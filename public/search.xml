<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MySQL日志系统之常见日志详解]]></title>
    <url>%2F2019%2F08%2F11%2FMySQL%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%B8%B8%E8%A7%81%E6%97%A5%E5%BF%97%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[innoDB事务日志包括redo log和undo log。redo log是重做日志，提供前滚操作，undo log是回滚日志，提供回滚操作，这两个日志都是innoDB特有的。而binlog（二进制日志）是MySQL数据库Server层实现的，所有引擎都可以使用。下面会对 redo log、undo log及binlog进行说明基本概念、相关作用及之间区别。 首先，我们先来看看一次查询/更新语句流程图：本文主要说明 执行器 存储引擎之间的交互。 mysql不是每次数据更改都立刻写到磁盘，而是会先将修改后的结果暂存在内存中，当一段时间后，再一次性将多个修改写到磁盘上，减少磁盘io成本，同时提高操作速度。 redo log（重做日志）基本概念重做日志用来实现事务的持久性，即事务ACID中的D。其由两部分组成： 一是内存中的重做日志缓存（redo log buffer）,其是易失的； 二是重做日志文件（redo log file)，其是持久的。 InnoDB是事务的存储引擎，其通过Force Log at Commit机制实现事务的持久性，即当事务提交（COMMIT）时，必须先将该事务的所有日志写入到重做日志文件进行持久化，待事务的COMMIT操作完成才算完成。 扩展：主从复制的原理：mysql中是基于binlog，来进行主从复置的。oracle中就没有binlog，只用redo log，复制是基于redo log来做。 redo log与二进制日志的区别redo log不是二进制日志。虽然二进制日志中也记录了innodb表的很多操作，也能实现重做的功能，但是它们之间有很大的区别。 二进制日志是在存储引擎的上层（即Server层）产生的，不管是什么存储引擎，对数据库进行了修改都会产生二进制日志。而redo log是innodb层产生的，只记录该存储引擎中表的修改。并且二进制日志先于redo log被记录。 二进制日志记录操作的方法是逻辑性的语句。即便它是基于行格式的记录方式，其本质也还是逻辑的SQL设置，如该行记录的每列的值是多少。而redo log是在物理格式上的日志，它记录的是数据库中每个页的修改。 二进制日志只在每次事务提交的时候一次性写入缓存中的日志“文件”。而redo log在数据准备修改前写入缓存中的redo log中，然后才对缓存中的数据执行修改操作；而且保证在发出事务提交指令时，先向缓存中的redo log写入日志，写入完成后才执行提交动作。 因为二进制日志只在提交的时候一次性写入，所以二进制日志中的记录方式和提交顺序有关，且一次提交对应一次记录。而redo log中是记录的物理页的修改，redo log文件中同一个事务可能多次记录，最后一个提交的事务记录会覆盖所有未提交的事务记录。例如：在binlog日志中记录方式如下：T1,T4,T3,T2，每个事务都会在提交之后被写入到二进制日志中。而redo log是并发写入的，不同事务之间的不同版本的记录会穿插写入到redo log 文件中，例如可能redo log的记录方式如下：T1-1,T1-2,T2-1,T2-2,T2*,T1-3,T1*，其中T1 表示最后提交时的日志记录，所以对应的数据页最终状态是 T1\对应的操作结果。 事务日志记录的是物理页的情况，它具有幂等性，因此记录日志的方式极其简练。幂等性的意思是多次操作前后状态是一样的，例如新插入一行后又删除该行，前后状态没有变化。而二进制日志记录的是所有影响数据的操作，记录的内容较多。例如插入一行记录一次，删除该行又记录一次。 innodb的恢复行为在启动innoDB的时候，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。且InnoDB可以保证数据库在异常重启后后的状态和使用binlog文件恢复出来的数据库状态保持一致。我们可以假设没有redo log，只有binlog，那么数据文件更新和写入binlog的顺序由两种可能： 第一种，更新数据文件； 写入binlog 第二种，写入binlog；更新数据文件 第一种情况如果在完成步骤1后服务器异常关闭，则导致binlog中缺少最后更新的数据；第二种情况如果在完成步骤1后服务器异常关闭，则数据库中比binlog中少了最后的数据变更记录。此时如果使用binlog文件进行恢复数据库（比如备库），则会导致数据库不一致的情况。redo log是怎么做的？先看下面的图片，是InnoDB更新数据时update语句的执行流程，橙色的流程在InnoDB内部执行，蓝色的部分在MySQL Server层执行器中执行。图片以下条SQL语句为例。update T set c=c+1 where ID=2;如上图所示，redo log的写入分为两个阶段（prepare 和 commit），这个称作两阶段提交，保证了数据的正确性。下面我们从上图4个可能发生异常关闭的时间点来分析InnoDB如何在MySQL启动时做崩溃恢复。 Point A如果服务器异常关闭发生在Point A 以及之前的时间点，这个时候redo log和binlog 都没有任何记录，事务还未提交，不会造成任何影响。 Point B当服务器启动的时候发现redo log里处于prepare状态的记录，这个时候需要检查binlog是否完整包含此条redo log的更新内容（通过全局事务ID对应），发现binlog中还未包含此事务变更，则丢弃此次变更。 Point C和Point B基本相同，只不过此时发现binlog中包含redo log的更新内容，此时事务会进行提交。 Point Dbinlog中和数据库中均含有此事务的变更，没有任何影响。 组提交上面关于崩溃恢复部分只是讲了写redo log和binlog 的步骤，那么一定很疑惑数据是何时被写入到磁盘文件中的呢，这里就要说下InnoDB通过redo log实现的组提交的策略了。因为更新数据时写磁盘的操作是随机写，这部分的IO消耗很大，而通过组提交（多个事务的变更统一写磁盘）的方式可以提升系统的吞吐量。 组提交实现下图是一组redo log文件的工作示意图，如图所示，一组redo log文件是一个类似环形的状态，循环利用。write pos表示日志当前记录的位置，当ib_logfile_4写满后，会从ib_logfile_1从头开始记录； checkpoint表示将日志记录的修改写进磁盘，完成数据落盘，数据落盘后checkpoint会将日志上的相关记录擦除掉，即write pos -&gt; checkpoint之间的部分是redo log空着的部分，用于记录新的记录，checkpoint -&gt; write pos之间是redo log待落盘的数据修改记录。当write pos追上checkpoint时，得先停下记录，先推动checkpoint向前移动，空出位置记录新的日志。（注：有了redo log，当数据库发生宕机重启后，可通过redo log将未落盘的数据恢复，即保证已经提交的事务记录不会丢失。） 日志刷盘的规则内存中（buffer pool，即log buffer）未刷到磁盘的数据称为脏数据（dirty data）。由于数据和日志都以页的形式存在，所以脏页表示脏数据和脏日志。在innoDB中，数据刷盘的规则只有一个：checkpoint。 但是触发checkpoint的情况却有集中。不管怎样，checkpoint触发后，会将buffer中脏数据页和脏日志页都刷到磁盘。innoDB存储引擎中checkpoint分为两种： sharp checkpoint：在重用redo log文件（例如切换日志文件）的时候，将所有已记录到redo log中对应的脏数据刷到磁盘。 fuzzy checkpoint：一次只刷一小部分的日志到磁盘，而非将所有脏日志刷盘。有以下几种情况会触发该检查点： master thread checkpoint： 由master线程控制，每秒或每10秒刷入一定比例的脏页到磁盘。 flush_lru_list checkpoint： 从MySQL5.6开始可通过 innodb_page_cleaners 变量指定专门负责脏页刷盘的page cleaner线程的个数，该线程的目的是为了保证lru列表有可用的空闲页。 async/sync flush checkpoint： 同步刷盘还是异步刷盘。 例如还有非常多的脏页没刷到磁盘（非常多是多少，有比例控制），这时候会选择同步刷到磁盘，但这很少出现；如果脏页不是很多，可以选择异步刷到磁盘，如果脏页很少，可以暂时不刷脏页到磁盘。 dirty page too much checkpoint： 脏页太多时强制出发检查点，目的是为了保证缓存有足够的空闲空间。too much的比例由变量innodb_max_dirty_pages_pct控制，MySQL5.6默认的值为75，即当脏页占缓冲池的百分之75后，就强制刷一部分脏页到磁盘。 （注：由于刷脏页需要一定的时间来完成，所以记录检查点的位置是在每次刷盘结束之后才在redo log中标记的。） 相关配置redo log的大小是固定的，在mysql中可以通过修改配置参数 innodb_log_files_in_group和innodb_log_file_size 配置日志文件数量和每个日志文件大小，redolog采用循环写的方式记录，当写到结尾时，会回到开头循环写日志。 注：binlog文件是通过追加的方式写入的。 innodb_flush_log_at_trx_commit={0|1|2} #指定何时将事务日志刷到磁盘，默认为1. 0 表示每秒将“log buffer”同步到“os buffer”且从”os buffer”刷到磁盘日志文件中。 1 表示每事务提交都将“log buffer”同步到“os buffer“且从”os buffer“刷到磁盘日志文件中。 2 表示每事务提交都将“log buffer”同步到“os buffer“但每秒才从”os buffer“刷到磁盘日志文件中。 innodb_log_buffer_size: #log buffer的大小，默认8M innodb_log_file_size: #事务日志的大小，默认5M innodb_log_fiel_group=2 #事务日志组中的事务日志文件个数，默认2个 innodb_log_group_home_dir=./ #事务日志组路径，当前目录表示数据目录 innodb_mirrored_log_groups=1 #指定事务日志组的镜像组个数，但镜像功能好像是强制关闭的，所以只有一个log group。在MySQL5.7中该变量已经移除。 undo log（回滚日志）基本概念undo log有两个作用：提供回滚和多个行版本控制（MVCC）。在数据修改的时候，不仅记录了redo，还记录了相对应的undo，如果因为某些原因导致事务失败或回滚了，可以借助该undo进行回滚。undo log不是redo log的逆向过程，其实它们都算是用来恢复的日志： redo log通常是物理日志，记录的是数据页的物理修改，而不是某一行或某几行修改成怎样怎样，他用来恢复提交后的物理数据页（恢复数据页，且只能恢复到最后一次提交的位置）。 undo用来回滚行记录到某个版本。undo log一般是逻辑日志，根据每行记录进行记录。 binlog（binary log，二进制日志）基本概念MySQL的二进制日志binlog可以说是MySQL最重要的日志，它记录了所有的DDL和DML语句（除了数据查询语句select），以事件形式记录，还包含语句所执行的消耗的时间，MySQL的二进制日志是事务安全型的。 binlog日志有两个最重要的使用场景： mysql主从复制： mysql replication在master端开启binlog，master把它的二进制日志传递给slaves来达到master-slave数据一致的目的。 数据恢复： 通过mysqlbinlog 工具来恢复数据。 binlog 日志包括两类文件： 二进制日志索引文件（文件名后缀为 .index）用于记录所有的二进制文件。 二进制日志文件（文件名后缀为 .00000*）记录数据库所有的DDL和DML（除了数据查询语句select）语句事件。 DDL（Data Definition Language 数据库定义语言），主要命令有create、alter、drop等，ddl主要是用在定义或改变表（table）的结构，数据类型，表之间的连接和约束等初始工作上，他们大多在建表时候使用。DML（Data Manipulation Language 数据操纵语言），主要命令是select、update、insert、delete，就像他的名字一样，这4条命令是用来对数据库里的数据进行操作的语言。 相关配置binlog文件是通过追加的方式写入的，可通过配置参数max_binlog_size设置每个binlog文件的大小，当文件大小大于给定值后，日志会发生滚动，之后的日志记录到新的文件上。binlog有两种记录模式，statement格式的话是记sql语句，row格式会记录行的内容，记两条，更新前和更新后都有。 注：binlog文件配置主从复制可查看我的另一篇文章。 博客：详细分析MySQL事务日志（redo log和undo log）MySQL崩溃恢复功臣——Redo LogMySQL日志系统之redo log和bin log]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常设计模式]]></title>
    <url>%2F2019%2F08%2F06%2F%E5%B8%B8%E7%94%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[参考博客：https://www.awaimai.com/patternshttps://learnku.com/docs/php-design-patterns/2018/about/1524http://c.biancheng.net/view/1395.html 一、创建型模式（*）1、简单工厂模式概述：通过传入参数，工厂就实例化出合适的对象，通过多态，返回对应的实例。简单代码实现：123456789101112131415class Factory&#123; public static function getObject($type) &#123; $object = null; switch ($type) &#123; case “A”: $object = new Object(); break; case “B”: $object = new Object(); break; &#125; return $object; &#125;&#125; 还有类似这种的：12345678class SimpleFactory&#123; public function createBicycle() &#123; return new Bicycle(); &#125;&#125;$factory = new SimpleFactory();$bicycle = $factory-&gt;createBicycle(); （注：简单工厂模式，可以理解为生产类的工厂。） 2、工厂模式概述：定义一个用于创建对象的接口，让子类决定实例化哪一个类。工厂方法使一个类的实例化延迟到其子类。简单代码实现：12345678910interface IFactory&#123; function getObject();&#125;class ObjectA implements IFactory&#123; public function getObject() &#123; return new self(); &#125;&#125; 3、抽象工厂模式概述：提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。其实就是简单工厂模式与抽象工厂模式的结合。简单代码实现：12345678910111213141516interface Product //抽象接口&#123; public function calculatePrice();&#125;class ShippableProduct implements Product //实现接口的具体类&#123; public function calculatePrice() &#123; return 66; &#125;&#125;class ProductFactory //工厂类&#123; public function createShippableProduct($price) &#123; return new ShippableProduct($price); &#125;&#125; 4、单例模式概述：保证一个类仅有一个实例，并提供一个访问它的全局访问点。简单代码实现：123456789101112class Singleton&#123; private static $instance = null; private function __construct() &#123;&#125; public static function getInstance() &#123; if (self::$instance === null) &#123; self::$instance = new self(); &#125; return self::$instance; &#125; private function __clone() &#123;&#125;&#125; 线上示例：连接数据库的代码1234567891011121314151617181920class Database&#123; private static $instance = null; private $db = null; private function __construct($config = array()) &#123; $dsn = sprintf(‘mysql:host=%s,dbname=%s’, $config[‘host’], $config[‘db_name’]); $this-&gt;db = new PDO($dsn, $config[‘db_user’], $config[‘db_pass’]); &#125; public static function getInstance($config = array()) &#123; if (self::$instance === null) &#123; self::$instance = new self($config); &#125; return self::$instance; &#125; public function db() &#123; return $this-&gt;db; &#125; private function __clone() &#123;&#125; private function __wakeup() &#123;&#125; //禁止重建对象&#125; 二、结构性模式1、适配器模式（*）即根据客户端需要，将某个类的接口转换成特定样式的接口，以解决类之间的兼容问题。 如果我们的代码依赖一些外部的API，或者依赖一些可能会经常更改的类，那么应该考虑用适配器模式。例如，支付接口，需要接入支付宝、微信等等。可对内部项目封装成一个支付接口，统一调用。外部接口变化，客户端接口并不需要调整。 简单代码实现：1234567891011interface PayAdapter&#123; public function pay();&#125;class AlipayAdapter implements PayAdapter&#123; public function pay() &#123; $alipay = new Alipay(); $alipay-&gt;sendPayment(); &#125;&#125; 客户端调用：12$alipay = new AlipayAdapter();$alipay-&gt;pay(); 如果当Alipay的支付方式改变，只需要修改AlipayAdapter类即可。 2、组合模式概述：组合模式将对象组合成树形结构，以表示“部分-整体”的层次结构。在组合模式，客户端访问独立对象和组合对象（或称对象集合）一样。 3、装饰器模式概述：又名包装（Wrapper）模式，该模式向一个已有的对象添加新的功能，而不改变其结构。 通常给对象添加功能有3种方法： 直接修改类，添加相应的功能。 直接修改类代码是一种侵入式修改，很容易对类功能造成损害。 派生对应的子类扩展新功能。 使用继承方式扩展功能，则在整个子类中添加功能，即使有的对象不需要，new出来也会有这些新功能。 使用对象组合的方式。 装饰器模式是典型的基于对象组合的方式，可以很灵活的给对象添加所需要的功能，它能动态的为一个对象增加功能，而且还能动态撤销。 4、依赖注入模式概述：用松散耦合的方式来更好的实现可测试、可维护和可扩展的代码。 5、注册模式（*）概述：目的是能够存储在应用程序中经常使用的对象实例，通常会使用只有静态方法的抽象类来实现（或使用单例模式）。需要注意的是这里可能会引入全局的状态，我们需要使用依赖注入来避免它。 三、行为型模式1、策略模式（*）概述：策略模式定义了一簇相同类型的算法，算法之间独立封装，并且可以互换代替。这些算法是同一类型问题的多种处理方式，它们具体行为有差别。每一个算法、或说每一种处理方式称为一个策略。在应用中，就可以根据环境的不同，选择不同的策略来处理问题。简单代码实现：12345678910111213141516interface OutputStrategy //策略接口&#123; public function render($array);&#125;class SerializeStrategy implements OutputStrategy //策略1&#123; public function render($array) &#123; return serialize($array); &#125;&#125;class JsonStrategy implements OutputStrategy //策略2&#123; public function render($array) &#123; return json_encode($array); &#125;&#125; 环境角色用来管理策略，实现不同策略的切换功能。同样，一旦写好，环境角色类以后也不需要修改了12345678910class Output //环境角色类&#123; private $outputStrategy; public function __construct(OutputStrategy $outputStrategy) &#123; $this-&gt;outputStrategy = $outputStrategy; &#125; public function renderOutput($array) &#123; return $this-&gt;outputStrategy-&gt;render($array); &#125;&#125; 客户端代码示例：123$test = [‘a’, ‘b’, ‘c’];$output = new Output(new JsonStrategy());$data = $output-&gt;renderOutput($test); 2、观察者模式（*）概述：也称发布-订阅模式，定义了一个被观察者和多个观察者的、一对多的对象关系。在被观察者状态发生变化的时候，它的所有观察者都会收到通知，并自动更新。观察者模式常用在 实时事件处理系统、组件间解耦、数据库驱动的消息队列系统，同时也是MVC设计模式中的重要组成部分。123456789/** *被观察者接口 */interface Observable&#123; public function attach(Observer $observer); //添加/注册观察者 public function detach(Observer $observer); //删除观察者 public function notify(); //触发通知&#125; 被观察者至少要实现attach()、detach()、notify()三个方法，用以添加、删除和通知观察者。通知的方式是，在类中的其他方法调用notify()方法。例如：123456789101112131415161718192021222324252627282930313233343536class Order implements Observable&#123; private $observers = array(); //保存观察者 private $state = 0; //订单状态 public function attach(Observer $observer) &#123; $key = array_search($observer, $this-&gt;observers); if ($key === false) &#123; $this-&gt;observers[] = $observer; &#125; &#125; public function detach(Observer $observer) &#123; $key = array_search($observer, $this-&gt;observers); if ($key !== false) &#123; unset($this-&gt;observers[$key]); &#125; &#125; public function notify() &#123; //遍历调用观察者的update()方法进行通知，不关心其具体实现方式 foreach($this-&gt;observers as $observer) &#123; $observer-&gt;update($this); //把本类对象传给观察者，以便观察者获取当前类对象的信息 &#125; &#125; public function addOrder() &#123; //订单状态有变化时发送通知 $this-&gt;state = 1; $this-&gt;notify(); &#125; public function getState() &#123; //观察者可能需要被观察者的一些状态信息，对外提供接口 return $this-&gt;state; &#125;&#125;/** *观察者接口 */interface Observer&#123; public function update(Observable $observable); //接收到通知的处理方法&#125; 观察者可能有多个，但每个观察者都必须实现Observer接口规定的update()接口，这是接收被观察者通知的唯一渠道。例如：1234567891011class Email implements Observer&#123; public function update(Observable $observable) &#123; $state = $observable-&gt;getState(); if ($state) &#123; echo &quot;发送邮件：您已经成功下单。&quot;; &#125; else &#123; echo “发送邮件：下单失败，请重试。”; &#125; &#125;&#125; 客户端代码示例：1234$email = new Email(); //创建观察者对象$order = new Order(); //创建订单对象$order-&gt;attach($email); //向订单对象中注册观察者$order-&gt;addOrder(); //添加订单，添加时会自动发送通知给观察者 3、迭代器模式概述：让对象变得可迭代并表现得像对象集合。 4、代理模式概述：在有些情况下，一个客户不能或者不想直接访问另一个对象，这时需要找一个中介帮忙完成某项任务，这个中介就是代理对象。 四、设计模式遵循的七大原则开闭原则、里氏替换原则、迪米特原则、单一职责原则、接口分割原则、依赖倒置原则、组合/聚合复用原则。设计目标：开闭原则、里氏替换原则、迪米特原则设计方法：单一职责原则、接口分割原则、依赖倒置原则、组合/聚合复用原则 开闭原则：软件实体（模块、类、方法等）应该对扩展开放，对修改关闭。里氏替换原则：子类能够替换基类。迪米特原则：又叫最少知道原则。只与你直接的朋友们通信，不要跟“陌生人”说话。单一职责原则：只能让一个类/接口/方法有且仅有一个职责。接口分割原则：使用多个专门的接口比使用单一的总接口要好。依赖倒置原则：高层模块不应该依赖底层模块，二者都应该依赖于抽象。组合/聚合复用原则：尽量使用组合/聚合，不要使用类继承。 五、面向对象的三个基本特征：1、封装 就是把客观事物封装成抽象的类，并且类可以把自己的数据和方法只让可信的类或者对象操作，对不可信的进行信息隐藏。 2、继承 他可以使用现有类的所有功能，并在无需重新编写原来的类的情况下对这些功能进行扩展。 继承的称为“子类”或“派生类”。 被继承的称为“基类”、“父类”或“超类”。 3、多态 不同类对象对相同行为的不同反应，就叫做多态。 与继承相对应的是多态提供了对同一类对象差异性的处理方法，子类通过多态重写父类继承的方法来实现子类的差异性。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP安装yaf扩展]]></title>
    <url>%2F2019%2F08%2F02%2FPHP%E5%AE%89%E8%A3%85yaf%E6%89%A9%E5%B1%95%2F</url>
    <content type="text"><![CDATA[Yaf，全称 Yet Another Framework，是一个C语言编写的PHP框架，是一个用PHP扩展形式提供的PHP开发框架, 相比于一般的PHP框架, 它更快. 它提供了Bootstrap, 路由, 分发, 视图, 插件, 是一个全功能的PHP框架。最大特点就是简单、高效、快速，已经在百度和新浪微博经过大平台验证。 一、下载Git仓库：https://github.com/laruence/yaf 资源整理： yaf官方文档 ：http://www.laruence.com/manual/ yaf手册：http://www.laruence.com/manual/index.html 二、安装依据git仓库中的说明，有两种方式可安装（也可使用其它方式）。 1、Yaf是PECL的一个扩展，因此，可以直接使用PECL安装即可pecl install yaf（注：没有pecl命令，可先安装该命令。） 2、编译安装（Linux）123/path/to/phpize./configure --with-php-config=/path/to/php-configMake &amp; make install 成功安装提示：Build process completed successfullyInstalling ‘/usr/local/lib/php/extensions/no-debug-non-zts-20170718/yaf.so’install ok: channel://pecl.php.net/yaf-3.0.8configuration option “php_ini” is not set to php.ini locationYou should add “extension=yaf.so” to php.ini 三、使用yaf框架1、启用yaf扩展由上面“成功安装提示”可知道，下面需要配置php.ini，将yaf.so添加进PHP扩展中，重新启动PHP即可。 下面是常用yaf配置选项：123456789[yaf]yaf.environ = productyaf.library = NULLyaf.cache_config = 0yaf.name_suffix = 1yaf.name_separator = &quot;&quot;yaf.forward_limit = 5yaf.use_namespace = 0yaf.use_spl_autoload = 0 2、生成yaf框架代码Yaf提供了代码生成工具yaf_code generator（在git项目中）, 所以也可以通过使用代码生成工具yaf_cg来完成这个简单的入门Demo（即yaf框架）。./yaf/tools/cg/yaf_cg sample会在 cg/output/目录下生成 sample文件夹，即yaf初版框架。将其粘贴到自己的项目中使用 3、访问yaf框架注：需要将’yaf_’ 更改为’/yaf/‘，才能识别出yaf的位置，否则会报找不到相应的类。然后就可以访问成功了。1Hello World! I am Stranger]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个URL的漫游详解]]></title>
    <url>%2F2019%2F08%2F01%2F%E4%B8%80%E4%B8%AAURL%E7%9A%84%E6%BC%AB%E6%B8%B8%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[详细分解网络请求一个网络请求通过TCP/IP五层架构（此处数据链路层和物理层是分开的）进行传输，都是怎么传输的，以及都干了些什么。此文将会从网络层、传输层和应用层进行详解，各个层都做了哪些操作，以及是如何进行传输的。 下面我们将从大的方向来说明一个请求从客户端到服务端都经历了哪些过程，此处可能会遗漏，以后达到该层次，再补充。 一、一个HTTP的URL的生命过程（注：此处会涉及到一个客户端缓存的问题，如果请求的是静态页面且本机上有该缓存页面，则直接读取，不会向服务器发送请求。此处忽略该请求）总体来说分为以下六个步骤： 1、DNS解析我们都知道，正常的url是通过域名来访问的，而在服务器之间则是通过IP地址来传输的。因此，首先客户端会进行URL解析——DNS解析。DNS会先在本机查找是否有缓存，没有的话，会向DNS服务器发送请求，而DNS服务器则会递归查询，直到找到该解析，则返回，客户端则会缓存该解析。 扩展：DNS缓存：DNS存在着多级缓存，从离浏览器的距离排序的话，有以下几种：浏览器缓存，系统缓存，路由缓存，IPS服务器缓存，根域名服务器缓存，顶级域名服务器缓存，DNS负载均衡：DNS可以返回一个合适的机器的IP给用户，例如可以根据每台服务器的负载量，该机器离用户地理位置的距离等，这种过程就是DNS负载均衡，又可以叫做DNS重定向，大家耳熟能详的CDN就是利用DNS的重定向技术，DNS服务器会返回一个跟用户最接近的点的IP地址给用户，CDN节点的服务器负责响应用户的请求，提供所需的内容。 2、TCP连接由DNS解析后，客户端就能够找到服务器，首先会通过TCP建立连接。 3、发送HTTP请求建立连接后，客户端就向服务器发送请求。 4、服务器处理请求并返回HTTP报文服务器接收到请求后，首先服务器会进行处理，如果是静态页面，则直接返回；否则，将该动态请求转发给后端服务处理，等待后端服务处理结束，则返回处理结果。 5、浏览器解析渲染页面客户端接收到服务端返回的结果后，进行页面渲染。展示给用户。 6、连接结束客户端断开连接。到此，一个请求周期结束。（注：如果是keepalive长连接模式，则该连接不会断开，在timeout时间结束后，连接才会断开。） 二、一个请求的建立及断开都经历了什么我们通过监听9000端口，来捕捉一个php-fpm的请求。仔细分析一下，该请求在服务端都进行了哪些处理，以及都进行了哪些步骤。（注：此分析相当于服务器处理请求并返回结果的过程）下面是通过tcpdump捕获的一个php请求：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126[root@VM_0_7_centos conf]# tcpdump -i lo port 9000 -S -XXtcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on lo, link-type EN10MB (Ethernet), capture size 262144 bytes15:07:47.787523 IP VM_0_7_centos.39980 &gt; VM_0_7_centos.cslistener: Flags [S], seq 682182615, win 43690, options [mss 65495,sackOK,TS val 3797694546 ecr 0,nop,wscale 7], length 0 0x0000: 0000 0000 0000 0000 0000 0000 0800 4500 ..............E. 0x0010: 003c 7f85 4000 4006 bd34 7f00 0001 7f00 .&lt;..@.@..4...... 0x0020: 0001 9c2c 2328 28a9 47d7 0000 0000 a002 ...,#((.G....... 0x0030: aaaa fe30 0000 0204 ffd7 0402 080a e25c ...0...........\ 0x0040: 3852 0000 0000 0103 0307 8R........00:58:18.801473 IP VM_0_7_centos.cslistener &gt; VM_0_7_centos.39980: Flags [S.], seq 1894922491, ack 682182616, win 43690, options [mss 65495,sackOK,TS val 3797694546 ecr 3797694546,nop,wscale 7], length 0 0x0000: 0000 0000 0000 0000 0000 0000 0800 4500 ..............E. 0x0010: 003c 0000 4000 4006 3cba 7f00 0001 7f00 .&lt;..@.@.&lt;....... 0x0020: 0001 2328 9c2c 70f2 38fb 28a9 47d8 a012 ..#(.,p.8.(.G... 0x0030: aaaa fe30 0000 0204 ffd7 0402 080a e25c ...0...........\ 0x0040: 3852 e25c 3852 0103 0307 8R.\8R....15:07:47.787544 IP VM_0_7_centos.39980 &gt; VM_0_7_centos.cslistener: Flags [.], ack 1894922492, win 342, options [nop,nop,TS val 3797694546 ecr 3797694546], length 0 0x0000: 0000 0000 0000 0000 0000 0000 0800 4500 ..............E. 0x0010: 0034 7f86 4000 4006 bd3b 7f00 0001 7f00 .4..@.@..;...... 0x0020: 0001 9c2c 2328 28a9 47d8 70f2 38fc 8010 ...,#((.G.p.8... 0x0030: 0156 fe28 0000 0101 080a e25c 3852 e25c .V.(.......\8R.\ 0x0040: 3852 8R15:07:47.787571 IP VM_0_7_centos.39980 &gt; VM_0_7_centos.cslistener: Flags [P.], seq 682182616:682183504, ack 1894922492, win 342, options [nop,nop,TS val 3797694546 ecr 3797694546], length 888 0x0000: 0000 0000 0000 0000 0000 0000 0800 4500 ..............E. 0x0010: 03ac 7f87 4000 4006 b9c2 7f00 0001 7f00 ....@.@......... 0x0020: 0001 9c2c 2328 28a9 47d8 70f2 38fc 8018 ...,#((.G.p.8... 0x0030: 0156 01a1 0000 0101 080a e25c 3852 e25c .V.........\8R.\ 0x0040: 3852 0101 0001 0008 0000 0001 0000 0000 8R.............. 0x0050: 0000 0104 0001 034f 0100 0f17 5343 5249 .......O....SCRI 0x0060: 5054 5f46 494c 454e 414d 452f 686f 6d65 PT_FILENAME/home 0x0070: 2f77 7777 726f 6f74 2f69 6e64 6578 2e70 /wwwroot/index.p 0x0080: 6870 0c00 5155 4552 595f 5354 5249 4e47 hp..QUERY_STRING 0x0090: 0e03 5245 5155 4553 545f 4d45 5448 4f44 ..REQUEST_METHOD 0x00a0: 4745 540c 0043 4f4e 5445 4e54 5f54 5950 GET..CONTENT_TYP 0x00b0: 450e 0043 4f4e 5445 4e54 5f4c 454e 4754 E..CONTENT_LENGT 0x00c0: 480b 0a53 4352 4950 545f 4e41 4d45 2f69 H..SCRIPT_NAME/i 0x00d0: 6e64 6578 2e70 6870 0b0a 5245 5155 4553 ndex.php..REQUES 0x00e0: 545f 5552 492f 696e 6465 782e 7068 700c T_URI/index.php. 0x00f0: 0a44 4f43 554d 454e 545f 5552 492f 696e .DOCUMENT_URI/in 0x0100: 6465 782e 7068 700d 0d44 4f43 554d 454e dex.php..DOCUMEN 0x0110: 545f 524f 4f54 2f68 6f6d 652f 7777 7772 T_ROOT/home/wwwr 0x0120: 6f6f 740f 0853 4552 5645 525f 5052 4f54 oot..SERVER_PROT 0x0130: 4f43 4f4c 4854 5450 2f31 2e31 0e04 5245 OCOLHTTP/1.1..RE 0x0140: 5155 4553 545f 5343 4845 4d45 6874 7470 QUEST_SCHEMEhttp 0x0150: 1107 4741 5445 5741 595f 494e 5445 5246 ..GATEWAY_INTERF 0x0160: 4143 4543 4749 2f31 2e31 0f0c 5345 5256 ACECGI/1.1..SERV 0x0170: 4552 5f53 4f46 5457 4152 456e 6769 6e78 ER_SOFTWAREnginx 0x0180: 2f31 2e31 342e 300b 0d52 454d 4f54 455f /1.14.0..REMOTE_ 0x0190: 4144 4452 3232 332e 3230 2e31 3633 2e39 ADDR223.20.163.9 0x01a0: 350b 0452 454d 4f54 455f 504f 5254 3437 5..REMOTE_PORT47 0x01b0: 3839 0b0a 5345 5256 4552 5f41 4444 5231 89..SERVER_ADDR1 0x01c0: 3732 2e32 372e 302e 370b 0253 4552 5645 72.27.0.7..SERVE 0x01d0: 525f 504f 5254 3830 0b09 5345 5256 4552 R_PORT80..SERVER 0x01e0: 5f4e 414d 456c 6f63 616c 686f 7374 0f03 _NAMElocalhost.. 0x01f0: 5245 4449 5245 4354 5f53 5441 5455 5332 REDIRECT_STATUS2 0x0200: 3030 090c 4854 5450 5f48 4f53 5431 3138 00..HTTP_HOST118 0x0210: 2e32 342e 382e 3232 390f 0a48 5454 505f .24.8.229..HTTP_ 0x0220: 434f 4e4e 4543 5449 4f4e 6b65 6570 2d61 CONNECTIONkeep-a 0x0230: 6c69 7665 1e01 4854 5450 5f55 5047 5241 live..HTTP_UPGRA 0x0240: 4445 5f49 4e53 4543 5552 455f 5245 5155 DE_INSECURE_REQU 0x0250: 4553 5453 310f 7948 5454 505f 5553 4552 ESTS1.yHTTP_USER 0x0260: 5f41 4745 4e54 4d6f 7a69 6c6c 612f 352e _AGENTMozilla/5. 0x0270: 3020 284d 6163 696e 746f 7368 3b20 496e 0.(Macintosh;.In 0x0280: 7465 6c20 4d61 6320 4f53 2058 2031 305f tel.Mac.OS.X.10_ 0x0290: 3134 5f34 2920 4170 706c 6557 6562 4b69 14_4).AppleWebKi 0x02a0: 742f 3533 372e 3336 2028 4b48 544d 4c2c t/537.36.(KHTML, 0x02b0: 206c 696b 6520 4765 636b 6f29 2043 6872 .like.Gecko).Chr 0x02c0: 6f6d 652f 3733 2e30 2e33 3638 332e 3130 ome/73.0.3683.10 0x02d0: 3320 5361 6661 7269 2f35 3337 2e33 360b 3.Safari/537.36. 0x02e0: 7648 5454 505f 4143 4345 5054 7465 7874 vHTTP_ACCEPTtext 0x02f0: 2f68 746d 6c2c 6170 706c 6963 6174 696f /html,applicatio 0x0300: 6e2f 7868 746d 6c2b 786d 6c2c 6170 706c n/xhtml+xml,appl 0x0310: 6963 6174 696f 6e2f 786d 6c3b 713d 302e ication/xml;q=0. 0x0320: 392c 696d 6167 652f 7765 6270 2c69 6d61 9,image/webp,ima 0x0330: 6765 2f61 706e 672c 2a2f 2a3b 713d 302e ge/apng,*/*;q=0. 0x0340: 382c 6170 706c 6963 6174 696f 6e2f 7369 8,application/si 0x0350: 676e 6564 2d65 7863 6861 6e67 653b 763d gned-exchange;v= 0x0360: 6233 140d 4854 5450 5f41 4343 4550 545f b3..HTTP_ACCEPT_ 0x0370: 454e 434f 4449 4e47 677a 6970 2c20 6465 ENCODINGgzip,.de 0x0380: 666c 6174 6514 0e48 5454 505f 4143 4345 flate..HTTP_ACCE 0x0390: 5054 5f4c 414e 4755 4147 457a 682d 434e PT_LANGUAGEzh-CN 0x03a0: 2c7a 683b 713d 302e 3900 0104 0001 0000 ,zh;q=0.9....... 0x03b0: 0000 0105 0001 0000 0000 ..........15:07:47.787577 IP VM_0_7_centos.cslistener &gt; VM_0_7_centos.39980: Flags [.], ack 682183504, win 356, options [nop,nop,TS val 3797694546 ecr 3797694546], length 0 0x0000: 0000 0000 0000 0000 0000 0000 0800 4500 ..............E. 0x0010: 0034 00ae 4000 4006 3c14 7f00 0001 7f00 .4..@.@.&lt;....... 0x0020: 0001 2328 9c2c 70f2 38fc 28a9 4b50 8010 ..#(.,p.8.(.KP.. 0x0030: 0164 fe28 0000 0101 080a e25c 3852 e25c .d.(.......\8R.\ 0x0040: 3852 8R15:07:49.735400 IP VM_0_7_centos.cslistener &gt; VM_0_7_centos.39980: Flags [P.], seq 1894922492:1894922596, ack 682183504, win 356, options [nop,nop,TS val 3797696494 ecr 3797694546], length 104 0x0000: 0000 0000 0000 0000 0000 0000 0800 4500 ..............E. 0x0010: 009c 00af 4000 4006 3bab 7f00 0001 7f00 ....@.@.;....... 0x0020: 0001 2328 9c2c 70f2 38fc 28a9 4b50 8018 ..#(.,p.8.(.KP.. 0x0030: 0164 fe90 0000 0101 080a e25c 3fee e25c .d.........\?..\ 0x0040: 3852 0106 0001 0050 0000 582d 506f 7765 8R.....P..X-Powe 0x0050: 7265 642d 4279 3a20 5048 502f 372e 302e red-By:.PHP/7.0. 0x0060: 3333 0d0a 436f 6e74 656e 742d 7479 7065 33..Content-type 0x0070: 3a20 7465 7874 2f68 746d 6c3b 2063 6861 :.text/html;.cha 0x0080: 7273 6574 3d55 5446 2d38 0d0a 0d0a 4865 rset=UTF-8....He 0x0090: 6c6c 6f20 576f 726c 6421 0103 0001 0008 llo.World!...... 0x00a0: 0000 0000 0000 003a 2074 .......:.t15:07:49.735419 IP VM_0_7_centos.39980 &gt; VM_0_7_centos.cslistener: Flags [.], ack 1894922596, win 342, options [nop,nop,TS val 3797696494 ecr 3797696494], length 0 0x0000: 0000 0000 0000 0000 0000 0000 0800 4500 ..............E. 0x0010: 0034 7f88 4000 4006 bd39 7f00 0001 7f00 .4..@.@..9...... 0x0020: 0001 9c2c 2328 28a9 4b50 70f2 3964 8010 ...,#((.KPp.9d.. 0x0030: 0156 fe28 0000 0101 080a e25c 3fee e25c .V.(.......\?..\ 0x0040: 3fee ?.15:07:49.735501 IP VM_0_7_centos.cslistener &gt; VM_0_7_centos.39980: Flags [F.], seq 1894922596, ack 682183504, win 356, options [nop,nop,TS val 3797696494 ecr 3797696494], length 0 0x0000: 0000 0000 0000 0000 0000 0000 0800 4500 ..............E. 0x0010: 0034 00b0 4000 4006 3c12 7f00 0001 7f00 .4..@.@.&lt;....... 0x0020: 0001 2328 9c2c 70f2 3964 28a9 4b50 8011 ..#(.,p.9d(.KP.. 0x0030: 0164 fe28 0000 0101 080a e25c 3fee e25c .d.(.......\?..\ 0x0040: 3fee ?.15:07:49.735547 IP VM_0_7_centos.39980 &gt; VM_0_7_centos.cslistener: Flags [F.], seq 682183504, ack 1894922597, win 342, options [nop,nop,TS val 3797696494 ecr 3797696494], length 0 0x0000: 0000 0000 0000 0000 0000 0000 0800 4500 ..............E. 0x0010: 0034 7f89 4000 4006 bd38 7f00 0001 7f00 .4..@.@..8...... 0x0020: 0001 9c2c 2328 28a9 4b50 70f2 3965 8011 ...,#((.KPp.9e.. 0x0030: 0156 fe28 0000 0101 080a e25c 3fee e25c .V.(.......\?..\ 0x0040: 3fee ?.15:07:49.735554 IP VM_0_7_centos.cslistener &gt; VM_0_7_centos.39980: Flags [.], ack 682183505, win 356, options [nop,nop,TS val 3797696494 ecr 3797696494], length 0 0x0000: 0000 0000 0000 0000 0000 0000 0800 4500 ..............E. 0x0010: 0034 00b1 4000 4006 3c11 7f00 0001 7f00 .4..@.@.&lt;....... 0x0020: 0001 2328 9c2c 70f2 3965 28a9 4b51 8010 ..#(.,p.9e(.KQ.. 0x0030: 0164 fe28 0000 0101 080a e25c 3fee e25c .d.(.......\?..\ 0x0040: 3fee ?. 分析分为三个步骤，建立连接、传输数据、断开连接，及TCP的一个周期。 1、三次握手建立连接一个连接包含了哪些数据，包头和包体。包头又包含了mac头、IP头、TCP头。下面扩展中分析了各头部所占用的位数。需要注意区分的是，IP头的协议版本、首部长度及IP地址是什么，TCP头的端口号、数据偏移及标识位是什么。对应上面二进制的表示，则为：IP头的版本号为4500中的’4’，表示IPv4，如果为’6’，表示IPv6。IP头的首部长度为4500中的’5’，表示4x5=20，说明IP头部有20个字节。IP头的IP地址为7f00 0001，表示’127.0.0.1’。TCP头的端口号为9c2c，表示nginx起的端口号为’39980’；2328，表示php-fpm的端口号为’9000’。TCP头的长度为a002中的’a’，表示10x4=40，说明TCP头部有40个字节。TCP头的标识位为a002中的’02’，表示‘SYN’；a012中的’12’，表示‘SYN’和‘ACK’。而断开连接的8011中的’11’，表示’FIN’和’ACK’。 因此，建立连接的步骤为：1) 客户端向服务端发送了一个‘SYN’，seq 682182615。2) 服务端回了客户端一个’ACK’，并向客户端发送了一个’SYN’，seq 1894922491, ack 682182616。3) 客户端回了服务端一个’ACK’，ack 1894922492。（注：此seq是接着服务端的seq的）至此，客户端与服务端建立了连接。下面就是客户端向服务端发送了请求。 扩展：Mac头包含什么？mac头包含了14个字节。对应上面的二进制，即为0000 0000 0000 0000 0000 0000 0800。IP头包含什么？对应上面的二进制，即为7f00 0001结束。TCP头包含什么？对应上面的二进制，即为fe30 0000结束。注：标识位区分TCP的标识位一共有6个，分别为URG、ACK（响应）、PSH（数据传输）、RST（连接重置）、SYN（建立连接）、FIN（关闭连接）。对应的位数分别表示为：10，01，1000，0100，0010，0001。转成16进制则为：2，1，8，4，2，1。TCP/IP头部大小通过TCP的数据偏移及IP的首部长度可说明头部的大小，因为都是4位，最大表示15，因此，TCP/IP头部最大头部位15x4=60个字节。由于最低头部有20字节，因此，可扩展的头部字节数为40字节。至于为什么要乘于4，因为4表示4字节，32位。而TCP/IP头部每一行占用32位。所以，TCP/IP的长度标识位最小数应为5。 2、传输数据在发送请求的时候，客户端给服务端回了一个’ACK‘，以及向服务端发送了数据’PSH’，seq 682182616:682183504, ack 1894922492（注：此处的682182616:682183504，表示传输的数据大小。）传输数据的协议都有哪些？HTTP、HTTPS、fastcgi,cgi 本次分析，以fastcgi协议来说明数据是如何传输的，以及如何保证数据的正确性。博客：https://mengkang.net/668.html传输数据时，nginx与PHP都会按照fastcig协议来进行通信，而最后，会通过nginx来进行组装数据，返回给客户端。 扩展：各协议详解？ 3、四次挥手断开连接首先，是服务端主动断开连接的，向服务端发送了‘FIN’，客户端回了一个‘ACK’，并发送了一个‘FIN’，服务端回了客户端一个‘ACK’。断开连接。 扩展：1、在浏览器与服务器通信的时候，哪端主动断开连接 那端都可主动断开连接，一般是客户端会断开连接。 如果服务端配置的有CLB，一般是CLB对服务端断开连接。 服务器上的TIME_WAIT一般都是请求其它服务断开的连接。例如：mysql、redis、mq、rpc等等。2、TIME_WAIT的产生，以及影响 那端主动断开，TIME_WAIT产生在那端，且存留时间为2MSL（MSL：数据报在网络中存留的最大时间）。如果并发量太高的话，会导致服务器端口被用尽，其它请求会报5xx。 解决办法，就是只能增加机器，横向扩展，服务器降级，多配几台低端机器，增加并发量。3、服务端连接都有几种状态，以及各个状态的含义三次握手状态：LISTEN、CLOSED、SYN_SENT、SYN_RCVD、ESTABLISHED四次挥手状态：FIN_WAIT_1、CLOSE_WAIT、FIN_WAIT_2、LAST_ACK、TIME_WAIT、CLOSED 扩展知识点：各层之间都是透明的，互不影响的，通过接口进行调用及传输。网络层的作用是找到主机（通过IP+mac地址）；传输层的作用是找到那个进程，即服务（通过端口）； 经典文章：https://www.cnblogs.com/jacklikedogs/articles/3848263.html]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP手册学习之类]]></title>
    <url>%2F2019%2F07%2F19%2FPHP%E6%89%8B%E5%86%8C%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[类PHP5中的新特性包括：访问控制、抽象类、final类与方法，附加的魔术方法、接口、对象复制和类型约束。 基本概念： 一个类可以包含有属于自己的常量、变量（称为“属性”）以及函数（称为“方法”）。 一个类可以在声明中用 extends 关键字继承另一个类的方法和属性。PHP不支持多重继承。 访问控制关键字 public、protected、private、static、final 静态成员、类常量访问使用“::“ self,parent和static这是那个特殊的关键字是用于在类定义的内部对其属性或方法进行访问的。 对象比较== 比较两个对象时，如果两个对象的属性和属性值 都相等，而且两个对象是同一个类的实例，那么这两个对象变量相等。=== 使用全等运算符，这两个对象变量一定要指向某个类的同一个实例（即同一个对象）。 static 声明类属性或方法为静态，就可以不实例化类而直接访问。静态属性不能通过一个已实例化的类对象来访问（但静态方法可以）。 由于静态方法不需要通过对象即可调用，所以伪变量 $this在静态方法中不可用。 静态属性不可以由对象通过 -&gt; 操作符来访问。 用静态方式调用一个非静态方法会导致一个 E_STRICT级别的错误 接口 通过关键字 interface来定义，通过implements关键字来实现接口。（类可以实现多个接口） 接口定义的所有方法都必须是公有的，这是接口的特性。 类中必须实现接口定义的所有方法，否则会报一个致命错误 Trait Trait是为类似PHP的单继承语言而准备的一种代码复用机制。利用组合的方式来复用代码。 优先级问题：从基类继承的成员会被trait插入的成员所覆盖。优先顺序是来自当前类的成员覆盖了trait的方法，而trait则覆盖了被继承的方法。 多个trait，通过都好分隔，在use声明列出多个trait，可以都插入到一个类中。 重载（类魔术方法）属性重载__set() 在给不可访问属性赋值时，会调用__get() 读取不可访问属性的值时，会调用__isset() 当对不可访问属性调用isset()或empty()时，会调用__unset() 当对不可访问属性调用unset()时，会调用 方法重载__call() 在对象中调用一个不可访问方法时，会调用__callStatic() 在静态上下文中调用一个不可访问方法时，会调用 魔术方法__construct()、__destruct()、__call()、__callStatic()、__get()、__set()、__isset()、__unset()、__sleep()、__wakeup()、__toString()、__invoke()、__clone()、__set_state()、__debugInfo()__toString() 用于当一个类被当成字符串时应该怎么回应。例如echo $obj;应该显示什么__invoke() 当尝试以调用函数的方式调用一个对象时，该方法会自动调用。__clone() 当克隆对象的时候，会调用该方法。 类型约束（PHP5可以使用） 函数的参数可以指定必须为对象（在函数原型里面指定类的名字），接口，数组或者callable 如果一个类或接口指定了类型约束，则其所有的子类或者实现也都如此。 类/对象函数__autoload - 尝试加载为定义的类 sql_autoload_register()class_exists - 检查类是否已定义method_exists - 检查类的方法是否存在trait_exists - 检查指定的trait是否存在property_exists - 检查对象或类是否具有该属性get_class - 返回对象的类名is_a - 如果对象属于该类或该类是此对象的父类则返回TRUE 其它一、错误处理和日志记录1、预定义常量E_ERROR 致命的运行时错误E_WARNING 运行时警告E_PARSE 编译时语法解析错误E_NOTICE 运行时通知E_STRICT 启用PHP对代码的修改建议，以确保代码具有最佳的互操作性和向前兼容性E_ALL 除E_STRICT外所有错误和警告信息 2、运行时配置项： 可通过ini_set()函数设置运行时配置error_reporting 设置错误报告的级别。运行时可直接使用error_reporting()函数设置 PHP5.3及以上版本中，默认值为E_ALL &amp; ~E_NOTICE &amp; ~E_STRICT &amp; ~E_DEPRECATED该设置不会显示E_NOTICE、E_STRICT、E_DEPRECATED级错误提示。display_errors 该选项设置是否将错误信息作为输出的一部分显示到屏幕，或者对用户隐藏而不显示。log_errors 设置是否将脚本运行的错误信息记录到服务器错误日志或者error_log之中。error_log 设置脚本错误将被记录到文件。ignore_repeated_errors 不记录重复的信息 3、错误处理函数：debug_backtrace 产生一条回溯信息debug_print_backtrace 打印一条回溯信息error_clear_last 清楚最近一次错误error_get_last 获取最后发生的错误error_log 发送错误信息到某个地方error_reporting 设置应该报告何种PHP错误set_error_handler 设置用户自定义的错误处理函数set_exception_handler 设置用户自定义的异常处理函数 二、PHP选项/信息函数set_time_limit - 设置脚本最大执行时间ini_set - 为一个配置选项设置值getenv - 获取一个环境变量的值getlastmod - 获取页面最后修改的时间ini_get - 获取一个配置选项的值memory_get_usage - 返回分配给PHP的内存量php_ini_loaded_file - 取得已加载的php.ini文件的路径php_uname - 返回运行PHP的系统的有关信息phpinfo - 输出关于PHP配置的信息phpversion - 获取当前的PHP版本putenv - 设置环境变量的值 三、扩展：压缩与归档扩展： Bzip2 对.bz2的压缩文件 Rar Zip Zlib 对gzip的编译加密： Hash 无需安装，自带 Mcrypt 加密扩展，需安装 OpenSSL 对称/非对称加解密数据库： Mysql(原始) 从PHP5.5.0起这个扩展已经被废弃，并且从PHP7.0.0开始被移除。可使用mysqli或者PDO_MySQL扩展代替。 Mysqli MySQL增强版扩展 Mysqlnd mysql的客户端 PDO_MYSQL mysql的PDO驱动国际化与字符编码支持： iconv 包含了iconv字符集转换功能的接口 gettext 实现了NLS API图像生成和处理： GD 图像处理和GD Gmagick、ImageMagick、Exif邮件相关扩展： 一般使用第三方邮件系统数学扩展： BC Math BCMath 任意精度数学 Math其它基本扩展： JSON、Sessions、libxml Lua、Swoole、Yaf、Yaml其它服务： cURL、Memcache、Memcached、Sockets]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>PHP知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP手册学习之函数整理]]></title>
    <url>%2F2019%2F07%2F19%2FPHP%E6%89%8B%E5%86%8C%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%87%BD%E6%95%B0%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[下面是此段时间看手册整理之后的函数： 数组函数：array_chunk(array, size, bool) 对数组进行分组，每个数组包含size个元素array_column(array, null, key) 获取数组中某一列数据，或以某列为键，某列为值array_combine(array1, array2) 合并两个数组，以array1为键，以array2为值，如果两个数组元素个数不一样，返回falsearray_diff(array1, array2,…) 比较数组的值，返回差集，存在array1,且不存在其它数组中的值。保持索引不变array_diff_key(array1, array2, …) 比较数组的键，返回差集，存在array1,且不存在其它数组中，保持索引不变array_intersect(array1, array2, …) 比较数组的值，返回交集，返回即存在array1,又存在array2种，保持索引不变array_intersect_key(array1, array2, …) 比较数组的键，返回交集，返回即存在array1,又存在array2种，保持索引不变array_filter(array, [callback]) 用回调函数过滤数组中的单元，如果没有回调函数，则过滤数组中false的单元array_flip(array) 交换数组中的键值array_keys(array) 返回数组中的键array_values(array) 返回数组中的值array_map(callback, array) 返回数组，对array中的每个元素都应用callback之后的数组array_walk(&amp;array, callback, [userdata]) 返回bool，对数组中的每个元素应用callbackarray_merge(array1, array2, …) 合并两个数组，关联数组后面的会覆盖前面。索引数组会重置索引值，从0开始。（比较 “+” 合并数组，索引数组会保持索引值，前面的会覆盖后面。）array_pop()、array_push()、array_shift()、array_unshift()array_rand(array, [int]) 从数组中随机取出一个或多个单元。只取出一个，返回键名。否则返回包含键名的数组。shuffle(array) 打乱数组，返回bool。array_reverse(array, [bool=false]) 返回单元顺序相反的数组。如果第二个值为true，会保留数字索引。array_key_exists(key, array) 检查数组中是否包含keyin_array(needle, array) 检查数组中是否存在某个值array_search(needle, array) 在数组中搜索给定的值，成功返回键名array_slice(array, offset, [length, bool=false]) 返回根据offset和length指定的数组序列。第四个值设为true，保持数字索引不变。array_sum(array) 对数组中所有值求和array_unique(array) 移除数组中重复的值，默认按照字符串排序sort()、rsort()、asort()、arsort()、ksort()、krsort()、usort(array, callback)、uksort()、uasort()compact(varname1) 建立一个数组，包含变量名和它们的值。会在当前符号表中查找该变量名并将它添加到输出的数组中，变量名成为键名而变量的内容成为该键的值。extract(array) 从数组中将变量导入到当前的符号表。 与compact正好相反count(array, [int = 0]) 获取数组的单元个数。 如果第二个值为1，则获取多维数组中单元个数。current() 返回数组当前单元，并不移动指针key() 从关联数组中取得键名，返回当前单元的键名，并不移动指针end() 返回数组最后一个单元，返回值next() 将数组中的内部指针向前移动一位，返回值prev() 将数组的内部指针倒回一位，返回值reset() 将数组的内部指针指向第一个单元，返回值each() 返回数组中当前的键/值对并将数组指针向前移动一位list(var1, …) 把数组中的值赋给一组变量range(start, end, [step=1]) 根据范围创建数组，包含指定的元素。 字符串函数：addslashes($str):str - 使用反斜线引用字符串stripcslashes($str):str - 反引用一个使用addslashes转义的字符串html_entity_decode - 转化html实体htmlspecialchars() - 将特殊字符转换为HTML实体htmlspecialchars_decode() - 将特殊的HTML实体转换回普通字符explode($delimiter, $str):array - 使用一个字符串分隔另一个字符串implode($glue, $pieces):string - 将一个一维数组的值转化为字符串trim()、ltrim()、rtrim() - 删除字符串两边的空白字符（或其它字符）md5($str, [$raw_output]):str - 计算字符串的MD5散列值sha1($str, [$raw_output]):str - 计算字符串的sha1散列值crc32($str):int - 计算一个字符串的crc32多项式sprintf($format, [$args,…]):str - 格式化字符串printf($format, [$args, …]):int - 输出格式化字符串number_format($number, $decimals, $dec_point=’.’, $thousands_sep=’,’) - 以千位分隔符方式格式化一个数字（注：本函数接受1个、2个或者4个参数，不能是3个）str_pad($input, $pad_length, [$pad_string, $pad_type=’left’]):str - 使用另一个字符串填充字符串为指定长度str_shuffle($str):str - 随机打乱一个字符串strlen($str):str - 获取字符串长度strrev($str):str - 反转字符串strpos($haystack, $needle, [$offset]):int - 查找字符串首次出现的位置stripos($haystack, $needle, [$offset]):int - 查找字符串首次出现的位置（不区分大小写）strrpos($haystack, $needle, [$offset]):int - 计算指定字符串在目标字符串中最后一次出现的位置strtolower($str):str - 将字符串转化为小写strtoupper($str):str - 将字符串转化为大写substr_replace($str, $replace, $start, [$length]):mixed - 替换字符串的子串substr($str, $start, $length):str - 返回字符串的子串ucfirst($str):str - 将字符串的首字母转换为大写ucwords($str):str - 将字符串中每个单词的首字母转换为大写 数学函数：ceil() - 进一法取整floor() - 舍去法取整round() - 对浮点数进行四舍五入fmod() - 返回除法的浮点数余数max() - 找出最大值min() - 找出最小值rand() - 产生一个随机整数（闭合区间）mt_rand() - 生成更好的随机数（闭合区间）pow() - 指数表达式abs() - 绝对值 BC数学函数：bcadd($left_operand, $right_operand, [$scale]) - 2个任意精度数字的加法计算bcsub($left_operand, $right_operand, [$scale]) - 2个任意精度数学的减法bcmul($left_operand, $right_operand, [$scale]) - 2个任意精度数字乘法计算bcdiv($left_operand, $right_operand, [$scale]) - 2个任意精度的数字除法计算bccomp($left_operand, $right_operand, [$scale]) - 比较两个任意精度的数字bcmod($left_operand, $modulus) - 对一个任意精度数字取模bcpow($left_operand, $right_operand, [$scale]) - 任意精度数字的乘方bcsqrt($operand, [$scale]) - 任意精度数学的二次方根bcscale($scale) - 设置所有bc数学函数的默认小数点保留位数 时间函数：date() - 格式化一个本地时间/日期microtime() - 返回当前Unix时间戳和微秒数mktime() - 取得一个日期的Unix时间戳strtotime() - 将任何字符串的日期时间描述解析为Unix时间戳time() - 返回当前的Unix时间戳localtime() - 取得本地时间date_default_timezone_set - 设定用于一个脚本中所有日期时间函数的默认时区date_default_timezone_get - 获取 文件函数：opendir($path):resource - 打开目录句柄closedir($dir_handle) - 关闭目录句柄readdir($dir_handle):str - 从目录句柄中读取条目scandir($directory, [$sorting_order]):array - 列出指定路径中的文件和目录chdir($directory):bool - 改变目录chroot($directory):bool - 改变根目录getcwd():str - 取得当前工作目录 文件系统函数；file_get_contents($filename):str - 将整个文件读取一个字符串file_put_contents($filename, $data):int - 将一个字符串写入文件basename($path, [$suffix]):str - 返回路径中的文件名部分（如果文件名是以suffix结尾，也会被去掉）dirname($path):str - 返回路径中的目录部分pathinfo($path):mixd - 返回文件路径信息（包含：surname,basename,extension,filename）fopen($filename, $mode):resource - 打开文件或者URLfread($handle, $length):str - 读取文件fgetc($handle):str - 从文件指针中读取字符fgets($handle, [$length]):str - 从文件指针中读取一行（如果没有指定length，则默认1K，即1024字节）fclose($handle):bool - 关闭一个已打开的文件指针feof($handle):bool - 测试文件指针是否到了文件结束的位置fwrite($handle, $str, [$length]):int - 写入文件mkdir($pathname, [$mode=0777, $recursive=false]):bool - 新建目录rmdir($dirname):bool - 删除目录（目录必须是空的）unlink($filename):bool - 删除文件fseek() - 在文件指针中定位move_uploaded_file($filename, $destination):bool - 将上传的文件移动到新位置filesize($filename):int - 取得文件大小filetype($filename):type - 取得文件类型fileatime($filename):int - 文件访问时间filectime($filename):int - 文件inode修改时间filemtime($filename):int - 取得文件修改时间is_dir($filename):bool - 是否是一个目录is_file($filename):bool - 是否为一个正常的文件is_executable($filename):bool - 文件是否可执行is_writable($filename):bool - 文件是否可写is_readable($filename):bool - 文件是否可读chgrp($filename, $group):bool - 改变文件所属的组chmod($filename, $mode):bool - 改变文件模式chown($filename, $user):bool - 改变文件的所有者copy($source, $dest):bool - 拷贝文件rename() - 重命名一个文件或目录realpath() - 返回规范化的绝对路径名touch() - 设定文件的访问和修改时间]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>PHP知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP手册学习之基础知识]]></title>
    <url>%2F2019%2F07%2F19%2FPHP%E6%89%8B%E5%86%8C%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[PHP类型：支持9种原始数据类型。四种标量类型： boolean、integer、float(double)、string 三种复合类型： array、object、callable(可调用) 两种特殊类型： resource、NULL(无类型) 知识点： 以下值被认为是false：false、0、0.0、”0”、””、array()、NULL 永远不要比较两个浮点数是否相等 一个字符串的表达方式：单引号、双引号、heredoc语法结构、nowdoc语法结构 在下列情况下一个变量被认为是NULL：被赋值为NULL；尚未被赋值；被unset()。（注：使用unset()将一个变量转换为null将不会删除该变量或unset其值。仅是返回NULL值而已） define()、const()定义常量，一个常量一旦被定义，就不能再改变或者取消定义。 1）常量只能包含标量数据 2）const()可以在类中定义常量 常量和变量的不同点 常量前面没有美元符号 $ 常量只能用define()函数定义，而不能通过赋值语句 常量可以不用理会变量的作用域而在任何地方定义和访问 常量一旦定义就不能被重新定义或者取消 常量的值只能是标量。 isset()与empty()isset()检测变量是否设置并且非NULL，注意，null字符并不等同于NULL常量。当变量被unset()之后，就变为NULL了。empty()检测变量是否为空。当一个变量不存在，或者它的值等同于false，那么就会被被认为不存在。以下的东西被认为是空的：“”、0、0.0、”0”、NULL、false、array()、$var(一个声明了，但是没有值的变量)（注：不存在的变量，empty()检测也是为true。） 其它： 允许强制转换的有：(int)、(bool)、(float)、(string)、(array)、(object)、(unset)对应函数：intval()、strval() 确定变量的类型：gettype()、is_array()、is_bool()、is_float()、is_int()、is_object()、is_string()、is_null()、is_numeric() 变量相关函数：empty()、isset()、serialize()、unserialize()、unset()、var_dump()、print_r()、var_export() PHP预定义变量：$_GLOBALS$_SERVER$_GET$_POST$_REQUEST$_FILES$_SESSION$_COOKIE$_ENV 环境变量$argc 传递给脚本的参数数目$argv 传递给脚本的参数数组$php_errormsg 前一个错误信息$HTTP_RAW_POST_DATA 原声POST数据$http_response_header HTTP响应头 魔术常量：__LINE__ 文件中当前行号__FILE__ 文件的完整路径和文件名__DIR__ 文件所在的目录__FUNCTION__ 函数名称（区分大小写）__CLASS__ 类的名称（区分大小写）__TRAIT__ Trait的名字（区分大小写）__METHOD__ 类的方法名（区分大小写）__NAMESPACE__ 当前命名空间的名称（区分大小写） 常见函数：get_class()、get_object_vars()、file_exists()、function_exists()call_user_func - 把第一个参数作为回调函数调用func_get_args - 返回一个包含函数参数列表的数组]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>PHP知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP异常处理机制]]></title>
    <url>%2F2019%2F05%2F06%2FPHP%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[异常与错误的区别首先要明白异常跟错误是两个不一样的概念，异常是出现正常逻辑之外的情况，而错误是指运行时出错了，比如，使用了一个未定义的变量等。异常需要抛出（throw）才能被捕捉到，而错误会导致程序执行终止。 PHP默认情况下，在代码出现了错误，如notice warning等消息时，错误信息会被直接打印到浏览器上，这个时候你通过 try catch是捕获不到错误信息的。php的try catch只能捕获到你自己 throw new Exception(“ “)抛出的错误，通过throw之后，程度会直接进入到catch中继续执行。如果你想抛弃php自身的错误处理机制，这个时候可以通过set_error_handler自定义一个函数用来处理，在这个函数中你可以抛出异常，然后再通过catch捕捉到异常。 异常介绍 PHP异常一般是指在业务逻辑上出现的不合预期、与正常流程不同的状况，不是语法错误。 PHP异常处理机制借鉴了java c++等，但是PHP的异常处理机制是不健全的。异常处理机制目的是将程序正常执行的代码与出现异常如何处理的代码分离。异常主要有检测（try）、抛出（throw）和捕获（catch）等操作。 PHP异常处理中需要注意的有，当代码中有throw出来的异常，则必须要catch到，也即是一个 try 至少要有一个与之对应的 catch。可以定义多个 catch 可以捕获不同的对象，php会按这些 catch 被定义的顺序执行，直到完成最后一个为止。而在这些 catch 内，又可以抛出新的异常。php的异常也像JAVA的异常的一样，可以在最外层catch捕捉，也可以在throw的地方捕捉。 当一个异常被抛出时，其后的代码将不会继续执行，PHP 会尝试查找匹配的 “catch” 代码块。如果一个异常没有被捕获，而且又没用使用set_exception_handler()作相应的处理的话，那么 PHP 将会产生一个严重的错误，并且输出未能捕获异常(Uncaught Exception …)的提示信息。 PHP是无法自动捕获异常的（绝大多数），只有主动抛出异常并捕捉。也就是说，对于异常，是可预见的。目前PHP能自动抛出的异常不多，如：PDO类。 异常相关函数（可自定义处理异常函数） set_exception_handler - 设置一个用户定义的异常处理函数1）callable set_exception_handler(callable $exception_handler)该函数设置默认的异常处理程序，用于没有用 try/catch 块来捕获的异常。在 exception_handler调用后异常会中止。2）如果把自定义的异常封装到一个类上，则可以使用数组的方式调用：set_exception_handler(array(&#39;MyExceptionHander&#39;, &#39;deal&#39;)); register_shutdown_function - 设置一个当执行关闭时可以调用的一个函数当脚本执行完成或意外死掉导致PHP执行即将关闭时，我们的这个函数将会被调用。该函数使用场景：1）页面被强制停止；2）程序代码意外终止或超时。说明：该函数接收一个回调函数作为参数（注意：如果函数里面有写路径一定要写绝对路径，因为执行回调函数时已经脱离了脚本，是从内存中调用该函数） ExceptionException 是系统自带的异常处理类，是所有异常的基类。（这个在PHP7中有变化） PHP7中的错误和异常在 PHP7 之前的 PHP 版本一个很大的痛点就是：发生了 E_ERROR 错误，无法捕获，导致数据库的事务无法回滚造成数据不一致。PHP 7 改变了大多数错误的报告方式。不同于传统（PHP 5）的错误报告机制，现在大多数错误被作为 Error 异常抛出（在 PHP7 中，只有 fatal error 和 recoverable error 抛出异常，其他 error 比如 warning 和 notice 的表现不变）。PHP7 中的 Error 和 Exception 的关系如图：123456789interface Throwable |- Exception implements Throwable |- ... |- Error implements Throwable |- TypeError extends Error |- ParseError extends Error |- ArithmeticError extends Error |- DivisionByZeroError extends ArithmeticError |- AssertionError extends Error 值得注意的是，Error 类表现上和 Exception 基本一致，可以像 Exception 异常一样被第一个匹配的 try / catch 块所捕获，如果没有匹配的 catch 块，则调用异常处理函数（事先通过 set_exception_handler() 注册7）进行处理。 如果尚未注册异常处理函数，则按照传统方式处理，被报告为一个致命错误（Fatal Error）。但并非继承自 Exception 类（要考虑到和 PHP5 的兼容性），所以不能用 catch (Exception $e) { … } 来捕获，而需要使用 catch (Error $e) { … }，当然，也可以使用 set_exception_handler 来捕获。 但是，用户不能自己定义类实现 Throwable，这是为了保证只有 Exception 和 Error 才可以抛出。 注：平常使用想要自定义处理错误的时候，可直接选择继承 Exception就可以。 对错误和异常的一种实践根据以上所述，我们提炼了一个对错误和异常处理较好的实践。对于业务中不应该出现错误的地方，抛出 InternalException，而不是 Error123456789&lt;?phpclass InternalException extends Exception &#123; /*...*/ &#125;function find(Array $ids) &#123; if (empty($ids)) &#123; throw new InternalException(&apos;ids should not be empty&apos;); &#125; ...&#125; 1. 只在需要清理现场的时候 catch Error12345&lt;?phptry &#123; /*...*/ &#125;catch (Throwable $t) &#123; // log, transaction rollback, cleanup...&#125; 未捕获的 Error 和Exception 通过 set_exception_handler 做后续清理和log 其他错误仍然通过 set_error_handler来处理，在处理的时候使用更加明确的 FriendlyErrorType，并抛出ErrorException 记录调用栈。 FriendlyErrorType:1234567891011121314151617181920212223242526272829303132333435363738&lt;?phpfunction FriendlyErrorType($type) &#123; switch($type) &#123; case E_ERROR: // 1 // return &apos;E_ERROR&apos;; case E_WARNING: // 2 // return &apos;E_WARNING&apos;; case E_PARSE: // 4 // return &apos;E_PARSE&apos;; case E_NOTICE: // 8 // return &apos;E_NOTICE&apos;; case E_CORE_ERROR: // 16 // return &apos;E_CORE_ERROR&apos;; case E_CORE_WARNING: // 32 // return &apos;E_CORE_WARNING&apos;; case E_COMPILE_ERROR: // 64 // return &apos;E_COMPILE_ERROR&apos;; case E_COMPILE_WARNING: // 128 // return &apos;E_COMPILE_WARNING&apos;; case E_USER_ERROR: // 256 // return &apos;E_USER_ERROR&apos;; case E_USER_WARNING: // 512 // return &apos;E_USER_WARNING&apos;; case E_USER_NOTICE: // 1024 // return &apos;E_USER_NOTICE&apos;; case E_STRICT: // 2048 // return &apos;E_STRICT&apos;; case E_RECOVERABLE_ERROR: // 4096 // return &apos;E_RECOVERABLE_ERROR&apos;; case E_DEPRECATED: // 8192 // return &apos;E_DEPRECATED&apos;; case E_USER_DEPRECATED: // 16384 // return &apos;E_USER_DEPRECATED&apos;; &#125; return &quot;&quot;; &#125; error_handler:12345678910&lt;?phpfunction exception_error_handler($severity, $message, $file, $line) &#123; if (!(error_reporting() &amp; $severity)) &#123; // This error code is not included in error_reporting return; &#125; log FriendlyErrorType($severity); throw new ErrorException($message, 0, $severity, $file, $line);&#125;set_error_handler(&quot;exception_error_handler&quot;); 总结：要注意，PHP中异常与错误是有区别的，并且处理机制也都是不一样的。因此，要区别对待。 参考博文： PHP异常处理机制 PHP的错误和异常处理机制 我们什么时候应该使用异常？ 深入理解PHP原理之异常机制]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP错误处理机制]]></title>
    <url>%2F2019%2F05%2F06%2FPHP%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[错误简介 PHP提供了错误处理和日志记录的功能. 这些函数允许你定义自己的错误处理规则，以及修改错误记录的方式. 这样，你就可以根据自己的需要，来更改和加强错误输出信息以满足实际需要。 常见的错误类型有：语法错误、环境错误、逻辑错误。平时遇到的warning、notice都是错误，只是级别不同而已。 常见的错误级别有：Deprecated（最低级别）、Notice（通知）、Warning（警告）、Fatal（致命）、Parser（语法解析），E_USER_相关错误。 注：在 PHP 中，默认的错误处理很简单。一条错误消息会被发送到浏览器，这条消息带有文件名、行号以及描述错误的消息。 错误处理函数（可自定义处理错误函数） error_reporting 设置PHP的报错级别1）通过php.ini设置如下： error_reporting = E_ALL2）通过error_reporting()函数设置，如：error_reporting(0); //关闭所有PHP错误报告error_reporting(-1); //报告所有PHP错误error_reporting(E_ALL); //和error_reporting(-1)一样3）通过ini_set()函数运行时设置ini_set(&#39;error_reporting&#39;, E_ALL); display_errors 设置是否将错误信息1）在php.ini设置如下：display_errors = On（注：不管是On还是Off都会记录到你错误日志里面，前提是配置了错误日志log_errors和error_log）2）通过ini_set(&#39;display_errors&#39;, 1); 设置运行时状态 set_error_handler 设置一个用户定义的错误处理函数1）set_error_handler(&#39;my_error&#39;); //my_error()函数为自定义的错误处理方法2）如果把自定义的错误封装到一个类上，则使用数组的方式调用：set_error_handler(array(&#39;MyErrorHander&#39;, &#39;deal&#39;)); //MyErrorHander为错误类，deal为处理方法3）set_error_handler() 参数介绍如下： 注：(1)以下级别的错误不能由用户定义的函数来处理： E_ERROR、 E_PARSE、 E_CORE_ERROR、 E_CORE_WARNING、 E_COMPILE_ERROR、E_COMPILE_WARNING，和在调用 set_error_handler() 函数所在文件中产生的大多数 E_STRICT。 官方没有给出原因，但不难看出这些错误要么是运行时的致命错误，要么是php核心或编译时的错误，因此也不难猜想：对于运行时的致命错误，php直接中断，导致了错误处理函数没有机会执行。(2)如果错误发生在脚本执行之前（比如文件上传时），将不会调用自定义的错误处理程序，因为它尚未在那时注册。 trigger_error() 产生一个用户级别的 error/warning/notice 信息 error_log —发送错误信息到某个地方1）在配置文件中： error_log = E:\phpStudy\MyError\test_error.txt2）运行时设置：int_set(&#39;error_log&#39;, &#39;E:\phpStudy\MyError\test_error.txt&#39;);3）使用error_log函数：error_log(&quot;You messed up!&quot;, 3, &quot;./error/my-errors.log&quot;); error_get_last() 获取最后发生的错误返回一个关联数组，描述了最后错误的信息，以该错误的“type”、“message”、“file”和“line”为数组的键。 如果该错误由PHP内置函数导致的，“message”会以该函数名开头。如果还没有错误则返回NULL。 错误控制配置我们按照php+php-fpm的模型来说，会影响php错误显示的其实是有两个配置文件，一个是php本身的配置文件php.ini，另外一个是php-fpm的配置文件，php-fpm.conf。 1）php.ini中的配置12345678910111213error_reporting = E_ALL // 报告错误级别，什么级别的error_log = /tmp/php_errors.log // php中的错误显示的日志位置display_errors = On // 是否把错误展示在输出上，这个输出可能是页面，也可能是stdoutdisplay_startup_errors = On // 是否把启动过程的错误信息显示在页面上，记得上面说的有几个Core类型的错误是启动时候发生的，这个就是控制这些错误是否显示页面的。log_errors = On // 是否要记录错误日志log_errors_max_len = 1024 // 错误日志的最大长度ignore_repeated_errors = Off // 是否忽略重复的错误track_errors = Off // 是否使用全局变量$php_errormsg来记录最后一个错误xmlrpc_errors = 0 //是否使用XML-RPC的错误信息格式记录错误xmlrpc_error_number = 0 // 用作 XML-RPC faultCode 元素的值。html_errors = On // 是否把输出中的函数等信息变为HTML链接docref_root = http://manual/en/ // 如果html_errors开启了，这个链接的根路径是什么fastcgi.logging = 0 // 是否把php错误抛出到fastcgi中 2）php-fpm中的配置123456789error_log = /var/log/php-fpm/error.log // php-fpm自身的日志log_level = notice // php-fpm自身的日志记录级别php_flag[display_errors] = off // 覆盖php.ini中的某个配置变量，可被程序中的ini_set覆盖php_value[display_errors] = off // 同php_flagphp_admin_value[error_log] = /tmp/www-error.log // 覆盖php.ini中的某个配置变量，不可被程序中的ini_set覆盖php_admin_flag[log_errors] = on // 同php_admin_valuecatch_workers_output = yes // 是否抓取fpmworker的输出request_slowlog_timeout = 0 // 慢日志时长slowlog = /var/log/php-fpm/www-slow.log // 慢日志记录 php-fpm的配置中也有一个error_log配置，这个很经常会和php.ini中的error_log配置弄混。但他们记录的东西是不一样的，php-fpm的error_log只记录php-fpm本身的日志，比如fpm启动，关闭。而php.ini中的error_log是记录php程序本身的错误日志。 那么在php-fpm中要覆盖php.ini中的error_log配置，就需要使用到下面几个函数： php_flag php_value php_admin_flag php_admin_value这四个函数admin的两个函数说明这个变量设置完之后，不能在代码中使用ini_set把这个变量重新赋值了。而php_flag/value就仍然以php代码中的ini_set为准 错误处理应该遵循的规则 一定要让PHP报告错误； 在开发环境中要显示错误； 在生产环境中不能显示错误； 在开发和生产环境中都要记录错误。 参考博文： PHP错误机制总结 PHP错误处理机制]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker使用docker-compose.yml管理LNMP环境]]></title>
    <url>%2F2019%2F01%2F22%2FDocker%E4%BD%BF%E7%94%A8docker-compose-yml%E7%AE%A1%E7%90%86LNMP%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[一、生成必备的镜像文件 - php版本说明：由于官方的镜像文件不满足实际使用需求，因此需要自定义Dockerfile文件来配置PHP。 1、基于php5.6.40镜像版本12345678910111213141516171819202122232425262728293031ROM php:5.6-fpmENV PHPREDIS_VERSION 3.0.0RUN apt-get update &amp;&amp; apt-get install -y \ libfreetype6-dev \ libjpeg62-turbo-dev \ libmcrypt-dev \ libpng-dev \ &amp;&amp; docker-php-ext-install iconv mcrypt \ &amp;&amp; docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \ &amp;&amp; docker-php-ext-install gd pdo pdo_mysql mysqli opcache# 安装memcachedRUN apt-get update \ # 手动安装依赖 &amp;&amp; apt-get install -y libmemcached-dev zlib1g-dev \ # 安装需要的扩展 &amp;&amp; pecl install memcached-2.2.0 \ # 启用扩展 &amp;&amp; docker-php-ext-enable memcached# 安装memcacheRUN pecl install memcache-2.2.7 \ &amp;&amp; docker-php-ext-enable memcache# 安装redisRUN pecl install -o -f redis \ &amp;&amp; rm -rf /tmp/pear \ &amp;&amp; docker-php-ext-enable redisWORKDIR /usr/local/CMD [&quot;./sbin/php-fpm&quot;, &quot;-c&quot;, &quot;/usr/local/etc/php-fpm.conf&quot;] 注意点： （1）安装扩展由于php的pecl支持安装memcached、memcache、redis扩展，则使用pecl安装即可。注：要使用docker命令 docker-php-ext-enable 来开启扩展。 2、基于php7.2.14镜像版本1234567891011121314151617181920212223242526272829303132333435363738394041FROM php:7.2.14-fpmENV PHPREDIS_VERSION 3.0.0RUN apt-get update &amp;&amp; apt-get install -y \ libfreetype6-dev \ libjpeg62-turbo-dev \ libpng-dev \ &amp;&amp; docker-php-ext-install iconv \ &amp;&amp; docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \ &amp;&amp; docker-php-ext-install gd pdo pdo_mysql mysqli opcache# 拷贝php.ini文件RUN cp /usr/local/etc/php/php.ini-development /usr/local/etc/php/php.ini# 注：php7不支持pecl安装memcached memcache# 安装memcachedRUN apt-get install -y libmemcached-devCOPY ./memcached /tmp/memcachedRUN cd /tmp/memcached &amp;&amp; /usr/local/bin/phpize \ &amp;&amp; ./configure -with-php-config=/usr/local/bin/php-config \ &amp;&amp; make \ &amp;&amp; make installRUN docker-php-ext-enable memcached# 安装memcacheCOPY ./memcache /tmp/memcacheRUN cd /tmp/memcache &amp;&amp; /usr/local/bin/phpize \ &amp;&amp; ./configure -with-php-config=/usr/local/bin/php-config \ &amp;&amp; make \ &amp;&amp; make installRUN docker-php-ext-enable memcache# 安装redisRUN pecl install -o -f redis \ &amp;&amp; rm -rf /tmp/pear \ &amp;&amp; docker-php-ext-enable redis# 清空脏数据RUN rm -rf /tmp/*WORKDIR /usr/local/CMD [&quot;./sbin/php-fpm&quot;, &quot;-c&quot;, &quot;/usr/local/etc/php-fpm.conf&quot;] 注意点： （1）安装memcache扩展php7不支持pecl安装memcached 和memcache扩展，因此只能使用源码安装，所以要下载源码，地址如下：Memcached: git clone https://github.com/php-memcached-dev/php-memcached memcachedMemcache: git clone https://github.com/websupport-sk/pecl-memcache memcache （2）安装memcache后，开启扩展时不要使用下面命令把扩展添加到php.ini文件，因为镜像中设置的--with-config-file-path参数找不到php.ini。RUN echo extension=memcached.so &gt;&gt; /usr/local/etc/php/php.ini因此，要用docker-php-ext-enable命令来开启扩展。 （3）清空脏数据时不要使用rm -rf /tmp/命令，该命令会删除tmp文件。导致docker构建错误：Fatal Error Unable to create lock file: Bad file descriptor (9) 3、构建镜像文件 （1）PHP基于上面书写的Dockerfile文件来构建镜像文件：docker build -t lnmp/php5.6:redis-memcache-001 . （2）Nginx、mysql使用官方镜像即可，此处不用构建！下面会贴出对应的版本。 二、使用docker-compose.yml管理镜像1、docker-compose.yml文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556version: &apos;2&apos;services: php-fpm7: container_name: php-fpm7 image: &quot;lnmp/php7.2:redis-memcache-001&quot; expose: - &quot;9000&quot; volumes: - /Users/zhengbenwu/Docker/wwwroot/:/usr/share/nginx/html networks: - lnmp php-fpm5: container_name: php-fpm5 image: &quot;lnmp/php5.6:redis-memcache-001&quot; depends_on: - mysql expose: - &quot;9000&quot; volumes: - /Users/zhengbenwu/Docker/wwwroot/:/usr/share/nginx/html networks: - lnmp nginx: container_name: nginx image: &quot;nginx:1.14.2&quot; depends_on: - php-fpm5 links: - php-fpm5:fpm5 - php-fpm7:fpm7 ports: - &quot;80:80&quot; volumes: - /Users/zhengbenwu/Docker/nginx/nginx.conf:/etc/nginx/nginx.conf - /Users/zhengbenwu/Docker/nginx/conf.d:/etc/nginx/conf.d - /Users/zhengbenwu/Docker/nginx/logs:/var/log/nginx - /Users/zhengbenwu/Docker/wwwroot/:/usr/share/nginx/html networks: - lnmp mysql: container_name: mysql image: mysql:5.7 ports: - &quot;3306:3306&quot; environment: - MYSQL_ROOT_PASSWORD=123456 volumes: - /Users/zhengbenwu/Docker/mysql/data:/var/lib/mysqlnetworks: lnmp: driver: bridge 附常用命令：1、docker-compose注： -f docker-compose.yml 参数可省略，若省略默认找docker-compose.yml文件，找不到会报错。且此参数必须紧跟docker-compse后面开启/关闭：docker-compose [-f docker-compose.yml] start/stop删除镜像：docker-compose rm构建并开启：docker-compose up -d （注：-d参数表示后台执行） 2、docker注：下面说明的container_id或image_id 可相应的使用container_name或image_name查看docker详细安装信息：docker info查看镜像文件：docker images查看容器：docker ps [-a] （注：添加-a表示查看所有）查看镜像或容器详细信息：docker inspect container_id/image_id进入容器：docker exec -it container_id /bin/bash容器关闭/开启：docker stop/start container_id删除容器：docker rm container_id删除镜像：docker rmi image_id构建镜像（Dockerfile）：docker build -t image_name . （注：“.” 表示使用当前目录的Dockerfile）生成容器：docker run -itd - -name container_name - -v $PWD/data:/var/data -p 8888:8888 image_id查看构建容器日志：docker logs container_id 错误记录：1、Error response from daemon: configured logging driver does not support reading参考文章：https://docs.docker.com/config/containers/logging/configure/解决方法：在生成容器的时候多加一个参数：docker run -itd --log-driver json-file --name container_name image_id注： --log-driver参数含义是指定日志驱动。 报错显示守护进程配置日志不支持，通过docker inspect container_id可查看到容器的 LoggingDriver 参数为null。因此需要指定一个日志驱动即可。 参考博客：使用docker创建集成服务-lnmp：https://www.cnblogs.com/s-b-b/p/8624491.htmlDockerfile构建LNMP平台：http://blog.51cto.com/ganbing/2074640秒懂Docker中安装扩展PHP：https://blog.csdn.net/u014389734/article/details/79683136Dockerfile文件中添加redis扩展：https://blog.csdn.net/xiaobinqt/article/details/83105807PHP7下安装memcache和memcached扩展：https://blog.csdn.net/u011547570/article/details/78325556]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Docker实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PhpStorm中添加自定义函数注释]]></title>
    <url>%2F2019%2F01%2F08%2FPhpStorm%E4%B8%AD%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%E6%B3%A8%E9%87%8A%2F</url>
    <content type="text"><![CDATA[点击查看更多 一、创建模板1、首先我们要添加自己的一套模板，点击 file-&gt;settings-&gt;editor-&gt;live Templates 2、点击右侧加号，选择第二个选项“template group”，创建一个分组。（注：不需要分组，可跳过此步） 3、选中刚才添加的分组，点击右侧加号，选择第一个选项“live template”添加一个具体模板。 4、按照图中步骤：其中第四步不一定要改，可以根据个人习惯，我习惯用enter键。 5、这是第五步点击内容，分别对DATE, TIME标签做设置。 设置完后点击”Apply”。 二、使用模板上面是设置要用的标签模板，下面是如何来使用它：点击“file and code templates-&gt;includes-&gt;PHP Function Doc Comment”，在里面输入自己想要的注释内容，注意Times对应添加的模板名称。 之后点击”apply”。 三、测试使用在项目中使用输入”/**”后点击enter键，会出现刚才添加的注释，再次点击enter键，会自动出现系统时间。注意：如果刚才第四步没有设置”enter”，要点击tab键，默认是tab键。]]></content>
      <categories>
        <category>编辑器</category>
      </categories>
      <tags>
        <tag>编辑器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP错误日志简单配置]]></title>
    <url>%2F2019%2F01%2F08%2FPHP%E9%94%99%E8%AF%AF%E6%97%A5%E5%BF%97%E7%AE%80%E5%8D%95%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[php.ini12345678; 错误日志log_errors = On; 显示错误display_errors = Off; 日志路径error_log = &quot;/usr/local/lnmp/php/var/log/error_log&quot;; 错误等级error_reporting = E_ALL&amp;~E_NOTICE php-fpm.conf1234567891011121314151617181920[global]; php-fpm pid文件pid = /usr/local/php/var/run/php-fpm.pid; php-fpm 错误日志路径error_log = /usr/local/php/var/log/php-fpm.log; php-fpm 记录错误日志等级log_level = notice[www]; 记录错误到php-fpm的日志中;catch_workers_output = yes; 慢日志slowlog = var/log/slow.log; 关闭打印日志php_flag[display_errors] = off; 错误日志php_admin_value[error_log] = /usr/local/php/var/log/www.log; 记录错误php_admin_flag[log_errors] = on; 内存使用量php_admin_value[memory_limit] = 32M 注：如果错误没有写入到文件，查看网站用户对php_admin_value[error_log]的路径是否有写入权限。]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图解TCP连接及对TIME_WAIT的理解]]></title>
    <url>%2F2018%2F12%2F19%2F%E5%9B%BE%E8%A7%A3TCP%E8%BF%9E%E6%8E%A5%E5%8F%8A%E5%AF%B9TIME-WAIT%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[环境说明：访问路径：http://118.24.8.229/index.htmlindex.html代码：123www.zhengbenwu.com&lt;img src=&quot;./apple.jpg&quot; /&gt;&lt;img src=&quot;./ceshi.webp&quot; /&gt; 监控工具：Wireshark监控筛选条件：ip.addr == 118.24.8.229 &amp;&amp; tcp.port == 80 监控说明：（1）建立连接客户端开启了3个连接请求，相应的客户端也会占用3个端口：59746、59747、59748。通过三次握手建立连接，图示连接建立过程很清晰。 客户端监控端口： 服务端监控端口： （2）传输数据index.html 文件传输：通过59747端口传输 apple.jpg 文件传输：通过59747端口传输 ceshi.webp 文件传输：通过59748端口传输 （3）断开连接① 59746端口连接由图示可以看出，首先断开的是59746端口的连接，且是客户端主动断开连接。因此，客户端会产生TIME_WAIT。 要注意的是，在此URL请求里，客户端建立的59746端口连接并没有用到；并且，很快就被断开了，并没有保持持久连接检测。 ② 59747、59748端口连接由上图示可以看出来，这两个端口是服务端主动断开连接的。中间客户端发送了 Keep-Alive保持连接，之后服务端在keepalive_timeout到时间后，自动断开了连接。因此，服务端会产生TIME_WAIT。 由此可知道，网站如果量比较大的话，尽量不要使用长连接，会产生大量的TIME_WAIT，导致网站瘫痪。量大的话，服务器可采用降配横向扩展，多部署几台服务器，提高端口数量，降低TIME_WAIT。 问题思考：长连接中TCP是如何请求的？实验一：客户端请求连接中，有多个额外请求时，TCP使用是怎么样的？服务端程序改成：（1）1个html和1个图片12www.zhengbenwu.com&lt;img src=&quot;./apple.jpg&quot; /&gt; 结论：客户端仍然会产生3个TCP连接，并且由于html的数据量少，导致服务端只使用了一个TCP连接就传输了html页面和图片数据。客户端产生2个TIME_WAIT，服务端产生1个TIME_WAIT。 （2）1个html1www.zhengbenwu.com 结论：客户端仍然会产生3个TCP连接，传输数据的TCP连接会保持持久连接，其余未传输数据的客户端会主动断开连接，不检测持久连接。客户端产生2个TIME_WAIT，服务端产生1个TIME_WAIT。 （3）1个html和4个图片12345www.zhengbenwu.com&lt;img src=&quot;./apple.jpg&quot; /&gt;&lt;img src=&quot;./ceshi.webp&quot; /&gt;&lt;img src=&quot;./timg.jpg&quot; /&gt;&lt;img src=&quot;./test001.jpg&quot; /&gt; 结论：客户端初始仍会产生3个TCP连接，首先建立连接的TCP会先传输html数据，当传输完成后，会自动的继续传输前3个图片，当发现3个TCP连接都用完后，仍有一个图片未传输，客户端会再新建一个TCP连接，用于传输第四个图片。 此时，TCP连接建立了4个，且都会检测是否保持持久连接，因此最后服务端会产生4个TIME_WAIT。 （4）1个html和7个图片12345678www.zhengbenwu.com&lt;img src=&quot;./apple.jpg&quot;&gt;&lt;img src=&quot;./ceshi.webp&quot;&gt;&lt;img src=&quot;./timg.jpg&quot;&gt;&lt;img src=&quot;./test001.jpg&quot;&gt;&lt;img src=&quot;./test002.jpg&quot;&gt;&lt;img src=&quot;./test003.jpg&quot;&gt;&lt;img src=&quot;./test004.jpg&quot;&gt; 结论：客户端初始会产生6个TCP连接，并且复用连接。断开TCP连接的是服务端，因此最后服务端会产生6个TIME_WAIT。 解决方案：线上环境可使用负载均衡。 短连接中TCP是如何请求的？实验二：使用短连接（在服务端Nginx配置 keepalive_timeout 0;）（1）1个html和7个图片12345678www.zhengbenwu.com&lt;img src=&quot;./apple.jpg&quot;&gt;&lt;img src=&quot;./ceshi.webp&quot;&gt;&lt;img src=&quot;./timg.jpg&quot;&gt;&lt;img src=&quot;./test001.jpg&quot;&gt;&lt;img src=&quot;./test002.jpg&quot;&gt;&lt;img src=&quot;./test003.jpg&quot;&gt;&lt;img src=&quot;./test004.jpg&quot;&gt; 结论：在使用短连接的情况下，每个请求都会建立一个TCP连接，不会复用。并且，测试显示都是服务端断开的连接，因此服务端会产生TIME_WAIT。 TIME_WAIT在服务端与客户端的理解我理解的TIME_WAIT有两种，一种是作为客户端产生的TIME_WAIT，一种是作为服务端产生的TIME_WAIT。这里，我们先说一下请求的过程。首先，客户端会新建一个进程，并随机分配一个端口号，来与服务端的固定端口号进行连接建立。因此，这一条请求就有两个端口，一个是客户端的随机端口，一个是服务端的固定端口。那么，下面来说结论：① 对于客户端来说客户端主动断开连接，TIME_WAIT产生在客户端。那么，服务端的这条TCP连接会被迅速回收；而客户端会耗时2MSL才被回收，端口才会被释放。 ② 对于服务端来说服务端主动断开连接，TIME_WAIT产生在服务端。客户端的TCP连接会被迅速回收，端口释放，重复使用；而服务端会耗时2MSL才会释放该条连接。 请注意，这里服务端与TCP建立连接并没有占用端口号，服务端的固定端口号一直在监听服务，并不会被请求占用，而是复用端口。这里与客户端建立连接的是服务端的线程或子进程，会复制主进程的资源，进行处理请求，然后，返回给客户端。因此，服务端产生的TIME_WAIT并不会消耗端口号，只会消耗线程或子进程资源。当请求量达到服务器承载的极限值时，服务端会返回错误。 （注：由上可知，一般说的TIME_WAIT，是指服务端作为客户端发起的请求产生的TIME_WAIT。过多会导致端口号不够，造成错误。） 那端会产生TIME_WAIT主动断开连接的那端会产生TIME_WAIT。 那又有个问题，那端会先断开连接呢？什么情况下客户端先断，什么情况下服务端先断？ 对于http1.0协议来说，如果响应头中有content-length头，则以content-length的长度就可以知道body的长度了，客户端在接收body时，就可以依照这个长度来接收数据，接收完后，就表示这个请求完成了。而如果没有content-length头，则客户端会一直接收数据，直到服务端主动断开连接，才表示body接收完了。 而对于http1.1协议来说，如果响应头中的Transfer-encoding为chunked传输，则表示body是流式输出，body会被分成多个块，每块的开始会标识出当前块的长度，此时，body不需要通过长度来指定。如果是非chunked传输，而且有content-length，则按照content-length来接收数据。否则，如果是非chunked，并且没有content-length，则客户端接收数据，直到服务端主动断开连接。 总结： http1.0带content-length，body长度可知，客户端在接收body时，就可以依据这个长度来接受数据。接受完毕后，就表示这个请求完毕了。客户端主动调用close进入四次挥手。不带content-length ，body长度不可知，客户端一直接受数据，直到服务端主动断开 http1.1带content-length，body长度可知，客户端主动断开带Transfer-encoding：chunked，body会被分成多个块，每块的开始会标识出当前块的长度，body就不需要通过content-length来指定了。但依然可以知道body的长度 客户端主动断开不带Transfer-encoding：chunked且不带content-length，客户端接收数据，直到服务端主动断开连接。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络之URL基础知识]]></title>
    <url>%2F2018%2F12%2F17%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8BURL%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[URI：服务器资源名被称为统一资源标识符（Uniform Resource Identifier,URI）。URL：统一资源定位符（URL）是资源标识符最常见的形式。URL描述了一台特定服务器上某资源的特定位置。他们可以明确说明如何从一个精确、固定的位置获取资源。URN：URI的第二种形式就是统一资源名（URN）。URN是作为特定内容的唯一名称使用的，与目前的资源所在地无关。使用这些与位置无关的URN，就可以将资源四处搬移。通过URN，还可以用同一个名字通过多种网络访问协议来访问资源。（注：仍然处于实验阶段，还未大范围使用。）URL趣谈 黑暗岁月：想要访问一个资源，需要使用FTP，连接到对应的域名上，匿名登录，切换到对应的目录，然后下载到本地。进行浏览这个文件。现在，只需要将URL输入到浏览器，直接回车进行浏览即可。 URL大多数URL方案的URL语法都建立在这个由9部分构成的通用格式上： &lt;scheme&gt;://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;path&gt;;&lt;params&gt;?&lt;query&gt;#&lt;frag&gt;几乎没有哪个URL中包含所有这些组件。URL最重要的3个部分是方案（scheme）、主机（host）和路径（path）。下表对各种组件进行了总结： 通用URL组件 组件 描述 默认值 方案 访问服务器以获取资源时要使用那种协议 无 用户 某些方案访问资源时需要的用户名 匿名 密码 用户名后面可能要包含的密码，中间由冒号（:）分隔 &lt;E-mail地址&gt; 主机 资源宿主服务器的主机名或点分IP地址 无 端口 资源宿主服务器正在监听的端口号。很多方案都默认端口号（HTTP的默认端口号为80） 每个方案特有 路径 服务器上资源的本地名，有一个斜杠（/）将其与前面的URL组件分隔开来。路径组件的语法是与服务器和方案有关的（本章稍后会讲到URL路径可以分为若干个段，每段都可以有其特有的组件） 无 参数 某些方案会用这个组件来指定输入参数。参数为名/值对。URL中可以包含多个参数字段，他们相互之间以及与路径的其余部分之间用分号（;）分隔 无 查询 某些方案会用这个组件传递参数以激活应用程序（比如数据库、公告板、搜索引擎以及其它因特网网关）。查询组件的内容没有通用格式。用字符“?”将其与URL的其余部分分割开来 无 片段 一小片或一部分资源的名字。引用对象时，不会将frag字段传送给服务器，这个字段是在客户端内部使用的。通过字符“#”将其与URL的其余部分分隔开来 无 常见的方案格式 方案 描述 http 超文本传输协议方案，除了没有用户名和密码之外，与通用的URL格式相符。如果省略了端口，就默认为80。基本格式：http://&lt;host&gt;:&lt;port&gt;/&lt;path&gt;?&lt;query&gt;#&lt;frag&gt; https 方案https与方案http是一对。唯一的区别在于方案https使用了网景的SSL，SSL为HTTP连接提供了端到端的加密机制。其语法与HTTP的语法相同，默认端口为443。基本格式：http://&lt;host&gt;:&lt;port&gt;/&lt;path&gt;?&lt;query&gt;#&lt;frag&gt; mailto Mailto URL指向的是E-mail地址。由于E-mail的行为与其他方案都有所不同（他并不指向任何可以直接访问的对象），所以mailto URL的格式与标准URL的格式也有所不同。因特网E-mail地址的语法记录在RFC 822中。基本格式：mailto:&lt;RFC-822-addr-spec&gt;示例：mailto:joe@joes-hardware.com ftp 文件传输协议URL可以用来从FTP服务器上下载或向其上载文件，并获取FTP服务器上的目录结构内容的列表。在Web和URL出现之前FTP就已经存在了。Web应用程序将FTP作为一种数据访问方案使用。URL语法遵循下列通用格式。基本格式：ftp://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;path&gt;;&lt;params&gt;ftp://anonymous:joe%40joes-hardware.com@prep.ai.mit.edu:21/pub/gnu/ file 方案file表示一台指定主机（通过本地磁盘、网络文件系统或其他一些文件共享系统）上可直接访问的文件。各字段都遵循通用格式。如果省略了主机名，就默认为正在使用URL的本地主机。基本格式：file://&lt;host&gt;/&lt;path&gt; telnet 方案telnet用于访问交互式业务。他表示的并不是对象自身，而是可通过telnet协议访问的交互式应用程序（资源）。基本格式：telnet://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/ rtsp,rtspu RTSP URL是可以通过实时流传输协议（Real Time Streaming Protocol）解析的音/视屏媒体资源的标识符。方案rtspu中的u表示它是使用UDP协议来获取资源的。基本格式：rtsp://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;path&gt;rtspu://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;path&gt; news 省略… URL是一种强有力的工具，但是并不完美。他们表示的是实际的地址，而不是准确的名字。这就意味着如果资源被移走了，URL就不再有效了。就无法对对象进行定位了。为了应对这个问题，因特网工程任务组（Internet Engineering Task Force，IETF）已经对一种名为统一资源名（uniform resource name,URN）的新标准做了一段时间的研究了。无论对象搬移到什么地方（在一个Web服务器内或是在不同的Web服务器间），URN都能为对象提供一个稳定的名称。永久统一资源定位符（persistent uniform resource locators,PURL）是用URL来实现URN功能的一个例子。其基本思想是在搜索资源的过程中引入另一个中间层，通过一个中间资源定位符（resource locator）服务器对资源的实际URL进行登记和跟踪。客户端可以向定位符请求一个永久URL，定位符可以以一个资源作为响应，将客户端重定向到资源当前实际的URL上去。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转]计算机网络之端口问题]]></title>
    <url>%2F2018%2F12%2F17%2F%E8%BD%AC-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%AB%AF%E5%8F%A3%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[博文出处：https://blog.csdn.net/weixin_42204641/article/details/83585277 搞清楚一些问题，端口就彻底了解了！ 1、 端口到底是什么？端口就是一个数字而已；这里的一个常识是：TCP/IP协议中的端口，端口号的范围从0到65535。 2、 端口到底是用来干嘛的？当系统调用一个应用程序的时候，会将该进程与一个端口绑定，这样一来，传输层传给该端口的数据都被相应的进程接收，与此同时，相应进程发给传输层的数据也都通过该端口输出。所以，端口就是用来识别系统中运行的应用程序的。 3、 一般80端口是用来www服务（网页服务）的，为什么有些网站不是80端口呢？而是别的什么端口呢？我们在IE的地址栏里输入一个网址的时候（比如www.baidu.com.cn）是不必指定端口号的，因为在默认情况下WWW服务的端口号是“80”。 网络服务是可以改的，使用其他端口号完全是可以的，要注意的是，如果不是默认端口号则应该在地址栏上指定端口号。 4、 主机和端口的关系是什么？大哥比方，就好比你要去银行存钱，这家银行就可以看成是一台主机，然后，银行不可能只有一种业务，对应每种业务就有很多的窗口，那么你一进银行大门的时候，在门口的服务人员就会问你说：你好！你要办什么业务？你跟他说：存钱！！服务员接着就会告诉你：请到三号窗口办理！这个时候你总该不会往其他的窗口跑吧？！这些窗口就可以看成是port。 5、 端口映射到底是怎么回事呢？就是建立内网主机IP地址和外网IP地址之间的一个映射；那么当我们向这个外网的IP地址发送请求时，该请求会被转发给内网的那台IP主机上去；从而实现了外网对内网的访问，端口映射的过程如果做个类比那就是：你的一个朋友来找你（网络请求），但是来找你但是不知道你住哪里（内网IP），但是他知道你的名字，（外网IP）于是你的朋友向物业告知你的名字后，物业查到了你的具体地址，几栋几单元几零几（内网IP），然后物业联系你，和你确认（端口映射），然后你的朋友就去对应的地址敲了你的门，然后你开门见到了你的朋友（外网访问到内网）这里的端口，是一种逻辑端口，是TCP/IP协议中定义的端口概念而已，通常就是一些装逼文章里的虚拟端口，那些看得见的端口，就叫做接口。 进程是个什么鬼呢？应用程序调入内存运行之后，就不能叫应用程序了，得叫进程；所以进程就是跑起来的应用程序。 6、 TCP和端口有什么关系呢？TCP和联机有关，所谓的联机就是指客户端Client机和服务端Server机的联系和通信；要想实现Client和Server的通信，必须先通过TCP来实现两端的联机！！TCP联机的过程是：第一步：客户端向服务端发送一个TCP封包（客户端发送的时候的端口是随机的） 这等同于，客户端打电话问服务端：服务端，听得到我说话么？第二步：服务端按端口的服务性质接受到请求后，向客户端发送第二个TCP封包，也就是第一个响应封包 这等同于，服务端回客户端说：我听得到！第三步：客户端获得这个响应封包之后，再向服务端发送一个确认封包； 这等同于，客户端向服务端说：好的！第四步：服务端接收到这个确认封包之后，客户端和服务端的联机就算真正建立了；在建立了tcp联机之后，才能进行服务资源的请求-响应。 了解到这里就可以完全用几句话把TCP协议和UDP协议的区别讲清楚了TCP协议是：不仅发送信息，然后还要确认信息是否送达UDP协议是：只发送信息，不确认信息是否送达]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络之端口解析]]></title>
    <url>%2F2018%2F12%2F17%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%AB%AF%E5%8F%A3%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[1、端口概述在Internet上，各主机间通过TCP/TP协议发送和接收数据报，各个数据报根据其目的主机的ip地址来进行互联网络中的路由选择。可见，把数据报顺利的传送到目的主机是没有问题的。问题出在哪里呢?我们知道大多数操作系统都支持多程序（进程）同时运行，那么目的主机应该把接收到的数据报传送给众多同时运行的进程中的哪一个呢？显然这个问题有待解决，端口机制便由此被引入进来。 请注意，端口号只具有本地意义（即只对本地机器来说才有意义），它只是为了标志本计算机应用层中的各个进程在和传输层交互时的层间接口。 本地操作系统会给那些有需求的进程分配协议端口 （protocal port，即我们常说的端口），每个协议端口由一个正整数标识，如：80，139，445，等等。当目的主机接收到数据报后，将根据报文首部的目的端口号，把数据发送到相应端口，而与此端口相对应的那个进程将会领取数据并等待下一组数据的到来。 端口其实就是队，操作系统为各个进程分配了不同的队，数据报按照目的端口被推入相应的队中，等待被进程取用，在极特殊的情况下，这个队也是有可能溢出的，不过操作系统允许各进程指定和调整自己的队的大小。 不光接受数据报的进程需要开启它自己的端口，发送数据报的进程也需要开启端口，这样，数据报中将会标识有源端口，以便接受方能顺利的回传数据报到这个端口。 传输层标识的端口号用16位来表示，说明只能允许有65535（2^16-1）个不同的端口号。 2、端口分类（1）服务器端使用的端口号这里又分为两类，最重要的一类叫做熟知端口号（well-known port number）或系统端口号，数值为0~1023。可在网址 www.iana.org查到。IANA把这些端口号指派给了TCP/IP最重要的一些应用程序，让所有的用户都知道。下面给出一些常用的熟知端口号： 应用程序 FTP TELNET SMTP DNS TFTP HTTP SNMP SNMP(trap) 熟知端口号 21 23 25 53 69 80 161 162 另一类叫做登记端口号，数值为 1024~49151。这类端口号是为没有熟知端口号的应用程序使用的。使用这类端口号必须在IANA按照规定的手续登记，以防止重复。 （2）客户端使用的端口号数值为 49152~65535。由于这类端口号仅在客户进程运行时才动态选择，因此又叫做短暂端口号。这类端口号是留给客户进程选择暂时使用。 短暂端口（ephemeral port）表示这种端口的存在时间是短期的。客户进程并不在意操作系统给它分配的是哪一个端口号，因为客户进程之所以必须有一个端口号（在本地主机中必须是唯一的），是为了让传输层的实体能够找到自己。这和熟知端口不同。服务器机器一接通电源，服务器程序就运行起来。为了让因特网上所有的客户程序都能够找到服务器程序，服务器程序所使用的端口（即熟知端口）就必须是固定的，并且是众所周知的。 端口在入侵中的作用有人曾经把服务器比作房子，而把端口比作通向不同房间（服务）的门，如果不考虑细节的话，这是一个不错的比喻。入侵者要占领这间房子，势必要破门而入（物理入侵另说），那么对于入侵者来说，了解房子开了几扇门，都是什么样的门，门后面有什么东西就显得至关重要。 入侵者通常会用扫描器对目标主机的端口进行扫描，以确定哪些端口是开放的，从开放的端口，入侵者可以知道目标主机大致提供了哪些服务，进而猜测可能存在的漏洞，因此对端口的扫描可以帮助我们更好的了解目标主机，而对于管理员，扫描本机的开放端口也是做好安全防范的第一步。 端口大小设置 端口大小需要先了解MTU是什么？MTU是Maximum Transmission Unit的缩写，意思是网络上传送的最大数据包，它的的单位是字节。 大部分网络设备都是1500。如果本机的MTU比网关的MTU大，大的数据包就会被拆开来传送，这样会产生很多数据包碎片，增加丢包率，降低网络速度。把本机的MTU设成比网关的MTU小或相同，就可以减少丢包 。通俗的说也就是，如果你上传一个大的文件，速度非常慢，可能就是这种原因，当你把MTU值改小时，就可以解决。 查看本机的mtu ： netstat -i 设置本机的mtu ： echo &quot;1450&quot; &gt; /sys/class/net/eth0/mtu 或直接编辑eth1网卡配置文件。 扩展：不同的协议层对数据包有不同的称谓，在传输层叫做段（segment），在网络层叫做数据报（datagram），在链路层叫做帧（frame）。数据封装成帧后发到传输介质上，到达目的主机后每层协议再剥掉相应的首部，最后将应用层数据交给应用程序处理。 在应用程序中我们用到的Data的长度最大是多少，直接取决于底层的限制。我们从下到上分析一下： 在链路层，由以太网的物理特性决定了数据帧的长度为(46＋18)－(1500＋18)，其中的18是数据帧的头和尾，也就是说数据帧的内容最大为1500(不包括帧头和帧尾)，即MTU(Maximum Transmission Unit)为1500； 在网络层，因为IP包的首部要占用20字节，所以这的MTU为1500－20＝1480； 在传输层，对于UDP包的首部要占用8字节，所以这的MTU为1480－8＝1472；（注：TCP的首部要占用20字节） 所以，在应用层，你的Data最大长度为1472。当我们的UDP包中的数据多于MTU(1472)时，发送方的IP层需要分片fragmentation进行传输，而在接收方IP层则需要进行数据报重组，由于UDP是不可靠的传输协议，如果分片丢失导致重组失败，将导致UDP数据包被丢弃。 由于以太网EthernetII最大的数据帧是1518Bytes这样，刨去以太网帧的帧头（DMAC目的MAC地址48bits=6Bytes+SMAC源MAC地址48bits=6Bytes+Type域2Bytes）14Bytes和帧尾CRC校验部分4Bytes那么剩下承载上层协议的地方也就是Data域最大就只能有1500Bytes这个值我们就把它称之为MTU。 如果我们定义的TCP和UDP包没有超过范围，那么我们的包在IP层就不用分包了，这样传输过程中就避免了在IP层组包发生的错误；如果超过范围，既IP数据报大于1500字节，发送方IP层就需要将数据包分成若干片，而接收方IP层就需要进行数据报的重组。更严重的是，如果使用UDP协议，当IP层组包发生错误，那么包就会被丢弃。接收方无法重组数据报，将导致丢弃整个IP数据报。UDP不保证可靠传输；但是TCP发生组包错误时，该包会被重传，保证可靠传输。 UDP数据报的长度是指包括报头和数据部分在内的总字节数，其中报头长度固定，数据部分可变。数据报的最大长度根据操作环境的不同而各异。从理论上说，包含报头在内的数据报的最大长度为65535字节(64K)。 注：此处的最大字节数，为系统内核缓存区大小。现在默认值已经不是65535字节，可依据机器查询。cat /proc/sys/net/core/rmem_max //读缓存区，单位字节16777216cat /proc/sys/net/core/wmem_max //写缓存区，单位字节16777216cat /proc/sys/net/core/rmem_default //默认读缓存区8388608cat /proc/sys/net/core/wmem_default //默认写缓存区8388608 我们在用Socket编程时，UDP协议要求包小于64K（需要减去IP头(20)+UDP头(8)=65507，否则用sendto函数发送数据会返回错误）。TCP没有限定，TCP包头中就没有“包长度”字段，而完全依靠IP层去处理分帧。这就是为什么TCP常常被称作一种“流协议”的原因，开发者在使用TCP服务的时候，不必去关心数据包的大小，只需讲SOCKET看作一条数据流的入口，往里面放数据就是了，TCP协议本身会进行拥塞/流量控制。 端口阻塞问题？（自我理解，不一定正确。待后期确认）端口号仅仅是为了区别本地机器的进程用的，并不会出现阻塞问题。出现阻塞问题，也都是一些TCP方面的，或者是IP层面的。 传输层或网络层的发送与接收缓存打满，导致阻塞丢失数据问题，出现阻塞。（这就是为什么TCP有拥塞控制。需要检测拥塞和减少包的发送率，降低拥塞，避免网络瘫痪。） （注：一般不会说UDP会阻塞网络，因为UDP是不可靠的传输，因此如果有阻塞发生，UDP就会丢包，没有重传机制。造成阻塞的情况，可能就是源主机一直发送数据，导致拥塞，造成网络不能正常收发数据。） 还有就是进程阻塞问题。 一般不会说端口阻塞。端口只会被禁止，通过防火墙一类的限制。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络之TCP、UDP解析]]></title>
    <url>%2F2018%2F12%2F17%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8BTCP%E3%80%81UDP%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[OSI模型OSI模型最初是用来作为开发网络通信协议族的一个工业参考标准。通过严格遵守OSI模型，不同的网络技术之间可以轻易地实现互操作。 OSI模型包含许多被分割成层的组件。在网络数据通信的过程中，每一层完成一个特定的任务。当传输数据的时候，每一层接收到上面层格式化后的数据，对数据进行操作，然后把它传给下面的层。当接收数据的时候，每一层接收到下面层传过来的数据，对数据进行解包，然后把它传给上一层。 OSI模型的一个关键概念是虚电路。兼容OSI模型的网络栈的每一部分都不知道其上面层和下面层的行为和细节；它只是向上和向下传输数据。就模型的层次而言，每一层都有一虚电路直接连接目的主机上的对应层。就每一层而言，它的数据在目的层被解包的方式和被打包的方式是完全一样的。层不知道传输数据的实际细节；它们只知道数据是从周围层中传过来的。 TCP/IP协议栈OSI模型是一种通用的、标准的、理论模型，今天市场上没有一个流行的网络协议完全遵守OSI模型，TCP/IP也不例外，TCP/IP协议族有自己的模型，被称为TCP/IP协议栈，又称DOD模型（Department of defense） 问：TCP/IP是什么？答：TCP/IP是协议栈，并不是单指TCP协议、IP协议或者HTTP协议。仅仅是一个协议族，是一个统称。 TCP、UDP概述（1）用户数据报协议UDP（User Datagram Protocol）：UDP用户数据报UDP在传送数据之前不需要先建立连接。远地主机的传输层在收到UDP报文后，不需要给出任何确认。虽然UDP不提供可靠交付，但在某些情况下UDP却是一种最有效的工作方式。 （2）传输控制协议TCP（Transmission Control Protocol）：TCP报文段TCP则提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。TCP不提供广播或多播服务。由于TCP要提供可靠的、面向连接的传输服务，因此不可避免地增加了许多的开销，如确认、流量控制、计时器以及连接管理等。这不仅使协议数据单元的首部增大很多，还要占用许多的处理机资源。 使用UDP和TCP协议的各种应用和应用层协议 应用 应用层协议 运输层协议 域名解析 DNS UDP 文件传送 TFTP UDP 路由选择协议 RIP UDP IP地址配置 BOOTP，DHCP UDP 网络管理 SNMP UDP 远程文件服务器 NFS UDP 多播 IGMP UDP 万维网 HTTP TCP 文件传送 FTP TCP 远程终端接入 TELNET TCP 题外话：如何将一个进程通过网络送到对应服务器的进程中？解决这个问题的方法就是在传输层使用协议端口号（protocol port number），或通常简称为端口（port）。这就是说，虽然通信的终点是应用进程，但我们只要把要传送的报文交到目的主机的某一个合适的目的端口（通过IP来完成），剩下的工作（即最后交付给目的进程）就由TCP来完成。 请注意，这种在协议栈层间的抽象的协议端口是软件端口，和路由器或交换机上的硬件端口（应该称为接口）是完全不同的概念。硬件端口是不同硬件设备进行交互的接口，而软件端口是应用层的各种协议进程与传输实体进行层间交互的一种地址。 扩展：TCP/IP的传输层用一个16位端口号来标志一个端口。但请注意，端口号只具有本地意义（即本机机器来说才有意义），它只是为了标志本计算机应用层中的各个进程在和传输层交互时的层间接口。16位的端口号可允许有65535个不同的端口号，这个数目对一个计算机来说是足够用的。 UDP解析1、主要特点：（1）UDP是无连接的，即发送数据之前不需要建立连接（当然发送数据结束时也没有连接可释放），因此减少了开销和发送数据之前的时延。（2）UDP使用尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的连接状态表（这里面有许多参数）。（3）UDP是面向报文的。发送方的UDP对应程序交下来的报文，在添加首部后就向下交付给IP层。UDP对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。这就是说，应用层交给UDP多长的报文，UDP就照样发送，即一次发送一个报文。如下图，在接收方的UDP，对IP层交上来的UDP用户数据报，在去除首部后就原封不动地交付给上层的应用进程。因此，应用程序必须选择合适大小的报文。若报文太长，UDP把它交给IP层后，IP层在传送时可能要进行分片，这回降低IP层的效率。反之，若报文太短，UDP把它交给IP层后，会使IP数据报的首部的相对长度太大，这也降低了IP层的效率。（4）UDP没有拥塞控制，因此网络出现的拥塞不会使源主机的发送速率降低。（这对某些实时应用是很重要的。）（5）UDP支持一对一、一对多、多对一和多对多的交互通信。（6）UDP的首部开销小，只有8个字节，比TCP的20个字节的首部要短。 虽然某些实时应用需要使用没有拥塞控制的UDP，但当很多的源主机同时都向网络发送高速率的实时视频流时，网络就有可能发生拥塞，结果大家都无法正常接收。因此，不适用拥塞控制功能的UDP有可能会引起网络产生严重的拥塞问题。 2、UDP的首部格式用户数据报UDP有两个字段：数据字段和首部字段。首部字段很简单，只有8个字节，由四个字段组成，每个字段的长度都是两个字节。各字段意义如下： 报头字段名 位数 说明 源端口号 16 发送主机的UDP端口 目的端口号 16 目标主机的UDP端口 消息长度 16 UDP用户数据报的长度，其最小值是8（仅有首部） 校验和 16 检测UDP用户数据报在传输中是否有错。有错就丢弃 当传输层从IP层收到UDP数据报时，就根据首部中的目的端口，把UDP数据报通过相应的端口，上交最后的终点——应用进程。 TCP解析1、主要特点：（1）TCP是面向连接的传输层协议。这就是说，应用程序在使用TCP协议之前，必须先建立TCP连接。在传送数据完毕后，必须释放已经建立的TCP连接。（2）每一条TCP连接只能有两个端点（endpoint），每一条TCP连接只能是点对点的（一对一）。（3）TCP提供可靠交付的服务。也就是说，通过TCP连接传送的数据，无差错、不丢失、不重复、并且按序到达。（4）TCP提供全双工通信。TCP允许通信双方的应用进程在任何时候都能发送数据。TCP连接的两端都设有发送缓存和接收缓存，用来临时存放双向通信的数据。（5）面向字节流。TCP中的“流”（stream）指的是流入到进程或从进程流出的字节序列。“面向字节流”的含义是：虽然应用程序和TCP的交互时一次一个数据块（大小不等），但TCP把应用程序交下来的数据看成仅仅是一连串的无结构的字节流。TCP并不知道所传送的字节流的含义。TCP不保证接收方应用程序所收到的数据块和发送方应用程序所发出的数据块具有对应大小的关系（例如，发送方应用程序交给发送方的TCP共10个数据块，但接收方的TCP可能只用了4个数据块就把收到的字节流交付给了上层的应用程序）。但接收方应用程序收到的字节流必须和发送方应用程序发出的字节流完全一样。当然，接收方的应用程序必须有能力识别收到的字节流，把它还原成有意义的应用层数据。 2、TCP的连接TCP把连接作为最基本的抽象。TCP的许多特性都与TCP是面向连接的这个基本特性有关。前面已经说过，每一条TCP连接有两个端点。那么，TCP连接的端点是什么呢？ 不是主机，不是主机的IP地址，不是应用进程，也不是传输层的协议端口。 TCP连接的端点叫做套接字（socket）或插口。根据RFC 793的定义：端口号拼接到（contatenated with）IP地址即构成了套接字。 每一条TCP连接唯一地被通信两端的两个端点（即两个套接字）所确定。即： TCP连接 ::= {socket1, socket2} = {(IP1:port1), (IP2:port2)} 应用编程接口API（Application Programming Interface），即传输层与应用程序之间的一种接口，称为socket API，并简称为socket。 3、TCP报文段的首部格式TCP虽然是面向字节流的，但TCP传送的数据单元却是报文段。一个TCP报文段分为首部和数据两部分，而TCP的全部功能都提现在它首部中各字段的作用。只有弄清楚TCP首部各字段的作用才能掌握TCP的工作原理。 TCP报文段首部的前20个字节是固定的，如下图，后面有4N字节是根据需要而增加的选项（N是整数）。因此TCP首部的最小长度是20字节。 报头字段名 位数 说明 源端口号 16 本地通信端口，支持TCP的多路复用机制 目的端口号 16 远地通信端口，支持TCP的多路复用机制 序号（SEQ） 32 数据段第一个数据字节的序号（除含有SYN的段外）；SYN段的SYN序号（建立本次连接的初始序号） 确认号（ACK） 32 表示本地希望接收的下一个数据字节的序号 数据偏移 4 指出该段中数据的初始位置（以32位为单位） 保留 6 保留为今后使用，但目前应置为0 控制字段（CTL） URG 1 紧急指针字段有效标志，即该段中携带紧急数据 ACK 1 确认号字段有效标志 PSH 1 PUSH操作的标志 RST 1 要求异常终止通信连接的标志 SYN 1 建立同步连接的标志 FIN 1 本地数据发送已结束，终止连接的标志 窗口 16 本地接收窗口尺寸，即本地接收缓冲区大小 校验和 16 包括TCP报头和数据在内的校验和 紧急指针 16 从段序号开始的正向位移，指向紧急数据的最后一个字节 选项 可变 提供任选的服务 填充 可变 保证TCP报头以32位为边界对齐 4、TCP的连接与释放用三次握手建立TCP连接： TCP连接释放的过程： 为什么A在TIME-WAIT状态必须等待2MSL的时间呢？这有两个理由。 第一，为了保证A发送的最后一个ACK报文段能够到达B。这个ACK报文段有可能丢失，因而使处在LAST-ACK状态的B收不到对已发送的FIN+ACK报文段的确认。B会超时重传这个FIN+ACK报文段，而A就能在2MSL时间内收到这个重传的FIN+ACK报文段。接着A重传一次确认，重新启动2MSL计时器。最后，A和B都正常进入到CLOSED状态。如果A在TIME-WAIT状态不等待一段时间，而是在发送完ACK报文段后立即释放连接，那么就无法收到B重传的FIN+ACK报文段，因而也不会再发送一次确认报文段。这样，B就无法按照正常步骤进入CLOSED状态。 第二，防止“已失效的连接请求报文段”出现在本连接中。A在发送完最后一个ACK报文段后，再经过时间2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样就可以使下一个新的连接中不会出现这种旧的连接请求报文段。B只要收到了A发出的确认，就进入CLOSED状态。同样，B在撤销相应的传输控制块TCB后，就结束了这次的TCP连接。我们注意到，B结束TCP连接的时间要比A早一些。 除时间等待计时器外，TCP还设有一个保活计时器（keepalive timer）。设想有这样的情况：客户已主动与服务器建立了TCP连接。但后来客户端的主机突然出故障。显然，服务器以后就不能再收到客户发来的数据。因此，应当有措施使服务器不要再白白等待下去。这就是使用保活计时器。服务器每收到一次客户的数据，就重新设置保活计时器，时间的设置通常是两小时。若两小时没有收到客户的数据，服务器就发送一个探测报文段，以后则每隔75秒发送一次。若一连发送10个探测报文段后仍无客户的响应，服务器就认为客户端出了故障，接着就关闭这个连接。 5、TCP高级知识点概述（1）TCP是怎么保证可靠传输的？通过超时重传、选择确认、滑动窗口机制来保证可靠传输。 （2）TCP的keepalive保活机制？通过保活计时器来确保TCP的keepalive的连接。 （3）TCP的时延、瓶颈及存在的障碍？最常见的TCP相关时延，其中包括： TCP连接建立握手； TCP慢启动拥塞控制； 数据聚集的Nagle算法； 用于捎带确认的TCP延迟确认算法； TIME_WAIT时延和端口耗尽。 （4）拥塞控制所谓拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。TCP的流量控制：利用滑动窗口实现流量控制。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线上TIME_WAIT日志记录总结]]></title>
    <url>%2F2018%2F12%2F13%2F%E7%BA%BF%E4%B8%8ATIME-WAIT%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[首先发现的是平响太高了，之后又降下来了。然后，开始查询原因。下面是过程记录 刚开始是追寻的问题是： CLB的“心跳”突然没了？（注：CLB是负载均衡，心跳应该是CLB那边用来检测的。是一个文件。）查看php-fpm数量，发现有434个；最后群里有人说是他调的 然后就开始查询为啥“心跳”没了。回答：CLB改心跳检查接口的话，如果改动比较大，应该是大面积报。最后问题回归到我们这边。 回答为啥将PHP-FPM数量调高？我理解是这样的，后端有请求时间过长的接口，然后PHP-FPM都用于处理了这些请求长的接口，然后堵住了，然后导致后边的请求都在等待，包括心跳的。心跳5s超时，前端CLB自己断的连接，报的499.所以，增加了fpm的进程数。 查看PHP的error。 只报进程数不够。突然频繁启动PHP进程，结果达到最大值了。 说明：在这个时间点，服务器内存和CPU也都没有问题。 在监控记录中发现，5xx的记录有一个高峰。如下图：并且，报这么多错误，项目日志中并没有记录。（说明不是项目导致的问题） 然后有人在监控记录中有了新发现，如下图： 注：在双十一期间，机器是10台。双十一前是2台（并且量不高）。双十一之后恢复到2台（量比以前高了点）。 现在确定原因了，由于双十一之后量比以前增多了，导致 timewait 增多，2台机器的端口不能够满足那么大的连接。导致频频报5xx。 解决方案：服务器降级，多开几台低配，降低 timewait。（注：timewait不可避免，只能通过增加机器，或者是调整服务端配置，提高timewait的回收速度。）]]></content>
      <categories>
        <category>问题</category>
        <category>线上业务</category>
      </categories>
      <tags>
        <tag>问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP之长连接、短连接]]></title>
    <url>%2F2018%2F12%2F11%2FHTTP%E4%B9%8B%E9%95%BF%E8%BF%9E%E6%8E%A5%E3%80%81%E7%9F%AD%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[1、什么是长连接、短连接？在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个HTML或其他类型的Web页中包含有其他的Web资源（如JavaScript文件、图像文件、CSS文件等），每遇到这样一个Web资源，浏览器就会重新建立一个HTTP会话。 而从HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头加入这行代码：Connection:keep-alive。在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。 2、TCP连接当网络通信时采用TCP协议时，在真正的读写操作之前，客户端与服务器端之间必须建立一个连接，当读写操作完成后，双方不再需要这个连接时可以释放这个连接。连接的建立依靠“三次握手”，而释放则需要“四次握手”，所以每个连接的建立都是需要资源消耗和时间消耗的。经典的三次握手建立连接示意图：经典的四次握手关闭连接示意图： （1）TCP短连接模拟一下TCP短连接的情况：client向server发起连接请求，server接到请求，然后双方建立连接。client向server发送消息，server回应client，然后一次请求就完成了。这时候双方任意都可以发起close操作，不过一般都是client先发起close操作。上述可知，短连接一般只会在 client/server间传递一次请求操作。 短连接的优点是：管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段。 （2）TCP长连接我们再模拟一下长连接的情况：client向server发起连接，server接受client连接，双方建立连接，client与server完成一次请求后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。 TCP的保活功能主要为服务器应用提供。如果客户端已经消失而连接未断开，则会使得服务器上保留一个半开放的连接，而服务器又在等待来自客户端的数据，此时服务器将永远等待客户端的数据。保活功能就是试图在服务端器端检测到这种半开放的连接。 如果一个给定的连接在两小时内没有任何动作，服务器就向客户发送一个探测报文段，根据客户端主机响应探测4个客户端状态： 客户主机依然正常运行，且服务器可达。此时客户的TCP响应正常，服务器将保活定时器复位。 客户主机已经崩溃，并且关闭或者正在重新启动。上述情况下客户端都不能响应TCP。服务端将无法收到客户端对探测的响应。服务器总共发送10个这样的探测，每个间隔75秒。若服务器没有收到任何一个响应，它就认为客户端已经关闭并终止连接。 客户端崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。 客户机正常运行，但是服务器不可达。这种情况与第二种状态类似。 易混淆概念：TCP的keep alive和HTTP的Keep-aliveTCP的keep alive是检查当前TCP连接是否活着；HTTP的Keep-alive是要让一个TCP连接活久点。它们是不同层次的概念。 TCP keep alive的表现：当一个连接“一段时间”没有数据通讯时，一方会发出一个心跳包（Keep Alive包），如果对方有回包则表明当前连接有效，继续监控。 HTTP的Keep-alive（1）HTTP Keep-Alive在http早期，每个http请求都要求打开一个tpc socket连接，并且使用一次之后就断开这个tcp连接。使用keep-alive可以改善这种状态，即在一次TCP连接中可以持续发送多份数据而不会断开连接。通过使用keep-alive机制，可以减少tcp连接建立次数，也意味着可以减少TIME_WAIT状态连接，以此提高性能和提高httpd服务器的吞吐率(更少的tcp连接意味着更少的系统内核调用,socket的accept()和close()调用)。但是，keep-alive并不是免费的午餐,长时间的tcp连接容易导致系统资源无效占用。配置不当的keep-alive，有时比重复利用连接带来的损失还更大。所以，正确地设置keep-alive timeout时间非常重要。（2）keepalvie timeoutHttpd守护进程，一般都提供了keep-alive timeout时间设置参数。比如nginx的keepalive_timeout，和Apache的KeepAliveTimeout。这个keepalive_timout时间值意味着：一个http产生的tcp连接在传送完最后一个响应后，还需要hold住keepalive_timeout秒后，才开始关闭这个连接。当httpd守护进程发送完一个响应后，理应马上主动关闭相应的tcp连接，设置 keepalive_timeout后，httpd守护进程会想说：”再等等吧，看看浏览器还有没有请求过来”，这一等，便是keepalive_timeout时间。如果守护进程在这个等待的时间里，一直没有收到浏览发过来http请求，则关闭这个http连接。 3、长短连接的优缺点：长连接和短连接的产生在于client和server采取的关闭策略。不同的应用场景适合采用不同的策略。 由上可以看出，长连接可以省去较多的TCP建立和关闭的操作，减少浪费，节约时间。对于频繁请求资源的客户来说，较适用长连接。不过这里存在一个问题，存活功能的探测周期太长，还有就是它只是探测TCP连接的存活，属于比较斯文的做法，遇到恶意的连接时，保活功能就不够使了。在长连接的应用场景下，client端一般不会主动关闭它们之间的连接，Client与server之间的连接如果一直不关闭的话，会存在一个问题，随着客户端连接越来越多，server早晚有扛不住的时候，这时候server端需要采取一些策略，如关闭一些长时间没有读写事件发生的连接，这样可 以避免一些恶意连接导致server端服务受损；如果条件再允许就可以以客户端机器为颗粒度，限制每个客户端的最大长连接数，这样可以完全避免某个蛋疼的客户端连累后端服务。 短连接对于服务器来说管理较为简单，存在的连接都是有用的连接，不需要额外的控制手段。但如果客户请求频繁，将在TCP的建立和关闭操作上浪费时间和带宽 4、长短连接操作过程： 短连接：操作步骤：连接-&gt;传输数据-&gt;关闭连接 长连接：操作步骤：连接-&gt;传输数据-&gt;保持连接 -&gt; 传输数据-&gt; 。。。 -&gt;关闭连接。 5、什么时候用长连接，短连接？ 长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况，。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。 而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。 参考文章：HTTP长连接、短连接究竟是什么？：https://www.cnblogs.com/gotodsp/p/6366163.htmlHTTP 的长连接和短连接：http://blog.jobbole.com/104108/http keep_alive 和 tcp keep_alive：https://blog.csdn.net/lys86_1205/article/details/21234867HTTP Keep-Alive模式：http://www.cnblogs.com/skynet/archive/2010/12/11/1903347.html]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP之TIME_WAIT解析]]></title>
    <url>%2F2018%2F12%2F11%2FHTTP%E4%B9%8BTIME-WAIT%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[1、查看服务器连接状态信息查看服务器TCP链接状态信息：netstat -n | awk &#39;/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}&#39; 它会显示例如下面的信息：TIME_WAIT 814CLOSE_WAIT 1FIN_WAIT1 1ESTABLISHED 634SYN_RECV 2LAST_ACK 1 常用的三个状态是：ESTABLISHED 表示正在通信（表示正常数据传输状态），TIME_WAIT 表示主动关闭（表示处理完毕，等待超时结束的请求数），CLOSE_WAIT 表示被动关闭。具体每种状态什么意思，看下图就明白了： 这么多状态不用都记住，只要了解到我上面提到的最常见的三种状态的意义就可以了。一般不到万不得已的情况也不会去查看网络状态，如果服务器出了异常，百分之八九十都是下面两种情况： 服务器保持了大量TIME_WAIT状态 服务器保持了大量CLOSE_WAIT状态 因为linux分配给一个用户的文件句柄是有限的，而TIME_WAIT和CLOSE_WAIT两种状态如果一直被保持，那么意味着对应数目的通道就一直被占着，而且是“占着茅坑不使劲”，一旦达到句柄数上限，新的请求就无法被处理了，接着就是大量Too Many Open Files异常，tomcat崩溃。。。 2、文件句柄说明在Linux里头“一切皆文件”，提示“Too Many Open Files异常”表示句柄不够用了。对应可能就是socket或设备打开太多导致的。 linux在文件句柄的数目上有两个级别的限制。一个是系统级别的总数限制，一个是针对用户的限制。 （1）系统级别的句柄操作 查看系统的句柄限制：sysctl -a | grep fs.file-max或者cat /proc/sys/fs/file-max 修改句柄数限制：sysctl -w fs.file-max 797692或者echo &quot;797692&quot; &gt; /proc/sys/fs/file-max两者作用是相同的，前者改内核参数，后者直接作用于内核参数在虚拟文件系统（procfs,psuedo file system）上对应的文件而已。或者修改内核参数 /etc/sysctl.confecho &quot;fs.file-max=797692&quot; &gt;&gt; /etc/sysctl.conf（注：查看内核参数： sysctl -p）*查看整个系统目前使用的文件句柄数量命令：cat /proc/sys/fs/file-nr （2）用户级别的句柄操作 查看当前用户的文件句柄限制： ulimit -a 修改句柄数限制：修改 /etc/security/limits.conf 增加下面代码：1234* soft nofile 262140* hard nofile 262140* soft nproc 102400* hard nproc 102400 （注：*标识所有用户）有两种限制，一种是soft软限制，在数目超过软限制的时候系统会给出warning警告，但是达到hard硬限制的时候系统将拒绝或者异常了。（注：修改之后可能需要重启shell生效。） （3）其它操作查看某个进程开了那些句柄： lsof -p pid某个进程开了几个句柄： lsof -p pid | wc -l可以看到某个目录/文件被什么进程占用了，显示已打开该目录或文件的所有进程信息： lsof path/filename 3、下面来讨论下这两种情况的处理方法：（1）服务器保持了大量TIME_WAIT状态这种情况比较常见，一些爬虫服务器或者WEB服务器（如果网管在安装的时候没有做内核参数优化的话）上经常会遇到这个问题，这个问题是怎么产生的呢？ 从 上面的示意图可以看得出来，TIME_WAIT是主动关闭连接的一方保持的状态，对于爬虫服务器来说他本身就是“客户端”，在完成一个爬取任务之后，他就 会发起主动关闭连接，从而进入TIME_WAIT的状态，然后在保持这个状态2MSL（max segment lifetime）时间之后，彻底关闭回收资源。为什么要这么做？明明就已经主动关闭连接了为啥还要保持资源一段时间呢？这个是TCP/IP的设计者规定 的，主要出于以下两个方面的考虑： 防止上一次连接中的包，迷路后重新出现，影响新连接（经过2MSL，上一次连接中所有的重复包都会消失） 可靠的关闭TCP连接。在主动关闭方发送的最后一个 ack(fin) ，有可能丢失，这时被动方会重新发fin, 如果这时主动方处于 CLOSED 状态 ，就会响应 rst 而不是 ack。所以主动方要处于 TIME_WAIT 状态，而不能是 CLOSED 。另外这么设计TIME_WAIT 会定时的回收资源，并不会占用很大资源的，除非短时间内接受大量请求或者受到攻击。 关于MSL引用下面一段话：MSL为一个TCP Segment（某一块TCP网路封包）从来源送到目的之间可续存的时间（也就是一个网路封包再网路上传输时能存活的时间），由于RFC 793 TCP传输协议是在1981年定义的，当时的网路速度不像现在的网路那样发达，你可以想象你从浏览器输入等到第一个byte出现要等4分钟吗？在现在的网路环境下几乎不可能有这种事情发生，因此我们大可将 TIME_WAIT 状态的续存时间大幅调低，好让连接口能更快空出来给其它连线使用。 再引用网络资源的一段话：值 得一说的是，对于基于TCP的HTTP协议，关闭TCP连接的是Server端，这样，Server端会进入TIME_WAIT状态，可 想而知，对于访 问量大的Web Server，会存在大量的TIME_WAIT状态，假如server一秒钟接收1000个请求，那么就会积压 240*1000=240，000个 TIME_WAIT的记录，维护这些状态给Server带来负担。当然现代操作系统都会用快速的查找算法来管理这些 TIME_WAIT，所以对于新的 TCP连接请求，判断是否hit中一个TIME_WAIT不会太费时间，但是有这么多状态要维护总是不好。HTTP协议1.1版规定default行为是Keep-Alive，也就是会重用TCP连接传输多个 request/response，一个主要原因就是发现了这个问题 也就是说HTTP的交互跟上面画的那个图是不一样的，关闭连接的不是客户端，而是服务器，所以web服务器也是会出现大量的TIME_WAIT的情况的。这也说清楚了，为什么我的客户端服务器会出现大量CLOSE_WAIT的情况。 现在来说如何来解决这个问题。解决思路很简单，就是让服务器能够快速回收和重用那些TIME_WAIT的资源。下面来看一下我们对/etc/sysctl.conf文件的修改：12345678910111213141516171819202122#对于一个新建连接，内核要发送多少个 SYN 连接请求才决定放弃,不应该大于255，默认值是5，对应于180秒左右时间 net.ipv4.tcp_syn_retries=2 #net.ipv4.tcp_synack_retries=2 #表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为300秒 net.ipv4.tcp_keepalive_time=1200 net.ipv4.tcp_orphan_retries=3 #表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间 net.ipv4.tcp_fin_timeout=30 #表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。 net.ipv4.tcp_max_syn_backlog = 4096 #表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭 net.ipv4.tcp_syncookies = 1 #表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭 net.ipv4.tcp_tw_reuse = 1 #表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭 net.ipv4.tcp_tw_recycle = 1 ##减少超时前的探测次数 net.ipv4.tcp_keepalive_probes=5 ##优化网络设备接收队列 net.core.netdev_max_backlog=3000 修改完之后执行/sbin/sysctl -p让参数生效。 这里头主要注意到的是1234net.ipv4.tcp_tw_reuse net.ipv4.tcp_tw_recycle net.ipv4.tcp_fin_timeout net.ipv4.tcp_keepalive_* 这几个参数。 net.ipv4.tcp_tw_reuse和net.ipv4.tcp_tw_recycle的开启都是为了回收处于TIME_WAIT状态的资源。net.ipv4.tcp_fin_timeout这个时间可以减少在异常情况下服务器从FIN-WAIT-2转到TIME_WAIT的时间。net.ipv4.tcp_keepalive_*一系列参数，是用来设置服务器检测连接存活的相关配置。 （2）服务器保持了大量CLOSE_WAIT状态TIME_WAIT状态可以通过优化服务器参数得到解决，因为发生TIME_WAIT的情况是服务器自己可控的，要么就是对方连接的异常，要么就是自己没有迅速回收资源，总之不是由于自己程序错误导致的。 但是CLOSE_WAIT就不一样了，从上面的图可以看出来，如果一直保持在CLOSE_WAIT状态，那么只有一种情况，就是在对方关闭连接之后服务器程序自己没有进一步发出ack信号。换句话说，就是在对方连接关闭之后，程序里没有检测到，或者程序压根就忘记了这个时候需要关闭连接，于是这个资源就一直被程序占着。个人觉得这种情况，通过服务器内核参数也没办法解决，服务器对于程序抢占的资源没有主动回收的权利，除非终止程序运行。 简单来说CLOSE_WAIT数目过大是由于被动关闭连接处理不当导致的。 我说一个场景，服务器A会去请求服务器B上面的apache获取文件资源，正常情况下，如果请求成功，那么在抓取完资源后服务器A会主动发出关闭连接的请求，这个时候就是主动关闭连接，连接状态我们可以看到是TIME_WAIT。如果一旦发生异常呢？假设请求的资源服务器B上并不存在，那么这个时候就会由服务器B发出关闭连接的请求，服务器A就是被动的关闭了连接，如果服务器A被动关闭连接之后自己并没有释放连接，那就会造成CLOSE_WAIT的状态了。 所以很明显，问题还是处在程序里头。 参考资料：服务器TIME_WAIT和CLOSE_WAIT详解和解决办法：https://www.cnblogs.com/sunxucool/p/3449068.htmlHttpClient4 TIME_WAIT和CLOSE_WAIT：https://www.cnblogs.com/caoyusongnet/p/9087633.html]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令之uniq]]></title>
    <url>%2F2018%2F11%2F20%2FLinux%E5%91%BD%E4%BB%A4%E4%B9%8Buniq%2F</url>
    <content type="text"><![CDATA[uniq - 检查及删除文本文件中重复出现的行列，一般与 sort 命令结合使用。12345678910111213语法： uniq [-cdu][-f&lt;栏位&gt;][-s&lt;字符位置&gt;][-w&lt;字符位置&gt;][--help][--version][输入文件][输出文件]参数： -c或--count 在每列旁边显示该行重复出现的次数。 -d或--repeated 仅显示重复出现的行列。 -f&lt;栏位&gt;或--skip-fields=&lt;栏位&gt; 忽略比较指定的栏位。 -s&lt;字符位置&gt;或--skip-chars=&lt;字符位置&gt; 忽略比较指定的字符。 -u或--unique 仅显示出一次的行列。 -w&lt;字符位置&gt;或--check-chars=&lt;字符位置&gt; 指定要比较的字符。 --help 显示帮助。 --version 显示版本信息。 [输入文件] 指定已排序好的文本文件。如果不指定此项，则从标准读取数据； [输出文件] 指定输出的文件。如果不指定此选项，则将内容显示到标准输出设备（显示终端）。 描述： uniq 命令删除文件中的重复行。 uniq 命令读取由 InFile 参数指定的标准输入或文件。该命令首先比较相邻的行，然后除去第二行和该行的后续副本。重复的行一定相邻。（在发出 uniq 命令之前，请使用 sort 命令使所有重复行相邻。） 实例：testfile中的原有内容为：12345678910$ cat testfile #原有内容 test 30 test 30 test 30 Hello 95 Hello 95 Hello 95 Hello 95 Linux 85 Linux 85 使用uniq 命令删除重复的行后，有如下输出结果：1234$ uniq testfile #删除重复行后的内容 test 30 Hello 95 Linux 85 检查文件并删除文件中重复出现的行，并在行首显示该行重复出现的次数。使用如下命令：1234$ uniq -c testfile #删除重复行后的内容 3 test 30 #前面的数字的意义为该行共出现了3次 4 Hello 95 #前面的数字的意义为该行共出现了4次 2 Linux 85 #前面的数字的意义为该行共出现了2次 （当重复的行并不相邻时，uniq 命令是不起作用的，即若文件内容为以下时，uniq 命令不起作用。必须辅以sort排序后再过滤。）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL中的SAFE UPDATE MODE]]></title>
    <url>%2F2018%2F11%2F20%2FMySQL%E4%B8%AD%E7%9A%84SAFE-UPDATE-MODE%2F</url>
    <content type="text"><![CDATA[写在前面：此SET SQL_SAFE_UPDATES=1命令可防止代码无WHERE条件更新或删除操作。 在业务中发现一个问题，报错如下：在使用 update 的时候，报如上错误。 根据图示可发现是因为开启了 update safe mode，然后where条件中的字段没有索引导致的。 经过查找，确实发现，MySQL有SAFE UPDATE MODE模式，可使用SQL语句更改：SET SQL_SAFE_UPDATES = 0;相当于是解除SAFE MODE模式，则可以更新删除了。 sql_safe_updates参数可以限制不带where条件的update/delete语句执行失败，这个参数设置后，可以防止业务bug/漏洞导致把整个表都更新或者删除（线上发生过的案例），也可以防止DBA在线误操作更新/删除整张表。 官方解释：当sql_safe_updates设置为1时，UPDATE :要有where，并查询条件必须使用为索引字段，或者使用limit，或者两个条件同时存在，才能正常执行。DELETE:where条件中带有索引字段可删除，where中查询条件不是索引，得必须有limit。主要是防止UPDATE和DELETE 没有使用索引导致变更及删除大量数据。系统参数默认值为0 （经验证发现，并不是唯一索引才行（网上某些说法如此），只要是含有索引的就OK。） 为了防止线上业务出现以下3种情况影响线上服务的正常使用和不小心全表数据删除: 没有加where条件的全表更新操作 加了where 条件字段，但是where 字段 没有走索引的表更新 全表delete 没有加where 条件 或者where 条件没有 走索引 建议: DBA 开启此参数限制 ，可以避免线上业务数据误删除操作，但是需要先在测试库开启，这样可以可以先在测试库上补充短缺的表索引，测试验证没问题再部署到线上库 邮件部从去年开始已经在严选电商线上运行。 业务逻辑：业务中开启SAFE MODE的场景是，由于出现过代码中有更新操作，而没有传where条件，导致全部更新（幸运的是，由于更新数据量过大，更新超时，导致数据并没有被更新成功。否则，数据就得回滚了。）。没有传where条件是因为没有进行过滤验证导致的。所以，DB那边就开启了UPDATE SAFE MODE模式。开启这个之后，线上数据库某些表中的字段没有索引，而代码中却使用了没有索引的字段做where条件进行更新。才引发上面的错误。]]></content>
      <categories>
        <category>MySQL</category>
        <category>优化</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>MySQL优化</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL加锁详解]]></title>
    <url>%2F2018%2F11%2F20%2FMySQL%E5%8A%A0%E9%94%81%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1. 新建数据表：123456789| test | CREATE TABLE `test` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(50) NOT NULL DEFAULT &apos;&apos;, `age` tinyint(3) NOT NULL, `hobby` varchar(200) NOT NULL DEFAULT &apos;&apos;, PRIMARY KEY (`id`), UNIQUE KEY `name` (`name`), KEY `hobby` (`hobby`)) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8 | 2. 设置事物隔离级别：12345678910mysql&gt; set tx_isolation=&apos;READ-COMMITTED&apos;;Query OK, 0 rows affectedmysql&gt; select @@tx_isolation;+-----------------+| @@tx_isolation |+-----------------+| READ-COMMITTED |+-----------------+1 row in set 3. 测试索引对于锁的影响（事务级别：READ-COMMITTED，默认级别（REPEATABLE-READ））① 只含有一个主键delete from test where id = 10;这个毋庸置疑，锁的肯定是主键。在id = 10的记录上加上锁即可。 ② 只含有一个唯一键delete from test where id = 10;这个id不是主键，而是一个Unique的二级索引键值。由于id是unique索引，因此delete语句会选择走id列的索引进行where条件的过滤，在找到id=10的记录后，首先会将unique索引上的id=10索引记录加上X锁，同时，会根据读取到的name列，回主键索引(聚簇索引)，然后将聚簇索引上的name = ‘d’ 对应的主键索引项加X锁。 为什么聚簇索引上的记录也要加锁？试想一下，如果并发的一个SQL，是通过主键索引来更新：update t1 set id = 100 where name = ‘d’; 此时，如果delete语句没有将主键索引上的记录加锁，那么并发的update就会感知不到delete语句的存在，违背了同一记录上的更新/删除需要串行执行的约束。 ③ 仅含有一个普通索引delete from test where id = 10;相对于前两个的变化，id列上的约束又降低了，id列不再唯一，只有一个普通的索引。根据此图，可以看到，首先，id列索引上，满足id = 10查询条件的记录，均已加锁。同时，这些记录对应的主键索引上的记录也都加上了锁。与②唯一的区别在于，②最多只有一个满足等值查询的记录，而③会将所有满足查询条件的记录都加锁。 ④ 即含有唯一键，也含有普通索引（唯一索引和普通索引都会 加锁，其它地方使用主键加锁，死锁概率会增加） ⑤ 不含有索引 （聚簇索引上全部加锁）delete from test where id = 10;(没有索引，只能走聚簇索引，进行全部扫描。)相对于前面三个组合，这是一个比较特殊的情况。id列上没有索引，where id = 10;这个过滤条件，没法通过索引进行过滤，那么只能走全表扫描做过滤。由于id列上没有索引，因此只能走聚簇索引，进行全部扫描。从图中可以看到，满足删除条件的记录有两条，但是，聚簇索引上所有的记录，都被加上了X锁。无论记录是否满足条件，全部被加上X锁。既不是加表锁，也不是在满足条件的记录上加行锁。 4. 死锁死锁是指两个或者多个事务在同一资源上相互作用，并请求锁定对方占用的资源，从而导致恶性循环的现象。当多个事务试图以不同的顺序锁定资源时，就可能产生死锁。多个事务同时锁定同一个资源时，也会产生死锁。 （1）互锁最常见的死锁，每个事务执行两条SQL，分别持有了一把锁，然后加另一把锁，产生死锁。 （2）数据查询顺序导致加锁虽然每个Session都只有一条语句，仍旧会产生死锁。要分析这个死锁，首先必须用到本文前面提到的MySQL加锁的规则。针对Session 1，从name索引出发，读到的[hdc, 1]，[hdc, 6]均满足条件，不仅会加name索引上的记录X锁，而且会加聚簇索引上的记录X锁，加锁顺序为先[1,hdc,100]，后[6,hdc,10]。而Session 2，从pubtime索引出发，[10,6],[100,1]均满足过滤条件，同样也会加聚簇索引上的记录X锁，加锁顺序为[6,hdc,10]，后[1,hdc,100]。发现没有，跟Session 1的加锁顺序正好相反，如果两个Session恰好都持有了第一把锁，请求加第二把锁，死锁就发生了。 （注：死锁的发生与否，并不在于事务中有多少条SQL语句，死锁的关键在于：两个(或以上)的Session加锁的顺序不一致。）]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP-FPM配置文件详解]]></title>
    <url>%2F2018%2F10%2F24%2FPHP-FPM%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[过滤php-fpm.conf文件中注释行。grep -Env &quot;^$|;&quot; php-fpm.confgrep - 过滤命令 -E - 使用正则表达示进行匹配 -n - 显示行号 -v - 剔除匹配的项（默认是筛选匹配的项） ^ - 开头匹配 $ - 代表空行 | - 正则中的或运算 ; - ;开头行 配置文件：1234567891011121314[www]user = www #进程的发起用户和用户组，用户user是必须设置，group不是group = wwwlisten = 127.0.0.1:9000 #监听ip和端口pm = dynamic #选择进程池管理器如何控制子进程的数量pm.max_children = 5pm.start_servers = 2pm.min_spare_servers = 1pm.max_spare_servers = 3 slowlog = /opt/webserver/logs/php/$pool.log.slow #用于记录慢请求request_slowlog_timeout = 3 #慢日志请求超时时间，对一个php程序进行跟踪。;request_terminate_timeout = 0 #终止请求超时时间，在worker进程被杀掉之后，提供单个请求的超时间隔。由于某种原因不停止脚本执行时，应该使用该选项，0表示关闭不启用 static： 对于子进程的开启数路给定一个锁定的值(pm.max_children)dynamic： 子进程的数目为动态的，它的数目基于下面的指令的值(以下为dynamic适用参数) pm.max_children： 同一时刻能够存货的最大子进程的数量 pm.start_servers： 在启动时启动的子进程数量 pm.min_spare_servers： 处于空闲”idle”状态的最小子进程，如果空闲进程数量小于这个值，那么相应的子进程会被创建 pm.max_spare_servers： 最大空闲子进程数量，空闲子进程数量超过这个值，那么相应的子进程会被杀掉。ondemand： 在启动时不会创建，只有当发起请求链接时才会创建(pm.max_children, pm.process_idle_timeout) 总结：在php-fpm的配置文件中，有两个指令非常重要，就是”pm.max_children” 和 “request_terminate_timeout” （1）第一个指令”pm.max_children” 确定了php-fpm的处理能力，原则上时越多越好，但这个是在内存足够打的前提下，每开启一个php-fpm进程要占用近30M左右的内存如果请求访问较多，那么可能会出现502，504错误。对于502错误来说，属于繁忙进程而造成的，对于504来说，就是客户发送的请求在限定的时间内没有得到相应，过多的请求导致“504 Gateway Time-out”。这里也有可能是服务器带宽问题。 （2）另外一个需要注意的指令”request_terminate_timeout”，它决定php-fpm进程的连接/发送和读取的时间，如果设置过小很容易出现”502 Bad Gateway” 和 “504 Gateway Time-out”，默认为0，就是说没有启用，不加限制，但是这种设置前提是你的php-fpm足够健康，这个需要根据实际情况加以限定 附属PHP-FPM配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879pid = run/php-fpm.pid#pid设置，默认在安装目录中的var/run/php-fpm.pid，建议开启error_log = log/php-fpm.log#错误日志，默认在安装目录中的var/log/php-fpm.loglog_level = notice#错误级别. 可用级别为: alert（必须立即处理）, error（错误情况）, warning（警告情况）, notice（一般重要信息）, debug（调试信息）. 默认: notice.emergency_restart_threshold = 60emergency_restart_interval = 60s#表示在emergency_restart_interval所设值内出现SIGSEGV或者SIGBUS错误的php-cgi进程数如果超过 emergency_restart_threshold个，php-fpm就会优雅重启。这两个选项一般保持默认值。process_control_timeout = 0#设置子进程接受主进程复用信号的超时时间. 可用单位: s(秒), m(分), h(小时), 或者 d(天) 默认单位: s(秒). 默认值: 0.daemonize = yes#后台执行fpm,默认值为yes，如果为了调试可以改为no。在FPM中，可以使用不同的设置来运行多个进程池。 这些设置可以针对每个进程池单独设置。listen = 127.0.0.1:9000#fpm监听端口，即nginx中php处理的地址，一般默认值即可。可用格式为: &apos;ip:port&apos;, &apos;port&apos;, &apos;/path/to/unix/socket&apos;. 每个进程池都需要设置.listen.backlog = -1#backlog数，-1表示无限制，由操作系统决定，此行注释掉就行。backlog含义参考：http://www.3gyou.cc/?p=41listen.allowed_clients = 127.0.0.1#允许访问FastCGI进程的IP，设置any为不限制IP，如果要设置其他主机的nginx也能访问这台FPM进程，listen处要设置成本地可被访问的IP。默认值是any。每个地址是用逗号分隔. 如果没有设置或者为空，则允许任何服务器请求连接listen.owner = wwwlisten.group = wwwlisten.mode = 0666#unix socket设置选项，如果使用tcp方式访问，这里注释即可。user = wwwgroup = www#启动进程的帐户和组pm = dynamic #对于专用服务器，pm可以设置为static。#如何控制子进程，选项有static和dynamic。如果选择static，则由pm.max_children指定固定的子进程数。如果选择dynamic，则由下开参数决定：pm.max_children #，子进程最大数pm.start_servers #，启动时的进程数pm.min_spare_servers #，保证空闲进程数最小值，如果空闲进程小于此值，则创建新的子进程pm.max_spare_servers #，保证空闲进程数最大值，如果空闲进程大于此值，此进行清理pm.max_requests = 1000#设置每个子进程重生之前服务的请求数. 对于可能存在内存泄漏的第三方模块来说是非常有用的. 如果设置为 &apos;0&apos; 则一直接受请求. 等同于 PHP_FCGI_MAX_REQUESTS 环境变量. 默认值: 0.pm.status_path = /status#FPM状态页面的网址. 如果没有设置, 则无法访问状态页面. 默认值: none. munin监控会使用到ping.path = /ping#FPM监控页面的ping网址. 如果没有设置, 则无法访问ping页面. 该页面用于外部检测FPM是否存活并且可以响应请求. 请注意必须以斜线开头 (/)。ping.response = pong#用于定义ping请求的返回相应. 返回为 HTTP 200 的 text/plain 格式文本. 默认值: pong.request_terminate_timeout = 0#设置单个请求的超时中止时间. 该选项可能会对php.ini设置中的&apos;max_execution_time&apos;因为某些特殊原因没有中止运行的脚本有用. 设置为 &apos;0&apos; 表示 &apos;Off&apos;.当经常出现502错误时可以尝试更改此选项。request_slowlog_timeout = 10s#当一个请求该设置的超时时间后，就会将对应的PHP调用堆栈信息完整写入到慢日志中. 设置为 &apos;0&apos; 表示 &apos;Off&apos;slowlog = log/$pool.log.slow#慢请求的记录日志,配合request_slowlog_timeout使用rlimit_files = 1024#设置文件打开描述符的rlimit限制. 默认值: 系统定义值默认可打开句柄是1024，可使用 ulimit -n查看，ulimit -n 2048修改。rlimit_core = 0#设置核心rlimit最大限制值. 可用值: &apos;unlimited&apos; 、0或者正整数. 默认值: 系统定义值.chroot =#启动时的Chroot目录. 所定义的目录需要是绝对路径. 如果没有设置, 则chroot不被使用.chdir =#设置启动目录，启动时会自动Chdir到该目录. 所定义的目录需要是绝对路径. 默认值: 当前目录，或者/目录（chroot时）catch_workers_output = yes#重定向运行过程中的stdout和stderr到主要的错误日志文件中. 如果没有设置, stdout 和 stderr 将会根据FastCGI的规则被重定向到 /dev/null . 默认值: 空.]]></content>
      <categories>
        <category>服务器</category>
        <category>PHP-FPM</category>
      </categories>
      <tags>
        <tag>PHP-FPM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[INI配置文件的格式]]></title>
    <url>%2F2018%2F10%2F24%2FINI%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%9A%84%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[最近学习Go语言的时候，发现好多Go框架中的配置文件都支持INI格式的。当然，不仅仅是Go框架，PHP语言的好多框架也使用INI格式的配置文件，例如Yaf等。因此，需要梳理一下INI格式。常用的配置文件格式还有很多，如XML配置文件等。 在早期的windows桌面系统中主要是用INI文件作为系统的配置文件，从win95以后开始转向使用注册表，但是还有很多系统配置是使用INI文件的。其实INI文件就是简单的text文件，只不过这种txt文件要遵循一定的INI文件格式。现在的WINCE系统上也常常用INI文件作为配置文件。“.INI ”就是英文 “initialization”的头三个字母的缩写；当然INI file的后缀名也不一定是”.ini”也可以是”.cfg”，”.conf ”或者是”.txt”。 INI文件由节、键、值组成节 [section] 参数（键=值） name=value 注解 注解使用分号表示（;）。在分号后面的文字，直到该行结尾都全部为注解。 INI文件的格式很简单，最基本的三个要素是：parameters，sections和comments。 什么是parameters？INI所包含的最基本的“元素”就是parameter；每一个parameter都有一个name和一个value，name和value是由等号“=”隔开。name在等号的左边。如： name = value 什么是sections ？所有的parameters都是以sections为单位结合在一起的。所有的section名称都是独占一行，并且sections名字都被方括号包围着（[ and ])。在section声明后的所有parameters都是属于该section。对于一个section没有明显的结束标志符，一个section的开始就是上一个section的结束，或者是end of the file。Sections一般情况下不能被nested，当然特殊情况下也可以实现sections的嵌套。section如下所示： [section] 什么是comments ？在INI文件中注释语句是以分号“；”开始的。所有的所有的注释语句不管多长都是独占一行直到结束的。在分号和行结束符之间的所有内容都是被忽略的。注释实例如下：;comments text 当然，上面讲的都是最经典的INI文件格式，随着使用的需求INI文件的格式也出现了很多变种； INI实例12345678910; last modified 1 April 2001 by John Doe[owner]name = John Doeorganization = Acme Products[database]server=192.0.2.42; use IP address in case network name resolution is not workingport=143file = &quot;acme payroll.dat&quot;]]></content>
      <categories>
        <category>服务器</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP状态码499解析]]></title>
    <url>%2F2018%2F10%2F24%2FHTTP%E7%8A%B6%E6%80%81%E7%A0%81499%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[日志记录中HTTP状态码出现499错误有多种情况，我遇到的一种情况是nginx反代到一个永远打不开的后端，就这样了，日志状态记录是499、发送字节数是0。 499错误是什么？让我们看看NGINX的源码中的定义：12345ngx_string(ngx_http_error_495_page), /* 495, https certificate error */ngx_string(ngx_http_error_496_page), /* 496, https no certificate */ngx_string(ngx_http_error_497_page), /* 497, http to https */ngx_string(ngx_http_error_404_page), /* 498, canceled */ngx_null_string, /* 499, client has closed connection */ 可以看到，499对应的是 “client has closed connection”。这很有可能是因为服务器端处理的时间过长，客户端“不耐烦”了。 Nginx 499错误的原因及解决方法打开Nginx的access.log发现在最后一次的提交是出现了HTTP1.1 499 0 -这样的错误，在百度搜索nginx 499错误，结果都是说客户端主动断开了连接。 但经过我的测试这显然不是客户端的问题，因为使用端口+IP直接访问后端服务器不存在此问题，后来测试nginx发现如果两次提交post过快就会出现499的情况，看来是nginx认为是不安全的连接，主动拒绝了客户端的连接. 但搜索相关问题一直找不到解决方法，最后终于在google上搜索到一英文论坛上有关于此错误的解决方法：12proxy_ignore_client_abort on;Don’t know if this is safe. 就是说要配置参数 proxy_ignore_client_abort on;表示代理服务端不要主要主动关闭客户端连接。 以此配置重启nginx,问题果然得到解决。只是安全方面稍有欠缺，但比总是出现找不到服务器好多了。 还有一种原因是 我后来测试发现 确实是客户端关闭了连接,或者说连接超时 ,无论你设置多少超时时间多没用 原来是php进程不够用了 改善一下php进程数 问题解决 默认测试环境才开5个子进程。]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell常用知识点总结]]></title>
    <url>%2F2018%2F10%2F23%2FShell%E5%B8%B8%E7%94%A8%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1、整型变量自增的几种方法12345678a=1a=$(($a+1))a=$[$a+1]a=`expr $a + 1`let a++let a+=1((a++))echo $a 2、读取文件中的内容（1）直接读取对应文件12345#!/bin/bashwhile read linedo echo $line #这里可根据实际用途变化done &lt; urfile （2）使用管道符形式读取12345#!/bin/bashcat urfile | while read linedo echo $linedone （注意：以上代码中urfile 为被读取的文件） （3）将文件赋值给变量12345#!/bin/bashlast | while read wOne wTwo wThreedo echo $wOne&quot;,&quot;$wTwo&quot;,&quot;$wThreedone 3、判断文件是否为空if [[ ! -s filename ]] # 如果文件存在且为空，-s代表存在不为空，！将它取反 4、比较两个字符串的大小&lt; 小于，在ASCII字母顺序下，如： if [[ &quot;$a&quot; &lt; &quot;$b&quot; ]] if [ &quot;$a&quot; \&lt; &quot;$b&quot; ]&#60; 大于，在ASCII字母顺序下，如： if [[ &quot;$a&quot; &gt; &quot;$b&quot; ]] if [ &quot;$a&quot; \&gt; &quot;$b&quot; ]注意：在[]结构中”&gt;”需要被转义。]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之常用命令集锦]]></title>
    <url>%2F2018%2F10%2F12%2FLinux%E4%B9%8B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E9%9B%86%E9%94%A6%2F</url>
    <content type="text"><![CDATA[Linux命令的格式是这样的： 命令名称 [命令参数] [命令对象]注意，命令名称、命令参数、命令对象之间请用空格键分隔。命令对象一般是指要处理的文件、目录、用户等资源，而命令参数可以用长格式（完整的选项名称），也可以用短格式（单个字母的缩写），两者分别用--与-作为前缀。 命令参数的长格式与短格式示例： 长格式 man –help 短格式 man -h 1、manman命令中常用按键以及用途 table th:first-of-type { width: 30%; } 按键 用途 空格键 向下翻一页 PaGe down 向下翻一页 PaGe up 向上翻一页 home 直接前往首页 end 直接前往尾页 / 从上至下搜索某个关键词，如“/linux” ? 从下至上搜索某个关键词，如“?linux” n 定位到下一个搜索到的关键词 N 定位到上一个搜索到的关键词 q 退出帮助文档 man 命令帮助信息的结构以及意义 结构名称 代表意义 NAME 命令的名称 SYNOPSIS 参数的大致使用方法 DESCRIPTION 介绍说明 EXAMPLES 演示（附带简单说明） OVERVIEW 概述 DEFAULTS 默认的功能 OPTIONS 具体的可用选项（带介绍） ENVIRONMENT 环境变量 FILES 用到的文件 SEE ALSO 相关的资料 HISTORY 维护历史与联系方式 一、常用系统工作命令1、echo [选项] 命令 参数 作用 -e 若字符串出现以下字符，则特别加以处理，而不会将它当成一般文字输出（类似一些特殊字符） -n 不要在最后自动换行 2、date命令格式： date [选项] [+指定的格式]date 命令中输入以“+”号开头的参数，即可按照指定格式来输出系统的时间或日期。date命令中的参数以及作用 参数 作用 %t 跳格[Tab 键] %Y 年 %m 月 %d 日 %H 小时（00～23） %I 小时（00～12） %M 分钟（00～59） %S 秒（00～59） %j 今年中的第几天 3、reboot命令4、poweroff命令5、wget命令wget 命令的参数以及作用 参数 作用 -b 后台下载模式 -P 下载到指定目录 -t 最大尝试次数 -c 断点续传 -p 下载页面内所有资源，包括图片、视频等 -r 递归下载 6、ps命令ps 命令的参数以及作用 参数 作用 -a 显示所有进程（包括其他用户的进程） -u 用户以及其他详细信息 -x 显示没有控制终端的进程 7、top命令8、pidof命令用于查询某个指定服务进程的PID值。格式：pidof [参数] [服务名称] 9、kill命令格式：kill [参数] [进程PID] 10、killall命令格式：killall [参数] [服务名称] 11、ln 命令用于创建链接文件，格式为“ln [选项] 目标”。 参数 作用 -s 创建“符号链接”（如果不带-s 参数，则默认创建硬链接） -f 强制创建文件或目录的链接 -i 覆盖前先询问 -v 显示创建链接的过程 12、scp 命令scp（secure copy）是一个基于 SSH 协议在网络之间进行安全传输的命令，其格式为“scp [参数] 本地文件 远程帐户@远程 IP 地址:远程目录”。 参数 作用 -v 显示详细的连接进度 -P 指定远程主机的 sshd 端口号 -r 用于传送文件夹 -6 使用 IPv6 协议 二、系统状态检测命令查看Linux主机公网IPcurl ifconfig.me 1、ifconfig命令其实主要查看的就是网卡名称、inet 参数后面的 IP 地址、ether 参数后面的网卡物理地址（又称为 MAC 地址），以及 RX、TX 的接收数据包与发送数据包的个数及累计流量。123456789[root@linuxprobe ~]# ifconfigeno16777728: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.10.10 netmask 255.255.255.0 broadcast 192.168.10.255 inet6 fe80::20c:29ff:fec4:a409 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:c4:a4:09 txqueuelen 1000 (Ethernet) RX packets 36 bytes 3176 (3.1 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 38 bytes 4757 (4.6 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 2、uname 命令格式：uname [-a] 3、uptime 命令查看系统的负载信息。格式为 uptime平均负载值指的是系统在最近 1 分钟、5 分钟、15 分钟内的压力情况（下面加粗的信息部分）；负载值越低越好，尽量不要长期超过 1，在生产环境中不要超过 5。 4、free 命令用于显示当前系统中内存的使用量信息，格式为“free [-h]”。 5、who命令用于查看当前登入主机的用户终端信息，格式为“who [参数]”。 6、last命令用于查看所有系统的登录记录，格式为“last [参数]”。 7、history命令用于显示历史执行过的命令，格式为“history [-c]”。默认显示1000，如果觉得 1000 不够用，还可以自定义/etc/profile 文件中的HISTSIZE 变量值。在使用 history 命令时，如果使用-c 参数则会清空所有的命令历史记录。 三、工作目录切换命令1、pwd2、cd3、ls四、文本文件编辑命令1、查看文件cat： 由第一行开始显示档案内容tac： 从最后一行开始显示，可以看出 tac 是 cat 的反向显示！nl： 显示的时候，顺便输出行号！more： 一页一页的显示档案内容，以百分比的形式分页显示。less 与 more 类似，但是比 more 更好的是，他可以[pg dn][pg up]翻页！对于显示的内容可以使用”/字符”输入要查找的字符或者字符串并高亮显示。head： 查看头几行tail： 查看尾几行od： 以二进制的方式读取档案内容！ 2、tr命令用于替换文本文件中的字符，格式为“tr [原始字符] [目标字符]”。 3、wc命令用于统计指定文本的行数（-l）、字数（-w）、字节数（-c），格式为“wc [参数] 文本”。 4、stat命令用于查看文件的具体存储信息和时间等信息，格式为“stat 文件名称”。 5、cut命令用于按“列”提取文本字符，格式为“cut [参数] 文本”。使用-f 参数来设置需要看的列数，还需要使用-d 参数来设置间隔符号示例：cut -d: -f1 /etc/passwd 6、diff命令用于比较多个文本文件的差异，格式为“diff [参数] 文件”。 五、文件目录管理命令1、touch命令用于创建空白文件或设置文件的时间，格式为“touch [选项] [文件]”。 2、mkdir命令用于创建空白的目录，格式为“mkdir [选项] 目录”。 3、cp命令用于复制文件或目录，格式为“cp [选项] 源文件 目标文件”。 4、mv命令用于剪切文件或将文件重命名，格式为“mv [选项] 源文件 [目标路径|目标文件名]”。 5、rm命令用于删除文件或目录，格式为“rm [选项] 文件”。 6、dd命令用于按照指定大小和个数的数据块来复制文件或转换文件，格式为“dd [参数]”。dd 命令的参数及其作用 参数 作用 if 输入的文件名称 of 输出的文件名称 bs 设置每个“块”的大小 count 设置要复制“块”的个数 Linux系统中有一个名为/dev/zero 的设备文件，这个文件不会占用系统存储空间，但却可以提供无穷无尽的数据，因此可以使用它作为 dd命令的输入文件，来生成一个指定大小的文件。例如我们可以用 dd 命令从/dev/zero 设备文件中取出一个大小为 560MB 的数据块，然后保存成名为 560_file 的文件。dd if=/dev/zero of=560_file count=1 bs=560MB 7、file命令用于查看文件的类型，格式为“file 文件名”。 六、打包压缩与搜索命令1、tar 命令用于对文件进行打包压缩或解压，格式为“tar [选项] [文件]”。tar 命令的参数及其作用 参数 作用 -c 创建压缩文件 -x 解开压缩文件 -t 查看压缩包内有哪些文件 -z 用 Gzip 压缩或解压 -j 用 bzip2 压缩或解压 -v 显示压缩或解压的过程 -f 目标文件名 -p 保留原始的权限与属性 -P 使用绝对路径来压缩 -C 指定解压到的目录 注：-c 参数用于创建压缩文件，-x 参数用于解压文件，因此这两个参数不能同时使用。 非常推荐使用-v 参数向用户不断显示压缩或解压的过程。 -C 参数用于指定要解压到哪个指定的目录。 -f 参数特别重要，它必须放到参数的最后一位，代表要压缩或解压的软件包名称。 常用组合命令：tar -czvf 压缩包名称.tar.gz 要打包的目录tar -xzvf 压缩包名称.tar.gz -C 解压到的目录 2、grep命令用于在文本中执行关键词搜索，并显示匹配的结果，格式为“grep [选项] [文件]”。grep 命令的参数及其作用 参数 作用 -b 将可执行文件（binary）当作文本文件（text）来搜索 -c 仅显示找到的行数 -i 忽略大小写 -n 显示行号 -v 反向选择—仅列出没有“关键词”的行 最常用的参数：-n 参数用来显示搜索到信息的行号；-v 参数用于反选信息（即没有包含关键词的所有信息行）。 3、find命令用于按照指定条件来查找文件，格式为“find [查找路径] 寻找条件 操作”。find 命令中的参数以及作用 参数 作用 -name 匹配名称 -perm 匹配权限（mode 为完全匹配，-mode 为包含即可） -user 匹配所有者 -group 匹配所有组 -mtime -n +n 匹配修改内容的时间（-n 指 n 天以内，+n 指 n 天以前） -atime -n +n 匹配访问文件的时间（-n 指 n 天以内，+n 指 n 天以前） -ctime -n +n 匹配修改文件权限的时间（-n 指 n 天以内，+n 指 n 天以前） -nouser 匹配无所有者的文件 -nogroup 匹配无所有组的文件 -newer f1 !f2 匹配比文件 f1 新但比 f2 旧的文件 –type b/d/c/p/l/f 匹配文件类型（后面的字母参数依次表示块设备、目录、字符设备、管道、链接文件、文本文件） -size 匹配文件的大小（+50KB 为查找超过 50KB 的文件，而-50KB 为查找小于 50KB 的文件） -prune 忽略某个目录 -exec …… {}\; 后面可跟用于进一步处理搜索结果的命令（下文会有演示） 重点是 “-exec {} \;” 参数，其中的{}表示find命令搜索出的每一个文件，并且命令的结尾必须是“\;”。[root@linuxprobe ~]# find / -user linuxprobe -exec cp -a {} /root/findresults/ \;]]></content>
      <categories>
        <category>书籍</category>
        <category>《Linux就该这么学》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之常用知识点总结]]></title>
    <url>%2F2018%2F10%2F12%2FLinux%E4%B9%8B%E5%B8%B8%E7%94%A8%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1、重置root管理员密码cat /etc/redhat-release 查看系统信息（1）重启 Linux 系统主机并出现引导界面时，按下键盘上的 e 键进入内核编辑界面（2）在 linux16 参数这行的最后面追加“rd.break”参数，然后按下 Ctrl + X 组合键来运行修改过的内核程序（3）大约 30 秒过后，进入到系统的紧急求援模式（4）依次输入以下命令，等待系统重启操作完毕，然后就可以使用新密码 linuxprobe 来登录Linux 系统了。123456mount -o remount,rw /sysrootchroot /sysrootpasswdtouch /.autorelabelexitreboot 2、常用的RPM软件包命令 命令 作用 安装软件的命令格式 rpm -ivh filename.rpm 升级软件的命令格式 rpm -Uvh filename.rpm 卸载软件的命令格式 rpm -e filename.rpm 查询软件描述信息的命令格式 rpm -qpi filename.rpm 列出软件文件信息的命令格式 rpm -qpl filename.rpm 查询文件属于哪个 RPM 的命令格式 rpm -qf filename 3、常见的Yum命令 命令 作用 yum repolist all 列出所有仓库 yum list all 列出仓库中所有软件包 yum info 软件包名称 查 看软件包信息 yum install 软件包名称 安装软件包 yum reinstall 软件包名称 重新安装软件包 yum update 软件包名称 升级软件包 yum remove 软件包名称 移除软件包 yum clean all 清除所有仓库缓存 yum check-update 检查可更新的软件包 yum grouplist 查看系统中已经安装的软件包组 yum groupinstall 软件包组 安装指定的软件包组 yum groupremove 软件包组 移除指定的软件包组 yum groupinfo 软件包组 查询指定的软件包组信息 4、RHEL7 systemctl常用命令systemctl 管理服务的启动、重启、停止、重载、查看状态等常用命令 System V init（RHEL 6） systemctl RHEL 7 作用 service foo start systemctl start foo.service 启动服务 service foo restart systemctl restart foo.service 重启服务 service foo stop systemctl stop foo.service 停止服务 service foo reload systemctl reload foo.service 重新加载配置文件（不终止服务） service foo status systemctl status foo.service 查看服务状态 systemctl 设置服务开机启动、不启动、查看各级别下服务启动状态等常用命令 System V init（RHEL 6） systemctl RHEL 7 作用 chkconfig foo on systemctl enable foo.service 开机自动启动 chkconfig foo off systemctl disable foo.service 开机不自动启动 chkconfig foo systemctl is-enabled foo.service 查看特定服务是否为开机自动启动 chkconfig –list systemctl list-unit-files –type=service 查看各个级别下服务的启动与禁用情况 5、输入、输出重定向输入重定向中用到的符号及其作用 符号 作用 命令 &lt; 文件 将文件作为命令的标准输入 命令 &lt;&lt; 分界符 从标准输入中读入，直到遇见分界符才停止 命令 &lt; 文件 1 &gt; 文件2 将文件 1 作为命令的标准输入并将标准输出到文件 2 输出重定向中用到的符号及其作用 符号 作用 命令 &gt; 文件 将标准输出重定向到一个文件中（清空原有文件的数据） 命令 2&gt; 文件 将错误输出重定向到一个文件中（清空原有文件的数据） 命令 &gt;&gt; 文件 将标准输出重定向到一个文件中（追加到原有内容的后面） 命令 2&gt;&gt; 文件 将错误输出重定向到一个文件中（追加到原有内容的后面） 命令 &gt;&gt; 文件 2&gt;&amp;1 或 命令 &amp;&gt;&gt; 文件 将标准输出与错误输出共同写入到文件中（追加到原有内容的后面） 6、常用的转义字符4 个最常用的转义字符如下所示。 反斜杠（\）：使反斜杠后面的一个变量变为单纯的字符串。 单引号（’’）：转义其中所有的变量为单纯的字符串。 双引号（””）：保留其中的变量属性，不进行转义处理。 反引号（``）：把其中的命令执行后返回结果。 7、Linux 系统中最重要的 10 个环境变量 变量名称 作用 HOME 用户的主目录（即家目录） SHELL 用户在使用的 Shell 解释器名称 HISTSIZE 输出的历史命令记录条数 HISTFILESIZE 保存的历史命令记录条数 MAIL 邮件保存路径 LANG 系统语言、语系名称 RANDOM 生成一个随机数字 PS1 Bash解释器的提示符 PATH 定义解释器搜索用户执行命令的路径 EDITOR 用户默认的文本编辑器 8、Linux系统中常见的目录名称以及相应内容 目录名称 应放置文件的内容 /boot 开启所需文件——内核、开机菜单以及所需配置文件等 /dev 以文件形式存放任何设备与接口 /etc 配置文件 /home 用户家目录 /bin 存放单用户模式下还可以操作的命令 /lib 开机时用到的函数库，以及/bin与/sbin下面的命令要调用的函数 /sbin 开机过程中需要的命令 /media 用于挂载设备文件的目录 /opt 放置第三方的软件 /root 系统管理员的家目录 /srv 一些网络服务的数据文件目录 /tmp 任何人均可使用的“共享”临时目录 /proc 虚拟文件系统，例如系统内核、进程、外部设备及网络状态等 /usr/local 用户自行安装的软件 /usr/sbin Linux系统开机时不会使用到的软件/命令/脚本 /usr/share 帮助与说明文件，也可放置共享文件 /var 主要存放经常变化的文件，如日志 /lost+found 当文件系统发生错误时，将一些丢失的文件片段存放在这里]]></content>
      <categories>
        <category>书籍</category>
        <category>《Linux就该这么学》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之用户身份与权限]]></title>
    <url>%2F2018%2F10%2F12%2FLinux%E4%B9%8B%E7%94%A8%E6%88%B7%E8%BA%AB%E4%BB%BD%E4%B8%8E%E6%9D%83%E9%99%90%2F</url>
    <content type="text"><![CDATA[在RHEL 7系统中，用户身份有下面这些： 管理员UID为0：系统的管理员用户。 系统用户UID为1~999：Linux系统为了避免因某个服务程序出现漏洞而被黑客提权至整台服务器，默认服务程序会有独立的系统用户负责运行，进而有效控制被破坏范围。 普通用户UID从1000开始：是由管理员创建的用于日常工作的用户。 Linux系统中创建每个用户时，将自动创建一个与其同名的基本用户组，而且这个基本用户组只有该用户一个人。如果该用户以后被归纳入其他用户组，则这个其他用户组称之为扩展用户组。（注：一个用户只有一个基本用户组，但是可以有多个扩展用户组。） 一、常用命令1、useradd 命令用于创建新的用户，格式为“useradd [选项] 用户名”。useradd 命令中的用户参数以及作用 table th:first-of-type { width: 20%; } 参数 作用 -d 指定用户的家目录（默认为/home/username） -e 账户的到期时间，格式为YYYY-MM-DD -u 指定改用户的默认UID -g 指定一个初始的用户基本组（必须已存在） -G 指定一个或多个扩展用户组 -N 不创建与用户同名的基本用户组 -s 指定该用户的默认Shell解释器 2、groupadd 命令用于创建用户组，格式为“groupadd [选项] 群组名”。 3、usermod 命令用于修改用户的属性，格式为“usermod [选项] 用户名”。usermod 命令中的参数及作用 参数 作用 -c 填写用户账户的备注信息 -d -m 参数-m 与参数-d 连用，可重新指定用户的家目录并自动把旧的数据转移过去 -e 账户的到期时间，格式为 YYYY-MM-DD -g 变更所属用户组 -G 变更扩展用户组 -L 锁定用户禁止其登录系统 -U 解锁用户，允许其登录系统 -s 变更默认终端 -u 修改用户的 UID 4、passwd 命令用于修改用户密码、过期时间、认证信息等，格式为“passwd [选项] [用户名]”。passwd 命令中的参数以及作用 参数 作用 -l 锁定用户，禁止其登录 -u 解除锁定，允许用户登录 –stdin 允许通过标准输入修改用户密码，如 echo “NewPassWord” &#124; passwd –stdin Username -d 使该用户可用空密码登录系统 -e 强制用户在下次登录时修改密码 -S 显示用户的密码是否被锁定，以及密码所采用的加密算法名称 5、userdel 命令用于删除用户，格式为“userdel [选项] 用户名”。userdel 命令的参数以及作用 参数 作用 -f 强制删除用户 -r 同时删除用户及用户家目录 二、文件权限与归属Linux系统中一切都是文件。文件的类型不尽相同，因此Linux系统使用了不同的字符来加以区分，常见的字符如下所示： -: 普通文件 d: 目录文件 l: 链接文件 b: 块设备文件 c: 字符设备文件 p: 管道文件 在 Linux 系统中，每个文件都有所属的所有者和所有组，并且规定了文件的所有者、所有组以及其他人对文件所拥有的可读（r）、可写（w）、可执行（x）等权限。对于一般文件来说，权限比较容易理解：“可读”表示能够读取文件的实际内容；“可写”表示能够编辑、新增、修改、删除文件的实际内容；“可执行”则表示能够运行一个脚本程序。对目录文件来说，“可读”表示能够读取目录内的文件列表；“可写”表示能够在目录内新增、删除、重命名文件；而“可执行”则表示能够进入该目录。 三、文件的特殊权限1、SUID（待补充）2、SGID（待补充）3、SBITSBIT 特殊权限位可确保用户只能删除自己的文件，而不能删除其他用户的文件。 4、文件的隐藏属性（1）chattr 命令用于设置文件的隐藏权限，格式为“chattr [参数] 文件”。如果想要把某个隐藏功能添加到文件上，则需要在命令后面追加“+参数”，如果想要把某个隐藏功能移出文件，则需要追加“-参数”。chattr 命令中用于隐藏权限的参数及其作用 参数 作用 i 无法对文件进行修改；若对目录设置了该参数，则仅能修改其中的子文件内容而不能新建或删除文件 a 仅允许补充（追加）内容，无法覆盖/删除内容（Append Only） S 文件内容在变更后立即同步到硬盘（sync） s 彻底从硬盘中删除，不可恢复（用 0 填充原文件所在硬盘区域） A 不再修改这个文件或目录的最后访问时间（atime） b 不再修改文件或目录的存取时间 D 检查压缩文件中的错误 d 使用 dump 命令备份时忽略本文件/目录 c 默认将文件或目录进行压缩 u 当删除该文件后依然保留其在硬盘中的数据，方便日后恢复 t 让文件系统支持尾部合并（tail-merging） X 可以直接访问压缩文件中的内容 （2）lsattr 命令用于显示文件的隐藏权限，格式为“lsattr [参数] 文件”。 5、su 命令与sudo服务（1）su 命令与用户名之间有一个减号（-），这意味着完全切换到新的用户，即把环境变量信息也变更为新用户的相应信息，而不是保留原始的信息。强烈建议在切换用户身份时添加这个减号（-）。（注：当从 root 管理员切换到普通用户时是不需要密码验证的，而从普通用户切换成 root管理员就需要进行密码验证了；） （2）sudo 命令把特定命令的执行权限赋予给指定用户用于给普通用户提供额外的权限来完成原本 root 管理员才能完成的任务，格式为“sudo [参数] 命令名称”。sudo 服务中的可用参数以及作用 参数 作用 -h 列出帮助信息 -l 列出当前用户可执行的命令 -u 用户名或 UID 值 以指定的用户身份执行命令 -k 清空密码的有效时间，下次执行 sudo 时需要再次进行密码验证 -b 在后台执行指定的命令 -p 更改询问密码的提示语 总结来说，sudo 命令具有如下功能： 限制用户执行指定的命令： 记录用户执行的每一条命令； 配置文件（/etc/sudoers）提供集中的用户管理、权限与主机等参数； 验证密码的后 5 分钟内（默认值）无须再让用户再次验证密码。 修改sudo服务的配置文件：（注：只有root管理员才可以使用） 使用visudo 命令打开 /etc/sudoers文件并编辑。 在root ALL=(ALL) ALL 改行下面新增加一行，修改对应用户名称 到此即可使用sudo服务，但是需要输入用户密码。免密使用可添加 NOPASSWD:ALL即可。]]></content>
      <categories>
        <category>书籍</category>
        <category>《Linux就该这么学》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker之Dockerfile配置LNMP服务实战篇]]></title>
    <url>%2F2018%2F09%2F30%2FDocker%E4%B9%8BDockerfile%E9%85%8D%E7%BD%AELNMP%E6%9C%8D%E5%8A%A1%E5%AE%9E%E6%88%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[参考博客：传送门supervisor：传送门详细Dockers学习记录参考：传送门 待整理…]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker之手动配置LNMP服务实战篇]]></title>
    <url>%2F2018%2F09%2F30%2FDocker%E4%B9%8B%E6%89%8B%E5%8A%A8%E9%85%8D%E7%BD%AELNMP%E6%9C%8D%E5%8A%A1%E5%AE%9E%E6%88%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[参考博客：传送门 1、安装MySQL 拉取MySQL5.6版本docker pull mysql:5.6 创建并启动一个容器docker run -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=xy123456 --name test_mysql mysql:5.6 进入到mysql容器后，我们通过创建一个远程可以访问的用户，这样我们就能从别的主机访问到我们的数据库了。（注：容器中默认是没有vim的，所以我们首先要安装vim,需要注意的是安装前记得先执行apt-get update命令，不然安装会出现问题。）docker exec -it test_mysql /bin/bash 123456进入数据库授权并新建用户&gt;mysql -uroot -p &quot;xy123456&quot;&gt;create database test;&gt;grant all privileges on test.* to &apos;test&apos;@&apos;%&apos; identified by &apos;123456&apos;;&gt;grant all privileges on *.* to &apos;root&apos;@&apos;%&apos;;&gt;exit; 2、安装PHP 拉取PHP版本docker pull php:7.0-fpm 创建一个phpfpm容器docker run -d -v $PWD/nginx/www/html:/var/www/html -p 9001:9000 --link test_mysql:mysql --name test_phpfpm php:7.0-fpm 进入容器中，并在 /var/www/html 下新建一个 index.php文件；查看宿主机中的 ./nginx/www/html下是否也有 index.php 在容器中，需要安装pdo_mysql模块，在docker容器中可以这样来安装docker-php-ext-install pdo_mysql然后，通过命令 php -m 查看我们的PHP所有模块（后面会用到PDO来测试数据库的连通性）。 3、安装Nginx 拉取Nginx版本docker pull nginx:1.10.3 创建一个Nginx容器docker run -d -p 8080:80 --name test_nginx -v $PWD/nginx/www/html:/var/www/html --link test_phpfpm:phpfpm nginx:1.10.3 进入容器中 123apt-get updateapt-get install -y vimapt-get install -y net-tools 再容器里找到nginx的配置文件，并修改如下：1234567location ~ \.php$ &#123; root /var/www/html; fastcgi_index index.php; fastcgi_pass phpfpm:9000;//这里改成我们之前--link进来的容器，也可以直接用php容器的ip fastcgi_param SCRIPT_FILENAME $document_root$fastcdi_script_name;//如果你的根目录和php容器的根目录不一样，这里的$document_root需要换成你php下的根目录，不然php就找不到文件了 include fastcgi_params;&#125; （注：docker容器中服务修改配置，需要重新启动，才能生效。） 4、测试连通性 直接访问nginx，检测nginx端口是否映射成功。（失败可在容器中查看nginx日志。） 访问php文件，检测nginx与php的连通性。（失败可查看容器日志，docker logs container_id -f） 通过php文件操作数据库，检测nginx、php与mysql的连通性。 检测代码：1234567891011&lt;?phptry &#123; $con = new PDO(&apos;mysql:host=mysql;dbname=test&apos;, &apos;xuye&apos;, &apos;xy123456&apos;); $con-&gt;query(&apos;SET NAMES UTF8&apos;); $res = $con-&gt;query(&apos;select * from test&apos;); while ($row = $res-&gt;fetch(PDO::FETCH_ASSOC)) &#123; echo &quot;id:&#123;$row[&apos;id&apos;]&#125; name:&#123;$row[&apos;name&apos;]&#125;&quot;; &#125;&#125; catch (PDOException $e) &#123; echo &apos;错误原因：&apos; . $e-&gt;getMessage();&#125; 总结说明： 同一个host下容器之间通信，由于现在的版本，只要默认生成的容器，都会加入到Docker默认的docker0网络下。相互之间可通信。（缺点：相同端口的容器只可开启一个容器。） 承接上面，在生成容器的时候，可不添加 –link仍可相互通信。（已验证。–link可更改容器中使用的容器名称） 常用命令特殊安装： 安装命令 安装包 命令 ps procps apt-get install -y procps netstat net-tools apt-get install -y net-tools ping inetutils-ping apt-get install inetutils-ping]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker之容器间通信]]></title>
    <url>%2F2018%2F09%2F30%2FDocker%E4%B9%8B%E5%AE%B9%E5%99%A8%E9%97%B4%E9%80%9A%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[容器之间可通过 IP，Docker DNS Server 或 joined 容器三种方式通信。 IP 通信两个容器要能通信，必须要有属于同一个网络的网卡。满足这个条件后，容器就可以通过 IP 交互了。具体做法是在容器创建时通过 –network 指定相应的网络，或者通过 docker network connect 将现有容器加入到指定网络。 Docker DNS Server通过 IP 访问容器虽然满足了通信的需求，但还是不够灵活。因为我们在部署应用之前可能无法确定 IP，部署之后再指定要访问的 IP 会比较麻烦。对于这个问题，可以通过 docker 自带的 DNS 服务解决。 从 Docker 1.10 版本开始，docker daemon 实现了一个内嵌的 DNS server，使容器可以直接通过“容器名”通信。方法很简单，只要在启动时用 –name 为容器命名就可以了。 下面启动两个容器 bbox1 和 bbox2：12docker run -it --network=my_net2 --name=bbox1 busyboxdocker run -it --network=my_net2 --name=bbox2 busybox 然后，bbox2 就可以直接 ping 到 bbox1 了： 使用 docker DNS 有个限制：只能在 user-defined 网络中使用。也就是说，默认的 bridge 网络是无法使用 DNS 的。下面验证一下：创建 bbox3 和 bbox4，均连接到 bridge 网络。12docker run -it --name=bbox3 busyboxdocker run -it --name=bbox4 busybox bbox4 无法 ping 到 bbox3。 joined 容器joined 容器是另一种实现容器间通信的方式。joined 容器非常特别，它可以使两个或多个容器共享一个网络栈，共享网卡和配置信息，joined 容器之间可以通过 127.0.0.1 直接通信。请看下面的例子： 先创建一个 httpd 容器，名字为 web1。docker run -d -it --name=web1 httpd然后创建 busybox 容器并通过 --network=container:web1 指定 jointed 容器为 web1： 请注意 busybox 容器中的网络配置信息，下面我们查看一下 web1 的网络： 看！busybox 和 web1 的网卡 mac 地址与 IP 完全一样，它们共享了相同的网络栈。busybox 可以直接用 127.0.0.1 访问 web1 的 http 服务。 joined 容器非常适合以下场景： 不同容器中的程序希望通过 loopback 高效快速地通信，比如 web server 与 app server。 希望监控其他容器的网络流量，比如运行在独立容器中的网络监控程序。 容器之间的通信我们已经搞清楚了，接下来要考虑的是容器如何与外部世界通信？ 容器如何访问到外部世界？容器默认就能访问外网。详细信息查看博客：传送门 外部世界如何访问容器？答案是：端口映射。 docker 可将容器对外提供服务的端口映射到 host 的某个端口，外网通过该端口访问容器。容器启动时通过-p参数映射端口： 容器启动后，可通过 docker ps 或者 docker port 查看到 host 映射的端口。在上面的例子中，httpd 容器的 80 端口被映射到 host 32773 上，这样就可以通过 &lt;host ip&gt;:&lt;32773&gt; 访问容器的 web 服务了。 除了映射动态端口，也可在 -p 中指定映射到 host 某个特定端口，例如可将 80 端口映射到 host 的 8080 端口： 每一个映射的端口，host 都会启动一个 docker-proxy 进程来处理访问容器的流量： 以 0.0.0.0:32773-&gt;80/tcp 为例分析整个过程： docker-proxy 监听 host 的 32773 端口。 当 curl 访问 10.0.2.15:32773 时，docker-proxy 转发给容器 172.17.0.2:80。 httpd 容器响应请求并返回结果。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker之容器网络解析]]></title>
    <url>%2F2018%2F09%2F30%2FDocker%E4%B9%8B%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[一、Docker原生网络Docker 网络从覆盖范围可分为单个 host 上的容器网络和跨多个 host 的网络。 1、单个 host 上的容器网络Docker 安装时会自动在 host 上创建三个网络，我们可用 docker network ls 命令查看： （1）none 网络故名思议，none 网络就是什么都没有的网络。挂在这个网络下的容器除了 lo，没有其他任何网卡。容器创建时，可以通过 –network=none 指定使用 none 网络。 我们不禁会问，这样一个封闭的网络有什么用呢？其实还真有应用场景。封闭意味着隔离，一些对安全性要求高并且不需要联网的应用可以使用 none 网络。比如某个容器的唯一用途是生成随机密码，就可以放到 none 网络中避免密码被窃取。 （2）host 网络连接到 host 网络的容器共享 Docker host 的网络栈，容器的网络配置与 host 完全一样。可以通过 –network=host 指定使用 host 网络。 在容器中可以看到 host 的所有网卡，并且连 hostname 也是 host 的。host 网络的使用场景又是什么呢？直接使用 Docker host 的网络最大的好处就是性能，如果容器对网络传输效率有较高要求，则可以选择 host 网络。当然不便之处就是牺牲一些灵活性，比如要考虑端口冲突问题，Docker host 上已经使用的端口就不能再用了。 Docker host 的另一个用途是让容器可以直接配置 host 网路。比如某些跨 host 的网络解决方案，其本身也是以容器方式运行的，这些方案需要对网络进行配置，比如管理 iptables。 （3）bridge 网络应用最广泛也是默认的 bridge 网络。Docker 安装时会创建一个 命名为 docker0 的 linux bridge。如果不指定–network，创建的容器默认都会挂到 docker0 上。 使用brctl命令，需要安装 当前 docker0 上没有任何其他网络设备，我们创建一个容器看看有什么变化。一个新的网络接口 veth28c57df 被挂到了 docker0 上，veth28c57df就是新创建容器的虚拟网卡。 下面看一下容器的网络配置。容器有一个网卡 eth0@if34。大家可能会问了，为什么不是veth28c57df 呢？ 实际上 eth0@if34 和 veth28c57df 是一对 veth pair。veth pair 是一种成对出现的特殊网络设备，可以把它们想象成由一根虚拟网线连接起来的一对网卡，网卡的一头（eth0@if34）在容器中，另一头（veth28c57df）挂在网桥 docker0 上，其效果就是将 eth0@if34 也挂在了 docker0 上。 我们还看到 eth0@if34 已经配置了 IP 172.17.0.2，为什么是这个网段呢？让我们通过 docker network inspect bridge 看一下 bridge 网络的配置信息: 原来 bridge 网络配置的 subnet 就是 172.17.0.0/16，并且网关是 172.17.0.1。这个网关在哪儿呢？大概你已经猜出来了，就是 docker0。 当前容器网络拓扑结构如图所示：容器创建时，docker 会自动从 172.17.0.0/16 中分配一个 IP，这里 16 位的掩码保证有足够多的 IP 可以供容器使用。 2、跨多个 host 的网络待补充… 二、自定义网络除了 none, host, bridge 这三个自动创建的网络，用户也可以根据业务需要创建 user-defined 网络。 Docker 提供三种 user-defined 网络驱动：bridge, overlay 和 macvlan。overlay 和 macvlan 用于创建跨主机的网络。 我们可通过 bridge 驱动创建类似前面默认的 bridge 网络，例如 查看一下当前 host 的网络结构变化： 新增了一个网桥 br-eaed97dc9a77，这里 eaed97dc9a77 正好新建 bridge 网络 my_net 的短 id。执行 docker network inspect 查看一下 my_net 的配置信息： 这里 172.18.0.0/16 是 Docker 自动分配的 IP 网段。 我们可以自己指定 IP 网段吗？答案是：可以。 只需在创建网段时指定 –subnet 和 –gateway 参数： 这里我们创建了新的 bridge 网络 my_net2，网段为 172.22.16.0/24，网关为 172.22.16.1。与前面一样，网关在 my_net2 对应的网桥 br-5d863e9f78b6 上： 容器要使用新的网络，需要在启动时通过 –network 指定： 容器分配到的 IP 为 172.22.16.2。 到目前为止，容器的 IP 都是 docker 自动从 subnet 中分配，我们能否指定一个静态 IP 呢？ 答案是：可以，通过–ip指定。注：只有使用 –subnet 创建的网络才能指定静态 IP。 my_net 创建时没有指定 –subnet，如果指定静态 IP 报错如下： 好了，我们来看看当前 docker host 的网络拓扑结构。 三、理解容器之间的连通性通过上面的实践，当前 docker host 的网络拓扑结构如下图所示 两个 busybox 容器都挂在 my_net2 上，应该能够互通，我们验证一下：可见同一网络中的容器、网关之间都是可以通信的。 my_net2 与默认 bridge 网络能通信吗？ 从拓扑图可知，两个网络属于不同的网桥，应该不能通信，我们通过实验验证一下，让 busybox 容器 ping httpd 容器：确实 ping 不通，符合预期。 原因：iptables DROP 掉了网桥 docker0 与 br-5d863e9f78b6 之间双向的流量。从规则的命名 DOCKER-ISOLATION 可知 docker 在设计上就是要隔离不同的 netwrok。 那么接下来的问题是：怎样才能让 busybox 与 httpd 通信呢？答案是：为 httpd 容器添加一块 net_my2 的网卡。这个可以通过docker network connect 命令实现。 我们在 httpd 容器中查看一下网络配置： 容器中增加了一个网卡 eth1，分配了 my_net2 的 IP 172.22.16.3。现在 busybox 应该能够访问 httpd 了，验证一下： busybox 能够 ping 到 httpd，并且可以访问 httpd 的 web 服务。当前网络结构如图所示：]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker之问题集锦]]></title>
    <url>%2F2018%2F09%2F29%2FDocker%E4%B9%8B%E9%97%AE%E9%A2%98%E9%9B%86%E9%94%A6%2F</url>
    <content type="text"><![CDATA[一、WINDOWS下错误集锦问题一解决 Docker pull 出现的net/http: TLS handshake timeout 的一个办法 解决思路：百度搜了下net/http: TLS handshake timeout 出现一个这个结果比较满意http://dockone.io/article/876?utm_source=tuicool&amp;utm_medium=referral我不用官方的dockhub了，转而使用国内的仓库daocloud12$ echo &quot;DOCKER_OPTS=\&quot;\$DOCKER_OPTS --registry-mirror=http://f2d6cb40.m.daocloud.io\&quot;&quot; | sudo tee -a /etc/default/docker$ sudo service docker restart 重启docker服务后，再次pull 就 ok了 问题二运行Dockerfile启动容器的时候报错，如下：docker: executable file not found in $PATH 问题出现的原因：主要原因是docker 执行的时候没有找到对应的Dockerfile文件位置。 解决方法：在执行docker run的时候，要指定Dockerfile的运行目录位置。如果DOckerfile在当前目录，则在命令后添加“.”即可。（必须项，否则会报此错误。） 问题三在Windows家庭版下安装了docker，并尝试在其中运行jupyter notebook等服务，但映射完毕之后，在主机的浏览器中，打开localhost:port无法访问对应的服务。 问题出现的原因： The reason you’re having this, is because on Linux, the docker daemon (and your containers) run on the Linux machine itself, so “localhost” is also the host that the container is running on, and the ports are mapped to.On Windows (and OS X), the docker daemon, and your containers cannot run natively, so only the docker client is running on your Windows machine, but the daemon (and your containers) run in a VirtualBox Virtual Machine, that runs Linux. 因为docker是运行在Linux上的，在Windows中运行docker，实际上还是在Windows下先安装了一个Linux环境，然后在这个系统中运行的docker。也就是说，服务中使用的localhost指的是这个Linux环境的地址，而不是我们的宿主环境Windows。 解决方法：通过命令 docker-machine ip default其中，default 是docker-machine的name，可以通过docker-machine -ls 查看找到这个Linux的ip地址，一般情况下这个地址是192.168.99.100，然后在Windows的浏览器中，输入这个地址，加上服务的端口即可启用了。（参考博客）]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker之Dockerfile详解]]></title>
    <url>%2F2018%2F09%2F28%2FDocker%E4%B9%8BDockerfile%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[一、Dockerfile详解（1）Dockerfile包含的信息 基础镜像信息 维护者信息 镜像操作指令 容器启动时执行指令 （2）文件的编写1234567891011121314151617# This is docker file# version v1# Author wangshibo# Base image(基础镜像)FROM centos# Maintainer(维护者信息)MAINTAINER wangshibo 2134728394@qq.com# Commands(执行命令)RUN rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpmRUN yum -y install nginx# Add(添加文件)ADD index.html /usr/share/nginx/html/index.html # index.html是自己编写的文件，放在后面的目录中，因为yum安装后Documentroot是在这里RUN echo &quot;daemon off;&quot; &gt;&gt;/etc/nginx/nginx.confEXPOSE 80 # 对外的端口CMD [&apos;nginx&apos;] # 执行的命令 （3）构建容器，并运行 建立newnginx容器，-t：标签，执行/opt/dockerfile/nginx/下面的默认的Dockerfile文件12[root@linux-node2 nginx]# docker build -t cgt/mynginx:v3 /opt/dockerfile/nginx/[root@linux-node2 nginx]# docker run -d -p 83:80 cgt/mynginx:v3 二、指令说明 指令 说明 FROM 指定所创建镜像的基础镜像 MAINTAINER 指定维护者信息 RUN 运行命令 CMD 指定启动容器时默认执行的命令 LABEL 指定生成镜像的元数据标签信息 EXPOSE 声明镜像内服务所监听的端口 ENV 指定环境变量 ADD 赋值指定的&lt;src&gt;路径下的内容到容器中的&lt;dest&gt;路径下，&lt;src&gt;可以为URL；如果为tar文件，会自动解压到&lt;dest&gt;路径下 COPY 赋值本地主机的&lt;src&gt;路径下的内容到容器中的&lt;dest&gt;路径下；一般情况下推荐使用COPY而不是ADD ENTRYPOINT 指定镜像的默认入口 VOLUME 创建数据挂载点 USER 指定运行容器时的用户名或UID WORKDIR 配置工作目录 ARG 指定镜像内使用的参数（例如版本号信息等） ONBUILD 配置当前所创建的镜像作为其他镜像的基础镜像时，所执行的创建操作的命令 STOPSIGNAL 容器退出的信号 HEALTHCHECK 如何进行健康检查 SHELL 指定使用SHELL时的默认SHELL类型 1. FROM指定所创建的镜像的基础镜像，如果本地不存在，则默认会去Docker Hub下载指定镜像。格式为：FROM &lt;image&gt;，或FROM &lt;image&gt;:&lt;tag&gt;，或FROM &lt;image&gt;@&lt;digest&gt;任何Dockerfile中的第一条指令必须为FROM指令。并且，如果在同一个Dockerfile文件中创建多个镜像，可以使用多个FROM指令(每个镜像一次)。 2. MAINTAINER指定维护者信息。格式为：MAINTAINER &lt;name&gt;例如：MAINTAINER image_creator@docker.com该信息将会写入生成镜像的Author属性域中。 3. RUN运行指定命令。格式为：RUN &lt;command&gt; 或 RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] 注意：后一个指令会被解析为json数组，所以必须使用双引号。前者默认将在shell终端中运行命令，即/bin/sh -c；后者则使用exec执行，不会启动shell环境。指定使用其他终端类型可以通过第二种方式实现，例如：RUN [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo hello&quot;]每条RUN指令将在当前镜像的基础上执行指定命令，并提交为新的镜像。当命令较长时可以使用\换行。例如：123RUN apt-get update \ &amp;&amp; apt-get install -y libsnappy-dev zliblg-dev libbz2-dev \ &amp;&amp; rm -rf /var/cache/apt##### 4. CMD #####CMD指令用来指定启动容器时默认执行的命令。它支持三种格式：1. CMD [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] 使用exec执行，是推荐使用的方式；2. CMD param1 param2 在/bin/sh中执行，提供给需要交互的应用；3. CMD [&quot;param1&quot;, &quot;param2&quot;] 提供给ENTRYPOINT的默认参数。每个Dockerfile只能有一条CMD命令。如果指定了多条命令，只有最后一条会被执行。如果用户启动容器时指定了运行的命令(作为run的参数)，则会覆盖掉CMD指定的命令。##### 5. LABEL #####LABEL指令用来生成用于生成镜像的元数据的标签信息。格式为：LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ...例如：12LABEL version=&quot;1.0&quot;LABEL description=&quot;This text illustrates \ that label-values can span multiple lines.&quot;##### 6. EXPOSE #####声明镜像内服务所监听的端口。格式为：EXPOSE &lt;port&gt; [&lt;port&gt;...]例如：EXPOSE 22 80 443 3306注意：该命令只是起到声明租用，并不会自动完成端口映射。在容器启动时需要使用-P(大写P)，Docker主机会自动分配一个宿主机未被使用的临时端口转发到指定的端口；使用-p(小写p)，则可以具体指定哪个宿主机的本地端口映射过来。 7. ENV指定环境变量，在镜像生成过程中会被后续RUN指令使用，在镜像启动的容器中也会存在。格式为：ENV &lt;key&gt;&lt;value&gt;或ENV&lt;key&gt;=&lt;value&gt;...例如：123456789ENV GOLANG_VERSION 1.6.3ENV GOLANG_DOWNLOAD_RUL https://golang.org/dl/go$GOLANG_VERSION.linux-amd64.tar.gzENV GOLANG_DOWNLOAD_SHA256 cdd5e08530c0579255d6153b08fdb3b8e47caabbe717bc7bcd7561275a87aebRUN curl -fssL &quot;$GOLANG_DOWNLOAD_RUL&quot; -o golang.tar.gz &amp;&amp; echo &quot;$GOLANG_DOWNLOAD_SHA256 golang.tar.gz&quot; | sha256sum -c - &amp;&amp; tar -C /usr/local -xzf golang.tar.gz &amp;&amp; rm golang.tar.gzENV GOPATH $GOPATH/bin:/usr/local/go/bin:$PATHRUN mkdir -p &quot;$GOPATH/bin&quot; &amp;&amp; chmod -R 777 &quot;$GOPATH&quot; 指令指定的环境变量在运行时可以被覆盖掉，如docker run --env &lt;key&gt;=&lt;value&gt; built_image。 8. ADD该指令将复制指定的路径下的内容到容器中的路径下。格式为：ADD&lt;src&gt; &lt;dest&gt;其中&lt;src&gt;可以使Dockerfile所在目录的一个相对路径(文件或目录)，也可以是一个URL，还可以是一个tar文件(如果是tar文件，会自动解压到&lt;dest&gt;路径下)。&lt;dest&gt;可以使镜像内的绝对路径，或者相当于工作目录(WORKDIR)的相对路径。路径支持正则表达式，例如：ADD *.c /code/ 9. COPY复制本地主机的&lt;src&gt;(为Dockerfile所在目录的一个相对路径、文件或目录)下的内容到镜像中的&lt;dest&gt;下。目标路径不存在时，会自动创建。路径同样支持正则。格式为：COPY &lt;src&gt; &lt;dest&gt;当使用本地目录为源目录时，推荐使用COPY。 10. ENTRYPOINT指定镜像的默认入口命令，该入口命令会在启动容器时作为根命令执行，所有传入值作为该命令的参数。支持两种格式： ENTRYPOINT [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] (exec调用执行)； ENTRYPOINT command param1 param2(shell中执行)。 此时，CMD指令指定值将作为根命令的参数。每个Dockerfile中只能有一个ENTRYPOINT，当指定多个时，只有最后一个有效。在运行时可以被--entrypoint参数覆盖掉，如docker run --entrypoint。 11. VOLUME创建一个数据卷挂载点。格式为：VOLUME [&quot;/data&quot;]可以从本地主机或者其他容器挂载数据卷，一般用来存放数据库和需要保存的数据等。 12. USER指定运行容器时的用户名或UID，后续的RUN等指令也会使用特定的用户身份。格式为：USER daemon当服务不需要管理员权限时，可以通过该指令指定运行用户，并且可以在之前创建所需要的用户。例如：RUN groupadd -r nginx &amp;&amp; useradd -r -g nginx nginx要临时获取管理员权限可以用gosu或者sudo。 13. WORKDIR为后续的RUN、CMD和ENTRYPOINT指令配置工作目录。格式为：WORKDIR /path/to/workdir可以使用多个WORKDIR指令，后续命令如果参数是相对的，则会基于之前命令指定的路径。例如：1234WORKDIR /aWORKDIR bWORKDIR cRUN pwd 则最终路径为/a/b/c 14. ARG指定一些镜像内使用的参数(例如版本号信息等)，这些参数在执行docker build命令时才以--build-arg&lt;varname&gt;=&lt;value&gt;格式传入。格式为：ARG&lt;name&gt;[=&lt;default value&gt;]则可以用docker build --build-arg&lt;name&gt;=&lt;value&gt;来指定参数值。 15. ONBUILD配置当所创建的镜像作为其他镜像的基础镜像的时候，所执行创建操作指令。格式为：ONBUILD [INSTRUCTION]例如Dockerfile使用如下的内容创建了镜像image-A：1234[...]ONBUILD ADD . /app/srcONBUILD RUN /usr/local/bin/python-build --dir /app/src[...] 如果基于image-A镜像创建新的镜像时，新的Dockerfile中使用FROM image-A指定基础镜像，会自动执行ONBUILD指令的内容，等价于在后面添加了两条指令：12345FROM image-A# Automatically run the followingONBUILD ADD . /app/srcONBUILD RUN /usr/local/bin/python-build --dir /app/src 使用ONBUILD指令的镜像，推荐在标签中注明，例如：ruby:1.9-onbuild。 三、后记从需求出发，定制适合自己需求、高效方便的镜像，可以参考他人优秀的Dockerfile文件，在构建中慢慢优化Dockerfile文件： 精简镜像用途： 尽量让每个镜像的用途都比较集中、单一，避免构造大而复杂、多功能的镜像； 选用合适的基础镜像： 过大的基础镜像会造成构建出臃肿的镜像，一般推荐比较小巧的镜像作为基础镜像； 提供详细的注释和维护者信息： Dockerfile也是一种代码，需要考虑方便后续扩展和他人使用； 正确使用版本号： 使用明确的具体数字信息的版本号信息，而非latest，可以避免无法确认具体版本号，统一环境； 减少镜像层数： 减少镜像层数建议尽量合并RUN指令，可以将多条RUN指令的内容通过&amp;&amp;连接； 及时删除临时和缓存文件： 这样可以避免构造的镜像过于臃肿，并且这些缓存文件并没有实际用途； 提高生产速度： 合理使用缓存、减少目录下的使用文件，使用.dockeringore文件等； 调整合理的指令顺序： 在开启缓存的情况下，内容不变的指令尽量放在前面，这样可以提高指令的复用性； 减少外部源的干扰： 如果确实要从外部引入数据，需要制定持久的地址，并带有版本信息，让他人可以重复使用而不出错。 不要在容器中存储数据： 容器可能被停止，销毁，或替换。一个运行在容器中的程序版本1.0，应该很容易被1.1的版本替换且不影响或损失数据。有鉴于此，如果你需要存储数据，请存在卷中，并且注意如果两个容器在同一个卷上写数据会导致崩溃。确保你的应用被设计成在共享数据存储上写入。 不要在镜像中存储凭据。使用环境变量： 不要将镜像中的任何用户名/密码写死。使用环境变量来从容器外部获取此信息。 使用非root用户运行进程： “docker容器默认以root运行。（…）随着docker的成熟，更多的安全默认选项变得可用。现如今，请求root对于其他人是危险的，可能无法在所有环境中可用。你的镜像应该使用USER指令来指令容器的一个非root用户来运行。” 不要依赖IP地址： 每个容器都有自己的内部IP地址，如果你启动并停止它地址可能会变化。如果你的应用或微服务需要与其他容器通讯，使用任何命名与（或者）环境变量来从一个容器传递合适信息到另一个。 附属实例：Nginx（已验证）1234567891011FROM centosMAINTAINER 2018-04-011 lipengcheng 777@qq.comRUN yum -y install gcc* make pcre-devel zlib-develRUN rpm -ivh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm &amp;&amp; yum -y install nginxEXPOSE 80CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker之基础篇]]></title>
    <url>%2F2018%2F09%2F28%2FDocker%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[一、常用命令1、容器生命周期管理 运行容器1234567docker run [-it] [-d] [--name container_name] [-p container_port:host_port] imageId参数详解：-i 打开容器的标准输入STDIN。-t 为容器建立一个命令行终端。Ctrl+D退出容器-d 以deamon后台运行-p 宿主机与容器端口建立映射--name 为容器命名一个name 进入运行中的容器docker exec -it container_id /bin/sh 查看运行中的容器日志信息docker logs container_id 停止所有容器docker stop $(docker ps -qa) 删除所有容器docker rm [-f] $(docker ps -qa) 删除所有镜像docker rmi [-f] $(docker ps -qa) 导入导出(未操作，待添加)docker loaddocker savedocker exportdocker import 2、容器操作 新建容器docker build -t container_name/container_tag ./ 容器开启/关闭/重启docker start/stop/restart container_id 查看容器详细信息docker inspect container_id 查看容器端口信息docker port container_id 查看运行中的容器docker ps 重新命名容器名称docker rename container_id new_name 删除容器docker rm [-f] container_id 进入容器（进入到容器启动命令的终端）docker attach 94ab7a046f7c 3、镜像仓库 查看docker信息docker info（要点：Registry: https://index.docker.io/v1/ 镜像仓库地址） 查看本地镜像文件docker images 查看镜像docker search whalesay //从镜像仓库中查询镜像文件 拉取镜像docker pull whalesay 推送本地镜像到远程仓库（注：必须先登录，之后才能成功推送。登录使用docker login）docker push myname/whalesay 删除镜像docker rmi [-f] container_id Docker重命名镜像名称和TAGdocker tag IMAGEID(镜像id) REPOSITORY:TAG（仓库：标签） 二、Docker概念Docker是开发人员和系统管理员开发、部署和运行带有容器的应用程序的平台。使用Linux容器来部署应用程序称为“容器化”。容器不是一个新概念，但是使用它们能够轻松部署应用程序。 容器越来越受欢迎，有如下优点： 灵活：即使是最复杂的应用程序也可以被容器化 轻量级：容器利用并共享宿主内核 可互换：您可以动态地部署更新和升级 可移植：您可以在本地构建，部署到云中，并在任何地方运行 可伸缩：您可以增加并自动分发容器副本。 Docker有三个大的概念：Images（镜像）、Containers（容器）、Registry（仓库） Docker 镜像Docker 镜像是Docker容器运行时的只读模板，每一个镜像由一系列的层 (layers) 组成。Docker 使用 UnionFS 来将这些层联合到单独的镜像中。UnionFS 允许独立文件系统中的文件和文件夹(称之为分支)被透明覆盖，形成一个单独连贯的文件系统。正因为有了这些层的存在，Docker 是如此的轻量。当你改变了一个 Docker 镜像，比如升级到某个程序到新的版本，一个新的层会被创建。因此，不用替换整个原先的镜像或者重新建立(在使用虚拟机的时候你可能会这么做)，只是一个新 的层被添加或升级了。现在你不用重新发布整个镜像，只需要升级，层使得分发 Docker 镜像变得简单和快速。 Docker 仓库Docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库。同样的，Docker 仓库也有公有和私有的概念。公有的 Docker 仓库名字是 Docker Hub。Docker Hub 提供了庞大的镜像集合供使用。这些镜像可以是自己创建，或者在别人的镜像基础上创建。Docker 仓库是 Docker 的分发部分。 Docker 容器Docker 容器和文件夹很类似，一个Docker容器包含了所有的某个应用运行所需要的环境。每一个 Docker 容器都是从 Docker 镜像创建的。Docker 容器可以运行、开始、停止、移动和删除。每一个 Docker 容器都是独立和安全的应用平台，Docker 容器是 Docker 的运行部分。 镜像和容器：一个容器是通过运行一个图像来启动的。图像是一个可执行的包，它包含运行应用程序所需的一切——代码、运行时、库、环境变量和配置文件。 容器和虚拟机：一个容器在Linux上运行，并与其他容器共享主机的内核。它运行一个离散的过程，没有比任何其他可执行文件更少的内存，使它变得轻量级。相比之下，虚拟机（VM）运行一个成熟的“客户”操作系统，通过虚拟机监控程序虚拟访问主机资源。一般来说，VMs提供的环境比大多数应用程序需要的资源都多。 三、安装1、Windows版win7、win8 等需要利用 docker toolbox 来安装，国内可以使用阿里云的镜像来下载，下载地址：传送门win10 直接下载官方的安装包：传送门 docker toolbox 是一个工具集，它主要包含以下一些内容：123456Docker CLI 客户端，用来运行docker引擎创建镜像和容器Docker Machine. 可以让你在windows的命令行中运行docker引擎命令Docker Compose. 用来运行docker-compose命令Kitematic. 这是Docker的GUI版本Docker QuickStart shell. 这是一个已经配置好Docker的命令行环境Oracle VM Virtualbox. 虚拟机 下载完成之后直接点击安装，安装成功后，桌边会出现三个图标，入下图所示： 2、Linux版 移除旧的版本 12345678910$ sudo yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine 安装一些必要的系统工具sudo yum install -y yum-utils device-mapper-persistent-data lvm2 添加软件源信息sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 如果报错：使用yum命令报错File “/bin/yum-config-manager”, line 133 except yum.Errors.RepoError, e: SyntaxError: invalid syntax问题vim /bin/yum-config-manager打开，可看见首行为 /usr/bin/python ，由报错可看出使用的是Python2的语法，而我单独安装了Python3，且设置为默认版本导致的语法错误。将/usr/bin/python2 更改为使用版本2的即可。 更新 yum 缓存sudo yum makecache fast 安装Docker-cesudo yum -y install docker-ce 启动Docker后台服务sudo systemctl start docker 测试运行 hello-worlddocker run hello-world 镜像加速鉴于国内网络问题，后续拉取 Docker 镜像十分缓慢，我们可以需要配置加速器来解决。比较常用的是网易的镜像中心和daocloud镜像市场。 网易镜像中心：https://c.163.com/hub#/m/home/ daocloud镜像市场：https://hub.daocloud.io/ 我使用的是网易的镜像地址：http://hub-mirror.c.163.com。请在该配置文件中加入（没有该文件的话，请先建一个。配置文件默认是在 /etc/default/docker）：123&#123; &quot;registry-mirrors&quot;: [&quot;http://hub-mirror.c.163.com&quot;]&#125; 所以刚开始我在寻找/etc/default/docker这个配置文件，一直找不到，后来发现是因为系统和版本的差异。在centos7上这个配置文件已经被更改为 /etc/docker/daemon.json 。可以在这个配置中添加相应的registry-mirrors路径 。原来是这样：1234[root@localhost docker]# cat daemon.json &#123; &quot;live-restore&quot;: true&#125; 添加后：1234&#123; &quot;registry-mirrors&quot;: [&quot;http://ef017c13.m.daocloud.io&quot;], &quot;live-restore&quot;: true&#125; 可以手动vim添加，也可以使用daocloud给出的命令直接更改（建议使用命令）[root@localhost docker]# curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://ef017c13.m.daocloud.io更改后重启dockersystemctl restart docker 删除Docker CE12yum remove docker-cerm -rf /var/lib/docker]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORDER BY与索引]]></title>
    <url>%2F2018%2F08%2F28%2FORDER-BY%E4%B8%8E%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[一条SQL实际上可以分为三步。 得到数据 处理数据 返回处理后的数据 比如这条语句select sid from zhuyuehua.student where sid &lt; 50000 and id &lt; 50000 order by id desc第一步：根据where条件和统计信息生成执行计划，得到数据。第二步：将得到的数据排序。当执行处理数据（order by）时，数据库会先查看第一步的执行计划，看order by 的字段是否在执行计划中利用了索引。如果是，则可以利用索引顺序而直接取得已经排好序的数据。如果不是，则排序操作。第三步：返回排序后的数据。 总结：当order by 中的字段出现在where条件中时，才会利用索引而不排序，更准确的说，order by 中的字段在执行计划中利用了索引时，不用排序操作。 这个结论不仅对order by有效，对其他需要排序的操作也有效。比如group by 、union 、distinct等。]]></content>
      <categories>
        <category>SQL</category>
        <category>SQL优化</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>SQL优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 配置支持IPv6]]></title>
    <url>%2F2018%2F08%2F28%2FLinux-%E9%85%8D%E7%BD%AE%E6%94%AF%E6%8C%81IPv6%2F</url>
    <content type="text"><![CDATA[由于ipv4已经不能满足当前的使用，ipv6出现了，也必将兼容ipv4的地位。因此，下面是配置ipv6的步骤： 一、检查Linux是否已经开启ipv6[root@iz2ze3oyrjbxg32wecre15z /]# ifconfig 从结果看出，输出结果没有 ipv6支持，如果支持ipv6，则输出结果会包含 “inet6”。 可以测试一下，如果环境已经支持，则可以不用往下看了！ 二、开启ipv6（1）找到配置sysctl.conf 文件，路径在：/etc/sysctl.conf ，找到如下配置： 如果已经存在，则直接修改，如果不存在，则新增。将列出的ipv6相关配置更改为0 （2）找到 disable_ipv6.conf 文件，路径在: /etc/modprobe.d/disable_ipv6.conf找到如下配置： 列出的配置更改为 0 （3）找到 network.conf 文件，路径在：/etc/sysconfig/network找到如下配置： 将列出的配置更改为 yes （4）重启网络服务[root@iz2ze3oyrjbxg32wecre15z /]# service network restart （5）通过ifconfig 命令检查是否已经启动ipv6[root@iz2ze3oyrjbxg32wecre15z /]# ifconfig|grep -i inet6 结果显示，已经包含 inet6 相关信息。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VPN 与 VPS的区别]]></title>
    <url>%2F2018%2F08%2F28%2FVPN-%E4%B8%8E-VPS%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[VPS：是Virtual Private Server的英文件缩写，说得是在一台服务器上创建多个相互隔离的虚拟服务器。这些虚拟服务器以最大化的效率共享硬件、软件许可证以及管理资源。对其用户和应用程序来讲，每一个VPS平台的运行和管理都和一台独立主机完全相同，因为每一个VPS都可以独立进行重启，并拥有自己的root访问权限、用户、IP地址、内存、过程、文件、应用程序、系统函数库以及配置文件。VPN：是Virtual Private Network的英文缩写，也就是虚拟专用网络。在公用网络上建立专用网络，进行加密通讯。在企业网络中有广泛应用。VPN网关通过对数据包的加密和数据包目标地址的转换实现远程访问。VPN有多种分类方式，主要是按协议进行分类。VPN可通过服务器、硬件、软件等多种方式实现。 VPN和VPS的区别：我们可以把VPS理解为服务器。一台服务器用软件分割开以后，就成了多台服务器，他们有独立的操作系统，具有独立的IP，这个时候每一个小的独立操作系统，就是一个VPS。也可以理解为，VPS就是一个配置低了点的服务器。VPN是一个软件。用一个帐号和密码，我们登陆了以后，我们的机器访问网站或者是上QQ或者是登陆一些网络软件的时候，所显示的和使用的IP都是国外的。也就是说，VPN是一个可以让我们的机器直接连接到国外的网线上的东西。VPN分为两种，一种是静态的VPN，另外一种就是动态的VPN。动态的VPN是每登陆一次，就变化一次IP的。 VPN的用途：VPN的主要用途：用来做国外的EBAY。VPN在国内最主要的用途：做国外的游戏。例如现在想打国外的游戏币，就要购买VPN，这也是VPN的主要用途。VPN的其它用途：供游戏币工作室来使用，其次是供一些做国外注册类网站的朋友使用。 VPS的用途：VPS主机主要是从空间转向服务器一个过渡。特点就是价格便宜，但是稳定性一般。VPS是桌面操作的，用远程登陆以后，和自己的机器是一样的布局，可以在上面安装软件等。VPS的另外一个常用的用途，就是用来做下载站。因为毕竟是虚拟主机，所以速度比一般的空间要快得多，同时硬盘也大，适合做下载站。所以一般做迅雷的，都会选择一个VPS主机。]]></content>
      <categories>
        <category>随记</category>
      </categories>
      <tags>
        <tag>随记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows 下设置定时任务]]></title>
    <url>%2F2018%2F08%2F28%2Fwindows-%E4%B8%8B%E8%AE%BE%E7%BD%AE%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Linux 系统可以通过crontab -e 设置定时任务，Windows系统没有crontab命令，但是Windows系统有跟crontab命令比较接近的命令： schtasks 命令。 schtasks 语法创建定时任务语法schtasks /create /tn TaskName /tr TaskRun /sc schedule [/mo modifier] [/d day] [/m month[,month...] [/i IdleTime] [/st StartTime] [/sd StartDate] [/ed EndDate] [/s computer [/u [domain\]user /p password]] [/ru {[Domain\]User | &quot;System&quot;} [/rp Password]] /? 参数/tn TaskName 指定任务的名称。/tr TaskRun 指定任务运行的程序或命令。键入可执行文件、脚本文件或批处理文件的完全合格的路径和文件名。（如果忽略该路径，SchTasks.exe 将假定文件在 Systemroot\System32 目录下。）/sc schedule 指定计划类型。有效值为 MINUTE、HOURLY、DAILY、WEEKLY、MONTHLY、ONCE、ONSTART、ONLOGON、ONIDLE。 值说明MINUTE、HOURLY、DAILY、WEEKLY、MONTHLY 指定计划的时间单位。ONCE 任务在指定的日期和时间运行一次。ONSTART 任务在每次系统启动的时候运行。可以指定启动的日期，或下一次系统启动的时候运行任务。ONLOGON 每当用户（任意用户）登录的时候，任务就运行。可以指定日期，或在下次用户登录的时候运行任务。ONIDLE 只要系统空闲了指定的时间，任务就运行。可以指定日期，或在下次系统空闲的时候运行任务。 /mo modifier 指定任务在其计划类型内的运行频率。这个参数对于 MONTHLY 计划是必需的。对于 MINUTE、HOURLY、DAILY 或 WEEKLY 计划，这个参数有效，但也可选。默认值为 1。 计划类型修饰符说明MINUTE1 ～ 1439任务每 n 分钟运行一次。HOURLY1 ～ 23任务每 n 小时运行一次。DAILY1 ～ 365任务每 n 天运行一次。WEEKLY1 ～ 52任务每 n 周运行一次。MONTHLY1 ～ 12任务每 n 月运行一次。LASTDAY任务在月份的最后一天运行。FIRST、SECOND、THIRD、FOURTH、LAST与 /d day 参数共同使用,并在特定的周和天运行任务。例如，在月份的第三个周三。 /d dirlist 指定周或月的一天。只与 WEEKLY 或 MONTHLY 计划共同使用时有效。 计划类型日期值WEEKLY可选项。有效值是 MON ~ SUN 和 * （每一天）。MON 是默认值。MONTHLY在使用 FIRST、SECOND、THIRD、FOURTH 或 LAST 修饰符 (/mo) 时，需要 MON ～ SUN 中的某个值。1 ～ 31 是可选的，只在没有修饰符或修饰符为 1 ～ 12 类型时有效。默认值是 1 （月份的第一天）。 /m month[,month…] 指定一年中的一个月。有效值是 JAN ～ DEC 和 （每个月）。/m 参数只对于 MONTHLY 计划有效。在使用 LASTDAY 修饰符时，这个参数是必需的。否则，它是可选的，默认值是 （每个月）。/i InitialPageFileSize 指定任务启动之前计算机空闲多少分钟。键入一个 1 ～ 999 之间的整数。这个参数只对于 ONIDLE 计划有效，而且是必需的。/st StartTime 以 HH:MM:SS 24 小时格式指定时间。默认值是命令完成时的当前本地时间。/st 参数只对于 MINUTE、HOURLY、DAILY、WEEKLY、MONTHLY 和 ONCE 计划有效。它只对于 ONCE 计划是必需的。/sd StartDate 以 MM/DD/YYYY 格式指定任务启动的日期。默认值是当前日期。/sd 参数对于所有的计划有效，但只对于 ONCE 计划是必需的。/ed EndDate 指定任务计划运行的最后日期。此参数是可选的。它对于 ONCE、ONSTART、ONLOGON 或 ONIDLE 计划无效。默认情况下，计划没有结束日期。/s Computer 指定远程计算机的名称或 IP 地址（带有或者没有反斜杠）。默认值是本地计算机。/u [domain]user 使用特定用户帐户的权限运行命令。默认情况下，使用已登录到运行 SchTasks 的计算机上的用户的权限运行命令。/p password 指定在 /u 参数中指定的用户帐户的密码。如果使用 /u 参数，则需要该参数。/ru {[Domain]User | “System”} 使用指定用户帐户的权限运行任务。默认情况下，使用用户登录到运行 SchTasks 的计算机上的权限运行任务。 值说明[domain}User?指定用户帐户。“System” 或 “”指定操作系统使用的 NT Authority\System 帐户。 /p Password 指定用户帐户的密码，该用户帐户在 /u 参数中指定。如果在指定用户帐户的时候忽略了这个参数，SchTasks.exe 会提示您输入密码而且不显示键入的文本。使用 NT Authority\System 帐户权限运行的任务不需要密码，SchTasks.exe 也不会提示索要密码。/? 在命令提示符显示帮助。 示例（1）计划任务每 20 分钟运行一次。（从脚本创建成功开始计时）schtasks /create /sc minute /mo 20 /tn &quot;Security Script&quot; /tr \\central\data\scripts\sec.vbs （2）计划命令在每小时过五分的时候运行。schtasks /create /sc hourly /st 00:05:00 /tn &quot;My App&quot; /tr c:\apps\myapp.exe （3）计划命令每五小时运行一次（它使用 /mo 参数来指定间隔时间，使用 /sd 参数来指定起始日期。）schtasks /create /sc hourly /mo 5 /sd 03/01/2001 /tn &quot;My App&quot; /tr c:\apps\myapp.exe （4）计划任务每天运行一次schtasks /create /tn &quot;My App&quot; /tr c:\apps\myapp.exe /sc daily /st 08:00:00 /ed 12/31/2001 （5）计划任务每隔一天运行一次（命令使用 /mo 参数来指定间隔天数。使用 /st 参数来指定起始时间， /sd 参数来指定起始日期。）schtasks /create /tn &quot;My App&quot; /tr c:\apps\myapp.exe /sc daily /mo 2 /st 13:00:00 /sd 12/31/2001 （6）计划任务每六周运行一次schtasks /create /tn &quot;My App&quot; /tr c:\apps\myapp.exe /sc weekly /mo 6 /s Server16 /ru Admin01（该命令使用 /mo 参数来指定间隔。它也使用 /s 参数来指定远程计算机，使用 /ru 参数来计划任务以用户的 Administrator 帐户权限运行。因为忽略了 /rp 参数，SchTasks.exe 会提示用户输入 Administrator 帐户密码。另外，因为命令是远程运行的，所以命令中所有的路径，包括到 MyApp.exe 的路径，都是指向远程计算机上的路径。） （7）计划任务每隔一周在周五运行schtasks /create /tn &quot;My App&quot; /tr c:\apps\myapp.exe /sc weekly /mo 2 /d FRI（下面的命令计划任务每隔一周在周五运行。它使用 /mo 参数来指定两周的间隔，使用 /d 参数来指定是一周内的哪一天。如计划任务在每个周五运行，要忽略 /mo 参数或将其设置为 1。） （8）计划任务运行一次schtasks /create /tn &quot;My App&quot; /tr c:\apps\myapp.exe /sc once /st 00:00:00 /sd 01/01/2002 /ru Admin23 /rp p@ssworD1（下面的命令计划 MyApp 程序在 2002 年 1 月 1 日午夜运行一次。它使用 /ru 参数指定以用户的 Administrator 帐户权限运行任务，使用 /rp 参数为 Administrator 帐户提供密码。） （9）计划任务在每次系统启动的时候运行（下面的命令计划 MyApp 程序在每次系统启动的时候运行，起始日期是 2001 年 3 月 15 日。）schtasks /create /tn &quot;My App&quot; /tr c:\apps\myapp.exe /sc onstart /sd 03/15/2001 （10）计划任务在用户登录到远程计算机的时候运行schtasks /create /tn &quot;Start Web Site&quot; /tr c:\myiis\webstart.bat /sc onlogon /s Server23（下面的命令计划批处理文件在用户（任何用户）每次登录到远程计算机上的时候运行。它使用 /s 参数指定远程计算机。因为命令是远程的，所以命令中所有的路径，包括批处理文件的路径，都指定为远程计算机上的路径。） （11）计划某项任务在计算机空闲的时候运行schtasks /create /tn &quot;My App&quot; /tr c:\apps\myapp.exe /sc onidle /i 10（下面的命令计划 MyApp 程序在计算机空闲的时候运行。它使用必需的 /i 参数指定在启动任务之前计算机必需持续空闲十分钟。） 立即执行计划任务立即运行计划任务。run 操作忽略计划，但使用程序文件位置、用户帐户和存储在任务中的密码立即运行任务。语法schtasks /run /tn TaskName [/s computer [/u [domain\]user /p password]] /?示例：在本地计算机上运行任务下面的命令启动 “Security Script” 任务。schtasks /run /tn &quot;Security Script&quot; 在远程计算机上运行任务下面的命令在远程计算机 Svr01 上运行 Update 任务：schtasks /run /tn Update /s Svr01 终止由任务启动的程序语法schtasks /end /tn TaskName [/s computer [/u [domain\]user /p password]] /?示例终止本地计算机上的任务下面的命令终止由 My Notepad 任务启动的 Notepad 实例：schtasks /end /tn &quot;My Notepad&quot;终止远程计算机上的任务下面的命令终止远程计算机 Svr01 上由 InternetOn 任务启动的 Internet Explorer 实例：schtasks /end /tn InternetOn /s Svr01 删除计划任务语法schtasks /delete /tn {TaskName | *} [/f] [/s computer [/u [domain\]user /p password]] [/?]示例（1）从远程计算机上的计划表中删除任务下面的命令从远程计算机上的计划表中删除 “Start Mail” 任务。它使用 /s 参数来标识远程计算机。schtasks /delete /tn &quot;Start Mail&quot; /s Svr16 （2）删除所有为本地计算机计划的任务。下面的命令从本地计算机的计划表中删除所有的任务，包括由其它用户计划的任务。它使用 /tn 参数代表计算机上所有的任务，使用/f 参数取消确认消息。`schtasks /delete /tn /f` 更改计划任务更改一个或多个下列任务属性。 任务运行的程序 (/tr)。 任务运行的用户帐户 (/ru)。 用户帐户的密码 (/rp)。 语法schtasks /change /tn TaskName [/s computer [/u [domain\]user /p password]] [/tr TaskRun] [/ru [Domain\]User | &quot;System&quot;] [/rp Password]示例（1）更改任务运行的程序下面的命令将 Virus Check 任务运行的程序由 VirusCheck.exe 更改为 VirusCheck2.exe。此命令使用 /tn 参数标识任务，使用 /tr 参数指定任务的新程序。（不能更改任务名称。）schtasks /change /tn &quot;Virus Check&quot; /tr C:\VirusCheck2.exe （2）更改远程任务的用户密码下面的命令更改用于远程计算机 Svr01 上 RemindMe 任务的用户帐户密码。命令使用 /tn 参数标识任务，使用 /s 参数指定远程计算机。它使用 /rp 参数指定新的密码 p@ssWord3。在用户帐户密码过期或更改的时候需要此过程。如果存储在任务中的密码无效，那么任务不会运行。schtasks /change /tn RemindMe /s Svr01 /rp p@ssWord3 显示计划任务显示计划在计算机上运行的所有任务，包括那些由其它用户计划的任务。语法schtasks [/query] [/fo {TABLE | LIST | CSV}] [/nh] [/v] [/s computer [/u [domain\]user /p password]] Windows下查看定时任务 Windows自定义定时任务（1）自定义脚本文件 “.bat”123c:cd \schtasksD:\dev\Go\src\myTest\ceshi.exe &gt;&gt; D:\go.txt 说明：在定时执行的bat文件开头加几行命令，先进入存放配置文件的目录。（schtasks的默认其实路径为：C:\Windows\System32） （2）设置定时任务schtasks /create /sc minute /mo 1 /tn &quot;windows_crontab&quot; /tr d:\schtasks\ceshi.bat]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 配置HTTPS及生成CA证书]]></title>
    <url>%2F2018%2F07%2F29%2FNginx-%E9%85%8D%E7%BD%AEHTTPS%E5%8F%8A%E7%94%9F%E6%88%90CA%E8%AF%81%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[原理详解（1）HTTPS 采用对称加密和非对称加密两种形式。对称加密：明文M -&gt; 加密算法（包含加密秘钥） -&gt; 生成密文Y发送 -&gt; 解密算法（包含加密秘钥） -&gt; 明文M（注：加密算法中的秘钥 = 解密算法中的秘钥） 非对称加密：发送方：明文M -&gt; 加密（通过会话秘钥（服务器端生成的公钥）及加密算法） -&gt; 生成密文Y接收方：接收密文Y -&gt; 解密（通过会话秘钥（服务器端生成的私钥）及加密算法） -&gt;明文M（注：服务器端生成公私钥，公钥可以公开用于客户端加密传输数据，秘钥保管在服务器端，用于解密数据。私钥很重要，不能泄露。） （2）HTTPS 加密协议原理：客户端 -&gt; 发起SSL连接 -&gt; 服务器端（保管唯一私钥） 发送对称密码（发送对称密码利用公钥加密） （利用对称秘钥传输数据） 此种方式安全漏洞：会出现一种中间人：① 在客户端“发起SSL连接”的时候，伪装成服务端接受连接，并再次伪装成客户端向服务端发送SSL连接；② 当服务端接受SSL连接的时候，会向客户端发送“公钥”，然后中间人会伪装成客户端接受“公钥”，就会获取公钥；③ 然后中间人会伪装成服务端向真正客户端发送“公钥”，获取客户端利用“公钥”返回的“对称密码”；④ 到此，中间人就把传输数据的安全信息都获取了，可以进行伪装传输数据了。 为解决此安全漏洞，HTTPS采用了数字证书的方式（即CA证书）。原理即是在发送公钥这一层，将服务端返回公钥改成发送“CA签名证书”，然后，客户端已经安装了服务端生成的证书。对服务端返回的证书进行验证即可。校验通过，客户端发送“对称密码”；校验失败则停止会话。 秘钥及证书生成准备工作：Nginx必备模块1234#openssl versionOpenSSL 1.0.2k-fips 26 Jan 2017#nginx -V--with-http_ssl_module （1）生成key秘钥openssl genrsa -idea -out jesonc.key 1024 （2）生成证书签名请求文件（csr文件）openssl req -new -key jesonc.key -out jesonc.csr （3）生成证书签名文件（CA文件）openssl x509 -req -days 3650 -in jesonc.csr -signkey jesonc.key -out jesonc.crt （4）通过秘钥直接生成证书文件openssl req -days 3650 -x509 -sha256 -nodes -newkey rsa:2048 -keyout jesonc.key -out jesonc_apple.crt （5）免密启动服务设置openssl rsa -in ./jesonc.key -out ./jesonc_nopass.key HTTPS语法配置12Syntax： ssl on | off;Context：http, server 示例：12345678server&#123; listen 443; server_name ip; ssl on; ssl_certificate /etc/nginx/ssl_key/jesonc.crt; ssl_certificate_key /etc/nginx/ssl_key/jesonc.key; ...&#125; 场景 - 配置苹果要求的证书 服务器所有的连接使用TLS1.2以上版本（openssl 1.0.2） HTTPS证书必须使用SHA256以上哈希算法签名 HTTPS证书必须使用RSA 2048位或ECC 256位以上公钥算法 使用向前加密技术 （注：查看本地证书要求：openssl x509 -noout -text -in ./jesonc.crt） HTTPS服务优化（1）激活keepalive长连接（2）设置 ssl session缓存示例：12345keepalive_timeout 100;ssl on;ssl_session_cache shared:SSL:10m;ssl_session_timeout 10m; 升级SSL版本（1）下载：wget http://www.openssl.org/source/openssl-1.0.2k.tar.gz（2）解压及安装1234tar -zxvf openssl-1.0.2k.tar.gzcd openssl-1.0.2k./config --prefix=/usr/local/opensslmake &amp;&amp; make install （3）将旧的OpenSSL备份12mv /usr/bin/openssl /usr/bin/openssl.OFFmv /usr/include/openssl /usr/include/openssl.OFF （4）将新的OpenSSL软链接到指定位置12ln -s /usr/local/openssl/bin/openssl /usr/bin/opensslln -s /usr/local/openssl/include/openssl /usr/include/openssl （5）将新安装的openssl的库路径追加到系统的库文件的搜索路径中echo &quot;/usr/local/openssl/lib&quot; &gt;&gt; /etc/ld.so.conf（6）加载系统库文件，使修改后的/etc/ld.so.conf生效ldconfig -v（7）查看openssl是否已更新成功openssl version -a]]></content>
      <categories>
        <category>服务器</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 配置常见问题]]></title>
    <url>%2F2018%2F07%2F29%2FNginx-%E9%85%8D%E7%BD%AE%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1. 相同server_name多个虚拟主机优先级访问答： 相同的server_name，在nginx.conf文件中，会由上往下匹配，匹配成功则不往下继续匹配；而在vhost/下，则依据文件顺序由上往下进行匹配（即文件名称的顺序）。 2. location匹配优先级location方式有三种： = 进行普通字符精确匹配，也就是完全匹配 ^~ 表示普通字符匹配，使用前缀匹配方式 ~/~* 表示执行一个正则匹配() / 通用匹配，如果没有其它匹配，任何请求都会匹配到 @ 用来定义“Named Location”,是专门用来处理“内部重定向”请求的 说明：(location =) &gt; (location 完整路径) &gt; (location ^~ 路径) &gt; (location ~,~*正则顺序) &gt; (location 部分起始路径) &gt; (/)（1）1、2优先级最高，匹配成功后，就不继续往下匹配。而3的优先级低，会一直往下匹配，直到找到最匹配的为止。（2）~与~*的区别：~ 表示区分大小写；~*则表示不区分大小写。 实际使用建议：所以实际使用中，个人觉得至少有三个匹配规则定义，如下：1234567891011121314151617181920#直接匹配网站根，通过域名访问网站首页比较频繁，使用这个会加速处理，官网如是说。#这里是直接转发给后端应用服务器了，也可以是一个静态首页# 第一个必选规则location = / &#123; proxy_pass http://tomcat:8080/index&#125;# 第二个必选规则是处理静态文件请求，这是nginx作为http服务器的强项# 有两种配置模式，目录匹配或后缀匹配,任选其一或搭配使用location ^~ /static/ &#123; root /webroot/static/;&#125;location ~* \.(gif|jpg|jpeg|png|css|js|ico)$ &#123; root /webroot/res/;&#125;#第三个规则就是通用规则，用来转发动态请求到后端应用服务器#非静态文件请求就默认是动态请求，自己根据实际把握#毕竟目前的一些框架的流行，带.php,.jsp后缀的情况很少了location / &#123; proxy_pass http://tomcat:8080/&#125; 3. try_files使用答：按顺序检查文件是否存在示例：1234567location / &#123; root /opt/app/code/cache; try_files $uri @php_page;&#125;location @php_page &#123; proxy_pass http://127.0.0.1:9090;&#125; 解释：检测$uri文件是否存在，如果不存在，则重定向到@php_page模块。 4. Nginx的alias和root区别（1）root123location /request_path/image/ &#123; root /local_path/image/;&#125; 请求链接：http://hostname/request_path/image/cat.png会指向：/local_path/image/request_path/image/cat.png（2）alias123location /request_path/image/ &#123; alias /local_path/image/;&#125; 请求链接：http://hostname/request_path/image/cat.png会指向：/local_path/image/cat.png 5. 用什么方法传递用户的真实IP可通过第一层代理增加http头变量来传递用户真实IP，后端服务可通过该变量来获取。set x_real_ip=$remote_addr（注：x-forwarded-for 可被用户篡改） 附属一个工具用例：Nginx压测工具 ab示例： ab -n 2000 -c 2 url说明：-n 总的请求数-c 并发数-k 是否开启长连接]]></content>
      <categories>
        <category>服务器</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 常见错误集锦]]></title>
    <url>%2F2018%2F07%2F21%2FGit-%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E9%9B%86%E9%94%A6%2F</url>
    <content type="text"><![CDATA[一、git add出现 “in unpopulated submodule ‘A’ “ 问题问题场景：我博客Hexo+GitHub中用到了Next主题，而Next主题是我直接从GitHub上下载使用的。因此我Hexo博客中有.git文件，而Next主题也有.git文件，且这两者文件含义不同。因此，在我git push的时候，报此错误。 原因解释：在 ./ 下有一文件夹 命名为“A”，A/ 有之前建立的仓库，我在 ./ 下add commit push 后发现远程仓库内并没有A/的内容，于是我在 A/ 下执行 ”git add .” 提示：“in unpopulated submodule ‘A’ ”（翻译为”在一个无人居住的子模块“，感觉意思是说位于子模块下，无法 add 0.0） 解决方法是：（1）删除 A/ 的.git 文件夹（2）在 ./ 下输入”git rm -rf –-cached A/“ //谨记：是 A/ ，意为A目录下（3）在 ./ 下输入”git add A”（4）git commit -m “”（5）git push origin master 二、GitHub上提示package-lock.json中依赖文件需要更新问题场景：将文章更新到GitHub上时，偶然间发现提示一个Warning！package-lock.json 依赖文件有漏洞，提示更新。一下就蒙圈了，package-lock.json是啥？怎么更新？经过查找发现，package-lock.json 是npm的包管理文件，即你下载的npm包都记录在里面。那需要怎么更新呢？度娘说直接使用“npm install” 即可，会自动的从package.json里面更新对应包，尝试失败。 解决方法：直接在Hexo根目录下使用命令：npm install packagename直接填写对应包名，更新即可。]]></content>
      <categories>
        <category>版本控制</category>
        <category>Git</category>
        <category>Git问题</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Git问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 实战场景配置]]></title>
    <url>%2F2018%2F07%2F21%2FNginx-%E5%AE%9E%E6%88%98%E5%9C%BA%E6%99%AF%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1、静态资源配置配置语法 - sendfile12Syntax : sendfile on | off;Contaxt: http, server, location, if in location 作用：开启sendfile，则静态数据不会被后端处理（即不会进入用户空间），直接在Nginx层返回数据（即经过内核空间直接返回）。配置语法 - tcp_nopush （注：sendfile 开启的情况下，提高网络包的传输效率）12Syntax : tcp_nopush on | off;Contaxt: http, server, location 配置语法 - tcp_nodelay（注：keepalive 连接下，提高网络包的传输实时性）12Syntax : tcp_nodelay on | off;Contaxt: http, server, location 配置语法 - 压缩12Syntax : gzip on | off;Contaxt: http, server, location, if in location 示例：1234567location ~ .*\.(jpg|gif|png)$ &#123; #gzip on; #gzip_http_version 1.1; #gzip_comp_level 2; #gzip_types text/plain application/javascript application/x-javascript text/css application/xml text/javascript application/x-httpd-php image/jpeg image/gif image/png; root /opt/app/code/images;&#125; 2、浏览器缓存配置客户端校验过期机制： 校验是否过期 Expires、Cache-Control(max-age) 协议中Etag头信息校验 Etag Last-Modified头信息校验 Last-Modified 服务端配置语法：123Syntax : expires [modified] time; expires epoch | max | off;Contaxt: http, server, location, if in location 示例：1234location ~ .*\.(html|htm)$ &#123; expires 24h; root /opt/app/code;&#125; 3、跨站访问配置客户端显示跨域信息：Access-Control-Allow-Origin服务端设置语法：12Syntax : add_header name value [always];Contaxt: http, server, location, if in location 示例：12345location ~ .*\.(html|htm)$ &#123; add_header Access-Control-Allow-Origin http://www.url.com; add_header Access-Control-Allow-Methods GET,POST,PUT,DELETE,OPTIONS; root /opt/app/code;&#125; 4、防盗链配置基于http_refer防盗链配置模块12Syntax : valid_referers none | blocked | server_names | string ...;Contaxt: server, location 示例：1234567location ~ .*\.(jpg|gif|png)$ &#123; valid_referers none blocked 116.62.103.228; （注：也可使用正则匹配） if ($invalid_referer) &#123; return 443; &#125; root /opt/app/code/images;&#125; 5、代理配置 代理区别：区别在于代理的对象不一样 正向代理代理的对象是客户端 反向代理代理的对象是服务端 配置语法：12Syntax : proxy_pass URL;Contaxt: location, if in location, limit_execpt （1）正向代理服务端需限制访问路径，下面服务端只开启对应的IP能够访问：1234567location / &#123; if ( $http_x_forwarded_for !~* &quot;^116\.62\.104\.228&quot; ) &#123; return 403; &#125; root /opt/app/code; index index.html index.htm;&#125; （注：$http_x_forwarded_for 为代理的所有信息，包括客户端的IP。）然后再对应的116.62.104.228服务器上配置正向代理即可。配置如下：1234resolver 8.8.8.8; //Google的一个dns解析location / &#123; proxy_pass http://$http_host$request_uri;&#125; 之后再客户端配置好代理服务器（116.62.104.228）即可进行访问。（注：此处服务端配置可不限制访问也可。理解正向代理就是客户端通过代理服务器向外访问。） （2）反向代理示例：123location ~ /test_proxy.html$ &#123; proxy_pass http://127.0.0.1:8080;&#125; 其它代理配置语法：可浏览Nginx官网进行查看。 6、负载均衡配置配置语法：12Syntax : upstream name &#123;...&#125;Context: http 示例：1234567891011upstream imooc &#123; server 116.62.103.228:8001 down; //不提供服务 server 116.62.103.228:8002 backup; //备份结点 server 116.62.103.228:8003 max_fails=1 fail_timeout=10s; //能够访问，最大试错为1次，服务暂停时间为10s&#125;server &#123; location / &#123; proxy_pass http://imooc; include proxy_params; &#125;&#125; 负载均衡默认规则是轮询。会依据访问请求来依次进行分配。当其中一台机器宕机，会停止对其访问。后端服务器在负载均衡调度中的状态： down 当前的server暂时不参与负载均衡 backup 预留的备份服务器 max_fails 允许请求失败的次数 fail_timeout 经过max_fails失败后，服务暂停的时间 max_conns 限制最大的接收的连接数 调度算法： 轮询 按时间顺序逐一分配到不同的后端服务器 加权轮询 weight值越大，分配到的访问几率越高 ip_hash 每个请求按访问IP的hash结果分配，这样来自同一个IP的固定访问一个后端服务器 url_hash 按照访问的URL的hash结果分配请求，是每个URL定向到同一个后端服务器 least_conn 最少链接数，那个机器连接数少就分发 hash关键数值 hash自定义的key url_hash 配置语法：123Syntax : hash key [consistent];Context: upstream（This directive appeared in version 1.7.2） 示例：123456upstream imooc &#123; hash $request_uri; server 116.62.103.228:8001; server 116.62.103.228:8002; server 116.62.103.228:8003; &#125; 上面示例的意思就是根据 $request_uri的值来计算hash值。此处的key可自定义，只要在配置文件中定义即可。 7、缓存服务配置（Nginx）proxy_cache 配置语法。 8、CPU亲和设置12345worker_processes 2;worker_cpu_affinity auto; //Nginx1.9之后worker_cpu_affinity 0101010101010101 1010101010101010;worker_rlimit_nofile 35535; //worker进程句柄数限制 9、文件上传漏洞http://hostname/upload/1.jpg/1.phpNginx 将1.jpg作为php代码执行123456location ^~ /upload &#123; root /opt/app/images; if ($request_filename ~* (.*)\.php) &#123; return 403; &#125;&#125; 10、Nginx动静分离实现原理：就是将静态内容，直接通过Nginx层就返回给用户，无需通过后端PHP处理。而动态数据，则发往后端处理后返回。静态数据包括：html/htm、js/css、jpg/png/gif、zip/rar 等。示例：动态请求：12345location ~ \.php$ &#123; proxy_pass 127.0.0.1:9000 proxy_params $SCRIPT$REQUIRT_URI; proxy_&#125; 静态内容：1234location ~ \.(jpg|png|gif)$ &#123; expires 1h; gzip on;&#125; 11、Nginx 之Rewrite配置配置语法：12Sysntax: rewrite regex replacement [flag];Context: server, location, if 正则表达式： . 匹配除换行符以外的任意字符 ? 重复0次或1次 + 重复1次或更多次 * 最少链接数，那个机器连接数少就分发 \d 匹配数字 ^ 匹配字符串的开始 $ 匹配字符串的结束 {n} 重复n次 {n,} 重复n次或更多次 [c] 匹配单个字符c [a-z] 匹配a-z小写字母的任意一个 \ 转义字符 () 用于匹配括号之间的内容，通过$1、$2调用 （注：可使用pcretest 来测试正则是否正确。） flag： last 停止rewrite检测 break 停止rewrite检测 redirect 返回302临时重定向，地址栏会显示跳转后的地址 permanent 返回301永久重定向，地址栏会显示跳转后的地址 （注：301与302的区别：临时重定向是客户端向服务器请求则返回，若服务器宕机，则不能够访问。永久重定向一旦服务器返回，再次访问则不需要请求服务器。永久性的指向重定向的服务器。）]]></content>
      <categories>
        <category>服务器</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime配置代码追踪]]></title>
    <url>%2F2018%2F07%2F18%2FSublime%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%A0%81%E8%BF%BD%E8%B8%AA%2F</url>
    <content type="text"><![CDATA[在阅读别人代码的时候，非常喜欢IDE中的追踪代码功能。也想在sublime text中也使用。从网上查找发现可以使用ctags插件。 下面介绍下如何在sublime text中安装使用ctags： 1、安装package control点击Preferences &gt; Browse Packages菜单，进入打开的目录的上层目录，然后再进入Installed Packages/目录，下载Package Control.sublime-package并复制到Installed Packages/目录pControl.sublime-package下载地址：点击下载 输入Ctrl+Shift+P（菜单 - Tools - Command Paletter），输入Install Package并回车，输入或选择你需要的插件回车就安装了。如果快捷键不好使，重启Sublime Text。 2、安装ctags（1）安装sublime text插件输入ctags安装，会发现编辑器左下角正在下载安装。 （2）安装可执行程序下载ctags可执行程序，地址为http://prdownloads.sourceforge.net/ctags/ctags58.zip，解压到一个目录，注意要是纯ASCII字符的目录不要带空格或中文命名的目录。 （3）配置配置可执行文件路径：打开菜单在Preferences菜单中打开Package settings-&gt;ctags-&gt;settings-user和settings-default把default中的配置全部复制到user中，然后改一下command配置项，为ctags的可执行文件路径,即ctags.exe路径123456789101112131415// Path to ctags executable.//// Alter this value if your ctags command is not in the PATH, or if using// a different version of ctags to that in the path (i.e. for OSX).//// NOTE: You *should not* place entire commands here. These commands are// built automatically using the values below. For example, this is OK://// &quot;command&quot;: &quot;/usr/bin/ctags&quot;//// This, on the other hand, won&apos;t work!//// &quot;command&quot;: &quot;ctags -R -f .tags --exclude=some/path&quot;//&quot;command&quot;: &quot;C:/Windows/System32/ctags58/ctags.exe&quot;, 配置快捷键：配置在sublime中使用Ctrl+左键单击函数跳转、Ctrl+右键单击跳回函数调用位置复制以下代码到 Preferences-&gt;Package Settings-&gt;Ctags-&gt;Mouse Bindings-User123456789101112131415[ &#123; &quot;button&quot;: &quot;button1&quot;, &quot;count&quot;: 1, &quot;press_command&quot;: &quot;drag_select&quot;, &quot;modifiers&quot;: [&quot;ctrl&quot;], &quot;command&quot;: &quot;navigate_to_definition&quot; &#125;, &#123; &quot;button&quot;: &quot;button2&quot;, &quot;count&quot;: 1, &quot;modifiers&quot;: [&quot;ctrl&quot;], &quot;command&quot;: &quot;jump_prev&quot; &#125;] （4）使用在使用函数调转功能前，需要先生成.tags文件，只需在项目文件管理器的项目文件上右键点击Ctags:Rebuild Tags即可（注意，在改动文件之后也许重新生成.tags） （5）所有工作都准备充分之后，就可以在函数名上 Ctrl+左键 点击指定函数跳转了，Ctrl+右键返回上一个跳转函数；]]></content>
      <categories>
        <category>编辑器</category>
      </categories>
      <tags>
        <tag>编辑器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建VPS记录]]></title>
    <url>%2F2018%2F07%2F18%2F%E6%90%AD%E5%BB%BAVPN%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[通过Stack Overflow搜索到一个靠谱点的搭建VPS方法，完成搭建，现整理如下： 一、服务器端安装1、软件安装Debian / Ubuntu: apt-get install python-pip pip install shadowsocks CentOS: yum install python-setuptools &amp;&amp; easy_install pip pip install shadowsocks Windows: See [Install Server on Windows] 2、账号配置及常用命令启动： ssserver -p 443 -k password -m aes-256-cfb后台启动： sudo ssserver -p 443 -k password -m aes-256-cfb –user nobody -d start停止： sudo ssserver -d stop检查log日志： sudo less /var/log/shadowsocks.log 3、使用配置文件后台启动： ssserver -c /etc/shadowsocks.json -d start关闭： ssserver -c /etc/shadowsocks.json -d stop配置文件：12345678910&#123;&quot;server&quot;: &quot;0.0.0.0&quot;,&quot;server_port&quot;: 443,&quot;local_address&quot;: &quot;127.0.0.1&quot;,&quot;local_port&quot;: 1080,&quot;password&quot;: &quot;123456&quot;,&quot;timeout&quot;: 300,&quot;method&quot;: &quot;aes-256-cfb&quot;,&quot;fast_open&quot;: false&#125; 4、防火墙相关设置 安装防火墙yum install firewalld 启动防火墙systemctl start firewalld 开启防火墙相应端口firewall-cmd -permanent -zone=putlic -add-port=443/tcpfirewall-cmd -reload 启动Shadowsocks服务 二、客户端安装wiki123456789101112131415161718[Android]: https://github.com/shadowsocks/shadowsocks-android[Build Status]: https://img.shields.io/travis/shadowsocks/shadowsocks/master.svg?style=flat[Configuration]: https://github.com/shadowsocks/shadowsocks/wiki/Configuration-via-Config-File[Coverage Status]: https://jenkins.shadowvpn.org/result/shadowsocks[Coverage]: https://jenkins.shadowvpn.org/job/Shadowsocks/ws/PYENV/py34/label/linux/htmlcov/index.html[Debian sid]: https://packages.debian.org/unstable/python/shadowsocks[iOS]: https://github.com/shadowsocks/shadowsocks-iOS/wiki/Help[Issue Tracker]: https://github.com/shadowsocks/shadowsocks/issues?state=open[Install Server on Windows]: https://github.com/shadowsocks/shadowsocks/wiki/Install-Shadowsocks-Server-on-Windows[Mailing list]: https://groups.google.com/group/shadowsocks[OpenWRT]: https://github.com/shadowsocks/openwrt-shadowsocks[OS X]: https://github.com/shadowsocks/shadowsocks-iOS/wiki/Shadowsocks-for-OSX-Help[PyPI]: https://pypi.python.org/pypi/shadowsocks[PyPI version]: https://img.shields.io/pypi/v/shadowsocks.svg?style=flat[Travis CI]: https://travis-ci.org/shadowsocks/shadowsocks[Troubleshooting]: https://github.com/shadowsocks/shadowsocks/wiki/Troubleshooting[Wiki]: https://github.com/shadowsocks/shadowsocks/wiki[Windows]: https://github.com/shadowsocks/shadowsocks-csharp 三、报错集锦1、启动服务的时候报： AttributeError: /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1: undefined symbol 这个问题是由于在openssl1.1.0版本中，废弃了EVP_CIPHER_CTX_cleanup函数，如官网中所说： EVP_CIPHER_CTX was made opaque in OpenSSL 1.1.0. As a result, EVP_CIPHER_CTX_reset() appeared and EVP_CIPHER_CTX_cleanup() disappeared.EVP_CIPHER_CTX_init() remains as an alias for EVP_CIPHER_CTX_reset(). 修改方法： (1) 用vim打开文件：vim /usr/local/lib/python2.7/dist-packages/shadowsocks/crypto/openssl.py (该路径请根据自己的系统情况自行修改，如果不知道该文件在哪里的话，可以使用find命令查找文件位置)(2) 跳转到52行（shadowsocks2.8.2版本，其他版本搜索一下cleanup）(3) 进入编辑模式(4) 将第52行libcrypto.EVP_CIPHER_CTX_cleanup.argtypes = (c_void_p,) 改为libcrypto.EVP_CIPHER_CTX_reset.argtypes = (c_void_p,)(5) 再次搜索cleanup（全文件共2处，此处位于111行），将libcrypto.EVP_CIPHER_CTX_cleanup(self._ctx) 改为libcrypto.EVP_CIPHER_CTX_reset(self._ctx)(6) 保存并退出(7) 启动shadowsocks服务：service shadowsocks start 或 sslocal -c ss配置文件目录(8) 问题解决 插曲：在 Vultr 上面购买的“东京”服务器用来搭建VPN，刚开始购买的是 $2.5/month，上面标注的是 only ipv6，服务器启动后，才发现国内目前还没有普及ipv6，ping都不通，在阿里云服务器上开启ipv6，然后再ping仍然不通。 应该是网络运营商方面没有开启ipv6，所以都出不去，才ping不通。迫不得已，又删除掉服务器，重新购买了 $5/month 的服务器，搭建完毕。 下面查看电脑是否支持IPV6访问：登陆http://test-ipv6.com/（如果能上网的话）根据网站给出的信息，判断是否支持IPv6。]]></content>
      <tags>
        <tag>VPS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试 - 数据库相关问题]]></title>
    <url>%2F2018%2F06%2F11%2F%E9%9D%A2%E8%AF%95-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1、什么是索引?SQL Server 2000里有什么类型的索引?任何有经验的数据库开发人员都应该能够很轻易地回答这个问题。一些经验不太多的开发人员能够回答这个问题，但是有些地方会说不清楚。简单地说，索引是一个数据结构，用来快速访问数据库表格或者视图里的数据。在SQL Server里，它们有两种形式:聚集索引和非聚集索引。聚集索引在索引的叶级保存数据。这意味着不论聚集索引里有表格的哪个(或哪些)字段，这些字段都会按顺序被保存在表格。由于存在这种排序，所以每个表格只会有一个聚集索引。非聚集索引在索引的叶级有一个行标识符。这个行标识符是一个指向磁盘上数据的指针。它允许每个表格有多个非聚集索引。 2、什么是主键?什么是外键?主键是表格里的(一个或多个)字段，只用来定义表格里的行;主键里的值总是唯一的。外键是一个用来建立两个表格之间关系的约束。这种关系一般都涉及一个表格里的主键字段与另外一个表格(尽管可能是同一个表格)里的一系列相连的字段。那么这些相连的字段就是外键。 3、什么是触发器?SQL Server 2000有什么不同类型的触发器?让未来的数据库开发人员知道可用的触发器类型以及如何实现它们是非常有益的。触发器是一种专用类型的存储过程，它被捆绑到SQL Server 2000的表格或者视图上。在SQL Server 2000里，有INSTEAD-OF和AFTER两种触发器。INSTEAD-OF触发器是替代数据操控语言(Data Manipulation Language，DML)语句对表格执行语句的存储过程。例如，如果我有一个用于TableA的INSTEAD-OF-UPDATE触发器，同时对这个表格执行一个更新语句，那么INSTEAD-OF-UPDATE触发器里的代码会执行，而不是我执行的更新语句则不会执行操作。AFTER触发器要在DML语句在数据库里使用之后才执行。这些类型的触发器对于监视发生在数据库表格里的数据变化十分好用。 您如何确一个带有名为Fld1字段的TableB表格里只具有Fld1字段里的那些值，而这些值同时在名为TableA的表格的Fld1字段里?这个与关系相关的问题有两个可能的答案。第一个答案(而且是您希望听到的答案)是使用外键限制。外键限制用来维护引用的完整性。它被用来确保表格里的字段只保存有已经在不同的(或者相同的)表格里的另一个字段里定义了的值。这个字段就是候选键(通常是另外一个表格的主键)。另外一种答案是触发器。触发器可以被用来保证以另外一种方式实现与限制相同的作用，但是它非常难设置与维护，而且性能一般都很糟糕。由于这个原因，微软建议开发人员使用外键限制而不是触发器来维护引用的完整性。 4、对一个投入使用的在线事务处理表格有过多索引需要有什么样的性能考虑?你正在寻找进行与数据操控有关的应聘人员。对一个表格的索引越多，数据库引擎用来更新、插入或者删除数据所需要的时间就越多，因为在数据操控发生的时候索引也必须要维护。 5、你可以用什么来确保表格里的字段只接受特定范围里的值?这个问题可以用多种方式来回答，但是只有一个答案是“好”答案。您希望听到的回答是Check限制，它在数据库表格里被定义，用来限制输入该列的值。触发器也可以被用来限制数据库表格里的字段能够接受的值，但是这种办法要求触发器在表格里被定义，这可能会在某些情况下影响到性能。因此，微软建议使用Check限制而不是其他的方式来限制域的完整性。 6、对关系型数据库而言，索引是相当重要的概念，请回答有关索引几个问题:a) 索引的目的是什么?b) 索引对数据库系统的负面影响是什么?c) 为数据表建立索引的原则有哪些?d) 什么情况下不宜建立索引? 答：索引的目的： 快速访问数据表中的特定信息，提高检索速度 创建唯一性索引，保证数据库表中每一行数据的唯一性 加速表和表之间的连接 使用分组和排序子句进行数据检索时，可以显著减少查询中分组和排序的时间 负面影响：创建索引和维护索引需要耗费时间，这个时间随着数据量的增加而增加；索引需要占用物理空间，不光是表需要占用数据空间，每个索引也需要占用物理空间；当对表进行增、删、改的时候索引也要动态维护，这样就降低了数据的维护速度。 建立索引的原则： 在最频繁使用的、用以缩小查询范围的字段上建立索引 在平频繁使用的、需要排序的字段上建立索引 什么情况下不宜建立索引： 对于查询中很少涉及的列或者重复值比较多的列，不宜建立索引 对于一些特殊的数据类型，不宜建立索引，比如文本字段(text)等。]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试 - php易混淆题集锦]]></title>
    <url>%2F2018%2F06%2F11%2F%E9%9D%A2%E8%AF%95-php%E6%98%93%E6%B7%B7%E6%B7%86%E9%A2%98%E9%9B%86%E9%94%A6%2F</url>
    <content type="text"><![CDATA[1. 写出如下程序的输出结果（考点：变量判空）12345678910111213141516171819202122232425$a1 = null;$a2 = false;$a3 = 0;$a4 = &apos;&apos;;$a5 = &apos;0&apos;;$a6 = &apos;null&apos;;$a7 = array();$a8 = array(array());echo empty($a1) ? &apos;true&apos; : &apos;false&apos;; //trueecho &quot;&lt;br/&gt;&quot;;echo empty($a2) ? &apos;true&apos; : &apos;false&apos;; //trueecho &quot;&lt;br/&gt;&quot;;echo empty($a3) ? &apos;true&apos; : &apos;false&apos;; //trueecho &quot;&lt;br/&gt;&quot;;echo empty($a4) ? &apos;true&apos; : &apos;false&apos;; //trueecho &quot;&lt;br/&gt;&quot;;echo empty($a5) ? &apos;true&apos; : &apos;false&apos;; //false trueecho &quot;&lt;br/&gt;&quot;;echo empty($a6) ? &apos;true&apos; : &apos;false&apos;; //falseecho &quot;&lt;br/&gt;&quot;;echo empty($a7) ? &apos;true&apos; : &apos;false&apos;; //trueecho &quot;&lt;br/&gt;&quot;;echo empty($a8) ? &apos;true&apos; : &apos;false&apos;; //true falseecho &quot;&lt;hr/&gt;&quot;; 2. 写出如下程序的输出结果（考点：变量引用）12345678$test = &apos;aaaaaa&apos;;$abc = &amp;$test;unset($test);echo $abc; //aaaaaaecho $test; //undefined valiableecho &quot;&lt;hr/&gt;&quot;; 3. 写出如下程序的输出结果（考点：静态变量）123456789101112$count = 5;function get_count()&#123; static $count = 0; return $count++;&#125;echo $count; //5++$count;echo get_count(); //0echo get_count(); //1echo &quot;&lt;hr/&gt;&quot;; 4. 写出如下程序的输出结果（考点：全局变量作用域）12345678910111213$GLOBALS[&apos;var1&apos;] = 5;$var2 = 1;function get_value()&#123; global $var2; $var1 = 0; return $var2++;&#125;get_value();echo $var1; //5echo $var2; //2echo &quot;&lt;hr/&gt;&quot;; 全局变量的作用域问题：12345678910function destroy_foo() &#123; global $foo; unset($foo); // unset($GLOBALS[&apos;foo&apos;]);//清除全局变量$foo&#125;$foo = &apos;bar&apos;;destroy_foo();echo $foo; //bar unset($foo) 的作用是清除函数内的 $foo变量，并没有清除全局变量的$foo，因此执行此函数后，变量仍存在。想要彻底清除$foo，需要清除掉全局变量里的$foo值。 5. 写出如下程序的输出结果（考点：unset对于数组的作用）1234567891011121314151617function get_arr($arr)&#123; unset($arr[0]);&#125;function get_arr2(&amp;$arr)&#123; unset($arr[0]);&#125;$arr1 = array(1, 2);$arr2 = array(1, 2);get_arr($arr1);get_arr2($arr2);echo count($arr1); //2echo count($arr2); //1echo &quot;&lt;hr/&gt;&quot;; 6. 字符串强制转换为整型为0例如：12&quot;a1&quot; =&gt; 0 //首字母a为字符，强制转换为整型为0&quot;1a&quot; =&gt; 1 //首字母1为数字，强制转换为整型为1 7. &amp; 与 &amp;&amp; 的区别1234567$a = 10;$b = 10;if($a &lt; 4 &amp;&amp; (++$a &gt; 10))&#123;&#125;if($b &lt; 4 &amp; (++$b &gt; 10))&#123;&#125;echo $a;echo $b; &amp; 是按位与操作符，两边都为 1 则为 1。左侧为false，也会运行右侧的代码。&amp;&amp; 是逻辑与操作符，左侧为false，不运行右侧的代码。相同点：都可进行 and 判断。 8. unset对于数组的作用123456789101112131415$arr = array( 1=&gt; &apos;a&apos;, 2=&gt; &apos;bb&apos;, 3=&gt; &apos;c&apos;, 4=&gt; &apos;dd&apos; );foreach($arr as $k=&gt;&amp;$v)&#123; if($k == 3)&#123; $v = &apos;x&apos;; &#125; // unset($v); //保持键4 的值不变&#125;foreach($arr as $k=&gt;$v)&#123; echo &quot;&#123;$k&#125;\t&#123;$v&#125;&lt;br/&gt;&quot;;&#125; 输出结果为：array(‘a’, ‘bb’, ‘x’, ‘x’);为什么结果是这样？ 因为满足条件语句之后，就把数组的值指向了’x’的内存地址了。因此后面的数组索引都是指向了’x’。怎么能让 键4 的值不变？ 使用 unset($v) 来清除 指针。]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令之netstat]]></title>
    <url>%2F2018%2F06%2F11%2FLinux%E5%91%BD%E4%BB%A4%E4%B9%8Bnetstat%2F</url>
    <content type="text"><![CDATA[1234567netstat命令各个参数说明如下： -a : 显示全部端口 -t : 指明显示TCP端口 -u : 指明显示UDP端口 -l : 仅显示监听套接字(所谓套接字就是使应用程序能够读写与收发通讯协议(protocol)与资料的程序) -p : 显示进程标识符和程序名称，每一个套接字/端口都属于一个程序。 -n : 不进行DNS轮询，显示IP(可以加速操作) 即可显示当前服务器上所有端口及进程服务，于grep结合可查看某个具体端口及服务情况··netstat -ntlp //查看当前所有tcp端口·netstat -ntulp |grep 80 //查看所有80端口使用情况·netstat -an | grep 3306 //查看所有3306端口使用情况· 查看一台服务器上面哪些服务及端口netstat -lanp 查看一个服务有几个端口。比如要查看mysqldps -ef |grep mysqld 查看某一端口的连接数量,比如3306端口netstat -pnt |grep :3306 |wc 查看某一端口的连接客户端IP 比如3306端口netstat -anp |grep 3306 netstat -an 查看网络端口 lsof -i :port，使用lsof -i :port就能看见所指定端口运行的程序，同时还有当前连接。 nmap 端口扫描netstat -nupl (UDP类型的端口)netstat -ntpl (TCP类型的端口)netstat -anp 显示系统端口使用情况 Netstat 命令用于显示各种网络相关信息，如网络连接，路由表，接口状态 (Interface Statistics)，masquerade 连接，多播成员 (Multicast Memberships) 等等。 输出信息含义：123456789101112131415[rd@qbj3-dwxk-ark-api-00 server_conf]$ netstatActive Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 0 10.40.10.219:21462 10.40.120.140:5511 TIME_WAIT tcp 0 0 10.40.10.219:35814 10.40.120.140:5511 TIME_WAIT...Active UNIX domain sockets (w/o servers)Proto RefCnt Flags Type State I-Node Pathunix 3 [ ] STREAM CONNECTED 286798166 /usr/local/yd.socket.clientunix 2 [ ] DGRAM 8807 /var/run/portreserve/socketunix 5 [ ] DGRAM 8837 /dev/logunix 2 [ ] DGRAM 7933 @/org/kernel/udev/udevdunix 3 [ ] STREAM CONNECTED 1144400163 ... 从整体上看，netstat的输出结果可以分为两个部分： 一个是Active Internet connections，称为有源TCP连接，其中”Recv-Q”和”Send-Q”指%0A的是接收队列和发送队列。这些数字一般都应该是0。如果不是则表示软件包正在队列中堆积。这种情况只能在非常少的情况见到。 另一个是Active UNIX domain sockets，称为有源Unix域套接口(和网络套接字一样，但是只能用于本机通信，性能可以提高一倍)。Proto显示连接使用的协议,RefCnt表示连接到本套接口上的进程号,Types显示套接口的类型,State显示套接口当前的状态,Path表示连接到套接口的其它进程使用的路径名。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP 传值与传引用详解]]></title>
    <url>%2F2018%2F06%2F05%2FPHP%E4%BC%A0%E5%80%BC%E4%B8%8E%E4%BC%A0%E5%BC%95%E7%94%A8%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[首先，看两个示例，进行比较：12345$a = 1; $b = &amp;$a; $c = 2; $a = &amp;$c; $c = 2;echo $a.&apos;&lt;br/&gt;&apos;;echo $b.&apos;&lt;br/&gt;&apos;;echo $c.&apos;&lt;br/&gt;&apos;;echo &quot;&lt;hr/&gt;&quot;; 此时，$a、$b、$c 的值是多少？ 12345$a = 1; $b = &amp;$a; $c = 2; $c = &amp;$a; $c = 2;echo $a.&apos;&lt;br/&gt;&apos;;echo $b.&apos;&lt;br/&gt;&apos;;echo $c.&apos;&lt;br/&gt;&apos;;echo &quot;&lt;hr/&gt;&quot;; 此时，$a、$b、$c 的值又是多少？ 12345$a = 1; $b = &amp;$a; $c = 2; $c = &amp;$a;echo $a.&apos;&lt;br/&gt;&apos;;echo $b.&apos;&lt;br/&gt;&apos;;echo $c.&apos;&lt;br/&gt;&apos;;echo &quot;&lt;hr/&gt;&quot;; 此时，$a、$b、$c 的值又是多少？ （先不要着急往下看，文章的末尾会附属答案。） 下面对赋值与赋引用进行说明。再看个示例，进行说明：1234567891011121314$a = 10;//将常量值赋给变量，会为a分配内存空间$b = $a;//变量赋值给变量，是不是copy了一份副本，b也分配了内存空间呢？$a = &amp;$c;//引用是不会为c分配空间的，c和a是共用一份空间的。 //此时，a和c都指向同一个内存空间，且为空。（注：a就不是10了）。echo $a;//无输出echo $c;//无输出echo PHP_EOL;echo $b;//由于b是副本，输出10$a = 5;echo $c;//输出5，因为a和c 是指向同一个内存空间echo PHP_EOL;echo $b;//由于b是副本，对a的操作不会影响b，输出10 （1）赋值由上面示例可看出，将常量值赋值给变量，会分配内存空间（属于新建变量）；当将变量赋值给变量，只是拷贝一份副本，并不会重新分配内存空间，PHP底层变量计数会加1。只有当这两个变量中的其中任一个改变，才会重新分配内存空间（属于新建变量），且此两个变量的更改与否互相无关。 （2）赋引用&amp;赋引用的时候，视为一个变量定义了一个别名，增加了一个对内存空间的引用。改变其中一个，会影响其他的引用。而使用unset()时，只是断开了对变量内存空间的引用，内存空间不会释放（底层就是释放了指向内存空间的指针）。由上面示例可看出，赋引用的先后是有影响的。$a=&amp;$c 指把$c的引用地址赋值给$a，且$c的引用地址对应的值为空，因此$a=$c为空。若$c=&amp;$a 指把$a的引用地址赋值给$c，且$a的引用地址对应的值为10，因此$a=$c=10。 划重点：赋引用的时候，先后赋引用的顺序是有影响的。 示例答案：示例一：$a=2,$b=1,$c=2;示例二：$a=2,$b=2,$c=2;示例三：$a=1,$b=1,$c=1;]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>PHP知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[接口文档说明]]></title>
    <url>%2F2018%2F06%2F04%2F%E6%8E%A5%E5%8F%A3%E6%96%87%E6%A1%A3%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[接口文档编写格式： 服务名称：video-clip-repeat 需求概述：很多视频是由相同片段重复播放合成，这种视频信息量低，用户观看体验差，检测出此类视频能有效降低feed、大搜等badcase。 核心功能：检测出视频是否包含重复片段，如果包含则给出重复片段时间点。 错误码表： 错误码 错误信息 解释 0 Success 成功 1 Failed to parse input json. 配置文件解析错误 2 Not found KEY:video_url in input json json格式里没有video_url字段 3 Video_url value is NULL video_url字段的内容为空 返回信息格式： 参数 参数类型 必须 说明 示例 err_no uint32 是 错误码 0正常，非零错误 err_msg string 是 错误信息 参考错误码表 result string 否 只在err_no为0时有结果 接口名称：1、接口地址：http://xxxx:8120/xxx 2、HTTP方法：POST 3、请求参数：格式：json 参数 类型 是否必选 说明 video_url string 是 视频的url链接，必选参数，不能为空 type_name string 是 固定值：video-clip-repeat 4、输出参数：格式：json 参数 类型 必须 说明 ratio double 是 重复片段占视频时长的比例 video_time string 是 视频总时长 result json 是 列表列表里面是重复片段组的json scopes json 是 列表列表包含了每个重复片段的起始时间 start_time string 是 片段起始时间，例如：”00:01:58” end_time string 是 片段终止时间，例如：”00:01:58” 5、正确返回值示例：{ “log_id”: 2874085801, “words_result”: [{ “location”: { “left”: 69, “top”: 35, “width”: 510, “height”: 100 }, “words”: “DOO”, “chars”: [{ “location”: { “left”: 84, “top”: 40, “width”: 330, “height”: 90 }, “char”: “DOO” }] }], “words_result_num”: 1}]]></content>
      <categories>
        <category>PHP</category>
        <category>接口</category>
      </categories>
      <tags>
        <tag>接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[接口代码架构]]></title>
    <url>%2F2018%2F06%2F04%2F%E6%8E%A5%E5%8F%A3%E4%BB%A3%E7%A0%81%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[下面列出接口代码架构格式，不一定完全一致，大概满足需求即可。12345678910111213— InterfaceBase.php //接口基文件— InterfaceFactory.php //接口工厂文件，即入口文件（按需求使用）— GoodInterface.php //商品接口基文件— CartInterface.php //购物车接口基文件— Output.php //定义错误码文件— ...... //其它接口文件— api_1/ // 1版本的接口文件夹— — Good.php //商品接口文件— — Cart.php //购物车接口文件— — ...... //其它接口文件— api_2/ //2版本的接口文件夹— — Good.php //商品接口文件— — ...... //其它接口文件 下面附属重要文件的代码格式，一般不需要改变。 接口基文件：InterfaceBase.php123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145&lt;?php/** * @file InterfaceBase.php * @author *** * @date 2017/05/26 * @brief * **/class Service_data_Ocr_Base_OcrBase &#123; protected $_fixed_args = null; //接口默认参数 protected $_args = null; //接口处理参数 protected $_requestData; //请求数据 protected $_resizeImage = null; //压缩图片数据 protected $_ocrResizeFactor = 1.0; //压缩因子 protected $_service_name = null; //请求服务 protected $_appid = null; //用户APPID /** *@param service name &amp; fixed args *@return null */ public function __construct($service_name,$fixed_args) &#123; $this-&gt;_service_name = $service_name; $this-&gt;_fixed_args = $fixed_args; &#125; /** * 入口方法 *@param input param *@return null */ public function run(&amp;$arrInput) &#123; $this-&gt;_checkParams($arrInput); $this-&gt;_buildRequest($arrInput); $this-&gt;_appid = $arrInput[&apos;appid&apos;]; $rpcResponse = $this-&gt;_getResult(); $result = $this-&gt;_buildResponse($arrInput,$rpcResponse); return $result; &#125; /** * 检测传入参数数据 *@param &amp;$arrInput input params from client *@return null */ protected function _checkParams(&amp;$arrInput) &#123; $ori_image_size = strlen($arrInput[&apos;image&apos;]); $this-&gt;_checkImage($arrInput); $resize_image_size = strlen($arrInput[&apos;image&apos;]); if($resize_image_size !== $ori_image_size) &#123; $this-&gt;_resizeImage = $arrInput[&apos;image&apos;]; &#125; &#125; /** * 对图片进行处理 （单独模块拉出来实现，避免一个方法的实现内容过长） *@param &amp;$arrInput, $ocrResizeFactor *@return null */ protected function _checkImage(&amp;$arrInput) &#123; //对图片进行处理 &#125; /** *@param &amp;$arrInput, $ocrResizeFactor * @param: $refusedLen 直接拒绝图片尺寸阈值;$refusedSize 直接拒绝图片大小阈值;$minBoderLen: 直接拒绝的最小尺寸图片 * @param: $resizeLen 符合压缩图片的边长上限; $resizeSize 符合压缩条件的大小上限 * @param: $resizedSizeThread 压缩后图片是否请求后端的条件大小 * @return null */ protected function _checkImageExtra(&amp;$arrInput, $refusedLen, $refusedSize, $resizeLen, $resizeSize, $resizedSizeThread, $minBoderLen = 15) &#123; pass; &#125; /** * 单独接口处理请求参数，在每个接口中实现 *@param build args *@return */ protected function _buildArgs(&amp;$arrInput) &#123; &#125; /** * 整合请求参数数据 *@param arrInput *@return */ protected function _buildRequest(&amp;$arrInput) &#123; $this-&gt;_buildArgs($arrInput); //调用接口处理参数并返回处理结果 $appid = $arrInput[&apos;appid&apos;]; $requestData = array( &apos;appid&apos; =&gt; $appid, &apos;logid&apos; =&gt; intval(Bd_Log::genLogID()), &apos;format&apos; =&gt; &apos;json&apos;, &apos;from&apos; =&gt; &apos;openapi&apos;, &apos;cmdid&apos; =&gt; &apos;123&apos;, &apos;clientip&apos; =&gt; strval($_SERVER[&apos;HTTP_X_REAL_IP&apos;]), &apos;data&apos; =&gt; base64_encode($this-&gt;_args), ); $this-&gt;_requestData = $requestData; &#125; /** * 向后端请求数据 *@param null *@return ral response */ protected function _getResult() &#123; $service_name = $this-&gt;_service_name; $arrayHeader = array( &apos;pathinfo&apos; =&gt; &apos;GeneralClassifyService/classify&apos;, &apos;Content-Type&apos; =&gt; &apos;application/json&apos;, ); $arrayReturn = ral($service_name, &apos;post&apos;, $this-&gt;_requestData, *, $arrayHeader); $intErrno = ral_get_errno(); $strErrmsg = ral_get_error(); if ($intErrno != 0) &#123; Bd_Log::warning(&quot;get from ocr ral error.[$intErrno][$strErrmsg] from service_name[$service_name]&quot;); throw new Exception(&apos;&apos;, Service_Data_Output::ERRNUM_INTERNAL_ERROR); &#125; return $arrayReturn; &#125; /** * 整合数据并返回 *@param input params &amp; return from ral *@return */ protected function _buildResponse($arrInput,$arrayReturn) &#123; Util::dealErrResponse($arrayReturn); //处理错误输出 $decodedResult = base64_decode($arrayReturn[&apos;result&apos;]); $result = json_decode($decodedResult, $assoc=true); return $this-&gt;buildResponse($arrInput, $result); &#125; /** *@param arrInput, array //用户传入数据 *@param ocrResult, array //后端处理后数据 *@return openapiResult, array //返回整合数组 */ protected function buildResponse(&amp;$arrInput, &amp;$ocrResult) &#123; // 建立数据并返回 return $openapiResult; &#125;&#125; 接口工厂文件：InterfaceFactory.php123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?php/*** @file InterfaceFactory.php* @brief * @author ** ** @version * @date 2016-08-03 */class InterfaceFactory &#123; /** *@param arrInput *@return object of Service_Data_InterfaceProxy */ public static function create(&amp;$arrInput) &#123; switch ($arrInput[&apos;openservice&apos;]) &#123; case &apos;good&apos;: switch ($arrInput[&apos;classify_type&apos;]) &#123; case &apos;clothing&apos;: if (strtolower($arrInput[&apos;version&apos;]) == &apos;v1&apos;) &#123; return new api_v1/ClothingInterface($arrInput); &#125;else &#123; return new api_v2/ClothingInterface($arrInput); &#125; case &apos;shoes&apos;: if (strtolower($arrInput[&apos;version&apos;]) == &apos;v2&apos;) &#123; return new api_v2/ShoesInterface($arrInput); &#125; else &#123; return new api_v1/ShoesInterface($arrInput); &#125; default: throw new Exception(&apos;&apos;, Output::ERRNUM_METHOD_NOT_SUPPORT); &#125; case &apos;cart&apos;: return new CartInterface($arrInput); default: try &#123; $className = ucfirst($arrInput[&apos;openservice&apos;]).&quot;Interface&quot;; $clazz = new ReflectionClass($className); return $clazz-&gt;newInstance($arrInput); &#125; catch (Exception $e) &#123; throw new Exception(&apos;&apos;, Output::ERRNUM_METHOD_NOT_SUPPORT); &#125; &#125; &#125;&#125; 商品接口文件：GoodInterface.php1234567891011121314151617181920212223242526272829303132333435363738394041/** * @file GoodInterface.php * @author *** * @date 2017/11/16 * @brief * **/class GoodInterface extends InterfaceBase &#123; /* *@param null *@return null */ public function __construct() &#123; $fixed_args = &quot;encoding=1&amp;recg_type=seq&amp;international=1&amp;save_img=true&quot;; //接口默认参数 parent::__construct(&quot;serverName&quot;,$fixed_args); //传入基文件数据 &#125; /** * 用户处理对应接口传入数据 *@param input params outside *@return protected memerber:args */ protected function _buildArgs(&amp;$arrInput) &#123; if($arrInput[&apos;object_type&apos;] == &apos;webimage_s1&apos;)&#123; $this-&gt;_service_name = &apos;serverName01&apos;; $object_type = &apos;webimage_v1&apos;; &#125;else if($arrInput[&apos;object_type&apos;] == &apos;webimage_s2&apos;)&#123; $this-&gt;_service_name = &apos;serverName01&apos;; $object_type = &apos;webimage_v2&apos;; &#125; $args = Util::dealArgs($arrInput); $this-&gt;_args = $this-&gt;_args . $args; if(isset($arrInput[&apos;language_type&apos;]))&#123; $this-&gt;_args = $this-&gt;_args . &quot;&amp;languagetype=CHN_ENG&quot;; &#125; $image = $arrInput[&apos;image&apos;]; //组合处理参数，供请求函数调用 $this-&gt;_args = $this-&gt;_fixed_args . $this-&gt;_args . &quot;&amp;object_type=&quot; .$object_type. &quot;&amp;image=&quot; .$image; &#125;&#125; 购物车接口文件：CartInterface.php123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?php/** * @file CartInterface.php * @author *** * @date 2018/3/9 * @brief * **/class CartInterface extends InterfaceBase &#123; /* *@param null *@return null */ public function __construct() &#123; $fixed_args = &quot;&quot;; parent::__construct(&quot;serverName&quot;,$fixed_args); &#125; /** *@param input params outside *@return protected memerber:args */ protected function _buildArgs(&amp;$arrInput) &#123; $image = $arrInput[&apos;image&apos;]; $this-&gt;_args = &quot;object_type=vat_invoice&amp;image=$image&quot;; &#125; /** * 接口自定义返回数据 *@param arrInput, array *@param ocrResult, array *@return openapiResult, array */ protected function buildResponse(&amp;$arrInput, &amp;$ocrResult) &#123; $words_result = $ocrResult[&apos;ret&apos;]; $words_result_num = count($words_result); if($words_result_num == 0)&#123; $words_result = array(); &#125; return array( &apos;words_result_num&apos; =&gt; $words_result_num, &apos;words_result&apos; =&gt; $words_result ); &#125;&#125; 版本1对应的商品接口文件：Good.php12345678910111213141516171819202122232425262728293031323334/** * @file Good.php * @author *** * @date 2017/11/16 * @brief * **/class Good extends GoodInterface &#123; /* *@param null *@return null */ public function __construct() &#123; //根据需求可自定义修改 parent::__construct(); &#125; /** * 用户处理对应接口传入数据 *@param input params outside *@return protected memerber:args */ protected function _buildArgs(&amp;$arrInput) &#123; $this-&gt;_service_name = &apos;serverName03&apos;; $object_type = &apos;webimage_v3&apos;; $args = Util::dealArgs($arrInput); $this-&gt;_args = $this-&gt;_args . $args; if(isset($arrInput[&apos;language_type&apos;]))&#123; $this-&gt;_args = $this-&gt;_args . &quot;&amp;languagetype=CHN_ENG&quot;; &#125; $image = $arrInput[&apos;image&apos;]; //组合处理参数，供请求函数调用 $this-&gt;_args = $this-&gt;_fixed_args . $this-&gt;_args . &quot;&amp;object_type=&quot; .$object_type. &quot;&amp;image=&quot; .$image; &#125;&#125; 输出错误整合文件：Output.php123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170&lt;?php/** * @author *** * @description 输出http请求结果 */class Output &#123; const ERROR_NO_INVALID_PARAM = 1; // 非法参数 const ERROR_NO_POST_RESPONSE_EMPTY = 2; // 后端返回为空 const ERROR_NO_POST_RESPONSE_ERROR = 3; // 后端返回错误 const ERROR_NO_MOLA_RESPONSE_ERROR = 4; // mola返回内容错误 const ERROR_NO_CACHE_RESPONSE_ERROR = 5; // cache返回内容错误 private static $objTpl = null; // 模板对象实例 /** * @param int Errornum to be translated * @return string The translated string * 根据错误码返回错误文字信息 */ public static function findErrorMsg($intErrNo) &#123; switch($intErrNo) &#123; case self::ERROR_NO_INVALID_PARAM: return &apos;参数不合法&apos;; case self::ERROR_NO_POST_RESPONSE_EMPTY: return &apos;抱歉，后端数据返回为空&apos;; case self::ERROR_NO_POST_RESPONSE_ERROR: return &apos;抱歉，后端数据返回错误&apos;; case self::ERROR_NO_MOLA_RESPONSE_ERROR: return &apos;抱歉，MOLA数据返回错误&apos;; case self::ERROR_NO_CACHE_RESPONSE_ERROR: return &apos;抱歉，CACHE数据返回错误&apos;; &#125; return &apos;未知错误&apos;; &#125; /** * 根据strCallbackName是否为空，返回json或者jsonp的错误结果 * @param string * @param integer * @param string * @param array * @param array * @return */ public static function showErrorJS( $strCallbackName, $intErrNo, $strErrmsg = &apos;&apos;, $arrData = array(), $arrExtra = array() ) &#123; &#125; /** * 根据strCallbackName是否为空，返回json或者jsonp的成功结果 * @param string * @param array * @param array * @return **/ public static function showSuccessJS( $strCallbackName, $arrData, $arrExtra = array() ) &#123; &#125; /** * 返回显示错误信息的json * @param integer * @param string * @param array * @param array * @return */ public static function showErrorJSON($intErrNo, $strErrmsg = &apos;&apos;, $arrData = array(), $arrExtra = array()) &#123; &#125; /** * 返回包含成功数据信息的json * @param array * @param array * @return */ public static function showSuccessJSON($arrData, $arrExtra = array()) &#123; &#125; /** * 返回显示错误信息的jsonp * @param string * @param integer * @param string * @param array * @param array * @return */ public static function showErrorJSONP( $strCallbackName, $intErrNo, $strErrmsg = &apos;&apos;, $arrData = array(), $arrExtra = array() ) &#123; &#125; /** * 返回包含成功数据信息的jsonp * @param string * @param array * @param array * @return */ public static function showSuccessJSONP( $strCallbackName, $arrData, $arrExtra = array() ) &#123; &#125; /** * 显示模版 * @param string * @return */ public static function showSuccessPage($strTemplatePath) &#123; &#125; /** * 显示出错模版 * @param integer * @param string * @return */ public static function showErrorPage($intErrNo, $strTemplatePath) &#123; &#125; /** * 获取模版对象 * @param * @return object */ public static function getTplInstance() &#123; &#125; /** * 如果url上带有debug=true参数，则输出所有模版变量； * 如果还带有debugParam参数，则输出特定某一个变量 * @param * @return bool */ public static function makeDebug() &#123; &#125; /** * 为openapi定制的输出格式 * @param array 未经处理的输出格式 * @param integer int64的日志号 * @return null */ public static function printOpenapi($arrInput, $intLogid) &#123; &#125;&#125; 公共处理函数：Util.php12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970&lt;?php/** * @file Util.php * @author *** * @date 2017/06/07 10:57:13 * @brief * **/class Util &#123; private static $OCR_PASSTHROUGH_PARAMS = array( &apos;version&apos; =&gt; &apos;version&apos;, &apos;auto_enhance&apos; =&gt; &apos;auto_enhance&apos;, &apos;fromproduct&apos; =&gt; &apos;fromproduct&apos;, ); /** *@param $arrayReturn *@return ret */ static public function dealErrResponse($arrayReturn) &#123; if($arrayReturn[&apos;err_no&apos;] == 0) return; if ( $arrayReturn[&apos;err_no&apos;] == 5 ) &#123; Bd_Log::addNotice(&apos;ocr_backend_error&apos;, &quot;[&quot; . $arrayReturn[&apos;err_no&apos;] . &quot;][&quot; . $arrayReturn[&apos;err_msg&apos;] . &quot;]&quot;); $innerError = json_decode($arrayReturn[&apos;err_msg&apos;], $assoc=true); if ( is_array($innerError) ) &#123; switch (intval($innerError[&apos;errno&apos;])) &#123; case -1: throw new Exception(&apos;&apos;, Service_Data_Output::ERRNUM_OCR_RECOGNIZE_UNKONWN_ERROR); break; case -2: throw new Exception(&apos;image too large&apos;, Service_Data_Output::ERRNUM_INVALID_IMGSIZE); break; case -3: throw new Exception(&apos;&apos;, Service_Data_Output::ERRNUM_INVALID_IMAGE_FORMAT); break; case -4: throw new Exception(&apos;&apos;, Service_Data_Output::ERRNUM_INVALID_IMAGE_FORMAT); break; case -5: throw new Exception(&apos;&apos;, Service_Data_Output::ERRNUM_INTERNAL_ERROR); break; default: throw new Exception(&apos;&apos;, Service_Data_Output::ERRNUM_UNKNOWN); break; &#125; &#125; else &#123; throw new Exception(&apos;&apos;, Service_Data_Output::ERRNUM_UNKNOWN); &#125; &#125; else if ($arrayReturn[&apos;err_no&apos;] != 0) &#123; Bd_Log::addNotice(&apos;ocr_backend_error&apos;, &quot;[&quot; . $arrayReturn[&apos;err_no&apos;] . &quot;][&quot; . $arrayReturn[&apos;err_msg&apos;] . &quot;]&quot;); Bd_Log::warning(&quot;get from service error.[&quot; . $arrayReturn[&apos;err_no&apos;] . &quot;][&quot; . $arrayReturn[&apos;err_msg&apos;] . &quot;]&quot;); throw new Exception(&apos;&apos;, Service_Data_Output::ERRNUM_UNKNOWN); &#125; &#125; /** * * @param $arrInput * @return string */ static public function dealArgs($arrInput) &#123; $args = &apos;&apos;; ; return $args; &#125;&#125; 注：以上代码信息只做说明架构思想，所含信息内容可忽略。]]></content>
      <categories>
        <category>PHP</category>
        <category>接口</category>
      </categories>
      <tags>
        <tag>接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask - SQLAlchemy 多条件模糊查询]]></title>
    <url>%2F2018%2F04%2F23%2FFlask-SQLAlchemy%E5%A4%9A%E6%9D%A1%E4%BB%B6%E6%A8%A1%E7%B3%8A%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[在做项目的时候，总会遇到搜索的需求，平常写SQL语句很简单。但是，在Flask 使用SQLAlchemy管理数据库的时候，应该如何使用，做了一下笔记，如下。 一、使用SQLALchemy一般的搜索都需要模糊查询，如果存在多条查询的需求，可如下操作：users = User.query.filter(User.name.like(&quot;%&quot;+搜索的内容+&quot;%&quot;), User.age.like(&quot;%&quot;+搜索的内容+&quot;%&quot;)).all() 此种方式有一个弊端，就是不能 “or” 查询，只能 “and” 查询。进行 “or” 查询，我的一个做法就是只能分开查询，然后合并数据即可。 二、使用原生SQL操作使用 session会话执行 SQL语句。（注：还有一种支持线程安全的方式，可使用 scoped_session()实现）123456789from app import dbsql = &quot;SELECT * FROM `moviecol` \ LEFT JOIN `movie` ON moviecol.movie_id=movie.id \ LEFT JOIN `user` ON moviecol.user_id=user.id \ WHERE movie.title like &apos;%&quot;+keywords+&quot;%&apos; or user.name like &apos;%&quot;+keywords+&quot;%&apos; \ ORDER BY moviecol.addtime DESC \ LIMIT &quot;+str(page)+&quot;,&quot;+str(page_config[&apos;moviecol_per_page&apos;])res = db.session.execute(sql).fetchall() 其中 Session类的相关方法如下：12345678public_methods = ( &apos;__contains__&apos;, &apos;__iter__&apos;, &apos;add&apos;, &apos;add_all&apos;, &apos;begin&apos;, &apos;begin_nested&apos;, &apos;close&apos;, &apos;commit&apos;, &apos;connection&apos;, &apos;delete&apos;, &apos;execute&apos;, &apos;expire&apos;, &apos;expire_all&apos;, &apos;expunge&apos;, &apos;expunge_all&apos;, &apos;flush&apos;, &apos;get_bind&apos;, &apos;is_modified&apos;, &apos;bulk_save_objects&apos;, &apos;bulk_insert_mappings&apos;, &apos;bulk_update_mappings&apos;, &apos;merge&apos;, &apos;query&apos;, &apos;refresh&apos;, &apos;rollback&apos;, &apos;scalar&apos;)]]></content>
      <categories>
        <category>Python</category>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 标准模块之 uuiid]]></title>
    <url>%2F2018%2F04%2F23%2FPython%E6%A0%87%E5%87%86%E6%A8%A1%E5%9D%97%E4%B9%8Buuiid%2F</url>
    <content type="text"><![CDATA[uuid是128位的全局唯一标识符（univeral unique identifier），通常用32位的一个字符串的形式来表现。有时也称guid(global unique identifier)。python中自带了uuid模块来进行uuid的生成和管理工作。 python中的uuid模块基于信息如MAC地址、时间戳、命名空间、随机数、伪随机数来uuid。具体方法有如下几个： uuid.getnode()：获取硬件地址为48位正整数。这是第一次运行，它可能会启动一个单独的程序，可能会很慢。如果所有尝试获取硬件地址都失败，我们选择一个随机的48位数字，其第8位设置为1，如RFC 4122中推荐的那样。“硬件地址”表示网络接口的MAC地址，以及具有多个网络接口可以返回其中任何一个的MAC地址。uuid.uuid1([ node [，clock_seq ] ])：从主机ID，序列号和当前时间生成一个UUID。如果 没有给出节点，getnode()则用于获取硬件地址。如果 给出clock_seq，它将用作序列号; 否则选择一个随机的14位序列号。（可以保证全球范围内的唯一性。但是可能会危害隐私，因为它会创建一个包含计算机网络地址的UUID。）uuid.uuid3(名称空间，名称)：根据名称空间标识（这是一个UUID）和一个名称（这是一个字符串）的MD5散列生成一个UUID。uuid.uuid4()：生成一个随机的UUID。（注：有一定概率重复的）uuid.uuid5 (名称空间，名称)：根据名称空间标识（这是一个UUID）和名称（它是一个字符串）的SHA-1散列生成一个UUID。（注：和uuid3基本相同，只不过采用的散列算法是sha1） 该uuid模块定义了以下用于uuid3()或的名称空间标识符 uuid5()。 uuid.NAMESPACE_DNS当指定此名称空间时，名称字符串是完全限定的域名。uuid.NAMESPACE_URL当这个名字空间被指定时，名字字符串就是一个URL。uuid.NAMESPACE_OID当这个名字空间被指定时，名字字符串就是一个ISO OID。uuid.NAMESPACE_X500当指定此名称空间时，名称字符串是DER中的X.500 DN或文本输出格式。 UUID 实例具有这些只读属性： UUID.bytesUUID作为一个16字节的字符串（包含以big-endian字节顺序的六个整数字段）。UUID.bytes_leUUID作为16字节的字符串（以 little-endian字节顺序包含time_low，time_mid和time_hi_version）。UUID.fieldsUUID的六个整数字段的元组，它们也可用作六个单独的属性和两个派生属性： 领域 含义 time_low UUID的前32位 time_mid UUID的接下来的16位 time_hi_version UUID的接下来的16位 clock_seq_hi_variant UUID的接下来的8位 clock_seq_low UUID的接下来的8位 node UUID的最后48位 time 60位时间戳 clock_seq 14位序列号 UUID.hexUUID作为32个字符的十六进制字符串。UUID.intUUID是一个128位整数。UUID.urnUUID作为RFC4122中规定的URN。UUID.variantUUID变体，它确定UUID的内部布局。这将是一个常量RESERVED_NCS，RFC_4122， RESERVED_MICROSOFT，或RESERVED_FUTURE。UUID.versionUUID版本号（1到5，仅在变体时才有意义 RFC_4122）。 以下是uuid模块典型用法的一些示例：1234567891011121314151617&gt;&gt;&gt; import uuid&gt;&gt;&gt; # make a UUID based on the host ID and current time&gt;&gt;&gt; uuid.uuid1()UUID(&apos;a8098c1a-f86e-11da-bd1a-00112444be1e&apos;)&gt;&gt;&gt; # make a UUID using an MD5 hash of a namespace UUID and a name&gt;&gt;&gt; uuid.uuid3(uuid.NAMESPACE_DNS, &apos;python.org&apos;)UUID(&apos;6fa459ea-ee8a-3ca4-894e-db77e160355e&apos;)&gt;&gt;&gt; # make a random UUID&gt;&gt;&gt; uuid.uuid4()UUID(&apos;16fd2706-8baf-433b-82eb-8c7fada847da&apos;)&gt;&gt;&gt; # make a UUID using a SHA-1 hash of a namespace UUID and a name&gt;&gt;&gt; uuid.uuid5(uuid.NAMESPACE_DNS, &apos;python.org&apos;)UUID(&apos;886313e1-3b8a-5372-9b90-0c9aee199e5d&apos;)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 常见错误篇]]></title>
    <url>%2F2018%2F04%2F16%2FFlask%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E7%AF%87%2F</url>
    <content type="text"><![CDATA[1、在 admin/forms.py 表单文件中使用如下语句报错：auth_list = Auth.query.all() 报错信息，如下： RuntimeError: No application found. Either work inside a view function or push an application contex. 报错信息意思是：没有找到应用程序。可以在视图函数内工作，也可以推动应用程序上下文。因此，我们就要建立程序上下文，如下：123456789app = Flask(__name__)app.config.from_object(config[config_name])config[config_name].init_app(app)with app.app_context(): db.init_app(app) ...return app 就是在初始化的时候，添加 app_context() 上下文。 2、元组赋值报错，如下： TypeError: ‘NoneType’ object is not iterable 这个错误提示一般发生在将None赋给多个值时。在判断语句中，当if条件不满足，并且没有else语句时，函数默认返回None。在没有return语句时，python也默认会返回None。调用时，将None赋给多个值时，会出现提示：TypeError: ‘NoneType’ object is not iterable. 3、模板中使用表单（wtf），报错如下： TypeError: html_params() got multiple values for keyword argument ‘name’ 这个是由于表单中有一个变量为 “render_kw”，其值是键值对字典，且其键不能为 “name”。否则就报错！ 4、Flask 的 validate_on_submit() 老是false ??在 flask中提交表单时使用了validate_on_submit()来验证，但是每次提交时都是false，不知道什么原因啊？但是只要把生成form表单的地方换成 quick_form自动生成，就正常了。最后，通过上网搜索知道问题的原因了，就是CSRF的原因，直接在form里加上csrf_token就行了。如下：12&#123;&#123; form.csrf_token &#125;&#125;&#123;&#123; form.submit() &#125;&#125;]]></content>
      <categories>
        <category>Python</category>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 使用session]]></title>
    <url>%2F2018%2F04%2F16%2FFlask%E4%BD%BF%E7%94%A8session%2F</url>
    <content type="text"><![CDATA[1、首先安装sessionpip install flask-session 2、引入框架中123from flask_session import Sessionsess = Session()sess.init_app(app) 3、下面就可以在视图中使用引用使用了from flask import session 报错集锦： Flask session报下面错误：RuntimeError: The session is unavailable because no secret key was set. Set the secret_key on the application to something unique and secret. 查找代码发现：SERECT_KEY 已经设定。但是，仍然报此错误。原因是 SESSION_TYPE 未设置，如果不使用 内存缓存的话，可以使文件缓存。即：SESSION_TYPE = &#39;filesystem&#39;]]></content>
      <categories>
        <category>Python</category>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 支持MySQL]]></title>
    <url>%2F2018%2F04%2F16%2FPython3%E6%94%AF%E6%8C%81MySQL%2F</url>
    <content type="text"><![CDATA[注：MySQLdb 只适用于Python2.x，在Python3的替代品是：pymysql 1、安装： pip install pymysql 一些框架默认仍然用的是MySQLdb，但是python3已经不支持MySQLdb，取而代之的是pymysql，因此运行的时候会报ModuleNotFoundError: No module named &#39;MySQLdb&#39; 2、使用方式： 安装成功后，可使用MySQLdb的语法使用pymysql 使用Flask集成的SQLAlchemy管理MySQL 在框架中引入下面语句：12import pymysqlpymysql.install_as_MySQLdb() 然后，在配置 SQLALCHEMY_DATABASE_URI 参数时，直接使用 mysql的使用方式即可。 使用方式：mysql://username:password@hostname/database 3、生成对应数据表123python manage.py db initpython manage.py db migratepython manage.py db upgrade]]></content>
      <categories>
        <category>Python</category>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP 和 Socket 的区别]]></title>
    <url>%2F2018%2F04%2F10%2FHTTP%E5%92%8CSocket%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[要弄明白 http 和 socket 首先要熟悉网络七层：物 数 网 传 会 表 应，如下图 HTTP 协议：超文本传输协议，对应于应用层，用于如何封装数据.TCP/UDP 协议：传输控制协议，对应于传输层，主要解决数据在网络中的传输。IP 协议：对应于网络层，同样解决数据在网络中的传输。 传输数据的时候只使用 TCP/IP 协议(传输层)，如果没有应用层来识别数据内容，传输后的协议都是无用的。应用层协议很多 FTP,HTTP,TELNET等，也可以自己定义应用层协议。web 使用 HTTP 作应用层协议，以封装 HTTP 文本信息，然后使用 TCP/IP 做传输层协议，将数据发送到网络上。 一、HTTP 连接HTTP协议即超文本传送协议(Hypertext Transfer Protocol )，是Web联网的基础，也是手机联网常用的协议之一，HTTP协议是建立在TCP协议之上的一种应用。 HTTP连接最显著的特点是客户端发送的每次请求都需要服务器回送响应，即使用的是“请求-响应”方式。在请求结束后，会主动释放连接。从建立连接到关闭连接的过程称为“一次连接”。即是“无状态”的协议。 1）在HTTP 1.0中，客户端的每次请求都要求建立一次单独的连接，在处理完本次请求后，就自动释放连接。 2）在HTTP 1.1中则可以在一次连接中处理多个请求，并且多个请求可以重叠进行，不需要等待一个请求结束后再发送下一个请求。 http 为短连接：由于HTTP在每次请求结束后都会主动释放连接，因此HTTP连接是一种“短连接”。通常要保持客户端程序的在线状态，需要不断地向服务器发起连接请求。通常的做法是即时不需要获得任何数据，客户端也保持每隔一段固定的时间向服务器发送一次“保持连接”的请求，服务器在收到该请求后对客户端进行回复，表明知道客户端“在线”。若服务器长时间无法收到客户端的请求，则认为客户端“下线”，若客户端长时间无法收到服务器的回复，则认为网络已经断开。 二、Socket 连接要想明白 Socket，必须要理解 TCP 连接。TCP 三次握手：握手过程中并不传输数据，在握手后服务器与客户端才开始传输数据，理想状态下，TCP 连接一旦建立，在通讯双方中的任何一方主动断开连接之前 TCP 连接会一直保持下去。 Socket 是对 TCP/IP 协议的封装，Socket 只是个接口不是协议，通过 Socket 我们才能使用 TCP/IP 协议，除了 TCP，也可以使用 UDP 协议来传递数据。 创建 Socket 连接的时候，可以指定传输层协议，可以是 TCP 或者 UDP，当用 TCP 连接，该Socket就是个TCP连接，反之。 Socket 连接,至少需要一对套接字，分为 clientSocket，serverSocket。连接分为3个步骤：服务器监听、客户端请求、连接确认。Socket 为长连接：通常情况下Socket 连接就是 TCP 连接，因此 Socket 连接一旦建立,通讯双方开始互发数据内容，直到双方断开连接。在实际应用中，由于网络节点过多，在传输过程中，会被节点断开连接，因此要通过轮询高速网络，该节点处于活跃状态。 很多情况下，都是需要服务器端向客户端主动推送数据，保持客户端与服务端的实时同步。 若双方是 Socket 连接，可以由服务器直接向客户端发送数据。 若双方是 HTTP 连接，则服务器需要等客户端发送请求后，才能将数据回传给客户端。因此，客户端定时向服务器端发送请求，不仅可以保持在线，同时也询问服务器是否有新数据，如果有就将数据传给客户端。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP 详解]]></title>
    <url>%2F2018%2F04%2F10%2FHTTP%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[一、基础概念篇1、介绍HTTP是Hyper Text Transfer Protocol（超文本传输协议）的缩写。它的发展是万维网协会（World Wide Web Consortium）和Internet工作小组IETF（Internet Engineering Task Force）合作的结果，（他们）最终发布了一系列的RFC，RFC 1945定义了HTTP/1.0版本。其中最著名的就是RFC 2616。RFC 2616定义了今天普遍使用的一个版本——HTTP 1.1。 HTTP协议（HyperText Transfer Protocol，超文本传输协议）是用于从WWW服务器传输超文本到本地浏览器的传送协议。它可以使浏览器更加高效，使网络传输减少。它不仅保证计算机正确快速地传输超文本文档，还确定传输文档中的哪一部分，以及哪部分内容首先显示(如文本先于图形)等。 HTTP是一个应用层协议，由请求和响应构成，是一个标准的客户端服务器模型。HTTP是一个无状态的协议。 2、在TCP/IP协议栈中的位置HTTP协议通常承载于TCP协议之上，有时也承载于TLS或SSL协议层之上，这个时候，就成了我们常说的HTTPS。如下图所示：默认HTTP的端口号为80，HTTPS的端口号为443。 3、HTTP的请求响应模型HTTP协议永远都是客户端发起请求，服务器回送响应。见下图：这样就限制了使用HTTP协议，无法实现在客户端没有发起请求的时候，服务器将消息推送给客户端。HTTP协议是一个无状态的协议，同一个客户端的这次请求和上次请求是没有对应关系。 4、工作流程一次HTTP操作称为一个事务，其工作过程可分为四步： 1）首先客户机与服务器需要建立连接。只要单击某个超级链接，HTTP的工作开始。2）建立连接后，客户机发送一个请求给服务器，请求方式的格式为：统一资源标识符（URL）、协议版本号，后边是MIME信息包括请求修饰符、客户机信息和可能的内容。3）服务器接到请求后，给予相应的响应信息，其格式为一个状态行，包括信息的协议版本号、一个成功或错误的代码，后边是MIME信息包括服务器信息、实体信息和可能的内容。4）客户端接收服务器所返回的信息通过浏览器显示在用户的显示屏上，然后客户机与服务器断开连接。 如果在以上过程中的某一步出现错误，那么产生错误的信息将返回到客户端，有显示屏输出。对于用户来说，这些过程是由HTTP自己完成的，用户只要用鼠标点击，等待信息显示就可以了。 5、客户端一个请求流程（详细版）待补充。。。 二、协议详解篇1、HTTP/1.0 与 HTTP/1.1 比较 异同点 HTTP/1.0 HTTP/1.1 建立连接方面 连接不能复用 可复用，减少TCP三次握手的开销 请求方式 GET、POST、HEAD GET、POST、HEAD、OPTIONS、PUT、DELETE、TRACE、CONNECT Request消息头 新增Host域 2、HTTP请求消息（1）请求消息格式HTTP请求消息实例：123456789GET /hello.htm HTTP/1.1Accept: */*Accept-Language: zh-cnAccept-Encoding: gzip, deflateIf-Modified-Since: Wed, 17 Oct 2007 02:15:55 GMTIf-None-Match: W/&quot;158-1192587355000&quot;User-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1)Host: 192.168.2.162:8080Connection: Keep-Alive 请求消息格式如下所示： 请求行通用信息头|请求头|实体头CRLF(回车换行)实体内容 其中“请求行”为：请求行 = 方法 [空格] 请求URI [空格] 版本号 [回车换行] （2）请求方法HTTP的请求方法包括如下几种：GET | POST | HEAD | PUT | DELETE | OPTIONS | TRACE | CONNECT 3、HTTP响应消息（1）响应消息格式HTTP响应消息实例如下所示：1234567HTTP/1.1 200 OKETag: W/&quot;158-1192590101000&quot;Last-Modified: Wed, 17 Oct 2007 03:01:41 GMTContent-Type: text/htmlContent-Length: 158Date: Wed, 17 Oct 2007 03:01:59 GMTServer: Apache-Coyote/1.1 HTTP响应消息的格式如下所示： 状态行通用信息头|响应头|实体头CRLF实体内容 其中：状态行 = 版本号 [空格] 状态码 [空格] 原因 [回车换行] （2）响应状态码1**：请求收到，继续处理100——客户必须继续发出请求101——客户要求服务器根据请求转换HTTP协议版本 2**：操作成功收到，分析、接受 200——交易成功201——提示知道新文件的URL202——接受和处理、但处理未完成203——返回信息不确定或不完整204——请求收到，但返回信息为空205——服务器完成了请求，用户代理必须复位当前已经浏览过的文件206——服务器已经完成了部分用户的GET请求3**：完成此请求必须进一步处理300——请求的资源可在多处得到301——删除请求数据302——在其他地址发现了请求数据303——建议客户访问其他URL或访问方式304——客户端已经执行了GET，但文件未变化305——请求的资源必须从服务器指定的地址得到306——前一版本HTTP中使用的代码，现行版本中不再使用307——申明请求的资源临时性删除4**：请求包含一个错误语法或不能完成400——错误请求，如语法错误401——未授权HTTP 401.1 - 未授权：登录失败 HTTP 401.2 - 未授权：服务器配置问题导致登录失败 HTTP 401.3 - ACL 禁止访问资源 HTTP 401.4 - 未授权：授权被筛选器拒绝 HTTP 401.5 - 未授权：ISAPI 或 CGI 授权失败402——保留有效ChargeTo头响应403——禁止访问HTTP 403.1 禁止访问：禁止可执行访问 HTTP 403.2 - 禁止访问：禁止读访问 HTTP 403.3 - 禁止访问：禁止写访问 HTTP 403.4 - 禁止访问：要求 SSL HTTP 403.5 - 禁止访问：要求 SSL 128 HTTP 403.6 - 禁止访问：IP 地址被拒绝 HTTP 403.7 - 禁止访问：要求客户证书 HTTP 403.8 - 禁止访问：禁止站点访问 HTTP 403.9 - 禁止访问：连接的用户过多 HTTP 403.10 - 禁止访问：配置无效 HTTP 403.11 - 禁止访问：密码更改 HTTP 403.12 - 禁止访问：映射器拒绝访问 HTTP 403.13 - 禁止访问：客户证书已被吊销 HTTP 403.15 - 禁止访问：客户访问许可过多 HTTP 403.16 - 禁止访问：客户证书不可信或者无效 HTTP 403.17 - 禁止访问：客户证书已经到期或者尚未生效404——没有发现文件、查询或URl405——用户在Request-Line字段定义的方法不允许406——根据用户发送的Accept拖，请求资源不可访问407——类似401，用户必须首先在代理服务器上得到授权408——客户端没有在用户指定的饿时间内完成请求409——对当前资源状态，请求不能完成410——服务器上不再有此资源且无进一步的参考地址411——服务器拒绝用户定义的Content-Length属性请求412——一个或多个请求头字段在当前请求中错误413——请求的资源大于服务器允许的大小414——请求的资源URL长于服务器允许的长度415——请求资源不支持请求项目格式416——请求中包含Range请求头字段，在当前请求资源范围内没有range指示值，请求也不包含If-Range请求头字段417——服务器不满足请求Expect头字段指定的期望值，如果是代理服务器，可能是下一级服务器不能满足请求长。5**：服务器执行一个完全有效请求失败HTTP 500 - 内部服务器错误 HTTP 500.100 - 内部服务器错误 - ASP 错误 HTTP 500-11 服务器关闭 HTTP 500-12 应用程序重新启动 HTTP 500-13 - 服务器太忙 HTTP 500-14 - 应用程序无效 HTTP 500-15 - 不允许请求 global.asaError 501 - 未实现HTTP 502 - 网关错误 4、请求头：HTTP最常见的请求头如下： 名称 说明 Accept： 浏览器可接受的MIME类型； Accept-Charset： 浏览器可接受的字符集； Accept-Encoding： 浏览器能够进行解码的数据编码方式，比如gzip。Servlet能够向支持gzip的浏览器返回经gzip编码的HTML页面。许多情形下这可以减少5到10倍的下载时间； Accept-Language： 浏览器所希望的语言种类，当服务器能够提供一种以上的语言版本时要用到； Authorization： 授权信息，通常出现在对服务器发送的WWW-Authenticate头的应答中； Connection： 表示是否需要持久连接。如果Servlet看到这里的值为“Keep-Alive”，或者看到请求使用的是HTTP 1.1（HTTP 1.1默认进行持久连接），它就可以利用持久连接的优点，当页面包含多个元素时（例如Applet，图片），显著地减少下载所需要的时间。要实现这一点，Servlet需要在应答中发送一个Content-Length头，最简单的实现方法是：先把内容写入ByteArrayOutputStream，然后在正式写出内容之前计算它的大小； Content-Length： 表示请求消息正文的长度； Cookie： 这是最重要的请求头信息之一； From： 请求发送者的email地址，由一些特殊的Web客户程序使用，浏览器不会用到它； Host： 初始URL中的主机和端口； If-Modified-Since： 只有当所请求的内容在指定的日期之后又经过修改才返回它，否则返回304“Not Modified”应答； Pragma： 指定“no-cache”值表示服务器必须返回一个刷新后的文档，即使它是代理服务器而且已经有了页面的本地拷贝； Referer： 包含一个URL，用户从该URL代表的页面出发访问当前请求的页面。 User-Agent： | 浏览器类型，如果Servlet返回的内容与浏览器类型有关则该值非常有用；UA-Pixels，UA-Color，UA-OS，UA-CPU： | 由某些版本的IE浏览器所发送的非标准的请求头，表示屏幕大小、颜色深度、操作系统和CPU类型。 5、响应头：HTTP最常见的响应头如下所示： table th:first-of-type{ width: 24%; } 名称 说明 Allow： 服务器支持哪些请求方法（如GET、POST等）； Content-Encoding： 文档的编码（Encode）方法。只有在解码之后才可以得到Content-Type头指定的内容类型。利用gzip压缩文档能够显著地减少HTML文档的下载时间。Java的GZIPOutputStream可以很方便地进行gzip压缩，但只有Unix上的Netscape和Windows上的IE 4、IE 5才支持它。因此，Servlet应该通过查看Accept-Encoding头（即request.getHeader(&quot;Accept-Encoding&quot;)）检查浏览器是否支持gzip，为支持gzip的浏览器返回经gzip压缩的HTML页面，为其他浏览器返回普通页面； Content-Length： 表示内容长度。只有当浏览器使用持久HTTP连接时才需要这个数据。如果你想要利用持久连接的优势，可以把输出文档写入ByteArrayOutputStram，完成后查看其大小，然后把该值放入Content-Length头，最后通过byteArrayStream.writeTo(response.getOutputStream()发送内容； Content-Type： 表示后面的文档属于什么MIME类型。Servlet默认为text/plain，但通常需要显式地指定为text/html。由于经常要设置Content-Type，因此HttpServletResponse提供了一个专用的方法setContentTyep。 可在web.xml文件中配置扩展名和MIME类型的对应关系； Date： 当前的GMT时间。你可以用setDateHeader来设置这个头以避免转换时间格式的麻烦； Expires： 指明应该在什么时候认为文档已经过期，从而不再缓存它。 Last-Modified： 文档的最后改动时间。客户可以通过If-Modified-Since请求头提供一个日期，该请求将被视为一个条件GET，只有改动时间迟于指定时间的文档才会返回，否则返回一个304（Not Modified）状态。Last-Modified也可用setDateHeader方法来设置； Location： 表示客户应当到哪里去提取文档。Location通常不是直接设置的，而是通过HttpServletResponse的sendRedirect方法，该方法同时设置状态代码为302； Refresh： 表示浏览器应该在多少时间之后刷新文档，以秒计。除了刷新当前文档之外，你还可以通过setHeader(&quot;Refresh&quot;, &quot;5; URL=http://host/path&quot;)让浏览器读取指定的页面。注意这种功能通常是通过设置HTML页面HEAD区的&lt;META HTTP-EQUIV=&quot;Refresh&quot; CONTENT=&quot;5;URL=http://host/path&quot;&gt;实现，这是因为，自动刷新或重定向对于那些不能使用CGI或Servlet的HTML编写者十分重要。但是，对于Servlet来说，直接设置Refresh头更加方便。注意Refresh的意义是“N秒之后刷新本页面或访问指定页面”，而不是“每隔N秒刷新本页面或访问指定页面”。因此，连续刷新要求每次都发送一个Refresh头，而发送204状态代码则可以阻止浏览器继续刷新，不管是使用Refresh头还是。注意Refresh头不属于HTTP 1.1正式规范的一部分，而是一个扩展，但Netscape和IE都支持它。 6、实体头实体头用坐实体内容的元信息，描述了实体内容的属性，包括实体信息类型，长度，压缩方法，最后一次修改时间，数据有效性等。 名称 说明 Allow： GET,POST Content-Encoding： 文档的编码（Encode）方法，例如：gzip，见“2.5 响应头”； Content-Language： 内容的语言类型，例如：zh-cn； Content-Length： 表示内容长度，eg：80，可参考“2.5响应头”； Content-Location： 表示客户应当到哪里去提取文档，例如：http://www.dfdf.org/dfdf.html，可参考“2.5响应头”； Content-MD5： MD5 实体的一种MD5摘要，用作校验和。发送方和接受方都计算MD5摘要，接受方将其计算的值与此头标中传递的值进行比较。Eg1：Content-MD5: 。Eg2：dfdfdfdfdfdfdff==； Content-Range： 随部分实体一同发送；标明被插入字节的低位与高位字节偏移，也标明此实体的总长度。Eg1：Content-Range: 1001-2000/5000，eg2：bytes 2543-4532/7898 Content-Type： 标明发送或者接收的实体的MIME类型。Eg：text/html; charset=GB2312 主类型/子类型； Expires： 为0证明不缓存； Last-Modified： WEB 服务器认为对象的最后修改时间，比如文件的最后修改时间，动态页面的最后产生时间等等。例如：Last-Modified：Tue, 06 May 2008 02:42:43 GMT. 7、扩展头在HTTP消息中，也可以使用一些再HTTP1.1正式规范里没有定义的头字段，这些头字段统称为自定义的HTTP头或者扩展头，他们通常被当作是一种实体头处理。 现在流行的浏览器实际上都支持Cookie,Set-Cookie,Refresh和Content-Disposition等几个常用的扩展头字段。 名称 说明 Refresh：1;url=http://www.dfdf.org //过1秒跳转到指定位置； Content-Disposition： 头字段,可参考“2.5响应头”； Content-Type： WEB 服务器告诉浏览器自己响应的对象的类型。 三、协议高级篇1、HTTP传输及与HTTPS的区别：TCP的数据是通过名为IP分组（或IP数据报）的小数据块来发送的。HTTP就是“HTTP over TCP over IP”这个“协议栈”中的最顶层。其安全版本HTTPS就是在HTTP和TCP之间插入了一个（称为TLS或SSL的）密码加密层。 HTTP要传送一条报文时，会以流的形式将报文数据的内容通过一条打开的TCP连接按序传输。TCP收到数据流之后，会将数据流砍成被称作段的小数据块，并将段封装在IP分组中，通过因特网进行传输。 每个TCP段都是由IP分组承载，从一个IP地址发送到另一个IP地址的。每个IP分组中都包括： 一个IP分组首部（通常为20字节）； 一个TCP段首部（通常为20字节）； 一个TCP数据块（0个或多个字节）。 IP首部包含了源和目的IP地址、长度和其它一些标记。TCP段的首部包含了TCP端口号、TCP控制标记，以及用于数据排序和完整性检查的一些数字值。 TCP连接是通过4个值来识别的： &lt;源IP地址、源端口号、目的IP地址、目的端口号&gt;这4个值一起唯一地定义了一条连接。两条不同的TCP连接不能拥有4个完全相同的地址组件值（但不同连接的部分组件可以拥有相同的值）。 2、HTTP事务的时延有以下几种主要原因。（1）客户端首先需要根据URI确定Web服务器的IP地址和端口号。如果最近没有对URI中的主机名进行访问，通过DNS解析系统将URI中的主机名转换成一个IP地址可能要花费数十秒的时间。（注：大多数HTTP客户端都有一个小的DNS缓存，用来保存近期所访问站点的IP地址。）（2）接下来，客户端向服务器发送一条TCP连接请求，并等待服务器回送一个请求接收应答。每条新的TCP连接都会有连接建立时延。这个值通常最多只有一两秒钟，但如果有数百个HTTP事务的话，这个值会快速地叠加上去。（3）一旦连接建立起来了，客户端就会通过新建立的TCP管道来发送HTTP请求。数据到达时，Web服务器会从TCP连接中读取请求报文，并对请求进行处理。（4）然后，Web服务器会回送HTTP响应，这也需要花费时间。 这些TCP网络时延的大小取决于硬件速度、网络和服务器的负载，请求和响应报文的尺寸，以及客户端和服务器之间的距离。TCP协议的技术复杂性也会对时延产生巨大的影响。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[socket 详解]]></title>
    <url>%2F2018%2F04%2F10%2Fsocket%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1、网络中进程之间是如何通信？本地的进程间通信（IPC）有很多种方式，但可以总结为下面4类： 消息传递（管道、FIFO、消息队列） 同步（互斥量、条件变量、读写锁、文件和写记录锁、信号量） 共享内存（匿名的和具名的） 远程过程调用（Solaris门和Sun RPC） 在本地我们通过进程 PID 来唯一标识一个进程，但是在网络中是行不通的。而 TCP/IP 协议簇帮我们解决了这个问题，网络层的“IP地址”可以唯一标识网络中的主机，传输层的“协议+端口”可以唯一标识主机中的应用程序（进程）。这样利用三元组（ip地址、协议、端口）就可以标识网络中的进程了，网络中的进程通信就可以利用这个标志与其它进程进行交互。 2、什么是socket那什么是socket呢？其实，Socket 就是编程接口（API），是对 TCP/IP 的封装，对外提供的接口。网络中进程间通信采用的就是 socket（套接字）。看下图即可明白 socket： 几种典型的应用编程接口： Berkeley UNIX 操作系统定义了一种 API，称为套接字接口（socket Interface），简称套接字（socket）。 微软公司在其操作系统中采用了套接字接口 API，形成了一个稍有不同的 API，并称为 Windows Socket Interface，WINSOCK。 AT&amp;T 为其 UNIX 系统 V 定义了一种 API，简写为 TLI（Transport Layer Interface）(已经被淘汰了)。 3、套接字（socket）概念套接字（socket）是通信的基石，是支持TCP/IP协议的网络通信的基本操作单元。它是网络通信过程中端点的抽象表示，包含进行网络通信必须的五种信息：连接使用的协议，本地主机的IP地址，本地进程的协议端口，远地主机的IP地址，远地进程的协议端口。 应用层通过传输层进行数据通信时，TCP会遇到同时为多个应用程序进程提供并发服务的问题。多个TCP连接或多个应用程序进程可能需要通过同一个 TCP协议端口传输数据。为了区别不同的应用程序进程和连接，许多计算机操作系统为应用程序与TCP／IP协议交互提供了套接字(Socket)接口（由图2可看出来）。应用层可以和传输层通过Socket接口，区分来自不同应用程序进程或网络连接的通信，实现数据传输的并发服务。 4、socket原理要想明白 Socket，必须要理解TCP连接。 建立TCP连接的“三次握手”： 第一次：客户端向服务器发送SYN包(syn=j)，同时自己处于SYN_SEND状态。 第二次：服务器端收到SYN包后，必须确认客户的SYN(syn=j+1)，同时也发送一个SYN包(syn=k)，即SYN+ACK包，此时服务器进入SYN_RECV状态。 第三次：客户端收到服务器发来的SYN+ACK包，就向服务器发送SYN(syn=k+1)，发送完毕后，服务器和客户端都进入ESTABLISHED状态。完成三次握手。 握手过程中，并不传输数据。在握手后，服务器与客户端才开始传输数据，理想状态下，TCP连接一旦建立，在通讯双方中的任何一方主动断开连接之前，TCP连接会一直保持下去。断开连接时服务器和客户端均可以主动发起断开TCP连接的请求，断开过程需要经过“四次握手”。 （1）socket 连接Socket连接，至少需要一对套接字，分为 clientSocket，serverSocket。连接分为3个步骤： 服务器监听：服务器并不定位具体客户端的套接字，而是时刻处于监听状态。 客户端请求：客户端的套接字要描述它要连接的服务器的套接字。提供地址和端口号，然后向服务器套接字提出连接请求。 连接确认：当服务器套接字收到客户端套接字发来的请求后，就响应客户端套接字的请求，并建立一个新的线程，把服务器端的套接字的描述发给客户端，一旦客户端确认了此描述，就正式建立连接。而服务器套接字继续处于监听状态，继续接收其他客户端套接字的连接请求。 创建Socket连接的时候，可以指定传输层协议。可以是TCP或者UDP，当用TCP连接，该Socket就是个TCP连接。 （2）socket 函数Socket接口对外提供的函数如下： table th:first-of-type{ width: 15%; } 函数名 说明 socket 创建套接字 connect “连接”远端服务器（仅用于客户端） closesocket 释放/关闭套接字 bind 绑定套接字的本地IP地址和端口号（通常客户端不需要） listen 配置服务端TCP套接字为监听模式，并设置队列大小（仅用于服务器端TCP套接字） accept 接受/提取一个连接请求，创建新套接字，通过新套接（仅用于服务器端的TCP套接字） recv 接收数据（用于TCP套接字或连接模式的客户端UDP套接字） recvfrom 接收数据报（用于非连接模式的UDP套接字） send 发送数据（用于TCP套接字或连接模式的客户端UDP套接字） sendto 发送数据报（用于非连接模式的UDP套接字） setsockopt 设置套接字选项参数 getsockopt 获取套接字选项参数 （3）socket 调用流程 （4）socket 管理通过 Socket描述符表来进行管理，每个进程中有一个描述符表。当应用进程创建套接字时，操作系统分配一个数据结构存储该套接字相关信息。如下图所示： （5）并发面向连接服务器基本流程主线程1：创建（主）套接字，并绑定熟知端口号；主线程2：创建（主）套接字为被动监听模式，准备用于服务器；主线程3：反复调用accept() 函数接收下一个连接请求（通过主套接字），并创建一个新的子线程处理该客户端响应；子线程1：接收一个客户端的服务请求（通过新创建的套接字）；子线程2：遵循应用层协议与特定客户进行交互；子线程3：关闭/释放连接并推出（线程终止）。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 配置文件详解]]></title>
    <url>%2F2018%2F04%2F09%2FNginx%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1、结构分析nginx配置文件中主要包括六块：main，events，http，server，location，upstream结构如下图： main块：主要控制nginx子进程的所属用户/用户组、派生子进程数、错误日志位置/级别、pid位置、子进程优先级、进程对应cpu、进程能够打开的文件描述符数目等 events块：控制nginx处理连接的方式 http块：是nginx处理http请求的主要配置模块，大多数配置都在这里面进行 server块：是nginx中主机的配置块，可以配置多个虚拟主机 location块：是server中对应的目录级别的控制块，可以有多个 upstream块：是nginx做反向代理和负载均衡的配置块，可以有多个 2、配置命令解释nginx中每条配置命令都必须要以分号“;”结束！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159#定义Nginx运行的用户和用户组user www www;#nginx进程数，建议设置为等于CPU总核心数。worker_processes 8;#全局错误日志定义类型，[ debug | info | notice | warn | error | crit ]error_log /var/log/nginx/error.log info;#指定nginx pid的存放路径pid /var/run/nginx.pid;#一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（系统的值ulimit -n）#与nginx进程数相除，但是nginx分配请求并不均匀，所以建议与ulimit -n的值保持一致。worker_rlimit_nofile 65535;#工作模式与连接数上限events &#123; #参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; #epoll模型是Linux 2.6以上版本内核中的高性能网络I/O模型，如果跑在FreeBSD上面，就用kqueue模型。 use epoll; #单个进程最大连接数（最大连接数=连接数*进程数） worker_connections 65535;&#125;#设定http服务器http &#123; include mime.types; #文件扩展名与文件类型映射表 default_type application/octet-stream; #默认文件类型 #charset utf-8; #默认编码 server_names_hash_bucket_size 128; #服务器名字的hash表大小 client_header_buffer_size 32k; #设定请求缓 large_client_header_buffers 4 64k; #设定请求缓 client_max_body_size 8m; #配置客户端能够上传的文件大小 sendfile on; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件， #对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off， #以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。 autoindex on; #开启目录列表访问，合适下载服务器，默认关闭。 tcp_nopush on; #防止网络阻塞 tcp_nodelay on; #防止网络阻塞 keepalive_timeout 120; #长连接超时时间，单位是秒 #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问 速度。下面参数看字面意思都能理解。 fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; #gzip模块设置 gzip on; #开启gzip压缩输出 gzip_min_length 1k; #最小压缩文件大小 gzip_buffers 4 16k; #压缩缓冲区 gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0） gzip_comp_level 2; #压缩等级 gzip_types text/plain application/x-javascript text/css application/xml; #压缩类型，默认就已经包含text/html，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。 gzip_vary on; #limit_zone crawler $binary_remote_addr 10m; #开启限制IP连接数的时候需要使用 #日志格式设定 log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; $http_x_forwarded_for&apos;; #启用访问日志，并且指定日志采用的格式 access_log /var/log/nginx/ha97access.log main; #自定义日志格式 #log_format access &apos;$http_x_forwarded_for $remote_addr [$time_local] &quot;http://$host&quot; &quot;$request&quot; &apos; #&apos;$status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot; &quot;$remote_user&quot; &apos;; #（x_forwarded_for表示用户的真实IP。） upstream blog.ha97.com &#123; #upstream的负载均衡，weight是权重，可以根据机器配置定义权重。 #weigth参数表示权值，权值越高被分配到的几率越大。 server 192.168.80.121:80 weight=3; server 192.168.80.122:80 weight=2; server 192.168.80.123:80 weight=3; &#125; #虚拟主机的配置 server &#123; #监听端口 listen 80; #域名可以有多个，用空格隔开 server_name www.ha97.com ha97.com; #默认索引文件 index index.html index.htm index.php; #主机站点根目录地址 root /data/www/ha97; #error_page 404 /404.html; //404页面地址（可以配置其它错误页面地址如500等，格式一样） #转发PHP文件到指定服务 location ~ .*\.(php|php5)?$&#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; &#125; #图片缓存时间设置 location ~ .*\.(gif|jpg|jpeg|png|bmp|swf)$&#123; expires 10d; &#125; #JS和CSS缓存时间设置 location ~ .*\.(js|css)?$&#123; expires 1h; &#125; #对 &quot;/&quot; 启用反向代理 location / &#123; proxy_pass http://127.0.0.1:88; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #以下是一些反向代理的配置，可选。 proxy_set_header Host $host; client_max_body_size 10m; #允许客户端请求的最大单文件字节数 client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数， proxy_connect_timeout 90; #nginx跟后端服务器连接超时时间(代理连接超时) proxy_send_timeout 90; #后端服务器数据回传时间(代理发送超时) proxy_read_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时) proxy_buffer_size 4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 4 32k; #proxy_buffers缓冲区，网页平均在32k以下的设置 proxy_busy_buffers_size 64k; #高负荷下缓冲大小（proxy_buffers*2） proxy_temp_file_write_size 64k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传 &#125; #设定查看Nginx状态的地址 location /NginxStatus &#123; stub_status on; access_log on; auth_basic &quot;NginxStatus&quot;; auth_basic_user_file conf/htpasswd; #htpasswd文件的内容可以用apache提供的htpasswd工具来产生。 &#125; #本地动静分离反向代理配置 #所有jsp的页面均交由tomcat或resin处理 location ~ .(jsp|jspx|do)?$ &#123; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:8080; &#125; #所有静态文件由nginx直接读取不经过tomcat或resin location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip |txt|flv|mid|doc|ppt|pdf|xls|mp3|wma)$&#123; expires 15d; &#125; location ~ .*.(js|css)?$&#123; expires 1h; &#125; &#125;&#125;]]></content>
      <categories>
        <category>服务器</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 中的事件驱动模型]]></title>
    <url>%2F2018%2F04%2F09%2FNginx%E4%B8%AD%E7%9A%84%E4%BA%8B%E4%BB%B6%E9%A9%B1%E5%8A%A8%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[什么是事件驱动模型？在计算机编程领域，事件驱动模型对应一种程序设计方式，Event-driven programming，即事件驱动程序设计。事件驱动模型一般是由事件收集器，事件发送器，事件处理器三部分基本单元组成。 事件收集器专门负责收集所有的事件，包括来自用户的（如鼠标单击事件、键盘输入事件等）、来自硬件的（如时钟事件等）和来自软件的（如操作系统、应用程序本身等）。 事件发送器负责将收集器收集到的事件分发到目标对象中。目标对象就是事件处理器所处的位置。 事件处理器主要负责具体事件的响应工作，它往往要到实现阶段才完全确定。 事件驱动程序可以由任何编程语言来实现，只是难易程度有别。如果一个系统是以事件驱动程序模型作为编程基础的，那么，它的架构基本上是这样的：预先设计一个事件循环所形成的程序，这个事件循环程序构成了“事件收集器”，它不断地检查目前要处理的事件信息，然后使用“事件发送器”传递给“事件处理器”。“事件处理器”一般运用虚函数机制来实现。 Nginx中的事件驱动模型Nginx服务器响应和处理Web请求的过程，就是基于事件驱动模型的，它也包含事件收集器、事件发送器和事件处理器等三部分基本单元。Nginx的“事件收集器”和“事件发送器”的实现没有太大的特点，重点介绍一下它的“事件处理器”。通常，我们在编写服务器处理模型的程序时，基于事件驱动模型，“目标对象”中的“事件处理器”可以有以下几种实现办法： “事件发送器”每传递过来一个请求，“目标对象”就创建一个新的进程，调用“事件处理器”来处理该请求。 “事件发送器”每传递过来一个请求，“目标对象”就创建一个新的线程，调用“事件处理器”来处理该请求。 “事件发送器”每传递过来一个请求，“目标对象”就将其放入一个待处理事件的列表，使用非阻塞I/O方式调用“事件处理器”来处理该请求。 以上的三种处理方式，各有特点，第一种方式，由于创建新的进程的开销比较大，会导致服务器性能比较差，但其实现相对来说比较简单。第二种方式，由于要涉及到线程的同步，故可能会面临死锁、同步等一系列问题，编码比较复杂。第三种方式，在编写程序代码时，逻辑比前面两种都复杂。大多数网络服务器采用了第三种方式，逐渐形成了所谓的“事件驱动处理库”。事件驱动处理库又被称为多路IO复用方法，最常见的包括以下三种：select模型，poll模型和epoll模型。Nginx服务器还支持rtsig模型、kqueue模型、dev/poll模型和eventport模型等。通过Nginx配置可以使得Nginx服务器支持这几种事件驱动处理模型。这里详细介绍以下它们。 select库select库，是各个版本的Linux和Windows平台都支持的基本事件驱动模型库，并且在接口的定义上也基本相同，只是部分参数的含义略有差异。使用select库的步骤一般是：首先，创建所关注事件的描述符集合。对于一个描述符，可以关注其上面的（Read)事件、写（Write)事件以及异常发送（Exception）事件，所以要创建三类事件描述符集合，分别用来收集读事件的描述符、写事件的描述符和异常事件的描述符。其次，调用底层提供的select()函数，等待事件发生。这里需要注意的一点是，select的阻塞与是否设置非阻塞I/O是没有关系的。然后，轮询所有事件描述符集合中的每一个事件描述符，检查是否有相应的事件发生，如果有，就进行处理。Nginx服务器在编译过程中如果没有为其指定其他高性能事件驱动模型库，它将自动编译该库。我们可以使用–with-select_module和–without-select_module两个参数强制Nginx是否编译该库。 缺点：1、单个进程可监视的fd（描述符）数量被限制，即能监听端口的大小有限。 一般来说这个数目和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。32位机默认是1024个。64位机默认是2048.2、对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低： 当套接字比较多的时候，每次select()都要通过遍历FD_SETSIZE个Socket来完成调度,不管哪个Socket是活跃的,都遍历一遍。这会浪费很多CPU时间。3、需要维护一个用来存放大量fd（描述符）的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大 poll库poll库，作为Linux平台上的基本事件驱动模型，实在Linux2.1.23中引入的。Windows平台不支持poll库。poll与select的基本工作方式是相同的，都是现创建一个关注事件的描述符集合，再去等待这些事件发生，然后在轮询描述符集合，检查有没有事件发生，如果有，就进行处理。poll库与select库的主要区别在于，select库需要为读事件、写事件和异常事件分别创建一个描述符集合，因此在最后轮询的时候，需要分别轮询这三个集合。而poll库只需要创建一个集合，在每个描述符对应的结构上分别设置读事件、写事件或者异常事件，最后轮询的时候，可以同时检查这三种事件是否发生。可以说，poll库是select库的优化实现。Nginx服务器在编译过程中如果没有为其制定其他高性能事件驱动模型库，它将自动编译该库。我们可以使用–with-poll_module和–without-poll_module两个参数强制Nginx是否编译该库。 缺点：1、大量的fd（描述符）的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义。2、poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。 epoll库epoll库是Nginx服务器支持的高性能事件驱动库之一，它是公认的非常优秀的事件驱动模型，和poll库及select库有很大的不同。epoll属于poll库的一个变种，是在Linux 2.5.44中引入的，在Linux 2.6以上的版本都可以使用它。poll库和select库在实际工作中，最大的区别在于效率。从前面的介绍我们知道，它们的处理方式都是创建一个待处理事件列表，然后把这个列表发给内核，返回的时候，再去轮询检查这个列表，以判断事件是否发生。这样在描述符比较多的应用中，效率就显得比较低下了。一种比较好的做法是，把描述符列表的管理交给内核负责，一旦有某种事件发生，内核把发生事件的描述符列表通知给进程，这样就避免了轮询整个描述符列表。epoll库就是这样一种模型。首先，epoll库通过相关调用通知内核创建一个由N个描述符的事件列表。然后，给这些描述符设置所关注的事件，并把它添加到内核的事件列表中去，在具体的编码过程中也可以通过相关调用对事件列表中的描述符进行修改和删除。完成设置之后，epoll库就开始等待内核通知事件发生了。某一事件发生后，内核将发生事件的描述符列表上报给epoll库。得到事件列表的epoll库，就可以进行事件处理了。epoll库在Linux平台上是最高效的。它支持一个进程打开大数目的事件描述符，上限是系统可以打开文件的最大数目。同时，epoll库的IO效率不随描述符数目增加而线性下降，因为它只会对内核上报的“活跃”的描述符进行操作。 优点：1、没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）；2、效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数； 即Epoll最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll。3、内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。 rtsig模型rtsig是Real-Time Signal的缩写，是实时信号的意思。从严格意义上说，rtsig模型并不是常用的事件驱动模型，但Nginx服务器使用了使用实时信号对事件进行响应的支持，官方文档中将rtsig模型与其他的事件驱动模型并列。使用rtsig模型时，工作进程会通过系统内核建立一个rtsig队列用于存放标记事件发生（在Nginx服务器应用中特指客户端请求发生）的信号。每个事件发生时，系统内核就会产生一个信号存放到rtsig队列中等待工作进程的处理。需要指出的是，rtsig队列有长度限制，超过该长度后就会发生溢出。默认情况下，Linux系统事件信号队列的最大长度设置为1024，也就是同时最多可以存放1024个发生事件的信号。在Linux 2.6.6-mm2之前的版本中，系统各个进程的事件信号队列是由内核统一管理的，用户可以通过修改内核参数/proc/sys/kernel/rtsig-max/来自定义该长度设置。在Linux 2.6.6-mm2之后的版本中，该内核参数被取消，系统各个进程分别拥有各自的事件信号队列，这个队列的大小由Linux系统的RLIMT_SIGPENGIND参数定义，在执行setrlimit()系统调用时确定该大小。Nginx提供了worker_rlimit_sigpending参数用于调节这种情况下的事件信号队列长度。当rtsig队列发生溢出时，Nginx将暂时停止使用rtsig模型，而调用poll库处理未处理的事件，直到rgsit信号队列全部清空，然后再次启动rtsig模型，以防止新的溢出发生。Nginx在配置文件中提供了相关参数对rtsig模型的使用配置。编译Nginx服务器时，使用–with-rtsig_module配置选项来启用rtsig模型的编译。 其他事件驱动模型除了以上四种主要的事件驱动模型，Nginx服务器针对特定的Linux平台提供了响应的事件驱动模型支持。目前实现的主要有kqueue模型、/dev/poll模型和eventport模型等。 kqueue模型，是用于支持BSD系列平台的高效事件驱动模型，主要用在FreeBSD 4.1及以上版本、OpenBSD 2.9及以上版本、NetBSD 2.0及以上版本以及Mac OS X平台上。该模型也是poll库的一个变种，其和epoll库的处理方式没有本质上的区别，都是通过避免轮询操作提供效率。该模型同时支持条件触发（level-triggered,也叫水平触发，只要满足条件就触发一个事件）和边缘触发（edge-triggered，每当状态变化时，触发一个事件）。如果大家在这些平台下使用Nginx服务器，建议选在该模型用于请求处理，以提高Nginx服务器的处理性能。 /dev/poll模型，适用于支持Unix衍生平台的高效事件驱动模型，其主要在Solaris711/99及以上版本、HP/UX 11.22及以上版本、IRIX 6.5.15及以上版本和Tru64 UNIX 5.1A及以上版本的平台中使用。该模型是Sun公司在开发Solaris系列平台时提出的用于完成事件驱动机制的方案，它使用了虚拟的/dev/poll设备，开发人员可以将要监视的文件描述符加入这个设备，然后通过ioctl()调用来获取事件通知。在以上提到的平台中，建议使用该模型处理请求。 eventport模型，适用于支持Solaris 10及以上版本平台的高效事件驱动模型。该模型也是Sun公司在开发Solaris系列平台时提出的用于完成事件驱动机制的方案，它可以有效防止内核崩溃情况的发生。Nginx服务器为此提供了支持。 以上就是Nginx服务器支持的事件驱动库。可以看到，Nginx服务器针对不同的Linux或Unix衍生平台提供了多种事件驱动模型的处理，尽量发挥系统平台本身的优势，最大程度地提高处理客户端请求事件的能力。在实际工作中，我们需要根据具体情况和应用情景选择使用不同的事件驱动模型，以保证Nginx服务器的高效运行。 select、poll、epoll 区别总结：1、支持一个进程所能打开的最大连接数 table th:first-of-type{ width: 10%; } select 单个进程所能打开的最大连接数有FD_SETSIZE宏定义，其大小是32个整数的大小（在32位的机器上，大小就是3232，同理64位机器上FD_SETSIZE为3264），当然我们可以对进行修改，然后重新编译内核，但是性能可能会受到影响，这需要进一步的测试。 poll poll本质上和select没有区别，但是它没有最大连接数的限制，原因是它是基于链表来存储的 epoll 虽然连接数有上限，但是很大，1G内存的机器上可以打开10万左右的连接，2G内存的机器可以打开20万左右的连接 2、FD剧增后带来的IO效率问题 select 因为每次调用时都会对连接进行线性遍历，所以随着FD的增加会造成遍历速度慢的“线性下降性能问题”。 poll 同上 epoll 因为epoll内核中实现是根据每个fd上的callback函数来实现的，只有活跃的socket才会主动调用callback，所以在活跃socket较少的情况下，使用epoll没有前面两者的线性下降的性能问题，但是所有socket都很活跃的情况下，可能会有性能问题。 3、 消息传递方式 select 内核需要将消息传递到用户空间，都需要内核拷贝动作 poll 同上 epoll epoll通过内核和用户空间共享一块内存来实现的。 综上，在选择select，poll，epoll时要根据具体的使用场合以及这三种方式的自身特点。1、表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。2、select低效是因为每次它都需要轮询。但低效也是相对的，视情况而定，也可通过良好的设计改善]]></content>
      <categories>
        <category>服务器</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx + FastCGI运行原理]]></title>
    <url>%2F2018%2F04%2F09%2FNginx%2BFastCGI%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[1、什么是 FastCGIFastCGI是一个可伸缩地、高速地在HTTP server和动态脚本语言间通信的接口。多数流行的HTTP server都支持FastCGI，包括Apache、Nginx和lighttpd等。同时，FastCGI也被许多脚本语言支持，其中就有PHP。 FastCGI是从CGI发展改进而来的。传统CGI接口方式的主要缺点是性能很差，因为每次HTTP服务器遇到动态程序时都需要重新启动脚本解析器来执行解析，然后将结果返回给HTTP服务器。这在处理高并发访问时几乎是不可用的。另外传统的CGI接口方式安全性也很差，现在已经很少使用了。FastCGI接口方式采用C/S结构，可以将HTTP服务器和脚本解析服务器分开，同时在脚本解析服务器上启动一个或者多个脚本解析守护进程。当HTTP服务器每次遇到动态程序时，可以将其直接交付给FastCGI进程来执行，然后将得到的结果返回给浏览器。这种方式可以让HTTP服务器专一地处理静态请求或者将动态脚本服务器的结果返回给客户端，这在很大程度上提高了整个应用系统的性能。 2、Nginx+FastCGI运行原理Nginx不支持对外部程序的直接调用或者解析，所有的外部程序（包括PHP）必须通过FastCGI接口来调用。FastCGI接口在Linux下是socket（这个socket可以是文件socket，也可以是ip socket）。 wrapper：为了调用CGI程序，还需要一个FastCGI的wrapper（wrapper可以理解为用于启动另一个程序的程序），这个wrapper绑定在某个固定socket上，如端口或者文件socket。当Nginx将CGI请求发送给这个socket的时候，通过FastCGI接口，wrapper接收到请求，然后Fork(派生）出一个新的线程，这个线程调用解释器或者外部程序处理脚本并读取返回数据；接着，wrapper再将返回的数据通过FastCGI接口，沿着固定的socket传递给Nginx；最后，Nginx将返回的数据（html页面或者图片）发送给客户端。这就是Nginx+FastCGI的整个运作过程，如图1-3所示。 所以，我们首先需要一个wrapper，这个wrapper需要完成的工作： 通过调用fastcgi（库）的函数通过socket和ningx通信（读写socket是fastcgi内部实现的功能，对wrapper是非透明的） 调度thread，进行fork和kill 和application（php）进行通信 3、spawn-fcgi与PHP-FPMFastCGI接口方式在脚本解析服务器上启动一个或者多个守护进程对动态脚本进行解析，这些进程就是FastCGI进程管理器，或者称为FastCGI引擎。 spawn-fcgi与PHP-FPM就是支持PHP的两个FastCGI进程管理器。因此HTTPServer完全解放出来，可以更好地进行响应和并发处理。 spawn-fcgi与PHP-FPM的异同：1）spawn-fcgi是HTTP服务器lighttpd的一部分，目前已经独立成为一个项目，一般与lighttpd配合使用来支持PHP。但是ligttpd的spwan-fcgi在高并发访问的时候，会出现内存泄漏甚至自动重启FastCGI的问题。即：PHP脚本处理器当机，这个时候如果用户访问的话，可能就会出现白页(即PHP不能被解析或者出错)。2）Nginx是个轻量级的HTTP server，必须借助第三方的FastCGI处理器才可以对PHP进行解析，因此其实这样看来nginx是非常灵活的，它可以和任何第三方提供解析的处理器实现连接从而实现对PHP的解析(在nginx.conf中很容易设置)。nginx也可以使用spwan-fcgi(需要一同安装lighttpd，但是需要为nginx避开端口，一些较早的blog有这方面安装的教程)，但是由于spawn-fcgi具有上面所述的用户逐渐发现的缺陷，现在慢慢减少用nginx+spawn-fcgi组合了。 由于spawn-fcgi的缺陷，现在出现了第三方(目前已经加入到PHP core中)的PHP的FastCGI处理器PHP-FPM，它和spawn-fcgi比较起来有如下优点： 由于它是作为PHP的patch补丁来开发的，安装的时候需要和php源码一起编译，也就是说编译到php core中了，因此在性能方面要优秀一些； 同时它在处理高并发方面也优于spawn-fcgi，至少不会自动重启fastcgi处理器。因此，推荐使用Nginx+PHP/PHP-FPM这个组合对PHP进行解析。 相对Spawn-FCGI，PHP-FPM在CPU和内存方面的控制都更胜一筹，而且前者很容易崩溃，必须用crontab进行监控，而PHP-FPM则没有这种烦恼。 FastCGI 的主要优点是把动态语言和HTTP Server分离开来，所以Nginx与PHP/PHP-FPM经常被部署在不同的服务器上，以分担前端Nginx服务器的压力，使Nginx专一处理静态请求和转发动态请求，而PHP/PHP-FPM服务器专一解析PHP动态请求。 4、Nginx+PHP-FPMPHP-FPM是管理FastCGI的一个管理器，它作为PHP的插件存在，在安装PHP要想使用PHP-FPM时在老php的老版本（php5.3.3之前）就需要把PHP-FPM以补丁的形式安装到PHP中，而且PHP要与PHP-FPM版本一致，这是必须的） PHP-FPM其实是PHP源代码的一个补丁，旨在将FastCGI进程管理整合进PHP包中。必须将它patch到你的PHP源代码中，在编译安装PHP后才可以使用。PHP5.3.3已经集成php-fpm了，不再是第三方的包了。PHP-FPM提供了更好的PHP进程管理方式，可以有效控制内存和进程、可以平滑重载PHP配置，比spawn-fcgi具有更多优点，所以被PHP官方收录了。在./configure的时候带 –enable-fpm参数即可开启PHP-FPM。 fastcgi已经在php5.3.5的core中了，不必在configure时添加 –enable-fastcgi了。老版本如php5.2的需要加此项。 当我们安装Nginx和PHP-FPM完后，配置信息： PHP-FPM的默认配置php-fpm.conf：1234listen_address 127.0.0.1:9000 #这个表示php的fastcgi进程监听的ip地址以及端口start_serversmin_spare_serversmax_spare_servers Nginx配置运行php： 编辑nginx.conf加入如下语句：1234567location ~ \.php$ &#123; root html; fastcgi_pass 127.0.0.1:9000; 指定了fastcgi进程侦听的端口,nginx就是通过这里与php交互的 fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME /usr/local/nginx/html$fastcgi_script_name;&#125; Nginx通过location指令，将所有以php为后缀的文件都交给127.0.0.1:9000来处理，而这里的IP地址和端口就是FastCGI进程监听的IP地址和端口。 其整体工作流程：1) FastCGI进程管理器php-fpm自身初始化，启动主进程php-fpm和启动start_servers个CGI 子进程。 主进程php-fpm主要是管理fastcgi子进程，监听9000端口。 fastcgi子进程等待来自Web Server的连接。2) 当客户端请求到达Web Server Nginx是时，Nginx通过location指令，将所有以php为后缀的文件都交给127.0.0.1:9000来处理，即Nginx通过location指令，将所有以php为后缀的文件都交给127.0.0.1:9000来处理。3) FastCGI进程管理器PHP-FPM选择并连接到一个子进程CGI解释器。Web server将CGI环境变量和标准输入发送到FastCGI子进程。4) FastCGI子进程完成处理后将标准输出和错误信息从同一连接返回Web Server。当FastCGI子进程关闭连接时，请求便告处理完成。5) FastCGI子进程接着等待并处理来自FastCGI进程管理器（运行在 WebServer中）的下一个连接。 5、Nginx+PHP正确配置一般web都做统一入口：把PHP请求都发送到同一个文件上，然后在此文件里通过解析「REQUEST_URI」实现路由。 Nginx配置文件分为好多块，常见的从外到内依次是「http」、「server」、「location」等等，缺省的继承关系是从外到内，也就是说内层块会自动获取外层块的值作为缺省值。例如：1234567891011121314151617server &#123; listen 80; server_name foo.com; root /path; location / &#123; index index.html index.htm index.php; if (!-e $request_filename) &#123; rewrite . /index.php last; &#125; &#125; location ~ \.php$ &#123; include fastcgi_params; fastcgi_param SCRIPT_FILENAME /path$fastcgi_script_name; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; &#125; &#125; 1) 不应该在location 模块定义index一旦未来需要加入新的「location」，必然会出现重复定义的「index」指令，这是因为多个「location」是平级的关系，不存在继承，此时应该在「server」里定义「index」，借助继承关系，「index」指令在所有的「location」中都能生效。 2) 使用try_files接下来看看「if」指令，说它是大家误解最深的Nginx指令毫不为过：123if (!-e $request_filename) &#123; rewrite . /index.php last;&#125; 很多人喜欢用「if」指令做一系列的检查，不过这实际上是「try_files」指令的职责：try_files $uri $uri/ /index.php; 除此以外，初学者往往会认为「if」指令是内核级的指令，但是实际上它是rewrite模块的一部分，加上Nginx配置实际上是声明式的，而非过程式的，所以当其和非rewrite模块的指令混用时，结果可能会非你所愿。 3）fastcgi_params」配置文件：include fastcgi_params; Nginx有两份fastcgi配置文件，分别是「fastcgi_params」和「fastcgi.conf」，它们没有太大的差异，唯一的区别是后者比前者多了一行「SCRIPT_FILENAME」的定义：fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; 注意：$document_root 和 $fastcgi_script_name 之间没有 /。原本Nginx只有「fastcgi_params」，后来发现很多人在定义「SCRIPT_FILENAME」时使用了硬编码的方式，于是为了规范用法便引入了「fastcgi.conf」。 不过这样的话就产生一个疑问：为什么一定要引入一个新的配置文件，而不是修改旧的配置文件？这是因为「fastcgi_param」指令是数组型的，和普通指令相同的是：内层替换外层；和普通指令不同的是：当在同级多次使用的时候，是新增而不是替换。换句话说，如果在同级定义两次「SCRIPT_FILENAME」，那么它们都会被发送到后端，这可能会导致一些潜在的问题，为了避免此类情况，便引入了一个新的配置文件。 此外，我们还需要考虑一个安全问题：在PHP开启「cgi.fix_pathinfo」的情况下，PHP可能会把错误的文件类型当作PHP文件来解析。如果Nginx和PHP安装在同一台服务器上的话，那么最简单的解决方法是用「try_files」指令做一次过滤：try_files $uri =404; 依照前面的分析，给出一份改良后的版本，是不是比开始的版本清爽了很多：1234567891011121314server &#123; listen 80; server_name foo.com; root /path; index index.html index.htm index.php; location / &#123; try_files $uri $uri/ /index.php; &#125; location ~ \.php$ &#123; try_files $uri =404; include fastcgi.conf; fastcgi_pass 127.0.0.1:9000; &#125; &#125;]]></content>
      <categories>
        <category>服务器</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 概述及工作原理]]></title>
    <url>%2F2018%2F04%2F09%2FNginx%E6%A6%82%E8%BF%B0%E5%8F%8A%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[一、nginx概述Nginx 是一款轻量级的 Web 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器，其特点是占有内存少，并发能力强。 1、什么是nginx？Nginx 是俄罗斯人编写的十分轻量级的 HTTP 服务器,Nginx，它的发音为“engine X”，是一个高性能的HTTP和反向代理服务器，同时也是一个 IMAP/POP3/SMTP 代理服务器。Nginx 是由俄罗斯人 Igor Sysoev 为俄罗斯访问量第二的 Rambler.ru 站点开发的，它已经在该站点运行超过两年半了。Igor Sysoev 在建立的项目时,使用基于 BSD 许可。 Nginx 以事件驱动的方式编写，所以有非常好的性能，同时也是一个非常高效的反向代理、负载平衡。其拥有匹配 Lighttpd 的性能，同时还没有 Lighttpd 的内存泄漏问题，而且 Lighttpd 的 mod_proxy 也有一些问题并且很久没有更新。 现在，Igor 将源代码以类 BSD 许可证的形式发布。Nginx 因为它的稳定性、丰富的模块库、灵活的配置和低系统资源的消耗而闻名．业界一致认为它是 Apache2.2＋mod_proxy_balancer 的轻量级代替者，不仅是因为响应静态页面的速度非常快，而且它的模块数量达到 Apache 的近 2/3。对 proxy 和 rewrite 模块的支持很彻底，还支持 mod_fcgi、ssl、vhosts ，适合用来做 mongrel clusters 的前端 HTTP 响应。 2、nginx的特点Nginx 做为 HTTP 服务器，有以下几项基本特性： I/O多路复用 epoll 轻量级：主要说功能模块少，代码模块化（Nginx仅保存了HTTP和核心功能相关的代码模块） CPU 亲和：是一种把CPU核心和Nginx工作进程绑定方式，把每个worker进程固定在一个CPU上执行，减少CPU切换，获得更好的性能 sendfile：使Nginx处理静态文件非常有优势 I/O多路复用：多个描述符的I/O操作都能在一个线程内并发交替地顺序完成，这就叫I/O多路复用，这里的“复用”指的是复用同一个线程。sendfile 运行原理：正常一个http请求会经过内核空间到用户空间，再传输给socket的流程，而请求会在内核空间和用户空间进行多次切换，静态文件是不需要经过用户空间的。因此开启sendfile，则静态文件直接通过内核空间传递给socket，返回给客户端。无需经过用户空间，提高性能 Nginx 专为性能优化而开发，性能是其最重要的考量,实现上非常注重效率 。它支持内核 Poll 模型，能经受高负载的考验,有报告表明能支持高达 50,000 个并发连接数。 Nginx 具有很高的稳定性。其它 HTTP 服务器，当遇到访问的峰值，或者有人恶意发起慢速连接时，也很可能会导致服务器物理内存耗尽频繁交换，失去响应，只能重启服务器。例如当前 apache 一旦上到 200 个以上进程，web响应速度就明显非常缓慢了。而 Nginx 采取了分阶段资源分配技术，使得它的 CPU 与内存占用率非常低。Nginx 官方表示保持 10,000 个没有活动的连接，它只占 2.5M 内存，所以类似 DOS 这样的攻击对 Nginx 来说基本上是毫无用处的。就稳定性而言,Nginx 比 lighthttpd 更胜一筹。 Nginx 支持热部署。它的启动特别容易, 并且几乎可以做到 7*24 不间断运行，即使运行数个月也不需要重新启动。你还能够在不间断服务的情况下，对软件版本进行进行升级。 二、nginx的架构众所周知，Nginx 性能高，而 Nginx 的高性能与其架构是分不开的。那么 Nginx 究竟是怎么样的呢？ Nginx 在启动后，在 unix 系统中会以 daemon 的方式在后台运行，后台进程包含一个 master 进程和多个 worker 进程。我们也可以手动地关掉后台模式，让 Nginx 在前台运行，并且通过配置让 Nginx 取消 master 进程，从而可以使 Nginx 以单进程方式运行。很显然，生产环境下我们肯定不会这么做，所以关闭后台模式，一般是用来调试用的。所以，我们可以看到，Nginx 是以多进程的方式来工作的，当然 Nginx 也是支持多线程的方式的，只是我们主流的方式还是多进程的方式，也是 Nginx 的默认方式。Nginx 采用多进程的方式有诸多好处，所以我就主要讲解 Nginx 的多进程模式吧。 1、Nginx 的进程模型刚才讲到，Nginx 在启动后，会有一个 master 进程和多个 worker 进程。master 进程主要用来管理 worker 进程，包含：接收来自外界的信号，向各 worker 进程发送信号，监控 worker 进程的运行状态，当 worker 进程退出后(异常情况下)，会自动重新启动新的 worker 进程。而基本的网络事件，则是放在 worker 进程中来处理了。多个 worker 进程之间是对等的，他们同等竞争来自客户端的请求，各进程互相之间是独立的。一个请求，只可能在一个 worker 进程中处理，一个 worker 进程，不可能处理其它进程的请求。 worker 进程的个数是可以设置的，一般我们会设置与机器cpu核数一致，这里面的原因与 Nginx 的进程模型以及事件处理模型是分不开的。Nginx 的进程模型，可以由下图来表示： 在 Nginx 启动后，如果我们要操作 Nginx，要怎么做呢？从上文中我们可以看到，master 来管理 worker 进程，所以我们只需要与 master 进程通信就行了。master 进程会接收来自外界发来的信号，再根据信号做不同的事情。所以我们要控制 Nginx，只需要通过 kill 向 master 进程发送信号就行了。比如kill -HUP pid，则是告诉 Nginx，从容地重启 Nginx，我们一般用这个信号来重启 Nginx，或重新加载配置，因为是从容地重启，因此服务是不中断的。master 进程在接收到 HUP 信号后是怎么做的呢？首先 master 进程在接到信号后，会先重新加载配置文件，然后再启动新的 worker 进程，并向所有老的 worker 进程发送信号，告诉他们可以光荣退休了。新的 worker 在启动后，就开始接收新的请求，而老的 worker 在收到来自 master 的信号后，就不再接收新的请求，并且在当前进程中的所有未处理完的请求处理完成后，再退出。当然，直接给 master 进程发送信号，这是比较老的操作方式，Nginx 在 0.8 版本之后，引入了一系列命令行参数，来方便我们管理。比如，./nginx -s reload，就是来重启 Nginx，./nginx -s stop，就是来停止 Nginx 的运行。如何做到的呢？我们还是拿 reload 来说，我们看到，执行命令时，我们是启动一个新的 Nginx 进程，而新的 Nginx 进程在解析到 reload 参数后，就知道我们的目的是控制 Nginx 来重新加载配置文件了，它会向 master 进程发送信号，然后接下来的动作，就和我们直接向 master 进程发送信号一样了。 现在，我们知道了当我们在操作 Nginx 的时候，Nginx 内部做了些什么事情，那么，worker 进程又是如何处理请求的呢？我们前面有提到，worker 进程之间是平等的，每个进程，处理请求的机会也是一样的。当我们提供 80 端口的 http 服务时，一个连接请求过来，每个进程都有可能处理这个连接，怎么做到的呢？首先，每个 worker 进程都是从 master 进程 fork 过来，在 master 进程里面，先建立好需要 listen 的 socket（listenfd）（socket不知道是啥的，可点击查询）之后，然后再 fork 出多个 worker 进程。所有 worker 进程的 listenfd 会在新连接到来时变得可读，为保证只有一个进程处理该连接，所有 worker 进程在注册 listenfd 读事件前抢 accept_mutex，抢到互斥锁的那个进程注册 listenfd 读事件，在读事件里调用 accept 接受该连接。当一个 worker 进程在 accept 这个连接之后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，这样一个完整的请求就是这样的了。我们可以看到，一个请求，完全由 worker 进程来处理，而且只在一个 worker 进程中处理。 那么，Nginx 采用这种进程模型有什么好处呢？当然，好处肯定会很多了。首先，对于每个 worker 进程来说，独立的进程，不需要加锁（因为需要争抢处理请求，不使用accept_mutex接受互斥量，会导致多个worker进程处理一个请求的混乱结果），所以省掉了锁带来的开销，同时在编程以及问题查找时，也会方便很多。其次，采用独立的进程，可以让互相之间不会影响，一个进程退出后，其它进程还在工作，服务不会中断，master 进程则很快启动新的 worker 进程。当然，worker 进程的异常退出，肯定是程序有 bug 了，异常退出，会导致当前 worker 上的所有请求失败，不过不会影响到所有请求，所以降低了风险。当然，好处还有很多，大家可以慢慢体会。 2、Nginx 事件模型上面讲了很多关于 Nginx 的进程模型，接下来，我们来看看 Nginx 是如何处理事件的。 有人可能要问了，Nginx 采用多 worker 的方式来处理请求，每个 worker 里面只有一个主线程，那能够处理的并发数很有限啊，多少个 worker 就能处理多少个并发，何来高并发呢？非也，这就是 Nginx 的高明之处，Nginx 采用了异步非阻塞的方式来处理请求，也就是说，Nginx 是可以同时处理成千上万个请求的。想想 apache 的常用工作方式（apache 也有异步非阻塞版本，但因其与自带某些模块冲突，所以不常用），每个请求会独占一个工作线程，当并发数上到几千时，就同时有几千的线程在处理请求了。这对操作系统来说，是个不小的挑战，线程带来的内存占用非常大，线程的上下文切换带来的 cpu 开销很大，自然性能就上不去了，而这些开销完全是没有意义的。 为什么 Nginx 可以采用异步非阻塞的方式来处理呢，或者异步非阻塞到底是怎么回事呢？我们先回到原点，看看一个请求的完整过程。首先，请求过来，要建立连接，然后再接收数据，接收数据后，再发送数据。具体到系统底层，就是读写事件，而当读写事件没有准备好时，必然不可操作，如果不用非阻塞的方式来调用，那就得阻塞调用了，事件没有准备好，那就只能等了，等事件准备好了，你再继续吧。阻塞调用会进入内核等待，cpu 就会让出去给别人用了，对单线程的 worker 来说，显然不合适，当网络事件越多时，大家都在等待呢，cpu 空闲下来没人用，cpu利用率自然上不去了，更别谈高并发了。好吧，你说加进程数，这跟apache的线程模型有什么区别，注意，别增加无谓的上下文切换。所以，在 Nginx 里面，最忌讳阻塞的系统调用了。不要阻塞，那就非阻塞喽。非阻塞就是，事件没有准备好，马上返回 EAGAIN，告诉你，事件还没准备好呢，你慌什么，过会再来吧。好吧，你过一会，再来检查一下事件，直到事件准备好了为止，在这期间，你就可以先去做其它事情，然后再来看看事件好了没。虽然不阻塞了，但你得不时地过来检查一下事件的状态，你可以做更多的事情了，但带来的开销也是不小的。所以，才会有了异步非阻塞的事件处理机制，具体到系统调用就是像 select/poll/epoll/kqueue 这样的系统调用。它们提供了一种机制，让你可以同时监控多个事件，调用他们是阻塞的，但可以设置超时时间，在超时时间之内，如果有事件准备好了，就返回。这种机制正好解决了我们上面的两个问题，拿 epoll 为例(在后面的例子中，我们多以 epoll 为例子，以代表这一类函数)，当事件没准备好时，放到 epoll 里面，事件准备好了，我们就去读写，当读写返回 EAGAIN 时，我们将它再次加入到 epoll 里面。这样，只要有事件准备好了，我们就去处理它，只有当所有事件都没准备好时，才在 epoll 里面等着。这样，我们就可以并发处理大量的并发了，当然，这里的并发请求，是指未处理完的请求，线程只有一个，所以同时能处理的请求当然只有一个了，只是在请求间进行不断地切换而已，切换也是因为异步事件未准备好，而主动让出的。这里的切换是没有任何代价，你可以理解为循环处理多个准备好的事件，事实上就是这样的。与多线程相比，这种事件处理方式是有很大的优势的，不需要创建线程，每个请求占用的内存也很少，没有上下文切换，事件处理非常的轻量级。并发数再多也不会导致无谓的资源浪费（上下文切换）。更多的并发数，只是会占用更多的内存而已。 我之前有对连接数进行过测试，在 24G 内存的机器上，处理的并发请求数达到过 200 万。现在的网络服务器基本都采用这种方式，这也是nginx性能高效的主要原因。 我们之前说过，推荐设置 worker 的个数为 cpu 的核数，在这里就很容易理解了，更多的 worker 数，只会导致进程来竞争 cpu 资源了，从而带来不必要的上下文切换。而且，nginx为了更好的利用多核特性，提供了 cpu 亲缘性的绑定选项，我们可以将某一个进程绑定在某一个核上，这样就不会因为进程的切换带来 cache 的失效。像这种小的优化在 Nginx 中非常常见，同时也说明了 Nginx 作者的苦心孤诣。比如，Nginx 在做 4 个字节的字符串比较时，会将 4 个字符转换成一个 int 型，再作比较，以减少 cpu 的指令数等等。 现在，知道了 Nginx 为什么会选择这样的进程模型与事件模型了。对于一个基本的 Web 服务器来说，事件通常有三种类型，网络事件、信号、定时器。从上面的讲解中知道，网络事件通过异步非阻塞可以很好的解决掉。如何处理信号与定时器？ 首先，信号的处理。对 Nginx 来说，有一些特定的信号，代表着特定的意义。信号会中断掉程序当前的运行，在改变状态后，继续执行。如果是系统调用，则可能会导致系统调用的失败，需要重入。关于信号的处理，大家可以学习一些专业书籍，这里不多说。对于 Nginx 来说，如果nginx正在等待事件（epoll_wait 时），如果程序收到信号，在信号处理函数处理完后，epoll_wait 会返回错误，然后程序可再次进入 epoll_wait 调用。 另外，再来看看定时器。由于 epoll_wait 等函数在调用的时候是可以设置一个超时时间的，所以 Nginx 借助这个超时时间来实现定时器。nginx里面的定时器事件是放在一颗维护定时器的红黑树里面，每次在进入 epoll_wait前，先从该红黑树里面拿到所有定时器事件的最小时间，在计算出 epoll_wait 的超时时间后进入 epoll_wait。所以，当没有事件产生，也没有中断信号时，epoll_wait 会超时，也就是说，定时器事件到了。这时，nginx会检查所有的超时事件，将他们的状态设置为超时，然后再去处理网络事件。由此可以看出，当我们写 Nginx 代码时，在处理网络事件的回调函数时，通常做的第一个事情就是判断超时，然后再去处理网络事件。 我们可以用一段伪代码来总结一下 Nginx 的事件处理模型：12345678910111213141516171819202122while (true) &#123; for t in run_tasks: t.handler(); update_time(&amp;now); timeout = ETERNITY; for t in wait_tasks: /* sorted already */ if (t.time &lt;= now) &#123; t.timeout_handler(); &#125; else &#123; timeout = t.time - now; break; &#125; nevents = poll_function(events, timeout); for i in nevents: task t; if (events[i].type == READ) &#123; t.handler = read_handler; &#125; else &#123; /* events[i].type == WRITE */ t.handler = write_handler; &#125; run_tasks_add(t);&#125; 好，本节我们讲了进程模型，事件模型，包括网络事件，信号，定时器事件。 三、Nginx的工作原理nginx 主要用于Web和代理服务器，接收请求，对请求进行解析，静态资源直接返回，动态请求转发到后端做处理。由上面对nginx的架构进行解析知道，nginx常采用多进程的工作方式，nginx启动后，会有一个master进程和多个worker进程生成。master进程主要用来管理worker进程，而worker进程主要就是用来处理请求的，多个worker进程之间是对等的，他们同等竞争客户端的请求，相互之间是独立的。 每个worker进程中只有一个主线程，且采用异步非阻塞的事件处理机制来处理请求。假设机制处理采用的是epoll，主线程就会循环epoll，如果发现事件已经准备好了，就进行处理，否则再次放入epoll，继续循环。epoll中的请求数就是worker进程的连接数。因此就可处理大量的并发了。要注意，worker同一时间只能处理一个请求（因为只有一个主线程），会来回进行请求切换处理。但是，请求间的切换是没有代价的，切换也是因为异步事件未准备好，而主动让出的。 工作原理要点：1. nginx采用多进程的工作方式，进程模型是master-worker形式。（注：常采用多进程方式，也有多线程，单进程方式）2. worker处理请求时采用异步非阻塞的处理方式。因此可支持高并发。3. 进程间、线程间是没有上下文切换，仅仅只是请求切换，是没有任何代价的。（注：进程与进程间是相互独立的；每个请求仅且只可在一个worker进程中完成；worker进程中只包含一个主线程；请求间的切换是通过循环事件来实现的。） 四、Nginx相关问题1、nginx为啥性能高？（1）nginx采用的是多进程模型首先，对于每个worker进程来说，独立的进程，不需要加锁，所以省掉了锁带来的开销，同时在编程以及问题查找时，也会方便很多。其次，采用独立的进程，可以让相互之间不会影响，一个进程退出之后，其它进程还在工作，服务不会中断，master进程则很快启动新的worker进程。当然，worker进程的异常退出，肯定是程序出BUG了，异常退出，会导致当前的worker上的所有请求失败，不过不会影响到所有请求，所以降低了风险。（2）nginx采用多进程事件模型：异步非阻塞虽然nginx采用多worker的方式来处理请求，每个worker里面只有一个主线程，那能够处理的并发数很有限啊，多少个worker就能处理多少个并发，何来高并发呢？非也，这就是nginx的高明之处，nginx采用了异步非阻塞的方式来处理请求，也就是说，nginx是可以同时处理成千上万个请求的。一个worker进程可以同时处理的请求数只受限于内存大小，而且在架构设计上，不同的worker进程之间处理并发请求时几乎没有同步锁的限制，worker进程通常不会进入睡眠状态，因此，当Nginx上的进程数与CPU核心数相等时（最好每一个worker进程都绑定特定的CPU核心），进程间切换的代价是最小的。 而apache的常用工作方式（apache也有异步非阻塞版本，但因其与自带某些模块冲突，所以不常用），每个进程在一个时刻只处理一个请求，因此，当并发数上到几千时，就同时有几千的进程在处理请求了。这对操作系统来说，是个不小的挑战，进程带来的内存占用非常大，进程的上下文切换带来的cpu开销很大，自然性能就上不去了，而这些开销完全是没有意义的。]]></content>
      <categories>
        <category>服务器</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试 - 常见算法试题总结（PHP）]]></title>
    <url>%2F2018%2F04%2F08%2F%E9%9D%A2%E8%AF%95-%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88PHP%EF%BC%89%2F</url>
    <content type="text"><![CDATA[试题目录：1、斐波那契数列实现2、楼梯有s阶台阶,上楼时可以一步上1阶,也可以一步上2阶，编程计算共有多少种不同的走法。3、输入两个整数n和m，从数列1，2…….n中随意取几个数，使其和等于m，要求将其中所有的可能组合列出来。4、求一个数组的最长递减子序列 比如{9，4，3，2，5，4，3，2}的最长递减子序列为{9，5，4，3，2}。5、一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。要求时间复杂度是O(n)，空间复杂度是O(1)。6、输入一个英文句子，翻转句子中单词的顺序，但单词内字符的顺序不变。句子中单词以空格符隔开。为简单起见，标点符号和普通字母一样处理。例如输入“I am a student.”，则输出“student. a am I”。 递归算法思想：1.递归过程一般通过函数或子过程实现；2.递归算法在函数或子过程的内部，直接或间接调用自己的算法3.递归算法实际上是把问题转化为规模缩小了的同类问题的子问题，然后再递归调用函数或过程来表示问题的解 注意：必须有一个明确的递归结束条件；如果递归次数过多，容易造成栈溢出。 斐波那契数列问题描述：斐波那契数列： 1 1 2 3 5 8 13 21 34 55 …概念：前两个值都为1，该数列从第三位开始，每一位都是当前位前两位的和规律公式为： Fn = F(n-1) + F(n+1) F：指当前这个数列 n：指数列的下标 12345678910111213141516171819202122232425/* 非递归实现 */function fbnq($n)&#123; if($n&lt;=0)&#123; return 0; &#125; $arr[1] = $arr[2] = 1; for($i=3; $i&lt;=$n; $i++)&#123; $arr[$i] = $arr[$i-1] + $arr[$i-2]; &#125; return $arr;&#125;// echo &quot;&lt;pre&gt;&quot;;// var_dump(fbnq(10));/* 递归实现 */function fbnq_2($n)&#123; if($n &lt;= 0) return 0; static $arr; $arr[1] = $arr[2] = 1; if($n &gt; 3)&#123; fbnq_2($n-1); &#125; $arr[$n] = $arr[$n-1] + $arr[$n-2]; return $arr;&#125;// var_dump(fbnq_2(10)); 上台阶问题问题描述：楼梯有s阶台阶,上楼时可以一步上1阶,也可以一步上2阶，编程计算共有多少种不同的走法。 解题思路：此问题可分解为 f(n-1) + f(n-2) 种不同的走法。且结束条件为 小于等于 2（最多一步上2阶） 12345678910function getStepSum($s)&#123; if($s &lt;= 0)&#123; return 0; &#125;else if($s &lt;= 2)&#123; return $s; &#125;else&#123; return getStepSum($s-1)+getStepSum($s-2); &#125;&#125;// var_dump(getStepSum(3)); 求数和问题问题描述：输入两个整数n和m，从数列1，2…….n中随意取几个数，使其和等于m，要求将其中所有的可能组合列出来。 解题思路：输入sum和n，要求输出1,2…n里所有和为sum的组合这是一个可划分子问题问题若用f(sum, n)表示问题的界，则元素组合共有两种情况 和为sum的组合里包括n，则f(sum, n) = f(sum - n, n - 1) 和为sum的组合里不包括n,则 f(sum, n) = f(sum, n - 1)所以 f(sum, n) = f(sum - n, n - 1) U f(sum, n - 1) 注：需要传入一个空数组来接收数，用于输出 1234567891011121314151617181920212223242526272829303132function combination($sum, $n, $comb) &#123; if ($n &lt; 0 || $sum &lt; 0) &#123; #当n &lt; 0或者sum &lt; 0都是不符合条件的结果 // print_r($comb); // echo &quot;sum: &#123;$sum&#125; n: &#123;$n&#125;&lt;br&gt;&quot;; return; &#125; if ($sum &lt; $n) &#123; combination($sum, $sum, $comb); #sum &lt; n，则不可能需要比sum大的数来构成组合 return; &#125; #结果求得 if ($sum == 0) &#123; #输出元素组合 echo &quot;Combination: &quot;; foreach ($comb as $val) &#123; echo $val . &quot; &quot;; &#125; echo &quot;&lt;br&gt;&quot;; return; &#125; #组合里包含n的情况 $comb[] = $n; combination($sum - $n, $n - 1, $comb); #组合里不包含n的情况 array_pop($comb); combination($sum, $n - 1, $comb);&#125; 最长递减子序列问题问题描述：求一个数组的最长递减子序列 比如{9，4，3，2，5，4，3，2}的最长递减子序列为{9，5，4，3，2}。 解题思路：此问题就是比较大小的问题，把列表看成栈比较容易些（且是有序的栈）。与栈头比较大小，满足条件入栈，否则出栈。注：需要判空数组的问题。 123456789101112131415161718192021222324// $arr = [9, 4, 3, 2, 5, 4, 3, 1];$arr = [9, 4, 3, 2, 5, 4, 3, 1, 10, 8, 5, 3, 2, 1];function maxDescList($arr)&#123; $len = count($arr); $list[] = $arr[0]; for($i=1; $i&lt;$len; $i++)&#123; $llen = count($list); for($j=$llen-1; $j&gt;=0; $j--)&#123; if($arr[$i] &gt; $list[$j])&#123; array_pop($list); if(empty($list))&#123; $list[] = $arr[$i]; break; &#125; &#125;else&#123; array_push($list, $arr[$i]); break; &#125; &#125; &#125; return $list;&#125;// echo &quot;&lt;pre&gt;&quot;;// var_dump(maxDescList($arr)); 找重复出现的数字问题描述：一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。要求时间复杂度是O(n)，空间复杂度是O(1)。 解题思路：将整型数组看成栈比较容易理解。每次出栈一个元素，然后查询该值是否存在数组中。如果存在，说明数组该值有两个，然后删除该值（原因是如果不删除，出栈元素会再次重复一次，再出现就会出现错误。因为该值已经查询过）。 123456789101112131415$arr = [8, 5, 3, 4, 2, 4, 3, 5, 9, 8];function searchNum($arr)&#123; $list = array(); while(!empty($arr))&#123; $val = array_pop($arr); if(!in_array($val, $arr))&#123; $list[] = $val; &#125;else&#123; $key = array_search($val, $arr); unset($arr[$key]); &#125; &#125; return $list;&#125;// var_dump(searchNum($arr)); 翻转字符串问题描述：输入一个英文句子，翻转句子中单词的顺序，但单词内字符的顺序不变。句子中单词以空格符隔开。为简单起见，标点符号和普通字母一样处理。例如输入“I am a student.”，则输出“student. a am I”。 解题思路：此题利用PHP函数很容易实现。注：要注意使用 array_reverse() 数组翻转函数 1234567function strrevStr($str)&#123; $str_arr = explode(&quot; &quot;, $str); $reverse_arr = array_reverse($str_arr); return implode(&quot; &quot;, $reverse_arr);&#125;// $str = &quot;I am a student.&quot;;// echo strrevStr($str); 问题描述：翻转字符串，但是不能使用PHP内部函数，只能使用 strlen 。 解题思路：可使用循环解决。 123456789101112131415161718function myStrrev($str)&#123; $len = strlen($str); $new_str = &apos;&apos;; $n = &apos;&apos;; for($i=0; $i&lt;$len-1; $i++)&#123; if($str[$i] != &apos; &apos;)&#123; $n .= $str[$i]; &#125;else&#123; $new_str = $n.&apos; &apos;.$new_str; $n = &apos;&apos;; &#125; &#125; $new_str = $n.&apos; &apos;.$new_str; return trim($new_str);&#125;$str = &quot;your name is benwu&quot;;echo myStrrev($str);]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试 - 常见函数试题总结（PHP）]]></title>
    <url>%2F2018%2F04%2F03%2F%E9%9D%A2%E8%AF%95-%E5%B8%B8%E8%A7%81%E5%87%BD%E6%95%B0%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88PHP%EF%BC%89%2F</url>
    <content type="text"><![CDATA[试题目录：1、写一个能创建多级目录的PHP函数2、请写一段代码，确保多个进程同时写入一个文件成功3、写一个函数，尽可能高效的，从一个标准url里取出文件的扩展名4、写一个函数，能够遍历一个文件夹下的所有文件和子文件夹。5、无限级分类 1、写一个能创建多级目录的PHP函数12345678function create_multidir($path, $mode=0777)&#123; if(is_dir($path))&#123; return &quot;目录已经存在！&quot;; &#125;else if(mkdir($path, $mode, true))&#123; return true; &#125; return false;&#125; 2、请写一段代码，确保多个进程同时写入一个文件成功123456789$fp = fopen($file, &apos;w+&apos;);if(flock($fp, LOCK_EX))&#123; //执行逻辑 fwrite($fp, &quot;content&quot;); flock($fp, LOCK_UN);&#125;else&#123; echo &quot;服务器繁忙...&quot;;&#125;fclose($fp); 3、写一个函数，尽可能高效的，从一个标准url里取出文件的扩展名例如:http://www.sina.com.cn/abc/de/fg.php?id=1需要取出php或.php12345678function getFileExtension($url)&#123; $url_arr = parse_url($url); $filename = basename($url_arr[&apos;path&apos;]); $file_arr = explode(&apos;.&apos;, $filename); return $file_arr[1];&#125;其中，最后获取扩展名，还可使用字符串截取函数。return strstr($filename, &apos;.&apos;); 4、写一个函数，能够遍历一个文件夹下的所有文件和子文件夹。1234567891011121314151617function getAllDirAndFile($path)&#123; $files = array(); if(is_dir($path))&#123; $handle = opendir($path); while(($file=readdir($handle)) !== false)&#123; if($file != &apos;.&apos; &amp;&amp; $file != &apos;..&apos;)&#123; if(is_dir($path.&apos;/&apos;.$file))&#123; $files[$file] = getAllDirAndFile($path.&apos;/&apos;.$file); &#125;else&#123; $files[] = $file; &#125; &#125; &#125; closedir($handle); &#125; return $files;&#125; 5、无限级分类创建类别表如下：12345CREATE TABLE category(cat_id smallint unsigned not null auto_increment primary key comment&apos;类别ID&apos;,cat_name VARCHAR(30)NOT NULL DEFAULT&apos;&apos;COMMENT&apos;类别名称&apos;,parent_id SMALLINT UNSIGNED NOT NULL DEFAULT 0 COMMENT&apos;类别父ID&apos;)engine=MyISAM charset=utf8; 编写一个函数，递归遍历，实现无限分类。 代码实现：（结果显示层级分明）12345678910function getAllCategory($arr, $pid=0)&#123; $categories = array(); foreach($arr as $val)&#123; if($val[&apos;parent_id&apos;] == $pid)&#123; $categories[$val[&apos;cat_id&apos;]] = $val; $categories[$val[&apos;cat_id&apos;]][&apos;childs&apos;] = getAllCategory($arr, $val[&apos;cat_id&apos;]); &#125; &#125; return $categories;&#125; 代码实现：（结果显示在一个层级，通过level标明层级）1234567891011function getAllCategory($arr, $pid=0, $level)&#123; static $categories = array(); foreach($arr as $val)&#123; if($val[&apos;parent_id&apos;] == $pid)&#123; $categories[] = $val; $categories[&apos;level&apos;] = $level; getAllCategory($arr, $val[&apos;cat_id&apos;], $level++); &#125; &#125; return $categories;&#125;]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典算法问题 - 排序算法与二分查找法（PHP实现）]]></title>
    <url>%2F2018%2F04%2F03%2F%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E9%97%AE%E9%A2%98-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E4%B8%8E%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E6%B3%95%EF%BC%88PHP%E5%AE%9E%E7%8E%B0%EF%BC%89%2F</url>
    <content type="text"><![CDATA[排序算法测试数组：$arr = [3,5,7,1,8,4,9,6,2]; 1、冒泡排序冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越大的元素会经由交换慢慢“浮”到数列的顶端。算法描述 比较相邻的元素。如果第一个比第二个大，就交换它们两个； 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； 针对所有的元素重复以上的步骤，除了最后一个； 重复步骤1~3，直到排序完成。 代码实现：123456789101112131415function bubbleSort($arr)&#123; if(!is_array($arr)) return false; $len = count($arr); if($len &lt;= 1) return $arr; for($i=0; $i&lt;$len; $i++)&#123; for($j=1; $j&lt;$len-$i; $j++)&#123; if($arr[$j-1] &gt; $arr[$j])&#123; $tmp = $arr[$j-1]; $arr[$j-1] = $arr[$j]; $arr[j] = $tmp; &#125; &#125; &#125; return $arr;&#125; 2、选择排序选择排序(Selection-sort)是一种简单直观的排序算法。它的工作原理：首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 算法描述n个记录的直接选择排序可经过n-1趟直接选择排序得到有序结果。具体算法描述如下： 初始状态：无序区为R[1..n]，有序区为空； 第i趟排序(i=1,2,3…n-1)开始时，当前有序区和无序区分别为R[1..i-1]和R(i..n）。该趟排序从当前无序区中-选出关键字最小的记录 R[k]，将它与无序区的第1个记录R交换，使R[1..i]和R[i+1..n)分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区； n-1趟结束，数组有序化了。 代码实现：12345678910111213141516171819function selectSort($arr)&#123; if(!is_array($arr)) return false; $len = count($arr); if($len &lt;= 1) return $arr; for($i=0; $i&lt;$len; $i++)&#123; $k = $i; for($j=$i+1; $j&lt;$len; $j++)&#123; if($arr[$k] &gt; $arr[$j])&#123; $k = $j; &#125; &#125; if($k != $i)&#123; $tmp = $arr[$k]; $arr[$k] = $arr[$i]; $arr[$i] = $tmp; &#125; &#125; return $arr;&#125; 3、插入排序插入排序（Insertion-Sort）的算法描述是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 算法描述一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下： 从第一个元素开始，该元素可以认为已经被排序； 取出下一个元素，在已经排序的元素序列中从后向前扫描； 如果该元素（已排序）大于新元素，将该元素移到下一位置； 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置； 将新元素插入到该位置后； 重复步骤2~5。 代码实现：1234567891011121314151617function insertSort($arr)&#123; if(!is_array($arr)) return false; $len = count($arr); if($len &lt;= 1) return $arr; for($i=1; $i&lt;$len; $i++)&#123; $tmp = $arr[$i]; for($j=$i-1; $j&gt;=0; $j--)&#123; if($tmp &lt; $arr[$j])&#123; $arr[$j+1] = $arr[$j]; $arr[$j] = $tmp; &#125;else&#123; break; &#125; &#125; &#125; return $arr;&#125; 4、快速排序快速排序的基本思想：通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。 算法描述快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。具体算法描述如下： 从数列中挑出一个元素，称为 “基准”（pivot）； 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。 代码实现：12345678910111213141516function quickSort($arr)&#123; if(!is_array($arr)) return false; $len = count($arr); if($len &lt;= 1) return $arr; $left = $right = array(); for($i=1; $i&lt;$len; $i++)&#123; if($arr[$i] &gt; $arr[0])&#123; $right[] = $arr[$i]; &#125;else&#123; $left[] = $arr[$i]; &#125; &#125; $left = quickSort($left); $right = quickSort($right); return array_merge($left, array($arr[0]), $right);&#125; 排序参考文章：https://www.cnblogs.com/onepixel/articles/7674659.html 二分查找法算法描述 二分查找又称折半查找，它是一种效率较高的查找方法。 二分查找要求：（1）必须采用顺序存储结构（2）.必须按关键字大小有序排列 原理：将数组分为三部分，依次是中值（所谓的中值就是数组中间位置的那个值）前，中值，中值后；将要查找的值和数组的中值进行比较，若小于中值则在中值前 面找，若大于中值则在中值后面找，等于中值时直接返回。然后依次是一个递归过程，将前半部分或者后半部分继续分解为三部分。 实现：二分查找的实现用递归和循环两种方式 （1）代码实现：（非递归实现）123456789101112131415function binarySearch1($arr, $searchV)&#123; $high = count($arr); $low = 0; while($low &lt;= $high)&#123; $middle = floor(($high+$low)/2); if($arr[$middle] &gt; $searchV)&#123; $high = $middle - 1; &#125;else if($arr[$middle] &lt; $searchV)&#123; $low = $middle +1; &#125;else&#123; return $middle; &#125; &#125; return false;&#125; （2）代码实现：（递归实现）12345678910111213function binarySearch2($arr, $searchV, $low, $high)&#123; if($low &lt;= $high)&#123; $middle = floor(($low+$high)/2); if($arr[$middle] &gt; $searchV)&#123; return binarySearch2($arr, $searchV, $low, $middle-1); &#125;else if($arr[$middle] &lt; $searchV)&#123; return binarySearch2($arr, $searchV, $middle+1, $high); &#125;else&#123; return $middle; &#125; &#125; return false;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典算法问题 - 最大连续子数列和与最大连续子数列乘积（PHP实现）]]></title>
    <url>%2F2018%2F04%2F03%2F%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E9%97%AE%E9%A2%98-%E6%9C%80%E5%A4%A7%E8%BF%9E%E7%BB%AD%E5%AD%90%E6%95%B0%E5%88%97%E5%92%8C%E4%B8%8E%E6%9C%80%E5%A4%A7%E8%BF%9E%E7%BB%AD%E5%AD%90%E6%95%B0%E5%88%97%E4%B9%98%E7%A7%AF%EF%BC%88PHP%E5%AE%9E%E7%8E%B0%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、最大连续子数列和问题给出测试数列：$arr = [-2, 6, -1, 5, 4, -7, 2, 3]; 1、穷举法 暴力求解法（穷举法）：暴力求解也是容易理解的做法，简单来说，我们只要用两层循环枚举起点和终点，这样就尝试了所有的子序列，然后计算每个子序列的和，然后找到其中最大的即可我们可以采用一个两重的循环，假设两个循环的循环变量分别为i, j。第一层循环从第一个元素遍历到后面，第二个元素从&gt;=第一个元素的位置开始到最后。这样就可以遍历到所有的点。然后，我们取所有从i到j的连续数组部分和再比较。这样最终就可以得到最大的连续和以及最大子序列的起始与结束点。 代码实现：12345678910111213141516171819function maxSubSum1($arr)&#123; $len = count($arr); $maxSum = 0; for($i=0; $i&lt;$len; $i++)&#123; for($j=$i; $j&lt;$len; $j++)&#123; $thisSum = 0; for($k=$i; $k&lt;=$j; $k++)&#123; $thisSum += $arr[$k]; &#125; if($thisSum &gt; $maxSum)&#123; $maxSum = $thisSum; $seqStart = $i; $seqEnd = $j; &#125; &#125; &#125; $length = $seqEnd-$seqStart+1; return array_slice($arr, $seqStart, $length);&#125; 2、优化穷举法 一个简单的优化：前面那个最简单暴力的方法虽然看起来能解决问题，但是循环遍历的次数太多了。里面还是有不少可以改进的空间。比如说，每次我们用变量k遍历i到j的时候，都要计算求和。实际上当每次j增加1时，k把前面计算的结果在循环里又计算了一遍。这是完全没必要的，完全可以重复利用前面的计算结果。 代码实现：1234567891011121314151617function maxSubSum2($arr)&#123; $len = count($arr); $maxSum = 0; for($i=1; $i&lt;$len; $i++)&#123; $thisSum = 0; for($j=$i; $j&lt;$len; $j++)&#123; $thisSum += $arr[$j]; if($thisSum &gt; $maxSum)&#123; $maxSum = $thisSum; $seqStart = $i; $seqEnd = $j; &#125; &#125; &#125; $length = $seqEnd-$seqStart+1; return array_slice($arr, $seqStart, $length);&#125; 3、线性算法 这个问题还是存在着一个线性时间复杂度的解法。需要我们对数组的序列进行进一步的分析。我们在数组中间找到的连续子序列，可能存在和为负的序列。如果需要找到一个最大的子数组的话，肯定该序列不是在最大子序列当中的。我们可以通过反证的方式来证明。假设数组A[i…j]，里面的元素和为负。如果A[i….j]在一个最大子序列的数组中间，假定为A[i…k]，k &gt; j。那么既然从i到j这一段是负的，我把这一段去掉剩下的部分完全比我们假定的这个最大子序列还要大。这就和我的假设矛盾了。这个假设还有一个限制，就是该数组就是从i开头的。否则有人可能会这么问，如果我A[i…j]这一部分确实是一个负数，但是在A[i]前面是一个很大的正数，使得他们的和为正数。那不就使得我们的结果不成立了么？如果我们是从某个数组的开头i开始的话，就不存在这个情况。结合前面的讨论，我们就可以发现一个有意思的事情，就是假设我们从数组的开头A[0]开始，不断的往后面走，每一步判断是否当前和最大，并保存结果。当发现当前字串和为负数的时候，我们可以直接跳过。假设当前的索引为i的话，从0到i这一段的和是负数，可以排除。然后再从当前元素的后面开始找。这样可以得到最终最大子串和以及串的起点和终点。 代码实现：123456789101112131415161718function maxSubSum3($arr)&#123; $len = count($arr); $maxSum = 0; $thisSum = 0; for($i=0, $j=0; $j&lt;$len; $j++)&#123; $thisSum += $arr[$j]; if($thisSum &gt; $maxSum)&#123; $maxSum = $thisSum; $seqStart = $i; $seqEnd = $j; &#125;else if($thisSum &lt; 0)&#123; $i = $j+1; $thisSum = 0; &#125; &#125; $length = $seqEnd-$seqStart+1; return array_slice($arr, $seqStart, $length);&#125; 二、最大连续子数列乘积问题给出测试数列：$arr = [2, 3, 0, -3, 3, -1, 0, 9]; 1、穷举法1234567891011121314151617function maxSubProduct1($arr)&#123; $len = count($arr); $maxPro = 0; for($i=0; $i&lt;$len; $i++)&#123; $thisPro = 1; for($j=$i; $j&lt;$len; $j++)&#123; $thisPro *= $arr[$j]; if($thisPro &gt; $maxPro)&#123; $maxPro = $thisPro; $seqStart = $i; $seqEnd = $j; &#125; &#125; &#125; $length = $seqEnd-$seqStart+1; return array_slice($arr, $seqStart, $length);&#125; 2、线性算法123456789101112131415161718function maxSubProduct2($arr)&#123; $len = count($arr); $maxPro = 0; $thisPro = 1; for($i=0, $j=0; $j&lt;$len; $j++)&#123; $thisPro *= $arr[$j]; if($thisPro &gt; $maxPro)&#123; $maxPro = $thisPro; $seqStart = $i; $seqEnd = $j; &#125;else if($thisPro == 0)&#123; $thisPro = 1; $i = $j+1; &#125; &#125; $length = $seqEnd-$seqStart+1; return array_slice($arr, $seqStart, $length);&#125; 总结：通过对“最大连续子数列和与最大连续子数列乘积”的算法实现，可发现共通之处，即算法原理都是一样的。最简单、最暴力的解决方法就是穷举法，同时，算法时间复杂度也是最差的。现实中使用必须要优化，否则特别影响性能。因此，就出现了线性算法，此算法时间复杂度只有 O(n)，极大的提高了性能。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试总结（一）]]></title>
    <url>%2F2018%2F04%2F02%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[上周面试了一个公司，下面做一下总结。 一、算法1、斐波那契数列实现12345678斐波那契数列： 1 1 2 3 5 8 13 21 34 55 …概念： 前两个值都为1，该数列从第三位开始，每一位都是当前位前两位的和 规律公式为： Fn = F(n-1) + F(n-2) F：指当前这个数列 n：指数列的下标 PHP代码实现：12345678910111213141516171819202122//非递归function fbnq($n)&#123; if($n&lt;=0)&#123; return 0; &#125; $arr[1] = $arr[2] = 1; for($i=3; $i&lt;=$n; $i++)&#123; $arr[$i] = $arr[$i-1] + $arr[$i-2]; &#125; return $arr;&#125;//递归实现function fbnq($n)&#123; if($n &lt;= 0) return 0; static $arr; $arr[1] = $arr[2] = 1; if($n &gt; 3)&#123; fbnq($n-1); &#125; $arr[$n] = $arr[$n-1] + $arr[$n-2]; return $arr;&#125; 2、最大连续子数列乘积问题。123给出一个数列为：$arr = [2, 3, 0, -3, 3, -1, 0, 9];求出最大连续子数列乘积。结果为：[-3, 3, -1] PHP代码实现：123456789101112131415161718function maxSubProduct($arr)&#123; $len = count($arr); $maxPro = 0; $thisPro = 1; for($i=0, $j=0; $j&lt;$len; $j++)&#123; $thisPro *= $arr[$j]; if($thisPro &gt; $maxPro)&#123; $maxPro = $thisPro; $seqStart = $i; $seqEnd = $j; &#125;else if($thisPro == 0)&#123; $thisPro = 1; $i = $j+1; &#125; &#125; $length = $seqEnd-$seqStart+1; return array_slice($arr, $seqStart, $length);&#125; 二、PHP基础1、比较empty()、isset()两个函数的区别，对应给出如下示例：1234567891011$a = 0;$b = &apos;0&apos;;$c = null;$d = &apos; &apos;;var_dump(empty($a)); //1var_dump(empty($b)); //1var_dump(empty($c)); //1var_dump(isset($c)); //0var_dump(isset($d)); //1var_dump($a == $b); //1var_dump($a == $c); //1 相关知识点介绍： bool isset ( mixed $var [, mixed $... ] ) — 检测变量是否已设置并且不是 NULL。返回值：如果 var 存在并且值不是 NULL 则返回 TRUE，否则返回 FALSE。（注：isset() 只能用于变量，因为传递任何其它参数都将造成解析错误。若想检测常量是否已设置，可使用 defined() 函数。因为是一个语言构造器而不是一个函数，不能被 可变函数 调用。）bool empty ( mixed $var ) — 检查一个变量是否为空判断一个变量是否被认为是空的。当一个变量并不存在，或者它的值等同于FALSE，那么它会被认为不存在。如果变量不存在的话，empty()并不会产生警告。返回值：当var存在，并且是一个非空非零的值时返回 FALSE 否则返回 TRUE. 以下的东西被认为是空的：“” (空字符串)0 (作为整数的0)0.0 (作为浮点数的0)“0” (作为字符串的0)NULLFALSEarray() (一个空数组)$var; (一个声明了，但是没有值的变量)（注：因为是一个语言构造器而不是一个函数，不能被 可变函数 调用。） 2、数组方面，给出如下：写出对应输出的结果，包括key=&gt;value值。1234567891011$a = array( 2, 1, &apos;3&apos;=&gt;2 );$b = array( 2=&gt;1, 4=&gt;2, 5=&gt;4 );foreach( $a as &amp;$k )&#123; $k++;&#125;foreach( $b as $k )&#123; $k++;&#125;var_dump($a); //0=&gt;3, 1=&gt;2, 3=&gt;5var_dump($b); //2=&gt;1, 4=&gt;2, 5=&gt;3var_dump($k); //5 答案解析： 根据程序自上到下的执行顺序，可知道首先遍历$a数组，且要注意遍历只有值，没有key，仅仅只是名字而已。此处，我就看错了。然后遍历$b数组，最后输出。首先要知道的一点是：$a数组将值传引用了，因此后面对$k的值更改则会更改对应的值。所以，$a的值相对应的都会+1，则其值为 0=&gt;3,1=&gt;2,3=&gt;3。下面，又再次对$b数组进行遍历，仅仅只是值传递。所以，对于$b数组未做改变，原样输出：2=&gt;1,4=&gt;2,5=&gt;4。虽然$b数组遍历未对数值的值进行改变，但是，对$k的值进行更改了。因此，$k的值再经过$b数组的遍历后为5，要注意，$k是传引用的，与$a数组中的最后一个值是引用关系。因此，$a数组中最后一个值现在变为5。且$k的值也是5。 相关知识点介绍：此处，考的知识点比较多一些，比较容易混淆且不被发现。首先是遍历的时候，命名问题容易让人产生是$key，没有值的幻觉，因此要注意看（面试官提醒了我好几次，最后还是看错了）。再次，就是传值与传引用的知识点运用。其中涉及到变量作用域的问题，此处我也考虑错了，理解成局部变量了。下面是变量作用域的总结： PHP中变量的作用域可以分为:超全局变量：在一个脚本的任何作用域里都可以被访问,可直接在局部范围里使用,不需要用global声明。比如$GLOBALS,$_ENV,$_SERVER,$_GET,$_POST,$_FILES,$_SESSION,$_COOKIE等.全局变量：声明的变量不在class,function等语言结构内部.如果要在class,function等内部使用全局变量,需要用关键词global或者超全局变量$GLOBALS.局部变量：在class,function等结构语句内部声明的变量.静态变量：在function中使用关键词static声明的变量,静态变量的值保留直至当前请求的脚本运行结束,比如可以用来保存数据库连接对象. 三、Linux命令给你一段文本文件，例如：file.txt102 baidu 100 5000101 google 110 5000104 sohu 100 4500103 guge 50 3000下面是需要执行的命令需求：1、输出前两行2、输出第3行3、输出第1列4、输出全部并且以第一列从小到大排序5、回到家目录，再回到上次目录下面是对上面命令的解答：123451、head -2 file.txt2、sed -n &apos;3p&apos; file.txt 或 cat file.txt | head -3 | tail -13、awk &apos;&#123;print $1&#125;&apos; file.txt4、sort -n -t &apos; &apos; -k 1 file.txt5、cd ~ ; cd - 四、操作系统方面1、进程间通信方式以及相对应的原理通信方式有：管道、信号、共享内存原理：管道：信号：共享内存： 2、操作系统中有个重复函数加载什么的3、I/O多路复用技术五、计算机网络1、计算机网络OSI七层协议模型、TCP/IP五层协议模型OSI七层协议模型包括：应用层、表示层、会话层、传输层、网络层、数据链路层、物理层。TCP/IP五层协议模型包括：应用层、传输层、网络层、数据链路层、物理层。以TCP/IP五层协议模型来说明每层作用及相关协议：应用层：对编程人员开发的接口，用于软件开发。主要协议包括：HTTP、SMTP、DNS等传输层：接受应用层的数据包，进行处理，并选择相应的协议进行转发到下层。主要协议包括：TCP、UDP等网络层：逻辑寻址，用于在网络中通过路由协议用IP:Port来进行子网定位。主要协议包括：路由协议、IP协议等数据链路层：物理寻址，用于在子网中通过交换机根据Mac地址来进行定位主机；同时，将数据进行处理，转换为位流。物理层：通过物理媒介，传输数据流。 2、TCP/UDP的区别及特点TCP：面向连接的、安全的协议。UDP：无连接的、不安全的协议。 3、HTTP相关知识点HTTP：超文本传输协议。是一个面向连接的（底层采用的是TCP协议）、无状态的协议。 六、原理方面1、web服务的工作原理WEB服务器也称为WWW(WORLD WIDE WEB)服务器，主要功能是提供网上信息浏览服务。通俗的说，Web服务器是可以向发出请求的浏览器提供文档的程序。1、服务器是一种被动程序：只有当Internet上运行在其他计算机中的浏览器发出请求时，服务器才会响应。2、最常用的Web服务器是Apache和Microsoft的Internet信息服务器（Internet Information Services，IIS）。3、Internet上的服务器也称为Web服务器，是一台在Internet上具有独立IP地址的计算机，可以向Internet上的客户机提供WWW、Email和FTP等各种Internet服务。4、Web服务器是指驻留于因特网上某种类型计算机的程序。当Web浏览器（客户端）连到服务器上并请求文件时，服务器将处理该请求并将文件反馈到该浏览器上，附带的信息会告诉浏览器如何查看该文件（即文件类型）。服务器使用HTTP（超文本传输协议）与客户机浏览器进行信息交流，这就是人们常把它们称为HTTP服务器的原因。Web服务器不仅能够存储信息，还能在用户通过Web浏览器提供的信息的基础上运行脚本和程序。 采用的协议：1、应用层使用HTTP协议。2、HTML（标准通用标记语言下的一个应用）文档格式。3、浏览器统一资源定位器（URL）。4、为了解决HTTP协议的这一缺陷，需要使用另一种协议：安全套接字层超文本传输协议HTTPS。为了数据传输的安全，HTTPS在HTTP的基础上加入了SSL协议，SSL依靠证书来验证服务器的身份，并为浏览器和服务器之间的通信加密。 Web服务的工作原理：Web服务器的工作原理并不复杂，一般可分成如下4个步骤：连接过程、请求过程、应答过程以及关闭连接。连接过程：通过三次握手建立连接。（客户端向服务器发送请求报文SYN；服务器接受请求连接，并向客户端发送ACK确认；客户端接受ACK确认并返回给服务器，服务器接受到客户端返回的ACK确认后建立起连接。）请求过程客户端与服务端建立连接后，客户端即可向服务端发送请求。（如果是持久连接，一次连接可多次请求。非持久连接，则一次连接只可请求一次。）应答过程客户端将请求发送到服务端，服务端接受链接，并进行处理。然后把处理结果再返回给客户端。关闭连接最后，通过四次挥手断开连接。（客户端向服务器发送断开连接请求报文FIN；服务器接受断开连接，并向客户端发送ACK确认报文，同时，待服务器端数据处理完后返回FIN关闭连接；客户端接受到服务端的确认报文ACK，待接收到服务端的关闭连接报文FIN后，返回ACK到服务端确认关闭连接。） 使用最多的Web Server服务器软件有：IIS、Apache、nginx。 为何使用三次握手机制：假设如下异常情况：客户端向服务器发送了第一条请求报文，但是该报文并未在网络中被丢弃，而是长时间阻滞在某处，而客户端收不到服务器确认，以为该报文丢失，于是重新发送该报文，这次的报文成功到达服务器，如果不使用三次握手，则服务器只需对该报文发出确认，就建立了一个连接。而在这个连接建立，并释放后，第一次发送的，阻滞在网络中的报文到达了服务器，服务器以为是客户端又重新发送了一个连接请求（实际上在客户端那里，该连接早已失效），就又向客户端发送一个确认，但客户端认为他没有发送该请求报文，因此不理睬服务器发送的确认，而服务器以为又建立了一个新的连接，于是一直等待A发来数据，造成了服务器资源的浪费，并且会产生安全隐患。因此，若使用三次握手机制，服务器发送了该确认后，收不到客户端的确认，也就知道并没有建立连接，因此不会将资源浪费在这种没有意义的等待上。 2、nginx的工作原理nginx类似是一个代理服务器，当浏览器请求的是一些静态资源，则直接返回；若接收的是动态请求，则通过配置文件直接将请求映射到对应的location block，而此location中所配置的各个指令则会启动不同的模块去完成工作，最后返回给浏览器。此处，处理动态请求的PHP文件，一般是PHP-FPM。 七、其它知识点1、session和cookie的区别首先，要知道session和cookie是怎么产生的。当客户端访问服务端的时候，服务器会对应生成一个唯一的SESSIONID，同数据一起返回给客户端，当客户端再次访问的时候，即可带着这个SESSIONID，服务端就可根据SESSIONID识别用户。HTTP是一个无状态的协议（即同一个会话的连续两次请求是互相不了解的，如果需要前面的信息，必须重传），session和cookie是会话层保持会话的一个措施。（1）因此，首先一个区别就是session保存在服务端，cookie保存在客户端。cookie保存在浏览器中，如果没有设置过期时间，则关闭浏览器的时候，cookie就消失了，相对应的SESSIONID也消失了。当设置过期时间的时候，cookie就保存在客户端硬盘上，当下次再访问网站的时候，可自动携带此cookie。（2）由于cookie保存在客户端，因此cookie是不安全的，容易被人给copy，伪造cookie登录。session保存在服务端，相对来说安全一些。假如浏览器禁止cookie，那应该怎么保持会话？① 可通过重写URL，即通过GET方法附带SESSIONID的方式传送到服务端。② 如果是表单，可通过隐藏表单传送SESSIONID。（3）由于session是保存在服务端，因此session的增加会导致服务器数据量增加，应定时清理无效数据。（4）cookie保存在客户端，但是也不能随意大批量使用，因为cookie的增加会增加数据传输量，导致带宽的增加。 2、堆区与栈区介绍内存分配有三种：静态存储区、堆区和栈区。他们的功能不同，对他们使用方式也就不同。静态存储区：内存在程序编译的时候就已经分配好，这块内存在程序的整个运行期间都存在。它主要存放静态数据、全局数据和常量。栈区：在执行函数时，函数（包括main函数）内局部变量的存储单元都可以在栈上创建，函数执行结束时这些存储单元自动被释放。栈内存分配运算内置于处理器的指令集中，效率很高，但是分配的内存容量有限。（任何变量都处于站区，例如int a[] = {1, 2},变量a处于栈区。数组的内容也存在于栈区。）堆区：亦称动态内存分配。程序在运行的时候用malloc或new申请任意大小的内存，程序员自己负责在适当的时候用free或delete释放内存。动态内存的生存期可以由我们决定，如果我们不释放内存，程序将在最后才释放掉动态内存。 但是，良好的编程习惯是：如果某动态内存不再使用，需要将其释放掉，并立即将指针置位NULL，防止产生野指针。]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令之sort]]></title>
    <url>%2F2018%2F03%2F30%2FLinux%E5%91%BD%E4%BB%A4%E4%B9%8Bsort%2F</url>
    <content type="text"><![CDATA[sort命令是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出。sort命令既可以从特定的文件，也可以从stdin中获取输入。 语法格式123456789[root@www ~]# sort [选项] [参数]选项：-n：依照数值的大小排序；-r：以相反的顺序来排序；-k：是指定需要爱排序的栏位；-t&lt;分隔字符&gt;：指定排序时所用的栏位分隔字符；-o&lt;输出文件&gt;：将排序后的结果存入制定的文件；（注：直接使用重定向会覆盖原文件，将其清空。）参数：文件：指定待排序的文件列表。 用法解析sort的工作原理sort将文件的每一行作为一个单位，相互比较，比较原则是从首字符向后，依次按ASCII码值进行比较，最后将他们按升序输出。123456789101112[root@www ~]$ cat seq.txtbananaapplepearorangepear[root@www ~]$ sort seq.txtapplebananaorangepearpear sort的-u选项它的作用很简单，就是在输出行中去除重复行。12345[root@www ~]$ sort -u seq.txtapplebananaorangepear sort的-r选项sort默认的排序方式是升序，如果想改成降序，就加个-r就搞定了。123456[root@www ~]$ sort -r seq.txtpearpearorangebananaapple sort的-n选项sort排序默认是采用ASCII码值进行比较来排序的，因此对比10与2的大小时，10就会比2小了，因为比较10与2的时候，真实比较的是1与2的大小，显然1小，所以就将10放在2的前面了。对于此类情况，我们只要使用-n选项，就限制sort使用数值来进行排序。1234567891011121314151617181920[root@www ~]$ cat number.txt11019112[root@www ~]$ sort number.txt110111925[root@www ~]$ sort -n number.txt125101119 sort的-o选项由于sort默认是把结果输出到标准输出，所以需要用重定向才能将结果写入文件，形如sort filename &gt; newfile。但是，如果你想把排序结果输出到原文件中，用重定向可就不行了。会将原文件覆盖，情况数据。对于这种情况，sort提供了-o选项解决了这个问题，可将结果写入原文件。1234567891011121314151617[root@www ~]$ sort -r number.txt &gt; number.txt[root@www ~]$ cat number.txt[root@www ~]$[root@www ~]$ cat number.txt11019112[root@www ~]$ sort -nr number.txt -o number.txt[root@www ~]$ cat number.txt191110521 sort的-t选项和-k选项 如果有一个文件的内容是这样：12345[root@www ~]$ cat seq.txtbanana:30:5.5apple:10:2.5pear:90:2.3orange:20:3.4 这个文件有三列，列与列之间用冒号隔开了，第一列表示水果类型，第二列表示水果数量，第三列表示水果价格。那么我想以水果数量来排序，也就是以第二列来排序，如何利用sort实现？ 此处就可使用sort提供的-t选项，指定以什么间隔符进行分割。然后，再使用-k选项指定对那一列或几列进行排序即可。12345[root@www ~]$ sort -n -k 2 -t : seq.txtapple:10:2.5orange:20:3.4banana:30:5.5pear:90:2.3 sort的-k选项进阶版 例如有一个文件内容是这样的：第一个域是公司名称，第二个域是公司人数，第三个域是员工平均工资。（仅作演示）12345[root@www ~]$ cat facebook.txtgoogle 110 5000baidu 100 5000guge 50 3000sohu 100 4500 （1）首先，按照公司人数排序，人数相同的按照员工平均工资的升序排序。12345678910[root@www ~]$ sort -n -t &apos; &apos; -k 2 facebook.txtguge 50 3000baidu 100 5000sohu 100 4500google 110 5000[root@www ~]$ sort -n -t &apos; &apos; -k 2 -k 3 facebook.txtguge 50 3000sohu 100 4500baidu 100 5000google 110 5000 （2）下面，按照员工平均工资的降序排序，如果员工工资相同，则按照工资人数升序排序。12345[root@www ~]$ sort -t &apos; &apos; -n -k 3r -k 3 facebook.txtbaidu 100 5000google 110 5000sohu 100 4500guge 50 3000]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 相关知识点整理]]></title>
    <url>%2F2018%2F03%2F28%2FRedis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[首先，要知道redis是一个key-value内存存储系统，且是单线程模型。与其他key-value缓存相比有以下特点： 支持数据持久化，可以将内存中的数据保存到磁盘，重启机器后可再次加载进来 不仅仅支持key-value类型数据，还有哈希、列表、集合数据结构类型 支持数据的备份，master-slave模式的数据备份 并且，redis的所有操作都具有原子性。 1、redis 的数据类型介绍String常用命令：set,get,decr,incr,mget 等。应用场景：String是最常用的一种数据类型，普通的key/value存储都可以归为此类，这里就不所做解释了。实现方式：String在redis内部存储默认就是一个字符串，被redisObject所引用，当遇到incr,decr等操作时会转成数值型进行计算，此时redisObject的encoding字段为int。 Hash常用命令：hget,hset,hgetall 等。应用场景：我们简单举个实例来描述下Hash的应用场景，比如我们要存储一个用户信息对象数据，包含以下信息：用户ID为查找的key，存储的value用户对象包含姓名，年龄，生日等信息，如果用普通的key/value结构来存储，主要有以下2种存储方式： 第一种方式将用户ID作为查找key,把其他信息封装成一个对象以序列化的方式存储，这种方式的缺点是，增加了序列化/反序列化的开销，并且在需要修改其中一项信息时，需要把整个对象取回，并且修改操作需要对并发进行保护，引入CAS等复杂问题。 第二种方法是这个用户信息对象有多少成员就存成多少个key-value对儿，用用户ID+对应属性的名称作为唯一标识来取得对应属性的值，虽然省去了序列化开销和并发问题，但是用户ID为重复存储，如果存在大量这样的数据，内存浪费还是非常可观的。 那么Redis提供的Hash很好的解决了这个问题，Redis的Hash实际是内部存储的Value为一个HashMap，并提供了直接存取这个Map成员的接口。 也就是说，Key仍然是用户ID, value是一个Map，这个Map的key是成员的属性名，value是属性值，这样对数据的修改和存取都可以直接通过其内部Map的 Key(Redis里称内部Map的key为field), 也就是通过 key(用户ID) + field(属性标签) 就可以操作对应属性数据了，既不需要重复存储数据，也不会带来序列化和并发修改控制的问题。很好的解决了问题。 这里同时需要注意，Redis提供了接口(hgetall)可以直接取到全部的属性数据,但是如果内部Map的成员很多，那么涉及到遍历整 个内部Map的操作，由于Redis单线程模型的缘故，这个遍历操作可能会比较耗时，而另其它客户端的请求完全不响应，这点需要格外注意。 上面已经说到Redis Hash对应Value内部实际就是一个HashMap，实际这里会有2种不同实现，这个Hash的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的value redisObject的encoding为zipmap,当成员数量增大时会自动转成真正的HashMap,此时encoding为ht。 List常用命令：lpush,rpush,lpop,rpop,lrange等。应用场景：Redis list的应用场景非常多，也是Redis最重要的数据结构之一，比如twitter的关注列表，粉丝列表等都可以用Redis的list结构来实现，比较好理解，这里不再重复。实现方式：Redis list的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销，Redis内部的很多实现，包括发送缓冲队列等也都是用的这个数据结构。 Set常用命令：sadd,spop,smembers,sunion 等。应用场景：Redis set对外提供的功能与list类似是一个列表的功能，特殊之处在于set是可以自动排重的，当你需要存储一个列表数据，又不希望出现重复数据时，set 是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。实现方式：set 的内部实现是一个 value永远为null的HashMap，实际就是通过计算hash的方式来快速排重的，这也是set能提供判断一个成员是否在集合内的原因。 Sorted Set常用命令：zadd,zrange,zrem,zcard等使用场景：Redis sorted set的使用场景与set类似，区别是set不是自动有序的，而sorted set可以通过用户额外提供一个优先级(score)的参数来为成员排序，并且是插入有序的，即自动排序。当你需要一个有序的并且不重复的集合列表，那么 可以选择sorted set数据结构，比如twitter 的public timeline可以以发表时间作为score来存储，这样获取时就是自动按时间排好序的。实现方式：Redis sorted set的内部使用HashMap和跳跃表(SkipList)来保证数据的存储和有序，HashMap里放的是成员到score的映射，而跳跃表里存放的 是所有的成员，排序依据是HashMap里存的score,使用跳跃表的结构可以获得比较高的查找效率，并且在实现上比较简单。 redis支持持久化的原理Redis由于支持非常丰富的内存数据结构类型，如何把这些复杂的内存组织方式持久化到磁盘上是一个难题，所以Redis的持久化方式与传统数据库的方式有比较多的差别，Redis一共支持四种持久化方式，分别是： 定时快照方式(snapshot)基于语句追加文件的方式(aof)虚拟内存(vm)Diskstore方式 redis支持小量数据落地功能，后两种种方式并不成熟，下面分别介绍下这几种持久化方式： 定时快照方式(snapshot)：该持久化方式实际是在Redis内部一个定时器事件，每隔固定时间去检查当前数据发生的改变次数与时间是否满足配置的持久化触发的条件（注：需要配置），如果满足则 通过操作系统fork调用来创建出一个子进程，这个子进程默认会与父进程共享相同的地址空间，这时就可以通过子进程来遍历整个内存来进行存储操作，而主进 程则仍然可以提供服务，当有写入时由操作系统按照内存页(page)为单位来进行copy-on-write保证父子进程之间不会互相影响。该持久化的主要缺点是定时快照只是代表一段时间内的内存映像，所以系统重启会丢失上次快照与重启之间所有的数据。 基于语句追加方式(aof)：aof方式实际类似mysql的基于语句的binlog方式，即每条会使Redis内存数据发生改变的命令都会追加到一个log文件中，也就是说这个log文件就是Redis的持久化数据。aof的方式的主要缺点是追加log文件可能导致体积过大，当系统重启恢复数据时如果是aof的方式则加载数据会非常慢，几十G的数据可能需要几小时才能加载完，当然这个耗时并不是因为磁盘文件读取速度慢，而是由于读取的所有命令都要在内存中执行一遍。另外由于每条命令都要写log,所以使用aof 的方式，Redis的读写性能也会有所下降。 虚拟内存方式：虚拟内存方式是Redis来进行用户空间的数据换入换出的一个策略，此种方式在实现的效果上比较差，主要问题是代码复杂，重启慢，复制慢等等，目前已经被作者放弃。 diskstore方式：diskstore方式是作者放弃了虚拟内存方式后选择的一种新的实现方式，也就是传统的B-tree的方式，目前仍在实验阶段，后续是否可用我们可以拭目以待。 Redis持久化磁盘IO方式及其带来的问题有Redis线上运维经验的人会发现Redis在物理内存使用比较多，但还没有超过实际物理内存总容量时就会发生不稳定甚至崩溃的问题，有人认为是 基于快照方式持久化的fork系统调用造成内存占用加倍而导致的，这种观点是不准确的，因为fork 调用的copy-on-write机制是基于操作系统页这个单位的，也就是只有有写入的脏页会被复制，但是一般你的系统不会在短时间内所有的页都发生了写 入而导致复制，那么是什么原因导致Redis崩溃的呢？ 答案是Redis的持久化使用了Buffer IO造成的，所谓Buffer IO是指Redis对持久化文件的写入和读取操作都会使用物理内存的Page Cache,而大多数数据库系统会使用Direct IO来绕过这层Page Cache并自行维护一个数据的Cache，而当Redis的持久化文件过大(尤其是快照文件)，并对其进行读写时，磁盘文件中的数据都会被加载到物理内 存中作为操作系统对该文件的一层Cache,而这层Cache的数据与Redis内存中管理的数据实际是重复存储的，虽然内核在物理内存紧张时会做 Page Cache的剔除工作，但内核很可能认为某块Page Cache更重要，而让你的进程开始Swap ,这时你的系统就会开始出现不稳定或者崩溃了。我们的经验是当你的Redis物理内存使用超过内存总容量的3/5时就会开始比较危险了。（注：Redis的复制功能是完全建立在基于内存快照的持久化策略基础上的，也就是说无论你的持久化策略选择的是什么，只要用到了 Redis的复制功能，就一定会有内存快照发生。） 3、redis备份原理由上面持久化的原理可知道，目前使用的是RDB和AOF。默认情况下60秒刷新到disk一次[save 60 10000 当有1w条keys数据被改变时]，Redis的数据集保存在叫dump.rdb一个二进制文件，这种策略被称为快照。 快照易恢复，文件也小，但是如果遇到宕机等情况的时候快照的数据可能会不完整。此时可能需要启用另一种持久化方式AOF，在配置文件中打开[appendonly yes]。 AOF刷新日志到disk的规则：appendfsync always #always 表示每次有写操作都进行同步，非常慢，非常安全。appendfsync everysec #everysec表示对写操作进行累积，每秒同步一次官方的建议的everysec，安全，就是速度不够快，如果是机器出现问题可能会丢失1秒的数据。 我们现在的做法是一主(Master)多从(Slave)，主库不开启AOF持久化，只是每天备份一下RDB[官方给的建议是每小时备份RDB文件，看你的策略了]，而在从库上开启AOF备份，并且会用脚本将相应的备份文件推送到备份服务器。当redis服务器挂掉时，重启时将按照以下优先级恢复数据到内存： 如果只配置AOF,重启时加载AOF文件恢复数据； 如果同时 配置了RBD和AOF,启动是只加载AOF文件恢复数据; 如果只配置RBD,启动是讲加载dump文件恢复数据。 恢复时需要注意，要是主库挂了不能直接重启主库，否则会直接覆盖掉从库的AOF文件，一定要确保要恢复的文件都正确才能启动，否则会冲掉原来的文件。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 底层索引原理【摘】]]></title>
    <url>%2F2018%2F03%2F28%2FMySQL%E5%BA%95%E5%B1%82%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86%E3%80%90%E6%91%98%E3%80%91%2F</url>
    <content type="text"><![CDATA[原文链接：MySQL索引背后的数据结构及算法原理 本文以MySQL数据库为研究对象，讨论与数据库索引相关的一些话题。特别需要说明的是，MySQL支持诸多存储引擎，而各种存储引擎对索引的支持也各不相同，因此MySQL数据库支持多种索引类型，如BTree索引，哈希索引，全文索引等等。本文将只关注于BTree索引。 文章主要内容分为三个部分。第一部分主要从数据结构及算法理论层面讨论MySQL数据库索引的数理基础。第二部分结合MySQL数据库中MyISAM和InnoDB数据存储引擎中索引的架构实现讨论聚集索引、非聚集索引及覆盖索引等话题。第三部分根据上面的理论基础，讨论MySQL中高性能使用索引的策略。 数据结构及算法基础索引的本质MySQL官方对索引的定义为：索引（Index）是帮助MySQL高效获取数据的数据结构。提取句子主干，就可以得到索引的本质：索引是一种数据结构。 数据库查询是数据库的主要功能之一，最基本的查询算法是顺序查找（linear search）时间复杂度为O(n)，显然在数据量很大时效率很低。优化的查找算法如二分查找（binary search）、二叉树查找（binary tree search）等，虽然查找效率提高了。但是各自对检索的数据都有要求：二分查找要求被检索数据有序，而二叉树查找只能应用于二叉查找树上，但是数据本身的组织结构不可能完全满足各种数据结构（例如，理论上不可能同时将两列都按顺序进行组织）。所以，在数据之外，数据库系统还维护着满足特定查找算法的数据结构。这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构就是索引。 看一个例子：图1展示了一种可能的索引方式。左边是数据表，一共有两列七条记录，最左边的是数据记录的物理地址（注意逻辑上相邻的记录在磁盘上也并不是一定物理相邻的）。为了加快Col2的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找在O(log2n)的复杂度内获取到相应数据。 虽然这是一个货真价实的索引，但是实际的数据库系统几乎没有使用二叉查找树或其进化品种红黑树（red-black tree）实现的，原因会在下文介绍。 B-Tree和B+Tree关于B树和B+树请参考关于B树的一些总结，这篇文章介绍的比较详细，同时容易理解。 目前大部分数据库系统及文件系统都采用B-Tree或其变种B+Tree作为索引结构，在本文的下一节会结合存储器原理及计算机存取原理讨论为什么B-Tree和B+Tree在被如此广泛用于索引，这一节先单纯从数据结构角度描述它们。 B-Tree为了描述B-Tree，首先定义一条数据记录为一个二元组[key, data]，key为记录的键值，对于不同数据记录，key是互不相同的；data为数据记录除key外的数据。那么B-Tree是满足下列条件的数据结构： d&gt;=2，即B-Tree的度； h为B-Tree的高； 每个非叶子结点由n-1个key和n个指针组成，其中d&lt;=n&lt;=2d； 每个叶子结点至少包含一个key和两个指针，最多包含2d-1个key和2d个指针，叶结点的指针均为NULL； 所有叶结点都在同一层，深度等于树高h； key和指针相互间隔，结点两端是指针； 一个结点中的key从左至右非递减排列； 如果某个指针在结点node最左边且不为null，则其指向结点的所有key小于v(key1)，其中v(key1)为node的第一个key的值。 如果某个指针在结点node最右边且不为null，则其指向结点的所有key大于v(keym)，其中v(keym)为node的最后一个key的值。 如果某个指针在结点node的左右相邻key分别是keyi和keyi+1且不为null，则其指向结点的所有key小于v(keyi+1)且大于v(keyi)。 由于B-Tree的特性，在B-Tree中按key检索数据的算法非常直观：首先从根节点进行二分查找，如果找到则返回对应节点的data，否则对相应区间的指针指向的节点递归进行查找，直到找到节点或找到null指针，前者查找成功，后者查找失败。B-Tree上查找算法的伪代码如下：12345678910BTree_Search(node, key) &#123; if(node == null) return null; foreach(node.key) &#123; if(node.key[i] == key) return node.data[i]; if(node.key[i] &gt; key) return BTree_Search(point[i]-&gt;node); &#125; return BTree_Search(point[i+1]-&gt;node);&#125;data = BTree_Search(root, my_key); 关于B-Tree有一系列有趣的性质，例如一个度为d的B-Tree，设其索引N个key，则其树高h的上限为logd((N+1)/2)，检索一个key，其查找结点个数的渐进复杂度为O(logdN)。从这点可以看出，B-Tree是一个非常有效率的索引数据结构。 B+TreeB-Tree有许多变种，其中最常见的是B+Tree，例如MySQL就普遍使用B+Tree实现其索引结构。与B-Tree相比，B+Tree有以下不同点： 每个结点的指针上限为2d而不是2d+1。 内结点不存储data，只存储key；叶子结点不存储指针。 由于并不是所有节点都具有相同的域，因此B+Tree中叶结点和内结点一般大小不同。这点与B-Tree不同，虽然B-Tree中不同节点存放的key和指针可能数量不一致，但是每个结点的域和上限是一致的，所以在实现中B-Tree往往对每个结点申请同等大小的空间。 一般来说，B+Tree比B-Tree更适合实现外存储索引结构，具体原因与外存储器原理及计算机存取原理有关，将在下面讨论。 带有顺序访问指针的B+Tree一般在数据库系统或文件系统中使用的B+Tree结构都在经典B+Tree的基础上进行了优化，增加了顺序访问指针。 如图4所示，在B+Tree的每个叶子结点增加一个指向相邻叶子结点的指针，就形成了带有顺序访问指针的B+Tree。做这个优化的目的是为了提高区间访问的性能，例如图4中如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着结点和指针顺序遍历就可以一次性访问到所有数据结点，极大提到了区间查询效率。 这一节对B-Tree和B+Tree进行了一个简单的介绍，下一节结合存储器存取原理介绍为什么目前B+Tree是数据库系统实现索引的首选数据结构。 为什么使用B-Tree（B+Tree）上文说过，红黑树等数据结构也可以用来实现索引，但是文件系统及数据库系统普遍采用B-/+Tree作为索引结构，这一节将结合计算机组成原理相关知识讨论B-/+Tree作为索引的理论基础。 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。下面先介绍内存和磁盘存取原理，然后再结合这些原理分析B-/+Tree作为索引的效率。 主存存取原理目前计算机使用的主存基本都是随机读写存储器（RAM），现代RAM的结构和存取原理比较复杂，这里本文抛却具体差别，抽象出一个十分简单的存取模型来说明RAM的工作原理。从抽象角度看，主存是一系列的存储单元组成的矩阵，每个存储单元存储固定大小的数据。每个存储单元有唯一的地址，现代主存的编址规则比较复杂，这里将其简化成一个二维地址：通过一个行地址和一个列地址可以唯一定位到一个存储单元。图5展示了一个4 x 4的主存模型。 主存的存取过程如下：当系统需要读取主存时，则将地址信号放到地址总线上传给主存，主存读到地址信号后，解析信号并定位到指定存储单元，然后将此存储单元数据放到数据总线上，供其它部件读取。写主存的过程类似，系统将要写入单元地址和数据分别放在地址总线和数据总线上，主存读取两个总线的内容，做相应的写操作。这里可以看出，主存存取的时间仅与存取次数呈线性关系，因为不存在机械操作，两次存取的数据的“距离”不会对时间有任何影响，例如，先取A0再取A1和先取A0再取D3的时间消耗是一样的。 磁盘存取原理上文说过，索引一般以文件形式存储在磁盘上，索引检索需要磁盘I/O操作。与主存不同，磁盘I/O存在机械运动耗费，因此磁盘I/O的时间消耗是巨大的。 一个磁盘由大小相同且同轴的圆形盘片组成，磁盘可以转动（各个磁盘必须同步转动）。在磁盘的一侧有磁头支架，磁头支架固定了一组磁头，每个磁头负责存取一个磁盘的内容。磁头不能转动，但是可以沿磁盘半径方向运动（实际是斜切向运动），每个磁头同一时刻也必须是同轴的，即从正上方向下看，所有磁头任何时候都是重叠的（不过目前已经有多磁头独立技术，可不受此限制）。 盘片被划分成一系列同心环，圆心是盘片中心，每个同心环叫做一个磁道，所有半径相同的磁道组成一个柱面。磁道被沿半径线划分成一个个小的段，每个段叫做一个扇区，每个扇区是磁盘的最小存储单元。为了简单起见，我们下面假设磁盘只有一个盘片和一个磁头。 当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点，磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间，然后磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间。 局部性原理与磁盘预读由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理： 当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 B-/+Tree索引的性能分析从使用磁盘I/O次数评价索引结构的优劣性：根据B-Tree的定义，可知检索一次最多需要访问h个结点。数据库系统的设计者巧妙的利用了磁盘预读原理，将一个结点的大小设为等于一个页面，这样每个结点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧： 每次新建结点时，直接申请一个页面的空间，这样可以保证一个结点的大小等于一个页面，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 B-Tree中一次检索最多需要h-1次I/O（根结点常驻内存），渐进复杂度为O(h)=O(logdN)。一般实际应用中，出读d是非常大的数字，通常超过100，因此h非常小。 综上所述，用B-Tree作为索引结构效率是非常高的。 而红黑树结构，h明显要深得多。由于逻辑上很近的结点（父子结点）物理上可能离得很远，无法利用局部性原理。所以即使红黑树的I/O渐进复杂度也为O(h)，但是查找效率明显比B-Tree差得多。 B+Tree更适合外存索引，是和内结点出度d有关。从上面分析可以看到，d越大索引的性能越好，而出度的上限取决于结点内key和data的大小：dmax=floor(pagesize/(keysize+datasize+pointsize))。 floor表示向下取整。由于B+Tree内结点去掉了data域，因此可以拥有更大的出度，拥有更好的性能。 这一章从理论角度讨论了与索引相关的数据结构与算法问题，下一章将讨论B+Tree是如何具体实现为MySQL中索引，同时将结合MyISAM和InnDB存储引擎介绍非聚集索引和聚集索引两种不同的索引实现形式。 MySQL索引实现在MySQL中，索引属于存储引擎级别的概念，不同存储引擎对索引的实现方式是不同的，本文主要讨论MyISAM和InnoDB两个存储引擎(MySQL数据库MyISAM和InnoDB存储引擎的比较)的索引实现方式。 MyISAM索引实现MyISAM引擎使用B+Tree作为索引结构，叶结点的data域存放的是数据记录的地址。下面是MyISAM索引的原理图：这里设表一共有三列，假设我们以Col1为主键，则图8是一个MyISAM表的主索引（Primary key）示意。可以看出MyISAM的索引文件仅仅保存数据记录的地址。在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。如果我们在Col2上建立一个辅助索引，则此索引的结构如下图所示：同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分。 InnoDB索引实现虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。 第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶结点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。图10是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶结点包含了完整的数据记录。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。 第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。例如，图11为定义在Col3上的一个辅助索引：这里以英文字符的ASCII码作为比较准则。聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 了解不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了InnoDB的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。再例如，用非单调的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择。 下一章将具体讨论这些与索引有关的优化策略。 索引使用策略及优化MySQL的优化主要分为结构优化（Scheme optimization）和查询优化（Query optimization）。本章讨论的高性能索引策略主要属于结构优化范畴。本章的内容完全基于上文的理论基础，实际上一旦理解了索引背后的机制，那么选择高性能的策略就变成了纯粹的推理，并且可以理解这些策略背后的逻辑。 示例数据库为了讨论索引策略，需要一个数据量不算小的数据库作为示例。本文选用MySQL官方文档中提供的示例数据库之一：employees。这个数据库关系复杂度适中，且数据量较大。下图是这个数据库的E-R关系图（引用自MySQL官方手册）： 最左前缀原理与相关优化高效使用索引的首要条件是知道什么样的查询会使用到索引，这个问题和B+Tree中的“最左前缀原理”有关，下面通过例子说明最左前缀原理。 这里先说一下联合索引的概念。在上文中，我们都是假设索引只引用了单个的列，实际上，MySQL中的索引可以以一定顺序引用多个列，这种索引叫做联合索引，一般的，一个联合索引是一个有序元组&lt;a1, a2, …, an&gt;，其中各个元素均为数据表的一列，实际上要严格定义索引需要用到关系代数，但是这里我不想讨论太多关系代数的话题，因为那样会显得很枯燥，所以这里就不再做严格定义。另外，单列索引可以看成联合索引元素数为1的特例。 以employees.titles表为例，下面先查看其上都有哪些索引：123456789SHOW INDEX FROM employees.titles;+--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Null | Index_type |+--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+| titles | 0 | PRIMARY | 1 | emp_no | A | NULL | | BTREE || titles | 0 | PRIMARY | 2 | title | A | NULL | | BTREE || titles | 0 | PRIMARY | 3 | from_date | A | 443308 | | BTREE || titles | 1 | emp_no | 1 | emp_no | A | 443308 | | BTREE |+--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+ 从结果中可以到titles表的主索引为&lt;emp_no, title, from_date&gt;，还有一个辅助索引&lt;emp_no&gt;。为了避免多个索引使事情变复杂（MySQL的SQL优化器在多索引时行为比较复杂），这里我们将辅助索引drop掉：ALTER TABLE employees.titles DROP INDEX emp_no;这样就可以专心分析索引PRIMARY的行为了。 全列匹配123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND title=&apos;Senior Engineer&apos; AND from_date=&apos;1986-06-26&apos;;+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+| 1 | SIMPLE | titles | const | PRIMARY | PRIMARY | 59 | const,const,const | 1 | |+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+ 很明显，当按照索引中所有列进行精确匹配（这里精确匹配指“=”或“IN”匹配）时，索引可以被用到。这里有一点需要注意，理论上索引对顺序是敏感的，但是由于MySQL的查询优化器会自动调整where子句的条件顺序以使用适合的索引，例如我们将where中的条件顺序颠倒：123456EXPLAIN SELECT * FROM employees.titles WHERE from_date=&apos;1986-06-26&apos; AND emp_no=&apos;10001&apos; AND title=&apos;Senior Engineer&apos;;+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+| 1 | SIMPLE | titles | const | PRIMARY | PRIMARY | 59 | const,const,const | 1 | |+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+ 效果是一样的。 最左前缀匹配123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos;;+----+-------------+--------+------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------+ 当查询条件精确匹配索引的左边连续一个或几个列时，如&lt;emp_no&gt;或&lt;emp_no, title&gt;，所以可以被用到，但是只能用到一部分，即条件所组成的最左前缀。上面的查询从分析结果看用到了PRIMARY索引，但是key_len为4，说明只用到了索引的第一列前缀。 查询条件用到了索引中列的精确匹配，但是中间某个条件未提供123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND from_date=&apos;1986-06-26&apos;;+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+| 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | Using where |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+ 此时索引使用情况和情况二相同，因为title未提供，所以查询只用到了索引的第一列，而后面的from_date虽然也在索引中，但是由于title不存在而无法和左前缀连接，因此需要对结果进行扫描过滤from_date（这里由于emp_no唯一，所以不存在扫描）。如果想让from_date也使用索引而不是where过滤，可以增加一个辅助索引&lt;emp_no, from_date&gt;，此时上面的查询会使用这个索引。除此之外，还可以使用一种称之为“隔离列”的优化方法，将emp_no与from_date之间的“坑”填上。 首先我们看下title一共有几种不同的值：123456789101112SELECT DISTINCT(title) FROM employees.titles;+--------------------+| title |+--------------------+| Senior Engineer || Staff || Engineer || Senior Staff || Assistant Engineer || Technique Leader || Manager |+--------------------+ 只有7种。在这种成为“坑”的列值比较少的情况下，可以考虑用“IN”来填补这个“坑”从而形成最左前缀：123456789EXPLAIN SELECT * FROM employees.titlesWHERE emp_no=&apos;10001&apos;AND title IN (&apos;Senior Engineer&apos;, &apos;Staff&apos;, &apos;Engineer&apos;, &apos;Senior Staff&apos;, &apos;Assistant Engineer&apos;, &apos;Technique Leader&apos;, &apos;Manager&apos;)AND from_date=&apos;1986-06-26&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 59 | NULL | 7 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 这次key_len为59，说明索引被用全了，但是从type和rows看出IN实际上执行了一个range查询，这里检查了7个key。看下两种查询的性能比较：1234567SHOW PROFILES;+----------+------------+-------------------------------------------------------------------------------+| Query_ID | Duration | Query |+----------+------------+-------------------------------------------------------------------------------+| 10 | 0.00058000 | SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND from_date=&apos;1986-06-26&apos;|| 11 | 0.00052500 | SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND title IN ... |+----------+------------+-------------------------------------------------------------------------------+ “填坑”后性能提升了一点。如果经过emp_no筛选后余下很多数据，则后者性能优势会更加明显。当然，如果title的值很多，用填坑就不合适了，必须建立辅助索引。 查询条件没有指定索引第一列123456EXPLAIN SELECT * FROM employees.titles WHERE from_date=&apos;1986-06-26&apos;;+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+| 1 | SIMPLE | titles | ALL | NULL | NULL | NULL | NULL | 443308 | Using where |+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+ 由于不是最左前缀，索引这样的查询显然用不到索引。 匹配某列前缀字符串123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND title LIKE &apos;Senior%&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 56 | NULL | 1 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 此时可以用到索引，但是如果通配符不是只出现在末尾，则无法使用索引。（原文表述有误，如果通配符%不出现在开头，则可以用到索引，但根据具体情况不同可能只会用其中一个前缀） 范围查询123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no &lt; &apos;10010&apos; and title=&apos;Senior Engineer&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 4 | NULL | 16 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 范围列可以用到索引（必须是最左前缀），但是范围列后面的列无法用到索引。同时，索引最多用于一个范围列，因此如果查询条件中有两个范围列则无法全用到索引。123456789EXPLAIN SELECT * FROM employees.titlesWHERE emp_no &lt; &apos;10010&apos;AND title=&apos;Senior Engineer&apos;AND from_date BETWEEN &apos;1986-01-01&apos; AND &apos;1986-12-31&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 4 | NULL | 16 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 可以看到索引对第二个范围索引无能为力。这里特别要说明MySQL一个有意思的地方，那就是仅用explain可能无法区分范围索引和多值匹配，因为在type中这两者都显示为range。同时，用了“between”并不意味着就是范围查询，例如下面的查询：123456789EXPLAIN SELECT * FROM employees.titlesWHERE emp_no BETWEEN &apos;10001&apos; AND &apos;10010&apos;AND title=&apos;Senior Engineer&apos;AND from_date BETWEEN &apos;1986-01-01&apos; AND &apos;1986-12-31&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 59 | NULL | 16 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 看起来是用了两个范围查询，但作用于emp_no上的“BETWEEN”实际上相当于“IN”，也就是说emp_no实际是多值精确匹配。可以看到这个查询用到了索引全部三个列。因此在MySQL中要谨慎地区分多值匹配和范围匹配，否则会对MySQL的行为产生困惑。 查询条件中含有函数或表达式很不幸，如果查询条件中含有函数或表达式，则MySQL不会为这列使用索引（虽然某些在数学意义上可以使用）。例如：123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND left(title, 6)=&apos;Senior&apos;;+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+| 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | Using where |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+ 虽然这个查询和情况五中功能相同，但是由于使用了函数left，则无法为title列应用索引，而情况五中用LIKE则可以。再如：123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no - 1=&apos;10000&apos;;+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+| 1 | SIMPLE | titles | ALL | NULL | NULL | NULL | NULL | 443308 | Using where |+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+ 显然这个查询等价于查询emp_no为10001的函数，但是由于查询条件是一个表达式，MySQL无法为其使用索引。看来MySQL还没有智能到自动优化常量表达式的程度，因此在写查询语句时尽量避免表达式出现在查询中，而是先手工私下代数运算，转换为无表达式的查询语句。 索引选择性与前缀索引既然索引可以加快查询速度，那么是不是只要是查询语句需要，就建上索引？答案是否定的。因为索引虽然加快了查询速度，但索引也是有代价的：索引文件本身要消耗存储空间，同时索引会加重插入、删除和修改记录时的负担，另外，MySQL在运行时也要消耗资源维护索引，因此索引并不是越多越好。一般两种情况下不建议建索引。 第一种情况是表记录比较少，例如一两千条甚至只有几百条记录的表，没必要建索引，让查询做全表扫描就好了。至于多少条记录才算多，这个个人有个人的看法，我个人的经验是以2000作为分界线，记录数不超过 2000可以考虑不建索引，超过2000条可以酌情考虑索引。 另一种不建议建索引的情况是索引的选择性较低。所谓索引的选择性（Selectivity），是指不重复的索引值（也叫基数，Cardinality）与表记录数（#T）的比值：Index Selectivity = Cardinality / #T 显然选择性的取值范围为(0, 1]，选择性越高的索引价值越大，这是由B+Tree的性质决定的。 这个问题就像是面试时提问我的一个问题：性别列适不适合建立索引？例如，上文用到的employees.titles表，如果title字段经常被单独查询，是否需要建索引，我们看一下它的选择性：123456SELECT count(DISTINCT(title))/count(*) AS Selectivity FROM employees.titles;+-------------+| Selectivity |+-------------+| 0.0000 |+-------------+ title的选择性不足0.0001（精确值为0.00001579），所以实在没有什么必要为其单独建索引。 有一种与索引选择性有关的索引优化策略叫做前缀索引，就是用列的前缀代替整个列作为索引key，当前缀长度合适时，可以做到既使得前缀索引的选择性接近全列索引，同时因为索引key变短而减少了索引文件的大小和维护开销。下面以employees.employees表为例介绍前缀索引的选择和使用。 从图12可以看到employees表只有一个索引&lt;emp_no&gt;，那么如果我们想按名字搜索一个人，就只能全表扫描了：123456EXPLAIN SELECT * FROM employees.employees WHERE first_name=&apos;Eric&apos; AND last_name=&apos;Anido&apos;;+----+-------------+-----------+------+---------------+------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+--------+-------------+| 1 | SIMPLE | employees | ALL | NULL | NULL | NULL | NULL | 300024 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+--------+-------------+ 如果频繁按名字搜索员工，这样显然效率很低，因此我们可以考虑建索引。有两种选择，建&lt;first_name&gt;或&lt;first_name, last_name&gt;，看下两个索引的选择性：123456789101112SELECT count(DISTINCT(first_name))/count(*) AS Selectivity FROM employees.employees;+-------------+| Selectivity |+-------------+| 0.0042 |+-------------+SELECT count(DISTINCT(concat(first_name, last_name)))/count(*) AS Selectivity FROM employees.employees;+-------------+| Selectivity |+-------------+| 0.9313 |+-------------+ &lt;first_name&gt;显然选择性太低，&lt;first_name, last_name&gt;选择性很好，但是first_name和last_name加起来长度为30，有没有兼顾长度和选择性的办法？可以考虑用first_name和last_name的前几个字符建立索引，例如&lt;first_name, left(last_name, 3)&gt;，看看其选择性：123456SELECT count(DISTINCT(concat(first_name, left(last_name, 3))))/count(*) AS Selectivity FROM employees.employees;+-------------+| Selectivity |+-------------+| 0.7879 |+-------------+ 选择性还不错，但离0.9313还是有点距离，那么把last_name前缀加到4：123456SELECT count(DISTINCT(concat(first_name, left(last_name, 4))))/count(*) AS Selectivity FROM employees.employees;+-------------+| Selectivity |+-------------+| 0.9007 |+-------------+ 这时选择性已经很理想了，而这个索引的长度只有18，比&lt;first_name, last_name&gt;短了接近一半，我们把这个前缀索引 建上：12ALTER TABLE employees.employeesADD INDEX first_name_last_name4 (first_name, last_name(4)); 此时再执行一遍按名字查询，比较分析一下与建索引前的结果：1234567SHOW PROFILES;+----------+------------+---------------------------------------------------------------------------------+| Query_ID | Duration | Query |+----------+------------+---------------------------------------------------------------------------------+| 87 | 0.11941700 | SELECT * FROM employees.employees WHERE first_name=&apos;Eric&apos; AND last_name=&apos;Anido&apos; || 90 | 0.00092400 | SELECT * FROM employees.employees WHERE first_name=&apos;Eric&apos; AND last_name=&apos;Anido&apos; |+----------+------------+---------------------------------------------------------------------------------+ 性能的提升是显著的，查询速度提高了120多倍。 前缀索引兼顾索引大小和查询速度，但是其缺点是不能用于ORDER BY和GROUP BY操作，也不能用于Covering index（即当索引本身包含查询所需全部数据时，不再访问数据文件本身）。 InnoDB的主键选择与优化在使用InnoDB存储引擎时，如果没有特别的需要，请永远使用一个与业务无关的自增字段作为主键。 经常看到有帖子或博客讨论主键选择问题，有人建议使用业务无关的自增主键，有人觉得没有必要，完全可以使用如学号或身份证号这种唯一字段作为主键。不论支持哪种论点，大多数论据都是业务层面的。如果从数据库索引优化角度看，使用InnoDB引擎而不使用自增主键绝对是一个糟糕的主意。 上文讨论过InnoDB的索引实现，InnoDB使用聚集索引，数据记录本身被存于主索引（一颗B+Tree）的叶子节点上。这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）。 如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。如下图所示： 这样就会形成一个紧凑的索引结构，近似顺序填满。由于每次插入时也不需要移动已有数据，因此效率很高，也不会增加很多开销在维护索引上。 如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置： 此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。 因此，只要可以，请尽量在InnoDB上采用自增字段做主键。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 存储引擎详解]]></title>
    <url>%2F2018%2F03%2F28%2FMySQL%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[数据库存储引擎是数据库底层软件组织，数据库管理系统（DBMS）使用数据引擎进行创建、查询、更新和删除数据。不同的存储引擎提供不同的存储机制、索引技巧、锁定水平等功能，使用不同的存储引擎，还可以 获得特定的功能。现在许多不同的数据库管理系统都支持多种不同的数据引擎。MySql的核心就是存储引擎。 存储引擎查看MySQL给开发者提供了查询存储引擎的功能，我这里使用的是 10.1.28-MariaDB，可以使用：show engines; Engine Suppert Comment Transactions XA Savepoints CSV YES CSV storage engine No No No InnoDB DEFAULT Comment: Percona-XtraDB, Supports transactions, row-level locking, foreignkeys and encryption for tables YES YES YES MEMORY YES Hash based, stored in memory, useful for temporary tables NO NO NO MyISAM YES MyISAM storage engine NO NO NO MRG_MyISAM YES Collection of identical MyISAM tables NO NO NO Aria YES Crash-safe tables with MyISAM heritage NO NO NO PERFORMANCE_SCHEMA YES Performance Schema NO NO NO NO SEQUENCE YES Generated tables filled with sequential values YES NO YES 看到MySQL给用户提供了这么多存储引擎，包括处理事务安全表的引擎和出来了非事物安全表的引擎。 InnoDBInnoDB是事务型数据库的首选引擎，支持事务安全表（ACID），支持行锁定和外键，上图也看到了，InnoDB是默认的MySQL引擎。InnoDB存储表和索引有一下两种方式： 使用共享表空间存储，这种方式创建的表的表结构保存在.frm文件中，数据和索引保存在innodb_data_home_dir和innodb_data_file_path定义的表空间中，可以是多个文件。 使用多表空间存储，这种方式创建的表的表结构仍然保存在.frm文件中，但是每个表的数据和索引单独保存在.ibd中，如果是个分区表，则每个分区对应单独的.ibd文件，文件名是”表名+分区名”，可以在创建分区的时候指定每个分区的数据文件位置，以此来将表的IO均匀分布在多个磁盘上。 选择理由：用于事务处理应用程序，支持外键。如果应用对事务的完整性有比较高的要求，在并发条件下要求数据一致性，数据操作除了插入和查询意外，还包括很多的更新删除操作，那么InnoDB比较合适。InnoDB存储引擎除了有效的降低由于删除和更新操作导致的锁定，还可以确保事务的完整提交和回滚。 MyISAM它是在Web、数据仓储和其他应用环境下最常使用的存储引擎之一。MyISAM拥有较高的插入、查询速度，但不支持事物。每个MyISAM在磁盘上存储成3个文件，文件名都和表名相同，但是扩展名不同，扩展名分别是： .frm（存储表定义）； .MYD（MYData，存储数据）； .MYI（MYIndex，存储索引）； MyISAM的表还支持3种不同的存储格式，分别是： 静态（固定长度）表； 动态表； 压缩表； 选择理由：如果应用是以读写操作和插入操作为主，只有很少的更新和删除操作，并且对事务的完整性、并发性要求不是很高可选用此种存储引擎。 MEMORYMEMORY存储引擎将表中的数据存储到内存中，未查询和引用其他表数据提供快速访问。MEMORY存储引擎使用存在于内存中的内容来创建表。每个MEMORY表只实际对应一个磁盘文件，格式.frm。MEMORY类型的表访问非常快，因为它的数据是存放在内存中的，并且默认使用HASH索引，但是一旦服务关闭，表中的数据就会丢失。 选择理由：将所有的数据保存在RAM中，在需要快速定位记录和其他类似数据的环境下，可提供极快的访问。MEMORY的缺陷是对表的大小有限制，太大的表无法缓存在内存中，其次要确保表数据可以恢复，数据库异常终止后表中的数据是可以恢复的。MEMORY表通常用于更新不太频繁的小表，用以快速得到访问结果。 下面对一些常用的引擎的特点进行汇总 功 能 MYISAM Memory InnoDB Archive 存储限制 256TB RAM 64TB None 支持事物 No No Yes No 支持全文索引 Yes No No No 支持数索引 Yes Yes Yes No 支持哈希索引 No Yes No No 支持数据缓存 No N/A Yes No 支持外键 No No Yes No 存储引擎的选择：如果要提供提交、回滚、崩溃恢复能力的事物安全（ACID兼容）能力，并要求实现并发控制，InnoDB是一个好的选择 如果数据表主要用来插入和查询记录，则MyISAM引擎能提供较高的处理效率 如果只是临时存放数据，数据量不大，并且不需要较高的数据安全性，可以选择将数据保存在内存中的Memory引擎，MySQL中使用该引擎作为临时表，存放查询的中间结果 如果只有INSERT和SELECT操作，可以选择Archive，Archive支持高并发的插入操作，但是本身不是事务安全的。Archive非常适合存储归档数据，如记录日志信息可以使用Archive 使用哪一种引擎需要灵活选择，一个数据库中多个表可以使用不同引擎以满足各种性能和实际需求，使用合适的存储引擎，将会提高整个数据库的性能 MyISAM与InnoDB的区别1、InnoDB支持事务处理，而MyISAM不支持2、InnoDB支持外键，而MyISAM不支持3、InnoDB是行锁，而MyISAM是表锁（行锁开销大，高并发；表锁开销小，并发低）4、MyISAM支持全文索引，而InnoDB不支持]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 的事务处理以及隔离级别]]></title>
    <url>%2F2018%2F03%2F28%2FMySQL%E7%9A%84%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86%E4%BB%A5%E5%8F%8A%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%2F</url>
    <content type="text"><![CDATA[以下内容出自《高性能MySQL》第三版，了解事务的ACID及四种隔离级有助于我们更好的理解事务运作。下面举一个银行应用是解释事务必要性的一个经典例子。假如一个银行的数据库有两张表：支票表（checking）和储蓄表（savings）。现在要从用户Jane的支票账户转移200美元到她的储蓄账户，那么至少需要三个步骤： 1、检查支票账户的余额高于或者等于200美元。2、从支票账户余额中减去200美元。3、在储蓄帐户余额中增加200美元。 上述三个步骤的操作必须打包在一个事务中，任何一个步骤失败，则必须回滚所有的步骤。 1、事务处理的基本语法可以用START TRANSACTION语句开始一个事务，然后要么使用COMMIT提交将修改的数据持久保存，要么使用ROLLBACK撤销所有的修改。事务SQL的样本如下： 1.start transaction;2.select balance from checking where customer_id = 10233276;3.update checking set balance = balance - 200.00 where customer_id = 10233276;4.update savings set balance = balance + 200.00 where customer_id = 10233276;5.commit; 2、事务处理的特性ACID表示原子性（atomicity）、一致性（consistency）、隔离性（isolation）和持久性（durability）。一个很好的事务处理系统，必须具备这些标准特性： 原子性（atomicity） 一个事务必须被视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功，要么全部失败回滚，对于一个事务来说，不可能只执行其中的一部分操作，这就是事务的原子性 一致性（consistency） 数据库总是从一个一致性的状态转换到另一个一致性的状态。（在前面的例子中，一致性确保了，即使在执行第三、四条语句之间时系统崩溃，支票账户中也不会损失200美元，因为事务最终没有提交，所以事务中所做的修改也不会保存到数据库中。） 隔离性（isolation） 通常来说，一个事务所做的修改在最终提交以前，对其他事务是不可见的。（在前面的例子中，当执行完第三条语句、第四条语句还未开始时，此时有另外的一个账户汇总程序开始运行，则其看到支票帐户的余额并没有被减去200美元。） 持久性（durability） 一旦事务提交，则其所做的修改不会永久保存到数据库。（此时即使系统崩溃，修改的数据也不会丢失。持久性是个有占模糊的概念，因为实际上持久性也分很多不同的级别。有些持久性策略能够提供非常强的安全保障，而有些则未必，而且不可能有能做到100%的持久性保证的策略。） 3、事务处理的隔离级别READ UNCOMMITTED（未提交读） 在READ UNCOMMITTED级别，事务中的修改，即使没有提交，对其他事务也都是可见的。事务可以读取未提交的数据，这也被称为脏读（Dirty Read）。这个级别会导致很多问题，从性能上来说，READ UNCOMMITTED不会比其他的级别好太多，但却缺乏其他级别的很多好处，除非真的有非常必要的理由，在实际应用中一般很少使用。 READ COMMITTED（提交读） 大多数数据库系统的默认隔离级别都是READ COMMTTED（但MySQL不是）。READ COMMITTED满足前面提到的隔离性的简单定义：一个事务开始时，只能”看见”已经提交的事务所做的修改。换句话说，一个事务从开始直到提交之前，所做的任何修改对其他事务都是不可见的。这个级别有时候叫做不可重复读（nonrepeatble read），因为两次执行同样的查询，可能会得到不一样的结果 REPEATABLE READ(可重复读) REPEATABLE READ解决了脏读的问题。该隔离级别保证了在同一个事务中多次读取同样记录结果是一致的。但是理论上，可重复读隔离级别还是无法解决另外一个幻读（Phantom Read）的问题。所谓幻读，指的是当某个事务在读取某个范围内的记录时，另一个事务又在该范围内插入了新的记录，当之前的事务再次读取该范围的记录时，会产生幻行（Phantom Row）。InnoDB和XtraDB存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）解决了幻读的问题。 SERIALIZABLE（可串行化） SERIALIZABLE是最高的隔离级别。它通过强制事务串行执行，避免了前面说的幻读的问题。简单来说，SERIALIZABLE会在读取每一行数据都加锁，所以可能导致大量的超时和锁争用问题。实际应用中也很少用到这个隔离级别，只有在非常需要确保数据的一致性而且可以接受没有并发的情况下，才考虑采用该级别。 下面对隔离级别进行汇总，如下表： 4、事务处理的相关语法总结1、查询当前这个点的下一个事务隔离级别：select @@tx_isolation; 2、查询全局事务隔离级别，也就是从这个点开始的下一个事务起所有的事务都采用设置的该隔离级别：select @@global.tx_isolation; 3、查询当前所在的事务的隔离级别：select @@session.tx_isolation; 4、设置事务隔离级别1.在my.cnf文件中的mysqld中进行更改。可供更改的leve 【READ-UNCOMMITTED | READ-COMMITTED | REPEATABLE-READ | SERIALIZABLE】如：transaction_isolation = SERIALIZABLE 2.用mysql语句进行更改，语法 set tx_isolation= level level级别【READ-UNCOMMITTED | READ-COMMITTED | REPEATABLE-READ | SERIALIZABLE】 注：可通过上述语法查询是否更改。 5、事务隔离级别示例详解1、脏读：脏读指的是一个事务修改本事务的数据，但没有提交的情况下，本事务可以查看到更改了却没有提交的数据。这种情况只可能发生在未提交读readuncommitted的隔离级别里。 首先设置事务隔离级别为read-uncommited事务一：给t表添加数据a=1但并不提交！查询t表的结果里本事务是可以看到未提交的操作结果的。事务二：查询t表的数据，这个时候事务一是没有提交的!要确保事务二的隔离级别是read-uncommited。 这个情况就是所谓的脏读！只可能出现在read-uncommit隔离级别里！自己可尝试其他级别试试 2、不重复读：指的是一个事务多次读取同一个数据，但此过程中第二个事务对第一个事务进行了修改，这就造成了第一个事务的多次读取过程中的结果出现了前后不一致的情况，这就是不可重复读！这个情况可以出现在read-uncommited和read-committed两个隔离级别里！ 先看反例repeatable-read隔离级别下的情况，这种级别下只会读取首次select的点的数据结果，以避免出现不重复读的情况：事务一：查询t表a=5，然后update进行修改a=22，查看修改后的结果最后提交事务二：在事务一开始后select操作时同时也进行两次查询，查询结果和事务一的结果一致，都是5，等到事务一update操作后事务二进行第三次查询，查询结果依旧为5，事务一提交后，事务二进行第四次查询，结果依旧是5，没有任何改变，所以repeatable-read的隔离级别解决了重复读的问题。 下面我们再来看read-commit隔离级别下的情况：事务一：事务二： 3、幻读：在repeatable-read级别下解决了不可重复读的问题，但是还有没有解决的地方，那就是幻读，幻读是指在一个事务中，事务一对数据进行了更改并提交了，而事务二也对该数据进行了更改，但是更改的情况可能会受到事务一更改的影响，从而引起更改仿佛不存在，这就是幻读！这是由于repeatable-read的隔离措施只读取初次select的点导致的。 t表的有一行数据，a=5事务一：首先读取t表，发现有一个a=5的数据，然后更改该数据为25并提交事务二：首先读取t表，发现有一个a=5的数据，这个时候事务一已经删除数据并期间了，但是由于隔离级别是repeatable-read导致读取的结果是a=5的数据还在，这个时候如果事务二对该数据进行更改，改为26，提交后发现之前的更改无效，该行数据a=25，这是因为事务一已经对a=5的数据进行了更改，改为了25，这个时候a=5的数据已经不存在了，因此这次更改无效，等同于幻影，这就是幻读！ 可以从这里看出，提交读解决了脏读问题，而重复读解决了不重复读的问题，串行化解决幻读的问题，但是也会造成锁竞争，可能造成大量的超时问题，因为串行化是给每个读的数据行加上共享锁，通过强制事务进行排序，以此防止相互冲突，具体原理以后再做说明。 6、极客时间精选笔记记录 务的特性：原子性、一致性、隔离性、持久性 多事务同时执行的时候，可能会出现的问题：脏读、不可重复读、幻读 事务隔离级别：读未提交、读提交、可重复读、串行化 不同事务隔离级别的区别： 读未提交：一个事务还未提交，它所做的变更就可以被别的事务看到 读提交：一个事务提交之后，它所做的变更才可以被别的事务看到 可重复读：一个事务执行过程中看到的数据是一致的。未提交的更改对其他事务是不可见的 串行化：对应一个记录会加读写锁，出现冲突的时候，后访问的事务必须等前一个事务执行完成才能继续执行 配置方法：启动参数transaction-isolation 事务隔离的实现：每条记录在更新的时候都会同时记录一条回滚操作。同一条记录在系统中可以存在多个版本，这就是数据库的多版本并发控制（MVCC）。 回滚日志什么时候删除？系统会判断当没有事务需要用到这些回滚日志的时候，回滚日志会被删除。 什么时候不需要了？当系统里么有比这个回滚日志更早的read-view的时候。 为什么尽量不要使用长事务。长事务意味着系统里面会存在很老的事务视图，在这个事务提交之前，回滚记录都要保留，这会导致大量占用存储空间。除此之外，长事务还占用锁资源，可能会拖垮库。 事务启动方式：一、显式启动事务语句，begin或者start transaction,提交commit，回滚rollback；二、set autocommit=0，该命令会把这个线程的自动提交关掉。这样只要执行一个select语句，事务就启动，并不会自动提交，直到主动执行commit或rollback或断开连接。 建议使用方法一，如果考虑多一次交互问题，可以使用commit work and chain语法。在autocommit=1的情况下用begin显式启动事务，如果执行commit则提交事务。如果执行commit work and chain则提交事务并自动启动下一个事务。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows 7设置WiFi热点]]></title>
    <url>%2F2018%2F03%2F27%2Fwindows7%E8%AE%BE%E7%BD%AEWiFi%E7%83%AD%E7%82%B9%2F</url>
    <content type="text"><![CDATA[点击可查看… 1、查看是否支持并开启首先打开自己的网络连接，看一下有没有上述网卡图标，若有则进行下一步，若没有则打开设备管理器，找到网卡选项，右击选择更新网卡驱动，若此时出现说明你的电脑支持此功能，若没有出现，则说明电脑不支持。 2、设置WiFi热点点击 开始， 选择 附件，找到命令提示符，右击选择以管理员方式运行输入：netsh wlan set hostednetwork mode=allow ssid=&quot;无线热点的名字&quot; key=“w ifi的密码”，然后回车，如果出现如下的信息，则表示设置成功： 3、开启WiFi至此wifi已经设置完成，现在就是开始运行问题，输入：netsh wlan start hostednetwork 然后回车，则启动wifi以后，每次重启电脑后都需要运行netsh wlan start hostednetwork命令来启动]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法概述及效率的度量]]></title>
    <url>%2F2018%2F03%2F26%2F%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0%E5%8F%8A%E6%95%88%E7%8E%87%E7%9A%84%E5%BA%A6%E9%87%8F%2F</url>
    <content type="text"><![CDATA[1、算法的定义算法是解决特定问题求解步骤的描述，在计算机中表现为指令的有限序列，并且每条指令表示一个或多个操作。 2、算法具有的武哥基本特性 有穷性： 指算法在执行有限的步骤之后，自动结束而不会出现无线循环，并且每一个步骤在可接受的时间内完成。 确定性： 算法的每一步骤都具有确定的含义，不会出现二义性。 可行性： 算法的每一步都必须是可行的，也就是说，每一步都能够通过执行有限次数完成。 输入： 一个算法有零个或多个的输入，这些输入取自于某个特定的对象的集合。 输出： 一个算法有一个或多个的输出，这些输出是同输入有着某些特定关系的量。 3、算法设计的要求设计算法时，通常应考虑达到以下目标： 正确性： 算法的正确性是指算法至少应该具有输入、输出和加工处理无歧义性、能正确反映问题的需求、能够得到问题的正确答案。 可读性： 算法设计的另一个目的是为了便于阅读、理解和交流。另一方面，晦涩难懂的程序易于隐藏较多错误而难以调试。 健壮性： 当输入数据不合法时，算法也能做出相关处理，而不是产生异常或莫名其妙的结果。 高效率与低存储量： 通常，效率指的是算法执行时间；存储量指的是算法执行过程中所需的最大存储空间，两者都与问题的规模相关。 4、算法效率的度量方法： 事后统计方法： 这种方法主要是通过设计好的测试程序和数据，利用计算机计时器对不同算法编制的程序的运行时间进行比较，从而确定算法效率的高低。 事前分析估算法： 在计算机程序编制前，依据统计方法对算法进行估算。一个程序的运行时间，依赖于算法的好坏和问题的输入规模。（所谓问题输入规模是指输入量的多少。） 5、算法的时间复杂度算法的时间复杂度，也就是算法的时间量度，记作：T(n) = O(f(n))。他表示岁问题规模n的增大，算法执行时间的增长率和f(n)的增长率相同，称作算法的渐进时间复杂度，简称为时间复杂度。 推导大O阶：1.用常数1取代运行时间中的所有加法常数。2.在修改后的运行次数函数中，只保留最高阶项。3.如果最高阶项存在且不是1，则去除与这个项相乘的常数。得到的结果就是大O阶。 执行次数函数 阶 非正式术语 12 O(1) 常数阶 2n+3 O(n) 线性阶 3n^2+2n+1 O(n^2) 平方阶 5log2n+20 O(logn) 对数阶 2n+3nlog2n+19 O(nlogn) nlogn阶 6n^3+2n^2+3n+4 O(n^3) 立方阶 2^n O(2^n) 指数阶 常用的时间复杂度所耗费的时间从小到大依次是：O(1) &lt; O(logn) &lt; O(n) &lt; O(nlogn) &lt; O(n^2) &lt; O(n^3) &lt; O(2^n) &lt; O(n!) &lt; O(n^n) 6、算法空间复杂度算法的空间复杂度通过计算算法所需的存储空间实现，算法空间复杂度的计算公式记作：S(n) = O(f(n))，其中，n为问题的规模，f(n)为语句关于n所占存储空间的函数。 （注：一般在没有特殊说明的情况下，都是指最坏时间复杂度。） 7、算法的时间复杂度推导公式待补充…]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 下二进制文件命令存放目录区别]]></title>
    <url>%2F2018%2F03%2F20%2FLinux%E4%B8%8B%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6%E5%91%BD%E4%BB%A4%E5%AD%98%E6%94%BE%E7%9B%AE%E5%BD%95%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[二进制文件命令： /bin # 系统的一些指令 /sbin # 一般是指超级用户指令 /usr/bin # 后期安装的一些软件的运行脚本 /usr/sbin # 一些用户安装的系统管理的必备程序 /usr/local/bin # 通常是源码编译的软件 /usr/local/sbin # 通常是源码编译的软件，用来管理系统的程序 首先看下PATH变量在不同用户下的值：root用户：12[root@VM_0_7_centos /]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin 普通用户：12[ben@VM_0_7_centos ~]$ echo $PATH/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/ben/.local/bin:/home/ben/bin （注：bin是binary的缩写，二进制。sbin意义为system binary。） 由上面可知道 root权限都没有 /bin、/sbin 的目录使用权。为什么呢？原因就是，/bin以及/sbin是和/在同一文件系统，在挂载其他文件系统之前就可以使用/bin以及/sbin下的命令。/usr/bin，/usr/sbin，/usr/local/bin，/usr/local/sbin可能与根文件系统不在同一文件系统之中，可能是在其他文件系统中后挂载上去的。而我的服务器是 腾讯云，因此也是挂载上去的。 从命令功能来看，/sbin 下的命令属于基本的系统命令，如shutdown，reboot，用于启动系统，修复系统，/bin下存放一些普通的基本命令，如ls,chmod等，这些命令在Linux系统里的配置文件脚本里经常用到。 从用户权限的角度看，/sbin目录下的命令通常只有管理员才可以运行，/bin下的命令管理员和一般的用户都可以使用。 从可运行时间角度看，/sbin,/bin能够在挂载其他文件系统前就可以使用。 下面来说一下常用的目录： /bin 是系统的一些指令。bin为binary的简写主要放置一些系统的必备执行档例如:cat、cp、chmod df、dmesg、gzip、kill、ls、mkdir、more、mount、rm、su、tar等。/sbin 一般是指超级用户指令。主要放置一些系统管理的必备程式例如:cfdisk、dhcpcd、dump、e2fsck、fdisk、halt、ifconfig、ifup、 ifdown、init、insmod、lilo、lsmod、mke2fs、modprobe、quotacheck、reboot、rmmod、 runlevel、shutdown等。/usr/bin 是你在后期安装的一些软件的运行脚本。主要放置一些应用软体工具的必备执行档例如c++、g++、gcc、chdrv、diff、dig、du、eject、elm、free、gnome、 gzip、htpasswd、kfm、ktop、last、less、locale、m4、make、man、mcopy、ncftp、 newaliases、nslookup passwd、quota、smb、wget等。/usr/sbin 放置一些用户安装的系统管理的必备程式例如:dhcpd、httpd、imap、in.*d、inetd、lpd、named、netconfig、nmbd、samba、sendmail、squid、swap、tcpd、tcpdump等。/usr/local/bin 很多时候我们自己安装的软件，可能在此处建立一个软连接（符号链接），指向实际的可执行文件。/usr/local/sbin也是我们自己安装的软件，一般用来管理系统的程序 （注：以上所说的并不是绝对的，例如ifconfig在/sbin下，但是普通用户一般具有可执行权限。）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7修改yum源]]></title>
    <url>%2F2018%2F03%2F20%2FCentOS7%E4%BF%AE%E6%94%B9yum%E6%BA%90%2F</url>
    <content type="text"><![CDATA[官方的yum源在国内访问效果不佳。需要改为国内比较好的阿里云或者网易的yum源。 1、配置 yum源12345# 备份当前的yum源cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS.Base.repo.bak# 下载阿里云的yum配置源wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 2、然后更新缓存12yum clean allyum makecache]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 源码安装升级CentOS 7中版本]]></title>
    <url>%2F2018%2F03%2F19%2FGit%E6%BA%90%E7%A0%81%E5%AE%89%E8%A3%85%E5%8D%87%E7%BA%A7CentOS7%E4%B8%AD%E7%89%88%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[服务器上报个漏洞警告： 漏洞名称：Git 远程代码执行漏洞 (CVE-2016-2315)漏洞描述：Git小于2.7.1的全版本中存在一个由于整数溢出导致的缓冲区边界溢出的远程代码执行漏洞，可使攻击者远程执行任意代码 因此，才有了这篇升级安装Git，使用源码编译安装指定版本。 1、下载 Git最新版的源码包登录https://github.com/git/git/releases查看git的最新版。不要下载带有-rc的，因为它代表了一个候选发布版本。安装指令：wget https://www.kernel.org/pub/software/scm/git/git-2.16.2.tar.gz 2、解压tar -xvf git-2.16.2 3、进行目录配置cd git-2.16.2./configure --prefix=/usr/local/git结果显示没有 configure 文件，尴尬了！根据网上搜索才了解到查找解压包中没有对应的 configure 文件，因此应该找README 或者 INSTALL 之类的文档。 通过 INSTALL安装文档可知，执行如下命令即可：1234$ make configure ;# as yourself$ ./configure --prefix=/usr ;# as yourself$ make all doc ;# as yourself# make install install-doc install-html;# as root ① 执行 make configure：报错 [root@VM_0_7_centos git-2.16.2]# make configureGIT_VERSION = 2.16.2 GEN configure/bin/sh: autoconf: command not foundmake: * [configure] Error 127 解决方法：**[root@VM_0_7_centos git-2.16.2]# yum install -y autoconf ② ./configure --prefix=/usr/local/git 成功执行 ③ make：报错 Can’t locate ExtUtils/MakeMaker.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at Makefile.PL line 3.BEGIN failed–compilation aborted at Makefile.PL line 3.make[1]: [perl.mak] Error 2make: [perl/perl.mak] Error 2解决方法：[root@VM_0_7_centos git-2.16.2]# yum install -y perl-ExtUtils-MakeMaker ④ make install 成功执行，安装完成。（注：如何确认是否安装完成，只要看 make 执行后的结果没有报错，基本都是ok的。） 4、配置全局路径12export PATH=&quot;/usr/local/git/bin:$PATH&quot; source /etc/profile # 使/etc/profile 文件中的环境变量立即生效 （注：此配置全局路径只是对当前生效，重启后就会失效。）可使用软连接的形式，来使命令长久生效：ln -s /usr/local/git/bin/git /usr/local/bin/git 5、查看Git版本12[root@VM_0_7_centos git-2.16.2]# git --versiongit version 2.16.2]]></content>
      <categories>
        <category>版本控制</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FTP 客户端报错集锦]]></title>
    <url>%2F2018%2F03%2F19%2FFTP%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%8A%A5%E9%94%99%E9%9B%86%E9%94%A6%2F</url>
    <content type="text"><![CDATA[1、ftp 客户端连接报错，如下： 经查看是因为 服务器端对应的 21号端口未开启，开启即可，命令如下：iptables -I INPUT 4 -p tcp -m tcp --dport 21 -j ACCEPT 2、ftp 客户端连接成功，而读取目录失败，如下： 经过查找，发现是因为ftp数据连接模式的问题，ftp连接模式有两种：PORT（主动模式）和PASV（被动模式）。两者的前面文章中已经说明两者的区别，在这里我选择“主动模式”，只开放ftp服务端20端口传输数据，客户端随机产生大于1024端口号。然后，开启服务端20端口权限即可，命令如下：iptables -I INPUT 4 -p tcp -m tcp --dport 20 -j ACCEPT 3、ftp 客户端能够下载数据，却不能上传数据，如下： 经过一番查找，发现应该是我本身一堆错误导致的。包括ftp 用户组使用但未创建，ftp用户根目录权限未给予正确的ftp用户组（注：根目录创建是root用户，因此所有者及所属组都是root。所以其他用户都不能操作）。因此，我做了如下修改：12345678# 首先，创建用户组groupadd ftpgroup# 将用户添加进用户组usermod -g ftpgroup ftp1# 更改用户根目录所属组chown -R :ftpgroup ceshi/# 更改用户根目录所属组权限为可写chmod -R g+w ceshi/ 4、ftp 拒绝登陆，如下： 原因是配置文件中 allow_writeable_chroot=YES 不存在，而chroot_local_user=YES已经开启。所以导致 ftp 拒绝登陆。]]></content>
      <categories>
        <category>服务器</category>
        <category>FTP</category>
      </categories>
      <tags>
        <tag>FTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FTP 服务器配置流程详解]]></title>
    <url>%2F2018%2F03%2F19%2FFTP%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E6%B5%81%E7%A8%8B%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1、修改 FTP服务器配置文件① 安装完之后在/etc/vsftpd/路径下会存在三个配置文件。 vsftpd.conf： 主配置文件ftpusers： 指定哪些用户不能访问FTP服务器,这里的用户包括root在内的一些重要用户。user_list： 指定的用户是否可以访问ftp服务器，通过vsftpd.conf文件中的userlist_deny的配置来决定配置中的用户是否可以访问，userlist_enable=YES ，userlist_deny=YES ，userlist_file=/etc/vsftpd/user_list 这三个配置允许文件中的用户访问FTP。（注：文件中用户名一行一个） userlist_enable=YES, userlist_deny=NO(或缺省), user_list文件中的用户允许登陆，未出现在此文件中的用户不允许（默认） userlist_enable=NO(或缺省), userlist_deny=YES, user_list文件中的用户不允许登陆，未出现再次文件中的用户允许 userlist_enable=YES, userlist_deny=YES, user_list文件中的用户不能访问FTP（注：ftpusers文件中的用户为禁止登陆用户，此文件的权限大于user_list文件的权限） ② vsftpd.conf 默认配置如下：1234567891011121314[root@VM_0_7_centos ~]# cat /etc/vsftpd/vsftpd.conf.bak | grep &apos;^[^#]&apos;anonymous_enable=YESlocal_enable=YESwrite_enable=YESlocal_umask=022dirmessage_enable=YESxferlog_enable=YESconnect_from_port_20=YESxferlog_std_format=YESlisten=NOlisten_ipv6=YESpam_service_name=vsftpduserlist_enable=YEStcp_wrappers=YES ③ 设置 chroot 来限制用户根目录（一般此处是为了限制FTP用户到网站根目录）123chroot_local_user=YESchroot_list_file=/etc/vsftpd/chroot_listallow_writeable_chroot=YES 选项 chroot_local_user=YES 意味着本地用户将进入 chroot 环境，当登录以后默认情况下是其 home 目录。 选项 chroot_list_file=/etc/vsftpd/chroot_list 指定在此文件中的用户被限制在根目录中。（此文件缺省时，表示所有用户都被限制） 选项 allow_writeable_chroot=YES 可以允许 chroot 目录具有可写权限。（注：vsftpd默认是不允许chroot目录有可写权限） 2、创建 FTP用户① 增加 FTP组groupadd ftpgroup ② 增加用户并设置其目录为 /data/www/ceshiuseradd -g ftpgroup -d /data/www/ceshi -M ftp1 ③ 设置用户口令passwd ftp1 ④ 更改用户根目录所属组及权限12chown -R :ftpgroup /data/www/ceshichmod -R g+w /data/www/ceshi ⑤ 将用户添加进 chroot_list文件：（即设置用户根目录）vi /etc/vsftpd/chroot_list（注：一行一个用户名） ⑥ 重启vsftpdsystemctl restart vsftpd.service 3、测试 FTP可通过 FileZilla 连接FTP服务器 ① 点击‘文件’，打开站点管理器② 如果有跳过此步，如果没有设置新站点③ 协议选择“FTP文件传输协议”即可④ 加密选择“普通FTP”即可⑤ 传输设置 选择主动模式（即PORT模式）⑥ 其它信息填写完整，点击连接 如果报错，请看另一篇《FTP 客户端报错集锦》。 4、常用 FTP命令总结（1）建立用户账号 useradd -c（备注） 加上备注文字 -d（登入目录） 指定用户登入时的起始目录 -g（群组） 指定用户所属的群组 -s 指定用户登入后所使用的shell例如：useradd -d /alidata/www81/ceshi -g ftp -s /sbin/nologin ftp3 （2）删除用户账号 userdel语法： userdel [-r] [用户账号]补充说明：userdel 可删除用户账号与相关的文件。若不加参数，则仅删除用户账号，而不删除相关文件。例如：userdel -r ftp3 （3）修改用户账号 usermod语法： usermod [-LU][-c &lt;备注&gt;][-d &lt;登入目录&gt;][-g &lt;群组&gt;][-s ]补充说明：可用来修改用户账号的各项设定例如：usermod -s /sbin/nologin tes 5、附本人配置的 vsftpd.conf12345678910111213141516anonymous_enable=YESlocal_enable=YESwrite_enable=YESlocal_umask=022dirmessage_enable=YESxferlog_enable=YESconnect_from_port_20=YESxferlog_std_format=YESchroot_local_user=YESchroot_list_file=/etc/vsftpd/chroot_listlisten=NOlisten_ipv6=YESpam_service_name=vsftpduserlist_enable=YEStcp_wrappers=YESallow_writeable_chroot=YES]]></content>
      <categories>
        <category>服务器</category>
        <category>FTP</category>
      </categories>
      <tags>
        <tag>FTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FTP 服务器概述及安装、卸载]]></title>
    <url>%2F2018%2F03%2F19%2FFTP%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%A6%82%E8%BF%B0%E5%8F%8A%E5%AE%89%E8%A3%85%E3%80%81%E5%8D%B8%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[1、FTP概述FTP（文件传输协议）是一个较老且最常用的标准网络协议，用于在两台计算机之间通过网络上传/下载文件。FTP采用客户/服务器模式，客户机与服务器之间利用TCP建立连接，客户可以从服务器上下载文件，也可以把本地文件上传至服务器。FTP服务器有匿名的和授权的两种。匿名的FTP服务器向公众开放，用户可以用“ftp”或“anonymous”为帐号，用电子邮箱地址为密码登录服务器；授权的FTP服务器必须用授权的账户名和密码才能登录服务器。通常匿名的用户权限较低，只能下载文件，不能上传文件。客户机访问FTP服务器通常有两种方法：用FTP命令访问和用FTP客户端软件访问。 然而， FTP 最初的时候并不安全，因为它仅通过用户凭证（用户名和密码）传输数据，没有进行加密。如果你打算使用 FTP，需要考虑通过 SSL/TLS 配置 FTP 连接。否则，使用安全 FTP，比如 SFTP 会更好一些。 2、知识点介绍（1）FTP采用双TCP 连接方式 控制连接-使用TCP端口号21 用于在FTP客户端和FTP服务器之间传输FTP控制命令及命令执行信息。控制连接在整个FTP会话期间一直保持打开。数据连接-使用TCP端口号20 用于传输数据，包括数据上传、下载、文件列表发送等。数据传输结束后数据连接将终止。 （2）FTP传输方式 FTP主动数据传输方式主动方式也称为PORT方式，是FTP协议最初定义的数据传输连接方式，主要特点是： FTP客户端通过向FTP服务器发送PORT命令，告诉服务器该客户端用于传输数据的临时端口号（大于1024的随机端口） 当需要传送数据时，服务器通过TCP端口号20与客户端的临时端口建立数据传输通道，完成数据传输 在建立数据连接的过程中，由服务器主动发起连接，因此被称为主动方式。 FTP被动数据传输方式被动方式也称为PASV方式，被动方式的主要特点是： FTP客户端通过向FTP服务器发送PASV命令，告诉服务器进入被动方式。服务器选择临时端口号(1024~5000之间的随机端口)并告知客户端 当需要传送数据时，客户端主动与服务器的临时端口号建立数据传输通道，完成数据传输。 在整个过程中，由于服务器总是被动接收客户端的数据连接，因此被称为被动方式。 （3）FTP用户的类型匿名用户：anonymous或ftp本地用户： 帐号名称、密码等信息保存在passwd、shadow文件中虚拟用户： 使用独立的帐号/密码数据文件 user_list zhangsan 123456 /var/pub 3、在Centos 7中安装FTP 服务器非常简单，使用 yum只需要一条命令即可。yum -y install vsftpd 4、卸载FTP 服务器① 查看当前服务器中的vsftpd 包rpm -qa|grep vsftpd例如结果为：vsftpd-2.2.2-13.el6_6.1.x86_64 ② 执行卸载rpm -e vsftpd-2.2.2-13.el6_6.1.x86_64返回：卸载时自动备份vsftp的用户列表文件 warning: /etc/vsftpd/vsftpd.conf saved as /etc/vsftpd/vsftpd.conf.rpmsavewarning: /etc/vsftpd/user_list saved as /etc/vsftpd/user_list.rpmsave ③ 删除上面的文件rm -rf /etc/vsftpd ④ 查看vsftpd是否还在开机启动项中chkconfig --list ⑤ 查看vsftpd运行状态service vsftpd status 返回：vsftpd: unrecognized service（无法识别vsftpd，说明卸载了vsftpd了）]]></content>
      <categories>
        <category>服务器</category>
        <category>FTP</category>
      </categories>
      <tags>
        <tag>FTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iptables 防火墙的规则设置]]></title>
    <url>%2F2018%2F03%2F19%2Fiptables%E9%98%B2%E7%81%AB%E5%A2%99%E7%9A%84%E8%A7%84%E5%88%99%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[iptables 常用命令及参数注释： 主要包含：命令表 用来增加(-A、-I)删除(-D)修改(-R)查看(-L)规则等；常用参数 用来指定协议(-p)、源地址(-s)、源端口(--sport)、目的地址(-d)、目的端口(--dport)、进入网卡(-i)、出去网卡(-o)等设定包信息（即什么样的包）；用来描述要处理包的信息。常用处理动作 用 -j 来指定对包的处理(ACCEPT、DROP、REJECT、REDIRECT等)。 常用参数注释： 命令 -P, --policy范例 iptables -P INPUT DROP说明 定义过滤政策。 也就是未符合过滤条件之封包，预设的处理方式。 参数 --line-numbers范例 iptables -nL --line-numbers说明 查询规则链表的number号。 参数 -m state --state范例 iptables -A INPUT -m state --state RELATED,ESTABLISHED说明 用来比对联机状态，联机状态共有四种：INVALID、ESTABLISHED、NEW 和 RELATED。 INVALID 表示该封包的联机编号（Session ID）无法辨识或编号不正确。ESTABLISHED 表示该封包属于某个已经建立的联机。NEW 表示该封包想要起始一个联机（重设联机或将联机重导向）。RELATED 表示该封包是属于某个已经建立的联机，所建立的新联机。例如：FTP-DATA 联机必定是源自某个 FTP 联机。 常用规则总结（1）启动、停止和重启 iptablessystemctl start/stop/restart/status iptables.service （2）查看iptables 现有规则iptables -L -niptables -Ln --line-numbers #查看对应的规则numbersiptables-save （3）屏蔽某个 IPiptables -A INPUT -s xxx.xxx.xxx.xxx -j DROP如果你只想屏蔽 TCP 流量，可以使用 -p 参数的指定协议，例如：iptables -A INPUT -p tcp -s xxx.xxx.xxx.xxx -j DROP （4）解封某个 IP地址（即删除）iptables -D INPUT -s xxx.xxx.xxx.xxx -j DROP或iptables -D INPUT numbers （5）关闭指定端口阻止特定的传出连接：iptables -A OUTPUT -p tcp --dport xxx -j DROP阻止特定的传入连接：iptables -A INPUT -p tcp --dport xxx -j ACCEPT （6）使用Multiport控制多端口使用 multiport 我们可以一次性在单条规则中写入多个端口，例如：iptables -A INPUT -p tcp -m multiport --dports 22,80,443 -j ACCEPTiptables -A OUTPUT -p tcp -m multiport --sports 22,80,443 -j ACCEPT （7）在规则中使用 IP 地址范围在 IPtables 中 IP 地址范围是可以直接使用 CIDR 进行表示的，例如：iptables -A OUTPUT -p tcp -d 192.168.100.0/24 --dport 22 -j ACCEPT （8）禁止PING对 Linux 禁 PING 可以使用如下规则屏蔽 ICMP 传入连接：iptables -A INPUT -p icmp -i eth0 -j DROP （9）允许访问回环网卡环回访问（127.0.0.1）是比较重要的，建议大家都开放：iptables -A INPUT -i lo -j ACCEPTiptables -A OUTPUT -o lo -j ACCEPT （10）丢弃无效数据包很多网络攻击都会尝试用黑客自定义的非法数据包进行尝试，我们可以使用如下命令来丢弃无效数据包：iptables -A INPUT -m conntrack --ctstate INVALID -j DROP （11）插入指定位置一条规则iptables -I INPUT number -p tcp -m tcp --dport 21 -j ACCEPT（注：插入对应的number位置，原number位置往后移） 常用端口总结 服务 端口 是否开启 协议类型 描述 HTTP 80 建议开启 tcp Web服务 HTTPS 443 建议开启 tcp Web加密服务 POP3 110 不建议开启 tcp POP邮局协议 SMTP 25 不建议开启 tcp SMTP邮件传输协议 FTP 21 不建议开启 tcp FTP服务 SFTP 115 不建议开启 tcp SFTP安全文本传输协议 SSH 22 必须开启 tcp SSH服务 Telnet 23 不建议开启 tcp 不安全的文本传送 DNS 53 建议开启 tcp DNS域名服务 TOMCAT 8080 不建议开启 tcp tomcat服务器端口 MySQL 3306 不建议开启 tcp MySQL服务器端口 附本人主机iptables规则脚本1234567891011121314151617181920212223242526272829#!/bin/bashiptables -F # 清空所有默认规则iptables -X # 清空所有自定义规则iptables -Z # 所有封包计数器归0# 设置默认的访问规则iptables -P INPUT DROPiptables -P OUTPUT ACCEPTiptables -P FORWARD ACCEPT# 允许接受本机请求之后的返回数据（ESTABLISHED 是已经建立连接的状态）；RELATED 是为了FTP设置的iptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPTiptables -A INPUT -p icmp -j ACCEPT # 允许 pingiptables -A INPUT -i lo -j ACCEPT # 允许访问回环网卡# 自定义开启一些端口iptables -A INPUT -p tcp --dport 22 -j ACCEPTiptables -A INPUT -p tcp --dport 80 -j ACCEPTiptables -A INPUT -p tcp --dport 443 -j ACCEPTiptables -A INPUT -p tcp --dport 3690 -j ACCEPT# 定义对应规则没有仍返回相应信息iptables -A INPUT -j REJECT --reject-with icmp-host-prohibitediptables -A FORWARD -j REJECT --reject-with icmp-host-prohibited# 保存更改并重启服务service iptables savesystemctl restart iptables.service]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[系统服务管理详解]]></title>
    <url>%2F2018%2F03%2F16%2F%E7%B3%BB%E7%BB%9F%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[透过systemctl 管理服务基本上，systemd 这个启动服务的机制，主要是透过一只名为systemctl 的指令来处理的！跟以前systemV 需要service / chkconfig / setup / init 等指令来协助不同，systemd 就是仅有systemctl 这个指令来处理而已！所以全部的行为都得要使用systemctl。 1、单一服务(service unit) 的启动/开机启动与观察状态一般来说，服务的启动有两个阶段，一个是『开机的时候设定要不要启动这个服务』，以及『你现在要不要启动这个服务』，这两者之间有很大的差异。123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@study ~]# systemctl [command] [unit]command主要有：start ：立刻启动后面接的unitstop ：立刻关闭后面接的unitrestart ：立刻关闭后启动后面接的unit，亦即执行stop 再start 的意思reload ：不关闭后面接的unit 的情况下，重新载入设定档，让设定生效enable ：设定下次开机时，后面接的unit 会被启动disable ：设定下次开机时，后面接的unit 不会被启动status ：目前后面接的这个unit 的状态，会列出有没有正在执行、开机预设执行否、登录等资讯等！is-active ：目前有没有正在运作中is-enabled：开机时有没有预设要启用这个unit范例一：看看目前atd这个服务的状态为何？[root@study ~]# systemctl status atd.serviceatd.service - Job spooling tools Loaded: loaded (/usr/lib/systemd/system/atd.service; enabled) Active: active (running) since Mon 2015-08-10 19:17:09 CST; 5h 42min ago Main PID: 1350 (atd) CGroup: /system.slice/atd.service └─1350 /usr/sbin/atd -fAug 10 19:17:09 study.centos.vbird systemd[1]: Started Job spooling tools.# 重点在第二、三行喔～# Loaded：这行在说明，开机的时候这个unit 会不会启动，enabled 为开机启动，disabled 开机不会启动# Active：现在这个unit 的状态是正在执行(running) 或没有执行(dead)# 后面几行则是说明这个unit 程序的PID 状态以及最后一行显示这个服务的登录档资讯！# 登录档资讯格式为：『时间』 『讯息发送主机』 『哪一个服务的讯息』 『实际讯息内容』# 所以上面的显示讯息是：这个atd 预设开机就启动，而且现在正在运作的意思！范例二：正常关闭这个atd服务 [root@study ~]# systemctl stop atd.service[root@study ~]# systemctl status atd.serviceatd.service - Job spooling tools Loaded: loaded (/usr/lib/systemd/system/atd.service; enabled ) Active: inactive (dead) since Tue 2015-08-11 01:04:55 CST; 4s ago Process: 1350 ExecStart=/usr/sbin/atd -f $OPTS (code=exited, status=0/SUCCESS) Main PID: 1350 (code=exited, status=0/SUCCESS)Aug 10 19:17:09 study.centos.vbird systemd[1]: Started Job spooling tools.Aug 11 01:04:55 study.centos.vbird systemd[1]: Stopping Job spooling tools...Aug 11 01:04:55 study.centos.vbird systemd[1]: Stopped Job spooling tools.#目前这个unit下次开机还是会启动，但是现在是没在运作的状态中！同时，# 最后两行为新增加的登录讯息，告诉我们目前的系统状态喔！ 上面的范例中，我们已经关掉了atd ！这样作才是对的！不应该使用kill 的方式来关掉一个正常的服务！否则systemctl 会无法继续监控该服务的！那就比较麻烦。而使用systemtctl status atd 的输出结果中，第2, 3 两行很重要，因为那个是告知我们该unit 下次开机会不会预设启动，以及目前启动的状态！相当重要！最底下是这个unit 的登录档。如果你的这个unit 曾经出错过，观察这个地方也是相当重要的！ 再回到systemctl status atd.service 的第三行，不是有个Active 的daemon 现在状态吗？除了running 跟dead 之外， 有没有其他的状态呢？有的，基本上有几个常见的状态： active (running)：正有一只或多只程序正在系统中执行的意思，举例来说，正在执行中的vsftpd就是这种模式。 active (exited)：仅执行一次就正常结束的服务，目前并没有任何程序在系统中执行。举例来说，开机或者是挂载时才会进行一次的quotaon功能，就是这种模式！quotaon不须一直执行～只须执行一次之后，就交给档案系统去自行处理啰！通常用bash shell写的小型服务，大多是属于这种类型(无须常驻记忆体)。 active (waiting)：正在执行当中，不过还再等待其他的事件才能继续处理。举例来说，列印的伫列相关服务就是这种状态！虽然正在启动中，不过，也需要真的有伫列进来(列印工作)这样他才会继续唤醒印表机服务来进行下一步列印的功能。 inactive：这个服务目前没有运作的意思。 既然daemon 目前的状态就有这么多种了，那么daemon 的预设状态有没有可能除了enable/disable 之外，还有其他的情况呢？当然有！ enabled：这个daemon将在开机时被执行 disabled：这个daemon在开机时不会被执行 static：这个daemon不可以自己启动(enable不可)，不过可能会被其他的enabled的服务来唤醒(相依属性的服务) mask：这个daemon无论如何都无法被启动！因为已经被强制注销(非删除)。可透过systemctl unmask方式改回原本状态 2、服务启动/关闭与观察的练习我们直接使用指令的方式来查询与设定看看：123456789101112131415161718# 1.观察一下状态，确认是否为关闭/未启动呢？[root@study ~]# systemctl status chronyd.servicehronyd.service - NTP client/server Loaded: loaded (/usr/lib/systemd/system/chronyd.service; enabled ) Active: active (running) since Mon 2015-08-10 19:17:07 CST; 24h ago .....(底下省略).....# 2.由上面知道目前是启动的，因此立刻将他关闭，同时开机不会启动才行！[root@study ~]# systemctl stop chronyd.service[root@study ~]# systemctl disable chronyd.servicerm &apos;/etc/systemd/system/multi-user.target.wants/chronyd.service&apos;# 看得很清楚～其实就是从/etc/systemd/system 底下删除一条连结档案而已～[root@study ~]# systemctl status chronyd.servicechronyd.service - NTP client/server Loaded: loaded (/usr/lib/systemd/system/chronyd.service; disabled ) Active: inactive (dead)#如此则将chronyd这个服务完整的关闭了！ 上面是一个很简单的练习，妳先不要知道chronyd 是啥东西，只要知道透过这个方式，可以将一个服务关闭就是了！ 3、强迫服务注销(mask) 的练习透过mask 的方式来将服务注销：12345678910111213# 1.保持刚刚的状态，关闭cups.service，启动cups.socket，然后注销cups.servcie [root@study ~]# systemctl stop cups.service[root@study ~]# systemctl mask cups.serviceln -s &apos;/dev/null&apos; &apos;/etc/systemd/system/cups.service&apos;# 喔耶～其实这个mask 注销的动作，只是让启动的脚本变成空的装置而已！[root@study ~]# systemctl status cups.servicecups.service Loaded: masked (/dev/null) Active: inactive (dead) since Tue 2015-08-11 23:14:16 CST; 52s ago[root@study ~]# systemctl start cups.serviceFailed to issue method call: Unit cups.service is masked. #再也无法唤醒 上面的范例你可以仔细推敲一下～原来整个启动的脚本设定档被连结到/dev/null 这个空装置～因此，无论如何你是再也无法启动这个cups.service 了！透过这个mask 功能，你就可以不必管其他相依服务可能会启动到这个想要关闭的服务了！虽然是非正规，不过很有效！ 那如何取消注销呢？当然就是unmask 即可啊！ 4、透过systemctl 观察系统上所有的服务上面谈到的是单一服务的启动/关闭/观察，以及相依服务要注销的功能。那系统上面有多少的服务存在呢？这个时候就得要透过list-units 及list-unit-files 来观察了！细部的用法如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@study ~]# systemctl [command] [--type=TYPE] [--all]command: list-units ：依据unit 列出目前有启动的unit。若加上--all 才会列出没启动的。 list-unit-files ：依据/usr/lib/systemd/system/ 内的档案，将所有档案列表说明。--type=TYPE：就是之前提到的unit type，主要有service, socket, target 等范例一：列出系统上面有启动的unit [root@study ~]# systemctlUNIT LOAD ACTIVE SUB DESCRIPTIONproc-sys-fs-binfmt_mis... loaded active waiting Arbitrary Executable File Formats File Systemsys-devices-pc...:0:1:... loaded active plugged QEMU_HARDDISKsys-devices-pc...0:1-0... loaded active plugged QEMU_HARDDISKsys-devices-pc...0:0-1... loaded active plugged QEMU_DVD-ROM.....(中间省略).....vsftpd.service loaded active running Vsftpd ftp daemon.....(中间省略).....cups.socket loaded failed failed CUPS Printing Service Sockets.....(中间省略).....LOAD = Reflects whether the unit definition was properly loaded.ACTIVE = The high-level unit activation state, ie generalization of SUB.SUB = The low-level unit activation state, values depend on unit type.141 loaded units listed. Pass --all to see loaded but inactive units, too.To show all installed unit files use &apos;systemctl list-unit-files&apos;.#列出的项目中，主要的意义是：# UNIT ：项目的名称，包括各个unit 的类别(看副档名)# LOAD ：开机时是否会被载入，预设systemctl 显示的是有载入的项目而已喔！# ACTIVE ：目前的状态，须与后续的SUB 搭配！就是我们用systemctl status 观察时，active 的项目！# DESCRIPTION ：详细描述啰# cups 比较有趣，因为刚刚被我们玩过，所以ACTIVE 竟然是failed 的喔！被玩死了！^_^# 另外，systemctl 都不加参数，其实预设就是list-units 的意思！范例二：列出所有已经安装的unit有哪些？[root@study ~]# systemctl list-unit-filesUNIT FILE STATEproc-sys-fs-binfmt_misc.automount staticdev-hugepages.mount staticdev-mqueue.mount staticproc-fs-nfsd.mount static.....(中间省略).....systemd-tmpfiles-clean.timer static336 unit files listed.范例三：假设我不想要知道这么多的unit 项目，我只想要知道service 这种类别的daemon 而已，而且不论是否已经启动，通通要列出来！那该如何是好？[root@study ~]# systemctl list-units --type=service --all #只剩下*.service的项目才会出现喔！范例四：查询系统上是否有以cpu为名的服务？[root@study ~]# systemctl list-units --type=service --all | grep cpucpupower.service loaded inactive dead Configure CPU power related settings# 确实有喔！可以改变CPU 电源管理机制的服务哩！ 使用systemctl list-unit-files 会将系统上所有的服务通通列出来～而不像list-units 仅以unit 分类作大致的说明。至于STATE 状态就是前两个小节谈到的开机是否会载入的那个状态项目啰！主要有enabled / disabled / mask / static 等等。 5、与systemd 的daemon 运作过程相关的目录简介我们在前几小节曾经谈过比较重要的systemd 启动脚本设定档在/usr/lib/systemd/system/, /etc/systemd/system/ 目录下，那还有哪些目录跟系统的daemon 运作有关呢？基本上是这样的： /usr/lib/systemd/system/：使用CentOS官方提供的软体安装后，预设的启动脚本设定档都放在这里，这里的资料尽量不要修改～要修改时，请到/etc/systemd/system底下修改较佳！ /run/systemd/system/：系统执行过程中所产生的服务脚本，这些脚本的优先序要比/usr/lib/systemd/system/高！ /etc/systemd/system/：管理员依据主机系统的需求所建立的执行脚本，其实这个目录有点像以前/etc/rc.d/rc5.d/Sxx之类的功能！执行优先序又比/run/systemd/system/高喔！ /etc/sysconfig/*：几乎所有的服务都会将初始化的一些选项设定写入到这个目录下，举例来说，mandb所要更新的man page索引中，需要加入的参数就写入到此目录下的man-db当中喔！而网路的设定则写在/etc/sysconfig/network-scripts/这个目录内。所以，这个目录内的档案也是挺重要的； /var/lib/：一些会产生资料的服务都会将他的资料写入到/var/lib/目录中。举例来说，资料库管理系统Mariadb的资料库预设就是写入/var/lib/mysql/这个目录下啦！ /run/：放置了好多daemon的暂存档，包括lock file以及PID file等等。 我们知道systemd 里头有很多的本机会用到的socket 服务，里头可能会产生很多的socket file ～那你怎么知道这些socket file 放置在哪里呢？很简单！还是透过systemctl 来管理！123456789101112131415161718192021[root@study ~]# systemctl list-socketsLISTEN UNIT ACTIVATES/dev/initctl systemd-initctl.socket systemd-initctl.service/dev/log systemd-journald.socket systemd-journald.service/run/dmeventd-client dm-event.socket dm-event.service/run/dmeventd-server dm-event.socket dm-event.service/run/lvm/lvmetad.socket lvm2-lvmetad.socket lvm2-lvmetad.service/run/systemd/journal/socket systemd-journald.socket systemd-journald.service/run/systemd/journal/stdout systemd-journald.socket systemd-journald.service/run/systemd/shutdownd systemd-shutdownd.socket systemd-shutdownd.service/run/udev/control systemd-udevd-control.socket systemd-udevd.service/var/run/avahi-daemon/socket avahi-daemon.socket avahi-daemon.service/var/run/cups/cups.sock cups.socket cups.service/var/run/dbus/system_bus_socket dbus.socket dbus.service/var/run/rpcbind.sock rpcbind.socket rpcbind.service@ISCSIADM_ABSTRACT_NAMESPACE iscsid.socket iscsid.service@ISCSID_UIP_ABSTRACT_NAMESPACE iscsiuio.socket iscsiuio.servicekobject-uevent 1 systemd-udevd-kernel.socket systemd-udevd.service16 sockets listed.Pass --all to see loaded but inactive sockets, too. 早期的System V 的init管理行为中 daemon的主要分类基本上init 的管理机制有几个特色如下： 服务的启动、关闭与观察等方式： 所有的服务启动脚本通通放置于/etc/init.d/底下，基本上都是使用bash shell script所写成的脚本程式，需要启动、关闭、重新启动、观察状态时，可以透过如下的方式来处理： 启动：/etc/init.d/daemon start 关闭：/etc/init.d/daemon stop 重新启动：/etc/init.d/daemon restart 状态观察：/etc/init.d/daemon status 服务启动的分类： init服务的分类中，依据服务是独立启动或被一只总管程式管理而分为两大类： 独立启动模式(stand alone)：服务独立启动，该服务直接常驻于记忆体中，提供本机或用户的服务行为，反应速度快。 总管程式(super daemon)：由特殊的xinetd 或inetd 这两个总管程式提供socket 对应或port 对应的管理。当没有用户要求某socket 或port 时， 所需要的服务是不会被启动的。若有用户要求时， xinetd 总管才会去唤醒相对应的服务程式。当该要求结束时，这个服务也会被结束掉～ 因为透过xinetd 所总管，因此这个家伙就被称为super daemon。好处是可以透过super daemon 来进行服务的时程、连线需求等的控制，缺点是唤醒服务需要一点时间的延迟。 服务的相依性问题： 服务是可能会有相依性的～例如，你要启动网路服务，但是系统没有网路，那怎么可能可以唤醒网路服务呢？如果你需要连线到外部取得认证伺服器的连线，但该连线需要另一个A服务的需求，问题是，A服务没有启动，因此，你的认证服务就不可能会成功启动的！这就是所谓的服务相依性问题。init在管理员自己手动处理这些服务时，是没有办法协助相依服务的唤醒的！ 执行等级的分类： 上面说到init是开机后核心主动呼叫的，然后init可以根据使用者自订的执行等级(runlevel)来唤醒不同的服务，以进入不同的操作界面。基本上Linux提供7个执行等级，分别是0, 1, 2…6 ，比较重要的是1)单人维护模式、3)纯文字模式、5)文字加图形界面。而各个执行等级的启动脚本是透过/etc/rc.d/rc[0-6]/SXXdaemon连结到/etc/init.d/daemon ，连结档名(SXXdaemon)的功能为： S为启动该服务，XX是数字，为启动的顺序。由于有SXX的设定，因此在开机时可以『依序执行』所有需要的服务，同时也能解决相依服务的问题。这点与管理员自己手动处理不太一样就是了。 制定执行等级预设要启动的服务： 若要建立如上提到的SXXdaemon的话，不需要管理员手动建立连结档，透过如下的指令可以来处理预设启动、预设不启动、观察预设启动否的行为： 预设要启动： chkconfig daemon on 预设不启动： chkconfig daemon off 观察预设为启动否： chkconfig –list daemon 执行等级的切换行为： 当你要从纯文字界面(runlevel 3)切换到图形界面(runlevel 5)，不需要手动启动、关闭该执行等级的相关服务，只要『 init 5 』即可切换，init这小子会主动去分析/etc/rc.d/rc[35].d/这两个目录内的脚本，然后启动转换runlevel中需要的服务～就完成整体的runlevel切换。 基本上init 主要的功能都写在上头了，重要的指令包括daemon 本身自己的脚本(/etc/init.d/daemon) 、xinetd 这个特殊的总管程式(super daemon)、设定预设开机启动的chkconfig， 以及会影响到执行等级的init N 等。虽然CentOS 7 已经不使用init 来管理服务了，不过因为考量到某些脚本没有办法直接塞入systemd 的处理，因此这些脚本还是被保留下来， 所以，我们在这里还是稍微介绍了一下。 systemd 使用的unit 分类从CentOS 7.x 以后，Red Hat 系列的distribution 放弃沿用多年的System V 开机启动服务的流程，就是前面提到的init 启动脚本的方法，改用systemd 这个启动服务管理机制。那么systemd 有什么好处呢？ 平行处理所有服务，加速开机流程：旧的init启动脚本是『一项一项任务依序启动』的模式，因此不相依的服务也是得要一个一个的等待。但目前我们的硬体主机系统与作业系统几乎都支援多核心架构了，没道理未相依的服务不能同时启动啊！systemd就是可以让所有的服务同时启动，因此你会发现到，系统启动的速度变快了！ 一经要求就回应的on-demand启动方式：systemd全部就是仅有一只systemd服务搭配systemctl指令来处理，无须其他额外的指令来支援。不像systemV还要init, chkconfig, service…等等指令。此外， systemd由于常驻记忆体，因此任何要求(on-demand)都可以立即处理后续的daemon启动的任务。 服务相依性的自我检查：由于systemd可以自订服务相依性的检查，因此如果B服务是架构在A服务上面启动的，那当你在没有启动A服务的情况下仅手动启动B服务时， systemd会自动帮你启动A服务喔！这样就可以免去管理员得要一项一项服务去分析的麻烦～(如果读者不是新手，应该会有印象，当你没有启动网路，但却启动NIS/NFS时，那个开机时的timeout甚至可达到10~30分钟…) 依daemon功能分类：systemd旗下管理的服务非常多，包山包海啦～为了厘清所有服务的功能，因此，首先systemd先定义所有的服务为一个服务单位(unit)，并将该unit归类到不同的服务类型(type)去。旧的init仅分为stand alone与super daemon实在不够看，systemd将服务单位(unit)区分为service, socket, target, path, snapshot, timer等多种不同的类型(type)，方便管理员的分类与记忆。 将多个daemons集合成为一个群组：如同systemV的init里头有个runlevel的特色，systemd亦将许多的功能集合成为一个所谓的target项目，这个项目主要在设计操作环境的建置，所以是集合了许多的daemons，亦即是执行某个target就是执行好多个daemon的意思！ 向下相容旧有的init服务脚本：基本上， systemd是可以相容于init的启动脚本的，因此，旧的init启动脚本也能够透过systemd来管理，只是更进阶的systemd功能就没有办法支援就是了。 虽然如此，不过systemd 也是有些地方无法完全取代init 的！包括： 在runlevel 的对应上，大概仅有runlevel 1, 3, 5 有对应到systemd 的某些target 类型而已，没有全部对应； 全部的systemd 都用systemctl 这个管理程式管理，而systemctl 支援的语法有限制，不像/etc/init.d/daemon 就是纯脚本可以自订参数，systemctl 不可自订参数。； 如果某个服务启动是管理员自己手动执行启动，而不是使用systemctl 去启动的(例如你自己手动输入crond 以启动crond 服务)，那么systemd 将无法侦测到该服务，而无法进一步管理。 systemd 启动过程中，无法与管理员透过standard input 传入讯息！因此，自行撰写systemd 的启动设定时，务必要取消互动机制～(连透过启动时传进的标准输入讯息也要避免！) 不过，光是同步启动服务脚本这个功能就可以节省你很多开机的时间。同时systemd 还有很多特殊的服务类型(type) 可以提供更多有趣的功能！确实值得学一学～ 而且CentOS 7 已经用了systemd 了！既然要学，首先就得要针对systemd 管理的unit 来了解一下。 1、systemd 的设定档放置目录基本上， systemd 将过去所谓的daemon 执行脚本通通称为一个服务单位(unit)，而每种服务单位依据功能来区分时，就分类为不同的类型(type)。基本的类型有包括系统服务、资料监听与交换的插槽档服务(socket)、储存系统状态的快照类型、提供不同类似执行等级分类的操作环境(target) 等等。 设定档都放置在底下的目录中： /usr/lib/systemd/system/：每个服务最主要的启动脚本设定，有点类似以前的/etc/init.d底下的档案； /run/systemd/system/：系统执行过程中所产生的服务脚本，这些脚本的优先序要比/usr/lib/systemd/system/高！ /etc/systemd/system/：管理员依据主机系统的需求所建立的执行脚本，其实这个目录有点像以前/etc/rc.d/rc5.d/Sxx之类的功能！执行优先序又比/run/systemd/system/高喔！ 也就是说，到底系统开机会不会执行某些服务其实是看/etc/systemd/system/ 底下的设定，所以该目录底下就是一大堆连结档。而实际执行的systemd 启动脚本设定档， 其实都是放置在/usr/lib/systemd/system/ 底下的喔！因此如果你想要修改某个服务启动的设定，应该要去/usr/lib/systemd/system/ 底下修改才对！/etc/systemd/system/ 仅是连结到正确的执行脚本设定档而已。所以想要看执行脚本设定，应该就得要到/usr/lib/systemd/system/ 底下去查阅才对！ 2、systemd 的unit 类型分类说明那/usr/lib/systemd/system/ 以下的资料如何区分上述所谓的不同的类型(type) 呢？很简单！看副档名！举例来说，我们来瞧瞧上一章谈到的vsftpd 这个范例的启动脚本设定， 还有crond 与纯文字模式的multi-user 设定：123456789101112[root@study ~]# ll /usr/lib/systemd/system/ | grep -E &apos;(vsftpd|multi|cron)&apos;-rw-r--r--. 1 root root 284 7月30 2014 crond. service-rw-r--r--. 1 root root 567 3月6 06:51 multipathd.service-rw-r--r--. 1 root root 524 3月6 13:48 multi-user.targetdrwxr-xr-x. 2 root root 4096 5月4 17:52 multi-user.target.wantslrwxrwxrwx. 1 root root 17 5月4 17:52 runlevel2.target -&gt; multi-user.targetlrwxrwxrwx. 1 root root 17 5月4 17:52 runlevel3.target -&gt; multi-user.targetlrwxrwxrwx. 1 root root 17 5月4 17:52 runlevel4.target -&gt; multi-user.target-rw-r--r--. 1 root root 171 6月10 2014 vsftpd.service-rw-r--r--. 1 root root 184 6月10 2014 vsftpd@.service-rw-r--r--. 1 root root 89 6月10 2014 vsftpd.target# 比较重要的是上头提供的那三行特殊字体的部份！ 所以我们可以知道vsftpd 与crond 其实算是系统服务(service)，而multi-user 要算是执行环境相关的类型(target type)。根据这些副档名的类型， 我们大概可以找到几种比较常见的systemd 的服务类型如下： table th:first-of-type { width: 15%; } 副档名 主要服务功能 .service 一般服务类型(service unit)：主要是系统服务，包括伺服器本身所需要的本机服务以及网路服务都是！比较经常被使用到的服务大多是这种类型！所以，这也是最常见的类型了！ .socket 内部程序资料交换的插槽服务(socket unit)：主要是IPC (Inter-process communication) 的传输讯息插槽档(socket file) 功能。这种类型的服务通常在监控讯息传递的插槽档，当有透过此插槽档传递讯息来说要连结服务时，就依据当时的状态将该用户的要求传送到对应的daemon， 若daemon 尚未启动，则启动该daemon 后再传送用户的要求。使用socket 类型的服务一般是比较不会被用到的服务，因此在开机时通常会稍微延迟启动的时间 (因为比较没有这么常用嘛！)。一般用于本机服务比较多，例如我们的图形界面很多的软体都是透过socket 来进行本机程序资料交换的行为。(这与早期的xinetd 这个super daemon 有部份的相似喔！) .target 执行环境类型(target unit)：其实是一群unit 的集合，例如上面表格中谈到的multi-user.target 其实就是一堆服务的集合～也就是说， 选择执行multi-user.target 就是执行一堆其他.service 或/及.socket 之类的服务就是了！ .mount.automount 档案系统挂载相关的服务(automount unit / mount unit)：例如来自网路的自动挂载、NFS 档案系统挂载等与档案系统相关性较高的程序管理。 .path 侦测特定档案或目录类型(path unit)：某些服务需要侦测某些特定的目录来提供伫列服务，例如最常见的列印服务，就是透过侦测列印伫列目录来启动列印功能！这时就得要.path 的服务类型支援了！ .timer 循环执行的服务(timer unit)：这个东西有点类似anacrontab 喔！不过是由systemd 主动提供的，比anacrontab 更加有弹性！ 其中又以.service 的系统服务类型最常见了！因为我们一堆网路服务都是透过这种类型来设计的啊！接下来，让我们来谈谈如何管理这些服务的启动与关闭。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7下执行firewall-cmd显示ImportError: No module named 'gi']]></title>
    <url>%2F2018%2F03%2F16%2FCentOS7%E4%B8%8B%E6%89%A7%E8%A1%8Cfirewall-cmd%E6%98%BE%E7%A4%BAImportError%20No%20module%20named%20'gi'%2F</url>
    <content type="text"><![CDATA[在命令行下执行报错提示：12345[root@VM_0_7_centos ~]# firewall-cmd -hTraceback (most recent call last): File &quot;/usr/bin/firewall-cmd&quot;, line 24, in &lt;module&gt; from gi.repository import GObjectModuleNotFoundError: No module named &apos;gi&apos; 由错误信息也能够看出来是因为缺少 ‘gi’ 模块。CentOS7 默认自带安装的是Python2.7版本，由于我前段时间安装了最新版的 Python3.6，且是共存的。更改了默认的Python版本为Python3，因此一些Linux命令不能使用，原因就是这些命令使用的Python2版本，由于我安装的Python3版本，并没有把所有的需要的包都安装，因此会提示缺少某些模块。（注：由此处就说明，升级Python2到Python3的时候，要保留Python2） 正确的做法就是，如果使用一些命令提示类似错误的时候，就把命令文件的头部Python版本改为CentOS默认的2.7版本即可。更改方法如下：1234第一步，vim /usr/bin/firewall-cmd将#！/usr/bin/python -Es 改为 #！/usr/bin/python2 -Es（到目前为止，上面提到的问题已解决）第二步，vim /usr/sbin/firewalld将#！/usr/bin/python -Es 改为 #！/usr/bin/python2 -Es (这一步是针对于防火墙报错，进行的修改)]]></content>
      <categories>
        <category>Linux</category>
        <category>Linux问题</category>
      </categories>
      <tags>
        <tag>firewalld</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[防火墙之iptables和firewalld]]></title>
    <url>%2F2018%2F03%2F16%2F%E9%98%B2%E7%81%AB%E5%A2%99%E4%B9%8Biptables%E5%92%8Cfirewalld%2F</url>
    <content type="text"><![CDATA[在RHEL 7系统中，firewalld防火墙取代了iptables防火墙。对于接触Linux系统比较早或学习过RHEL 6系统的读者来说，当他们发现曾经掌握的知识在RHEL 7中不再适用，就需要全新学习firewalld。其实，iptables与firewalld都不是真正的防火墙，它们都只是用来定义防火墙策略的防火墙管理工具而已，或者说，它们只是一种服务。iptables服务会把配置好的防火墙策略交由内核层面的netfilter网络过滤器来处理，而firewalld服务则是把配置好的防火墙策略交由内核层面的nftables包过滤框架来处理。换句话说，当前在Linux系统中其实存在多个防火墙管理工具，旨在方便运维人员管理Linux系统中的防火墙策略，我们只需要配置妥当其中的一个就足够了。虽然这些工具各有优劣，但它们在防火墙策略的配置思路上是保持一致的。只要在这多个防火墙管理工具中任选一款并将其学透，就足以满足日常的工作需求了。 防火墙的分类：根据工作的层次的不同来划分，常见的防火墙工作在OSI第三层，即网络层防火墙，工作在OSI第七层的称为应用层防火墙，或者代理服务器（代理网关）。 1、网络层防火墙网络层防火墙又称包过滤防火墙，在网络层对数据包进行选择，选择的依据是系统内设置的过滤逻辑，被称为访问控制列表（ACL），通过检查数据流中每个数据的源地址，目的地址，所用端口号和协议状态等因素,来确定是否允许该数据包通过。特点：对用户来说透明，处理速度快且易于维护。但是一旦黑客突破防火墙，就可以轻易地伪造数据包的源地址，目的地址和IP的端口号，即“IP地址伪造”。 2、应用层防火墙代理服务型防火墙（Proxy Service）将所有跨越防火墙的网络通信链路分为两段。当代理服务器接收到用户对某个站点的访问请求后会检查该请求是否符合控制规则。如果规则允许，则代理服务器会替用户去那个站点取回所需要的信息，转发给用户。内外网用户的访问都是通过代理服务器上的“链接”来实现的，从而起到了隔离防火墙内外计算机系统的作用。特点：在应用层对数据进行检查，比较安全。但是会增加防火墙的负载。 包过滤防火墙将对每一个接收到的包做出允许或拒绝的决定。具体地讲，它针对每一个数据包的包头，按照包过滤规则进行判定，与规则相匹配的包依据路由信息继续转发，否则就丢弃。包过滤是在IP层实现的，包过滤根据数据包的源IP地址、目的IP地址、协议类型（TCP包、UDP包、ICMP包）、源端口、目的端口等包头信息及数据包传输方向等信息来判断是否允许数据包通过。包过滤也包括与服务相关的过滤，这是指基于特定的服务进行包过滤，由于绝大多数服务的监听都驻留在特定TCP/UDP端口，因此，为阻断所有进入特定服务的链接，防火墙只需将所有包含特定TCP/UDP目的端口的包丢弃即可。 现实生产环境中所使用的防火墙一般都是二者结合体。即先检查网络数据，通过之后再送到应用层去检查。 iptables 防火墙在早期的Linux系统中，默认使用的是iptables防火墙管理服务来配置防火墙。尽管新型的firewalld防火墙管理服务已经被投入使用多年，但是大量的企业在生产环境中依然出于各种原因而继续使用iptables。对于如何在CentOS7系统中，从firewalld防火墙更改为iptables防火墙，可查看我的一篇文章《解决CentOS7关闭/开启防火墙出现Unit iptables.service failed to load: No such file or directory.》。 1、策略与规则链防火墙会从上至下的顺序来读取配置的策略规则，在找到匹配项后就立即结束匹配工作并去执行匹配项中定义的行为（即放行或阻止）。如果在读取完所有的策略规则之后没有匹配项，就去执行默认的策略。一般而言，防火墙策略规则的设置有两种：一种是“通”（即放行），一种是“堵”（即阻止）。当防火墙的默认策略为拒绝时（堵），就要设置允许规则（通），否则谁都进不来；如果防火墙的默认策略为允许时，就要设置拒绝规则，否则谁都能进来，防火墙也就失去了防范的作用。 iptables服务把用于处理或过滤流量的策略条目称之为规则，多条规则可以组成一个规则链，而规则链则依据数据包处理位置的不同进行分类，具体如下： 在进行路由选择前处理数据包（PREROUTING）；处理流入的数据包（INPUT）；处理流出的数据包（OUTPUT）；处理转发的数据包（FORWARD）；在进行路由选择后处理数据包（POSTROUTING）。 一般来说，从内网向外网发送的流量一般都是可控且良性的，因此我们使用最多的就是INPUT规则链，该规则链可以增大黑客人员从外网入侵内网的难度。 2、基本的命令参数iptables是一款基于命令行的防火墙策略管理工具，具有大量参数，学习难度较大。好在对于日常的防火墙策略配置来讲，大家无需深入了解诸如“四表五链”的理论概念，只需要掌握常用的参数并做到灵活搭配即可，这就足以应对日常工作了。 iptables命令可以根据流量的源地址、目的地址、传输协议、服务类型等信息进行匹配，一旦匹配成功，iptables就会根据策略规则所预设的动作来处理这些流量。另外，再次提醒一下，防火墙策略规则的匹配顺序是从上至下的，因此要把较为严格、优先级较高的策略规则放到前面，以免发生错误。表8-1总结归纳了常用的iptables命令参数。表8-1：iptables中常用的参数以及作用 table th:first-of-type { width: 35%; } 参数 作用 -P 设置默认策略 -F 清空规则链 -L 查看规则链 -A 在规则链的末尾加入新规则 -I num 在规则链的头部加入新规则 -D num 删除某一条规则 -s 匹配来源地址IP/MASK，加叹号“!”表示除这个IP外 -d 匹配目标地址 -i 网卡名称 匹配从这块网卡流入的数据 -o 网卡名称 匹配从这块网卡流出的数据 -p 匹配协议，如TCP、UDP、ICMP –dport num 匹配目标端口号 –sport num 匹配来源端口号 至于，具体的示例可参考我的另一篇文章《Linux 防火墙的设定》。 firewalld 防火墙RHEL 7系统中集成了多款防火墙管理工具，其中firewalld（Dynamic Firewall Manager of Linux systems，Linux系统的动态防火墙管理器）服务是默认的防火墙配置管理工具，它拥有基于CLI（命令行界面）和基于GUI（图形用户界面）的两种管理方式。 相较于传统的防火墙管理配置工具，firewalld支持动态更新技术并加入了区域（zone）的概念。简单来说，区域就是firewalld预先准备了几套防火墙策略集合（策略模板），用户可以根据生产场景的不同而选择合适的策略集合，从而实现防火墙策略之间的快速切换。例如，我们有一台笔记本电脑，每天都要在办公室、咖啡厅和家里使用。按常理来讲，这三者的安全性按照由高到低的顺序来排列，应该是家庭、公司办公室、咖啡厅。当前，我们希望为这台笔记本电脑指定如下防火墙策略规则：在家中允许访问所有服务；在办公室内仅允许访问文件共享服务；在咖啡厅仅允许上网浏览。在以往，我们需要频繁地手动设置防火墙策略规则，而现在只需要预设好区域集合，然后只需轻点鼠标就可以自动切换了，从而极大地提升了防火墙策略的应用效率。firewalld中常见的区域名称（默认为public）以及相应的策略规则如表8-2所示。表8-2：firewalld中常用的区域名称及策略规则 区域 默认规则策略 trusted 允许所有的数据包 home 拒绝流入的流量，除非与流出的流量相关；而如果流量与ssh、mdns、ipp-client、amba-client与dhcpv6-client服务相关，则允许流量 internal 等同于home区域 work 拒绝流入的流量，除非与流出的流量数相关；而如果流量与ssh、ipp-client与dhcpv6-client服务相关，则允许流量 public 拒绝流入的流量，除非与流出的流量相关；而如果流量与ssh、dhcpv6-client服务相关，则允许流量 external 拒绝流入的流量，除非与流出的流量相关；而如果流量与ssh服务相关，则允许流量 dmz 拒绝流入的流量，除非与流出的流量相关；而如果流量与ssh服务相关，则允许流量 block 拒绝流入的流量，除非与流出的流量相关 drop 拒绝流入的流量，除非与流出的流量相关 1、终端管理工具命令行终端是一种极富效率的工作方式，firewall-cmd是firewalld防火墙配置管理工具的CLI（命令行界面）版本。它的参数一般都是以“长格式”来提供的，大家不要一听到长格式就头大，因为RHEL 7系统支持部分命令的参数补齐，其中就包含这条命令（很酷吧）。也就是说，现在除了能用Tab键自动补齐命令或文件名等内容之外，还可以用Tab键来补齐表8-3中所示的长格式参数了（这太棒了）。表8-3：firewall-cmd命令中使用的参数以及作用 参数 作用 –get-default-zone 查询默认的区域名称 –set-default-zone=&lt;区域名称&gt; 设置默认的区域，使其永久生效 –get-zones 显示可用的区域 –get-services 显示预先定义的服务 –get-active-zones 显示当前正在使用的区域与网卡名称 –remove-source= 将源自此IP或子网的流量导向指定的区域 –remove-source= 不再将源自此IP或子网的流量导向某个指定区域 –add-interface=&lt;网卡名称&gt; 将源自该网卡的所有流量都导向某个指定区域 –change-interface=&lt;网卡名称&gt; 将某个网卡与区域进行关联 –list-all 显示当前区域的网卡配置参数、资源、端口以及服务等信息 –list-all-zones 显示所有区域的网卡配置参数、资源、端口以及服务等信息 –add-service=&lt;服务名&gt; 设置默认区域允许该服务的流量 –add-port=&lt;端口号/协议&gt; 设置默认区域允许该端口的流量 –remove-service=&lt;服务名&gt; 设置默认区域不再允许该服务的流量 –remove-port=&lt;端口号/协议&gt; 设置默认区域不再允许该端口的流量 –reload 让“永久生效”的配置规则立即生效，并覆盖当前的配置规则 –panic-on 开启应急状况模式 –panic-off 关闭应急状况模式 与Linux系统中其他的防火墙策略配置工具一样，使用firewalld配置的防火墙策略默认为运行时（Runtime）模式，又称为当前生效模式，而且随着系统的重启会失效。如果想让配置策略一直存在，就需要使用永久（Permanent）模式了，方法就是在用firewall-cmd命令正常设置防火墙策略时添加–permanent参数，这样配置的防火墙策略就可以永久生效了。但是，永久生效模式有一个“不近人情”的特点，就是使用它设置的策略只有在系统重启之后才能自动生效。如果想让配置的策略立即生效，需要手动执行firewall-cmd –reload命令。 2、常用命令汇总（1）安装firewalldyum install firewalld firewall-config （2）运行、停止、禁用firewalld启动：# systemctl start firewalld查看状态：# systemctl status firewalld 或者 firewall-cmd –state停止：# systemctl disable firewalld禁用：# systemctl stop firewalldsystemctl mask firewalldsystemctl unmask firewalld （3）配置firewalld查看版本：$ firewall-cmd --version查看帮助：$ firewall-cmd --help 查看设置：显示状态：$ firewall-cmd --state查看区域信息: $ firewall-cmd --get-active-zones查看指定接口所属区域：$ firewall-cmd --get-zone-of-interface=eth0拒绝所有包：# firewall-cmd --panic-on取消拒绝状态：# firewall-cmd --panic-off查看是否拒绝：$ firewall-cmd --query-panic 更新防火墙规则：# firewall-cmd --reload# firewall-cmd --complete-reload两者的区别就是第一个无需断开连接，就是firewalld特性之一动态添加规则，第二个需要断开连接，类似重启服务 将接口添加到区域，默认接口都在public# firewall-cmd --zone=public --add-interface=eth0永久生效再加上 –permanent 然后reload防火墙 设置默认接口区域# firewall-cmd --set-default-zone=public立即生效无需重启 打开端口（貌似这个才最常用）查看所有打开的端口：# firewall-cmd --zone=dmz --list-ports加入一个端口到区域：# firewall-cmd --zone=dmz --add-port=8080/tcp若要永久生效方法同上 打开一个服务，类似于将端口可视化，服务需要在配置文件中添加，/etc/firewalld 目录下有services文件夹，这个不详细说了，详情参考文档# firewall-cmd --zone=work --add-service=smtp移除服务# firewall-cmd --zone=work --remove-service=smtp 3、示例接下来的实验都很简单，但是提醒大家一定要仔细查看使用的是Runtime模式还是Permanent模式。如果不关注这个细节，就算是正确配置了防火墙策略，也可能无法达到预期的效果。 查看firewalld服务当前所使用的区域：12[root@linuxprobe ~]# firewall-cmd --get-default-zonepublic 查询eno16777728网卡在firewalld服务中的区域：12[root@linuxprobe ~]# firewall-cmd --get-zone-of-interface=eno16777728public 把f默认irewalld服务中eno16777728网卡的区域修改为external，并在系统重启后生效。分别查看当前与永久模式下的区域名称：123456[root@linuxprobe ~]# firewall-cmd --permanent --zone=external --change-interface=eno16777728success[root@linuxprobe ~]# firewall-cmd --get-zone-of-interface=eno16777728public[root@linuxprobe ~]# firewall-cmd --permanent --get-zone-of-interface=eno16777728external 把firewalld服务的当前默认区域设置为public：1234[root@linuxprobe ~]# firewall-cmd --set-default-zone=publicsuccess[root@linuxprobe ~]# firewall-cmd --get-default-zone public 启动/关闭firewalld防火墙服务的应急状况模式，阻断一切网络连接（当远程控制服务器时请慎用）：1234[root@linuxprobe ~]# firewall-cmd --panic-onsuccess[root@linuxprobe ~]# firewall-cmd --panic-offsuccess 查询public区域是否允许请求SSH和HTTPS协议的流量：1234[root@linuxprobe ~]# firewall-cmd --zone=public --query-service=sshyes[root@linuxprobe ~]# firewall-cmd --zone=public --query-service=httpsno 把firewalld服务中请求HTTPS协议的流量设置为永久允许，并立即生效：123456[root@linuxprobe ~]# firewall-cmd --zone=public --add-service=httpssuccess[root@linuxprobe ~]# firewall-cmd --permanent --zone=public --add-service=httpssuccess[root@linuxprobe ~]# firewall-cmd --reloadsuccess 把firewalld服务中请求HTTP协议的流量设置为永久拒绝，并立即生效：1234[root@linuxprobe ~]# firewall-cmd --permanent --zone=public --remove-service=http success[root@linuxprobe ~]# firewall-cmd --reload success 把在firewalld服务中访问8080和8081端口的流量策略设置为允许，但仅限当前生效：1234[root@linuxprobe ~]# firewall-cmd --zone=public --add-port=8080-8081/tcpsuccess[root@linuxprobe ~]# firewall-cmd --zone=public --list-ports 8080-8081/tcp 把原本访问本机888端口的流量转发到22端口，要且求当前和长期均有效：流量转发命令格式为firewall-cmd --permanent --zone=&lt;区域&gt; --add-forward-port=port=&lt;源端口号&gt;:proto=&lt;协议&gt;:toport=&lt;目标端口号&gt;:toaddr=&lt;目标IP地址&gt;1234[root@linuxprobe ~]# firewall-cmd --permanent --zone=public --add-forward-port=port=888:proto=tcp:toport=22:toaddr=192.168.10.10success[root@linuxprobe ~]# firewall-cmd --reloadsuccess 在客户端使用ssh命令尝试访问192.168.10.10主机的888端口：1234567[root@client A ~]# ssh -p 888 192.168.10.10The authenticity of host &apos;[192.168.10.10]:888 ([192.168.10.10]:888)&apos; can&apos;t be established.ECDSA key fingerprint is b8:25:88:89:5c:05:b6:dd:ef:76:63:ff:1a:54:02:1a.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &apos;[192.168.10.10]:888&apos; (ECDSA) to the list of known hosts.root@192.168.10.10&apos;s password:此处输入远程root管理员的密码Last login: Sun Jul 19 21:43:48 2017 from 192.168.10.10 firewalld中的富规则表示更细致、更详细的防火墙策略配置，它可以针对系统服务、端口号、源地址和目标地址等诸多信息进行更有针对性的策略配置。它的优先级在所有的防火墙策略中也是最高的。比如，我们可以在firewalld服务中配置一条富规则，使其拒绝192.168.10.0/24网段的所有用户访问本机的ssh服务（22端口）：1234[root@linuxprobe ~]# firewall-cmd --permanent --zone=public --add-rich-rule=&quot;rule family=&quot;ipv4&quot; source address=&quot;192.168.10.0/24&quot; service name=&quot;ssh&quot; reject&quot;success[root@linuxprobe ~]# firewall-cmd --reloadsuccess 在客户端使用ssh命令尝试访问192.168.10.10主机的ssh服务（22端口）：123[root@client A ~]# ssh 192.168.10.10Connecting to 192.168.10.10:22...Could not connect to &apos;192.168.10.10&apos; (port 22): Connection failed.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>firewalld</tag>
        <tag>网络安全</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVN 详解之多仓库管理及使用钩子hooks/post-commit实现代码自动部署（四）]]></title>
    <url>%2F2018%2F03%2F16%2FSVN-%E8%AF%A6%E8%A7%A3%E4%B9%8B%E5%A4%9A%E4%BB%93%E5%BA%93%E7%AE%A1%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8%E9%92%A9%E5%AD%90hooks-post-commit%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2%EF%BC%88%E5%9B%9B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[多仓库管理1、首先，需要明白几个概念：（1）多仓库管理，则每个仓库对应的配置都要单独管理。即该仓库允许那些人员访问，人员所具有的的权限等都需设定。 （2）每个仓库的配置文件只可管理本仓库的属性，不需要涉及到其他仓库，涉及到也没用。即仓库与仓库之间是独立的管理。 2、实战演示首先，新建2个仓库，project和common。12svnadmin create /var/svn/project //开放仓库svnadmin create /var/svn/common //共享资源仓库 分别对应仓库的conf/ 下文件进行配置，passwd与svnserve.conf配置类似，不予说明。其中，authz文件内容如下：12345678910111213# project:[groups]admin = admin, bentest = nana[/] == [project:/]@admin = rw@test = r @只有读的权限，可用于测试# common:[common:/]@test = rwben = rw 注：对于用户享有单独某个目录权限，可如下操作：1234[svn:/special1] == [/special1]ben = rw[svn:/special1/special2]nana = rw 3、问题解决 Invalid authz configurationsvn: Authorization failed解决办法 报此类错误，都是因为auth权限文件配置错误，仔细检查就可发现原因。 使用钩子hooks/post-commit实现代码自动部署配置了台svn服务器，用来保存公司项目的代码，同时svn服务器也是一台web服务器。因此希望当我本地代码commit到svn服务器时,能够触发svn服务器的钩子hooks/post-commit将新版本的代码自动update到站点目录上去。 svn 目录：/var/svn/project站点目录：/data/www/project/ 1、新建post-commit钩子找到svn项目的hooks目录，这里是/var/svn/project/hooks。目录中默认会几个对应操作的钩子模板，我们需要创建一个post-commit的文件。复制钩子文件，进行修改：cp hooks/post-commit.tmpl hooks/post-commit12345678910111213141516#!/bin/shexport LANG=zh_CN.UTF-8REPOS=&quot;$1&quot;REV=&quot;$2&quot;SVN_PATH=/usr/bin #svn命令路径WEB_PATH=/data/www/project/python_movie #项目路径，即已经检出的项目，具体到项目目录LOG_PATH=/data/www/project/python_movie/logs/svn_deploy.log #日志文件SVN_USER=benSVN_PASS=ben1234echo `date &quot;+%Y-%m-%d %H:%M:%S&quot;` &gt;&gt; $LOG_PATHecho `whoami`,$REPOS,%REV &gt;&gt; %LOG_PATH$SVN_PATH/svn update $WEB_PATH --username $SVN_USER --password $SVN_PASS --no-auth-cache &gt;&gt; $LOG_PATH 说明：whoami #执行此程序的用户REPOS=”$1” #svn项目绝对路径值REV=”$2” #最新版本号–no-auth-cache #不保存账户认证信息 2、修改post-commit文件权限12chown www:www /var/svn/python_movie/hooks/post-commit #设置脚本所属用户组，www为web服务运行账户和组chmod +x /var/svn/python_movie/hooks/post-commit #添加脚本执行权限 3、客户端测试测试的话，我这里是在本地修改版本库，点commit，然后再看web(nginx)服务器上的数据是否更新来测试。 4、问题解决Checkout一份代码到web服务器上12# cd /data/www/project/python_movie# /usr/bin/svn checkout svn://127.0.0.1/python_movie 在日志文件中提示Skipped “/data/www/project/python_movie”然后提交的文件并没有自动更新到web目录下 解决方法是：首先，需要在Web目录下检出SVN项目，生成 .svn目录。原因是 Web目录下没有 .svn目录，更新时钩子不能识别Web目录下的 .svn（因为没有），因此会跳过Web目录。12cd /data/www/project/python_moviesvn checkout svn://服务器的ip地址/python_movie ./ 然后再次提交的文件就可以自动更新到web目录下了。]]></content>
      <categories>
        <category>版本控制</category>
        <category>SVN</category>
      </categories>
      <tags>
        <tag>SVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 源码包与RPM包详解]]></title>
    <url>%2F2018%2F03%2F15%2FLinux%E6%BA%90%E7%A0%81%E5%8C%85%E4%B8%8ERPM%E5%8C%85%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[源码包与RPM包的区别1、安装之前的区别：概念上的区别比如说：源码包是开源的，比RPM包安装更自由，但是它安装更慢，更容易报错；RPM包是经过编译的，不能看到源代码，但是它安装更快，报错更容易解决，只有依赖性问题。 2、安装之后的区别：安装位置不同RPM包不需要指定安装位置，它会安装到系统默认位置；而源码包是人为手工设置的，下面我们就来看看到底位置有什么区别 （1）RPM包安装位置是按照在默认位置中RPM包默认安装路径 安装位置 对应目录功能 /etc/ 配置文件安装目录 /usr/bin/ 可执行的命令安装目录 /usr/lib/ 程序所使用的函数库保存位置 /usr/share/doc/ 基本的软件使用手册保存位置 /usr/share/man/ 帮助文件保存位置 （2）源码包安装位置安装在指定位置当中，一般是 /usr/local/软件名/ 3、安装位置不同带来的影响RPM包安装的服务可以使用系统服务管理命令（service）来管理，例如RPM包安装的apache的启动方法是：/etc/rc.d/init.d/httpd startservice httpd start 而源码包安装的服务则不能被服务管理命令管理，因为没有安装到默认路径中。所以只能用绝对路径进行服务的管理，如：/usr/local/apache2/bin/apachectl start RPM包详解1、rpm包命名结构rpm包的组成：name-version-release.arch.rpm name：表示包的名称，包括主包名和分包名version：表示包的版本信息release：用于标识rpm包本身的发行号，可还包含适应的操作系统arch:表示主机平台,noarch表示此包能安装到所以平台上面 例如：gd-devel-2.0.35-11.el6.x86_64.rpm gd是这个包的主包名，devel是这个包的分包名，2.0.35是表示版本信息，2为主版本，0为此版本，35为修订号，11.el6中的11是表示发行号，el6表示是RHEL6，x86_64是表示包适合的平台，如果是noarch这表示与平台无关 2、rpm包命令操作总结 table th:first-of-type { width: 15%; } table th:nth-of-type(2) { width: 17%; } table th:nth-of-type(3) { width: 50%; } table th:nth-of-type(4) { width: 40%; } &nbsp; option 参数解释 Example 安装 -i 安装 &#32; -v&#124;-vv&#124;-vvv 显示详细信息 &#32; -h 以#显示安装进度一个#表示2%的进度 rpm -ivh zsh-4.3.10-5.el6.x86_64.rpm –nodeps 忽略依赖关系 &#32; –test 测试安装 &#32; –replacepkgs 重新安装(安装的包已经安装了) &#32; 升级 -U -Uvh 升级+安装 &#32; -F -Fvh 升级(此包已经安装了) &#32; –force 有冲突强制升级 &#32; –nodeps 忽略包依赖性关系 &#32; 卸载 -e 卸载 rpm -e 包名 –nodeps 忽略包依赖性关系 &#32; 查询 -q&#124;–query &#32; rpm -q&#124;–query 包名 -qa 查看所有已经安装的包 rpm -qa 查看所有包名 rpm -qa &#124; grep 包名查看某个包名 -qi 查看包的摘要信息 &#32; -qf 查看文件是有那个包安装的 rpm -qf /path/to/file -ql 查看包安装生成的文件清单 &#32; -qc 查看包安装生成的配置文件 &#32; -qd 查看包安装生成的帮助文档 &#32; -q –scripts 查看相关的脚本 rpm -q–script 包名 -qp[i&#124;l&#124;d&#124;c] 查看尚未安装包的详细信息 rpm -qpi /path/to/rpm_file 校验 -V &#32; rpm -V 包名 数据库管理 –initdb 新建 rpm –initdb –rebuilddb 重建 rpm –rebuilddb 3、RPM包安装方法之 yum 安装yum(Yellowdog Update Manager),yum是RPM的前端工具，是基于RPM的一个管理工具，他能自动的解决安装rpm包产生的依赖关系。 yum 的配置文件 /etc/yum.confyum 的repository仓库的配置文件 /etc/yum.repos.d/*.repo yum常用命令总结 table th:nth-of-type(2) { width: 40%; } &#32; 操作命令 命令解释 列表 yum list &lt;package_name&gt; 列出指定安装软件的清单 yum list installed 列出所有已安装的软件包 yum list extras 列出所有已安装但不在 Yum 仓库內的软件包 yum grouplist 列出所有的组 yum grouplist “Group1” 列出指定组的软件包列表 安装 yum -y install &lt;package_name&gt; 安装指定的软件 yum -y groupinstall “Group1” “Group2” 安装指定的组 yum -y localinstall &lt;package_name&gt; 用yum安装下载到本地的rpm包 卸载 yum -y remove &lt;package_name&gt; 卸载指定的软件 更新 yum check-update 列出所有可更新的软件清单 yum update 安装所有更新软件 yum update &lt;package_name&gt; 更新指定的软件 信息 yum info 显示所有包的信息 yum info &lt;package_name&gt; 显示指定包的信息 yum groupinfo “Group1” “Group2” 显示指定组的信息 清除 yum clean all 清除所有yum所保存的信息 yum clean metadata 只清空保存的数据信息 其它操作 yum repolist [all&#124;enable&#124;disable] 查看yum仓库的个数，默认显示启用的 yum makecache 手动生成缓存 yum search &lt;package_name&gt; 查询rpm包 yum reinstall &lt;package_name&gt; 重新安装一遍 yum provides &lt;package_name&gt; 列出软件包提供哪些文件 源码包详解1、tar 源码包编译安装编译安装的三部曲:在安装三部曲之前，建议先看看解压之后目录里面的包含README, INSTALL文件，这里面的文件会告诉你详细安装步骤。 （1）configure 检测编译环境（2）make 进行编译（3）make install 编译安装]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决CentOS7关闭/开启防火墙出现Unit iptables.service failed to load: No such file or directory.]]></title>
    <url>%2F2018%2F03%2F15%2F%E8%A7%A3%E5%86%B3CentOS7%E5%85%B3%E9%97%AD-%E5%BC%80%E5%90%AF%E9%98%B2%E7%81%AB%E5%A2%99%E5%87%BA%E7%8E%B0Unit-iptables-service-failed-to-load-No-such-file-or-directory%2F</url>
    <content type="text"><![CDATA[CentOS7中执行service iptables start/stop 会报错Failed to start iptables.service: Unit iptables.service failed to load: No such file or directory. 在CentOS 7或RHEL 7或Fedora中防火墙由firewalld来管理，当然你可以还原传统的管理方式。或则使用新的命令进行管理。 1、还原传统的管理方式执行一下命令：12systemctl stop firewalld #停止 firewalldsystemctl mask firewalld #禁用 firewalld 并且安装iptables-services：yum install iptables-services 设置开机启动：systemctl enable iptables 启动iptables：systemctl start iptables 保存设置：service iptables save 或者 /usr/libexec/iptables/iptables.init save 常用命令：systemctl [stop|start|restart|reload] iptables（分开执行） OK，再试一下传统管理方式应该就好使了。 2、使用新的防火墙firewalld进行管理 待补充…]]></content>
      <categories>
        <category>Linux</category>
        <category>Linux问题</category>
      </categories>
      <tags>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVN 详解之常用操作命令（三）]]></title>
    <url>%2F2018%2F03%2F15%2FSVN%E8%AF%A6%E8%A7%A3%E4%B9%8B%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1、检出123svn checkout http://路径(目录或文件的全路径) [本地目录全路径] --username 用户名svn checkout svn://路径(目录或文件的全路径) [本地目录全路径] --username 用户名简写：svn co xxx 例子：svn checkout svn://localhost/测试工具 /home/testtools –username bensvn checkout http://localhost/test/testapp –username ben注：如果不带–password 参数传输密码的话，会提示输入密码，建议不要用明文的–password 选项。 其中 username 与 password前是两个短线，不是一个。 不指定本地目录全路径，则检出到当前目录下。 2、导出123svn export [-r 版本号] http://路径(目录或文件的全路径) [本地目录全路径] --username 用户名svn export [-r 版本号] svn://路径(目录或文件的全路径) [本地目录全路径] --username 用户名svn export 本地检出的(即带有.svn文件夹的)目录全路径 要导出的本地目录全路径 例子：svn export svn://localhost/测试工具 /home/testtools –username bensvn export svn://localhost/test/testapp –username bensvn export /home/testapp /home/testtools注：第一种从版本库导出干净工作目录树的形式是指定URL，如果指定了修订版本号，会导出相应的版本，如果没有指定修订版本，则会导出最新的，导出到指定位置。如果省略本地目录全路径，URL的最后一部分会作为本地目录的名字。第二种形式是指定 本地检出的目录全路径 到 要导出的本地目录全路径，所有的本地修改将会保留，但是不在版本控制下(即没提交的新文件，因为.svn文件夹里没有与之相关的信息记录)的文件不会拷贝。 3、添加新文件svn add 文件名注：告诉SVN服务器要添加文件了，还要用svn commint -m真实的上传上去！ 例子：svn add test.php #添加test.phpsvn commit -m “添加我的测试用test.php“ test.phpsvn add .php #添加当前目录下所有的php文件svn commit -m “添加我的测试用全部php文件“ .php 4、提交12svn commit -m “提交备注信息文本“ [-N] [--no-unlock] 文件名简写：svn ci xxx 注：必须带上-m参数，参数可以为空，但是必须写上-m 例子：svn commit -m “提交当前目录下的全部在版本控制下的文件“ #注意这个表示全部文件svn commit -m “提交我的测试用test.php“ test.phpsvn commit -m “提交我的测试用test.php“ -N –no-unlock test.php #保持锁就用–no-unlock开关svn ci -m “提交当前目录下的全部在版本控制下的文件“ #注意这个表示全部文件svn ci -m “提交我的测试用test.php“ test.phpsvn ci -m “提交我的测试用test.php“ -N –no-unlock test.php #保持锁就用–no-unlock开关 5、更新123svn updatesvn update -r 修正版本 文件名svn update 文件名 例子：svn update #后面没有目录，默认将当前目录以及子目录下的所有文件都更新到最新版本svn update -r 200 test.cpp #将版本库中的文件 test.cpp 还原到修正版本（revision）200svn update test.php #更新与版本库同步。 提交的时候提示过期冲突，需要先 update 修改文件，然后清除svn resolved，最后再提交commit。 6、删除1234svn delete svn://路径(目录或文件的全路径) -m “删除备注信息文本”推荐如下操作：svn delete 文件名 svn ci -m “删除备注信息文本” 例子：svn delete svn://localhost/testapp/test.php -m “删除测试文件test.php”推荐如下操作：svn delete test.phpsvn ci -m “删除测试文件test.php” 7、比较差异12svn diff 文件名 svn diff -r 修正版本号m:修正版本号n 文件名 例子：svn diff test.php #将修改的文件与基础版本比较svn diff -r 200:201 test.php #对 修正版本号200 和 修正版本号201 比较差异 8、查看文件或者目录状态123456svn st 目录路径/名svn status 目录路径/名 #目录下的文件和子目录的状态，正常状态不显示 【?：不在svn的控制中； M：内容被修改；C：发生冲突；A：预定加入到版本库；K：被锁定】 svn st -v 目录路径/名svn status -v 目录路径/名 #显示文件和子目录状态 【第一列保持相同，第二列显示工作版本号，第三和第四列显示最后一次修改的版本号和修改人】 注：svn status、svn diff和 svn revert这三条命令在没有网络的情况下也可以执行的，原因是svn在本地的.svn中保留了本地版本的原始拷贝。 9、查看日志1svn log 文件名 例子：svn log test.php #显示这个文件的所有修改记录，及其版本号的变化 10、查看文件详细信息1svn info file 例子：svn info test.php 11、帮助12svn help #全部功能选项svn help ci #具体功能的说明 12、加锁/解锁12svn lock -m “加锁备注信息文本“ [--force] 文件名 svn unlock 文件名 例子：svn lock -m “锁信测试用test.php文件“ test.phpsvn unlock test.php 13、查看版本库下的文件和目录列表12svn list svn://路径（目录或文件的全路径） 简写：svn ls xx 例子：svn list svn://localhost/testsvn ls svn://localhost/test #显示svn://localhost/test目录下的所有属于版本库的文件和目录 14、创建纳入版本控制下的新目录12svn mkdir 目录名svn mkdir -m &quot;新增目录备注文本&quot; http://目录全路径 例子：svn mkdir newdirsvn mkdir -m “Making a new dir.” svn://localhost/test/newdir注：添加完子目录后，一定要回到根目录更新一下，不然在该目录下提交文件会提示“提交失败” svn update 注：如果手工在 checkout 出来的目录中新建目录 newsubdir，再用 svn mkdir newsubdir 命令后，SVN会提示：svn: 尝试用 “svn add”或 “svn add –non-recursive”代替？svn: 无法创建目录“hello”: 文件已经存在此时用如下命令解决：svn add --non-recursive newsubdir在进入这个newsubdir文件夹，用ls -a查看它下面的全部目录与文件，会发现多了：.svn目录。再用 svn mkdir -m “添hello功能模块文件” svn://localhost/test/newdir/newsubdir 命令。 15、恢复本地修改1svn revert [--recursive] 文件名 注意: 本子命令不会存取网络，并且会解除冲突的状况。但是它不会恢复被删除的目录。 例子：svn revert foo.c #丢弃对一个文件的修改svn revert –recursive . #恢复一整个目录的文件，. 为当前目录 16、把工作拷贝更新到别的URL1svn switch http://目录全路径 本地目录全路径 例子：svn switch http://localhost/test/456 . # (原为123的分支)当前所在目录分支到localhost/test/456 17、解决冲突1svn resolved [本地目录全路径] 例子：$ svn updateC foo.cUpdated to revision 31.如果你在更新时得到冲突，你的工作拷贝会产生三个新的文件：$ lsfoo.cfoo.c.minefoo.c.r30foo.c.r31当你解决了foo.c的冲突，并且准备提交，运行svn resolved让你的工作拷贝知道你已经完成了所有事情。你可以仅仅删除冲突的文件并且提交，但是svn resolved除了删除冲突文件，还修正了一些记录在工作拷贝管理区域的记录数据，所以我们推荐你使用这个命令。 18、不checkout而查看输出特定文件或URL的内容1svn cat http://文件全路径 例子：svn cat http://localhost/test/readme.txt 19、新建一个分支copy1svn copy branchA branchB -m &quot;make B branch&quot; // 从branchA拷贝出一个新分支branchB 20、合并内容到分支merge1svn merge branchA branchB // 把对branchA的修改合并到分支branchB]]></content>
      <categories>
        <category>版本控制</category>
        <category>SVN</category>
      </categories>
      <tags>
        <tag>SVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVN 详解之客户端安装（二）]]></title>
    <url>%2F2018%2F03%2F15%2FSVN%E8%AF%A6%E8%A7%A3%E4%B9%8B%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%89%E8%A3%85%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Linux 下安装CentOS 系统可使用yum 软件包管理器直接安装，Debian、Ubuntu 系统可使用apt-get 软件包管理器直接安装。此处，我们使用一种通用的安装方式，源码安装。 1、下载及解压下载地址：传送门tar xvf subversion-1.9.7.tar.bz2 2、配置文件123mkdir /usr/local/subersioncd subversion-1.9.7./configure --prefix=/usr/local/subversion #指定安装路径 3、解析及安装文件1make &amp; make install 4、测试及建立软链接12345cd /usr/local/subversion/binsvnln -s /usr/local/subversion/bin/svn /sbin/svncd /svn Windows下安装1、SVN客户端下载及安装下载地址：传送门安装根据下一步操作即可完成… 2、SVN客户端配置这里要特别声明一下——SVN客户端不是指一个桌面应用程序，而是集成到系统的右键菜单中的插件。因此使用客户端向资源库下载项目资源、提交项目资源等都是通过右键菜单来完成的。 在桌面空白处右键： 选择 设置 ，打开设置面板：可以设置语言：也可以设置 项目资源的图标，通过不同图标来指示下载到本地的项目资源文件发生了什么变化，比如：修改、新增、删除等等。 3、SVN 基础操作（1）检出在你的本地项目文件夹或随便一个地方，右键空白处弹出菜单，选择 SVN检出。然后，通过从SVN服务端获取的 资源库URL+具体的项目文件夹名 下载相应项目，并可以知道下载项目的保存位置 （2）修改及提交 把项目下载到本机后，其实就是一个普通的项目文件而已，你可以在里面添加文件、修改文件、删除文件等等。 提交修改在项目文件空白处右键，选择 SVN提交。然后，输入 本次提交的版本更新信息（所作修改的注释）、勾选要提交的操作内容，点击 确定，即可把本机项目提交到SVN服务器资源库，覆盖掉资源库项目从而实现更新。（如果发生提交冲突，即两人都提交修改，后提交者由于版本落后会提交失败。这时可以先把自己的项目备份，然后从服务端下载最新的项目（下面有讲SVN更新），再把自己的项目覆盖到本地项目文件夹，最后SVN提交即可成功提交）（SVN不提供历史版本功能，所以项目被覆盖后就找不回来了，所以切记备份。如果需要历史版本的保存功能，推荐使用Git） （3）更新如果别人修改了SVN服务端资源库上的项目，你想下载最新的项目，则在 本机项目文件空白处单击鼠标右键，选择 SVN更新 ，即可自动完成下载，并会提示所作的更新有哪些。注意：在原项目文件夹内选择SVN更新的话，会自动覆盖掉原有内容。建议：先备份，再更新，防止自己本来的项目内容丢失。]]></content>
      <categories>
        <category>版本控制</category>
        <category>SVN</category>
      </categories>
      <tags>
        <tag>SVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVN 详解之服务器搭建（一）]]></title>
    <url>%2F2018%2F03%2F15%2FSVN%E8%AF%A6%E8%A7%A3%E4%B9%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1、使用 yum 安装SVN[root@singledb ~]# yum install -y subversion 2、验证安装版本及是否成功安装12[root@singledb ~]# svnserve --versionsvnserve, version 1.4.2 (r22196) 3、创建 SVN 版本库12[root@singledb ~]# mkdir /var/svn //创建存放版本库文件的目录[root@singledb ~]# svnadmin create /var/svn/testsvn //创建版本库 4、SVN 配置创建版本库后，在这个目录下会生成3个配置文件：1234[root@singledb conf]# pwd/var/svn/testsvn/conf[root@singledb conf]# lsauthz passwd svnserve.conf （1）svnserve.conf： svn服务配置文件。（2）passwd： 用户名口令文件，负责账号和密码的用户名单管理。（3）authz： 权限配置文件，控制账号的读写权限。 svnserve.conf 文件， 该文件配置项分为以下5项： anon-access： 控制非鉴权用户访问版本库的权限。 auth-access： 控制鉴权用户访问版本库的权限。 password-db： 指定用户名口令文件名。 authz-db：指定权限配置文件名，通过该文件可以实现以路径为基础的访问控制。 realm：指定版本库的认证域，即在登录时提示的认证域名称。若两个版本库的认证域相同，建议使用相同的用户名口令数据文件 （注意：将此五项都给去注释的时候，一定要顶格，不然会报错； realm指要认证的版本库，填写自己要配置的版本库即可） authz 文件 ：1234567891011121314151617181920#这里把不同用户放到不同的组里面，下面在设置目录访问权限的时候，用用户组来操作就可以了。（注：此处可不使用groups概念，在下方可直接添加可操作的人员即可。）[groups]admin = admindev = ben,nanatest = test# 为所有库指定默认访问规则# 所有人可以读，管理员可以写[/]* = r@admin = rw# 允许开发人员可以完全访问他们的项目版本库[devproject:/]@dev = rw# 允许测试人员访问其中的test目录[devproject:/test]@test = rw Passwd 文件 ：12345[users]# harry = harryssecret# sally = sallyssecretben = ben1234nana = nana1234 5、启动和停止SVN服务启动SVN服务：svnserve -d -r /home/svn （/home/svn 为版本库的根目录）关闭SVN服务：① 使用以下命令查找进程123[root@VM_0_7_centos ~]# ps aux | grep svnroot 8530 0.0 0.0 112660 972 pts/0 R+ 14:44 0:00 grep --color=auto svnroot 31964 0.0 0.0 166336 908 ? Ss 11:28 0:00 svnserve -d -r /var/svn ② 使用Kill命令杀死进程kill -s 9 31964 （31964为进程ID） 6、客户端检出访问 svn 服务器svn co svn://118.24.8.229/&lt;repo&gt; 即可检出代码（注：repo为代码库名称）。会弹出输入用户名和密码。 注：我们搭建的SVN是独立服务器形式运行的，没有和Apache整合，所以SVN地址为：svn://xxx/xxx，而不是http://xxx/xxx或https://xxx/xxx。Apache 搭建HTTP方式访问SVN服务器：传送门 问题集锦（1）SVN 检出错误 在 Windows下检出SVN仓库报错：Unable to connect to a repository at URL ‘svn://118.24.8.229/testsvn’Can’t connect to host ‘118.24.8.229’: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。 经查找知道，是因为服务器端的对应端口未开启：3690.在服务端开启即可：iptables -A INPUT -p tcp -m tcp --dport 3690 -j ACCEPT]]></content>
      <categories>
        <category>版本控制</category>
        <category>SVN</category>
      </categories>
      <tags>
        <tag>SVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码版本管理系统]]></title>
    <url>%2F2018%2F03%2F15%2F%E4%BB%A3%E7%A0%81%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[代码版本管理系统的概述1、什么是版本控制版本控制的核心是这样一个简单的概念，即对一个或者多个文件的追踪过程，随着这些文件演变成一个或多个产品的过程。特别是版本控制追踪什么变化，是什么改变了它，为什么会这样。版本控制系统提供了一个有益的说明，这些说明在传统的文件管理中是找不到的。 需要注意的是版本控制使用不仅是局限于程序员。版本控制可以被任何人用来维护文件目录，因此即便你不是程序也可以因此受益。 2、集中式版本控制介绍集中化的版本控制系统(Centralized Version Control Systems，简称 CVCS)应运而生。 这类系统，诸如 CVS、Subversion 以及 Perforce 等，都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。 多年以来，这已成为版本控制系统的标准做法。 这种做法带来了许多好处，特别是相较于老式的本地 VCS 来说。 现在，每个人都可以在一定程度上看到项目中的其他人正在做些什么。 而管理员也可以轻松掌控每个开发者的权限，并且管理一个 CVCS 要远比在各个客户端上维护本地数据库来得轻松容易。 事分两面，有好有坏。 这么做最显而易见的缺点是中央服务器的单点故障。 如果宕机一小时，那么在这一小时内，谁都无法提交更新，也就无法协同工作。 如果中心数据库所在的磁盘发生损坏，又没有做恰当备份，毫无疑问你将丢失所有数据——包括项目的整个变更历史，只剩下人们在各自机器上保留的单独快照。 本地版本控制系统也存在类似问题，只要整个项目的历史记录被保存在单一位置，就有丢失所有历史更新记录的风险。 3、分布式版本控制介绍于是分布式版本控制系统(Distributed Version Control System，简称 DVCS)面世了。 在这类系统中，像 Git、Mercurial、Bazaar 以及 Darcs 等，客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。 这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。 因为每一次的克隆操作，实际上都是一次对代码仓库的完整备份。 4、SVN与Git的最主要的区别？ SVN是集中式版本控制系统，版本库是集中放在中央服务器的，而干活的时候，用的都是自己的电脑，所以首先要从中央服务器哪里得到最新的版本，然后干活，干完后，需要把自己做完的活推送到中央服务器。集中式版本控制系统是必须联网才能工作，如果在局域网还可以，带宽够大，速度够快，如果在互联网下，如果网速慢的话，就纳闷了。 Git是分布式版本控制系统，那么它就没有中央服务器的，每个人的电脑就是一个完整的版本库，这样，工作的时候就不需要联网了，因为版本都是在自己的电脑上。既然每个人的电脑都有一个完整的版本库，那多个人如何协作呢？比如说自己在电脑上改了文件A，其他人也在电脑上改了文件A，这时，你们两之间只需把各自的修改推送给对方，就可以互相看到对方的修改了。 5、总结：分布式代码版本管理系统并不一定适合所有团队，比如中小团队可能更关心的只是成本更低，简单易用，那么SVN等这类集中式版本管理工具还是更为适合。但是不管团队最终选用什么代码版本管理工具，只要适合自己的团队的开发流程和工作方式，并且代码管理顺畅就可以了。 代码版本管理系统的历史代码版本管理系统大致可以分为三个时代： 第一代：本地式这代主要的特点提供本地代码版本控制，比如SCCS(1972)、 PVCS(1985)等。这代主要实现了基本的代码版本管理，但缺点是无法让多人同时对一个版本库进行修改。这个也和当时软件规模不够大有关，也没有这样的需求。 第二代：客户端-服务器式这代主要的特点是提供集中式服务器端代码版本控制,比如 CVS(1986), ClearCase(1992), Visual SourceSafe(1994), Perforce(1995), Subversion(2000) 等。 这代主要是实现了中心服务器端的代码版本管理，特点是可以让多人同时对一个代码版本库进行同步和修改，但缺点也相当明显： 在无法连接服务器的情况下，无法查看日志以及提交和比较代码版本（慢速网络和远程异地工作的程序员的痛），以及当服务或者网络出现问题的时候很多人员就会无法工作。 不支持local branch，导致branch创建管理复杂，并且一旦创建就很难修改（快速迭代开发中的程序员的痛） 由于只有一个中心端服务器，一旦发生灾难性问题，那么所有日志都会丢失，所以需要经常做备份（备份需要不小的成本） 如果软件代码量过于庞大，一般会出现速度缓慢的情况，因为每次的日志查询、不同版本之间的代码比较和代码提交等操作都需要和服务器通信，造成服务器端的负载过大。 第三代：分布式这代主要的特点是提供分布式代码版本控制,比如Git(2005), Mercurial(2005)等。这代结合了第一代和第二代的优点并实现了分布式的代码版本管理。这代的优点：分布式管理，在没有和服务器有连接的情况下仍然可以查看日志，提交代码，创建分支；支持local branch，可以快速方便的实现各种分支管理；支持分布式，从而可以实现分块管理，以及负载分流管理。缺点是有一定的学习曲线，比如分布方式下的代码同步，local branch的理解与运用，分布式代码管理的理解与运用等。详细的比较可以参考：这里。]]></content>
      <categories>
        <category>版本控制</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>SVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 详解之常用命令汇总（四）]]></title>
    <url>%2F2018%2F03%2F14%2FGit%E8%AF%A6%E8%A7%A3%E4%B9%8B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%B1%87%E6%80%BB%EF%BC%88%E5%9B%9B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Git提交三步走：（1）git add xxgit add xx命令可以将xx文件添加到暂存区，如果有很多改动可以通过 git add -A .来一次添加所有改变的文件。注意 -A 选项后面还有一个句点。 git add -A表示添加所有内容， git add . 表示添加新文件和编辑过的文件不包括删除的文件; git add -u 表示添加编辑或者删除的文件，不包括新添加的文件（2）git commit -m “注释”（3）git push origin 分支名称，一般使用：git push origin master 正常来说，这三步基本满足需求了。 下面进行命令的详细介绍git checkout 查看当前分支 git branch name 新建分支namegit checkout name 切换当前分支为name 合并语句：git checkout -b name 新建分支并切换到分支name git add filename 增加文件到分支git commit -m &quot;remark&quot; 提交修改内容到分支并添加注释 合并语句：git commit -m &quot;remark&quot; filename git diff filename 查看文件修改了什么内容 git status 查看当前工作区状态 git checkout mastergit merge name 合并分支到master分支 git branch -d name 删除name分支 git log --graph --pretty=oneline --abbrev-commit 用git log查看分支历史 特殊需求详解：1、合并文件：git merge name 这种合并 Git会优先选择 Fast forward模式，这种模式下，删除分支后，会丢掉分支信息如果要强制禁用Fast forward模式，Git就会在merge时生成一个新的commit，这样从分支历史上就可以看出来分支信息。git merge --no-ff -m &quot;merge with no-ff&quot; dev 合并分支dev并禁用Fast forward模式 2、冲突解决：① 手动修改冲突文件并再次提交打开产生冲突的文件后，Git用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======，>>>>>>>标记出不同分支的内容，其中&lt;&lt;&lt;HEAD是指主分支修改的内容，&gt;&gt;&gt;&gt;&gt;fenzhi1 是指fenzhi1上修改的内容，我们可以修改后保存即解决冲突。 3、版本回退：（1）使用HEAD^回退版本，HEAD^回退上个版本，HEAD^^回退上上个版本，以此类推。如果回退前100个版本的话，使用HEAD~100即可。git reset --hard HEAD^git reset --hard HEAD~100 （2）使用commit_id回退版本git log 查看需要回退的 commit_idgit reflog 查看未来版本的 commit_idgit reset --hard commit_id（注：这个回退可以回到以前版本，也可回到未来版本，只要知道commit_id即可） 4、撤销修改：在工作区：（就是开发环境本地）git checkout -- file 可以丢弃工作区的修改。（注：如果没有 -- 的话，那么命令就变成创建分支了。） 在暂存区：（已经 git add了）git reset HEAD file 可以把暂存区的修改撤销掉，重新放回到工作区 在本地版本中：（已经提交到分支）只能版本回退了 5、删除文件：工作区删除：rm file工作区删除后文件恢复：git checkout -- file版本库删除：git rm filegit commit -m &#39;remark&#39;（注：删除后必须得提交更新版本库） 6、BUG分支git stash 把当前工作现场“储藏”起来，等以后恢复现场后继续工作git stash save &quot;说明&quot; 同上，可增加注释git stash list 查看工作现场存储到什么地方git stash apply 恢复，但stash内容并不删除git stash pop 恢复，但同时把stash内容也删除git stash drop 移除git stash show 查看指定stash的diff （后面添加 -p 或 –patch 可以查看特定stash的全部diff）（注：实际使用中，如果一次git stash不行的话，可多使用一次；记得恢复的时候，也是需要的。） 7、多人协作git remote 查看远程库的信息 -v 更详细信息推送分支： 就是把该分支上的所有本地提交推送到远程库。git push origin master抓取分支：把线上最新的提交抓下来git pull 工作模式通常是这样：1、首先，可以试图用git push origin branch-name推送自己的修改；2、如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并；3、如果合并有冲突，则解决冲突，并在本地提交；4、没有冲突或者解决掉冲突后，再用git push origin branch-name推送就能成功！如果git pull提示“no tracking information”，则说明本地分支和远程分支的链接关系没有创建，用命令git branch --set-upstream branch-name origin/branch-name。这就是多人协作的工作模式，一旦熟悉了，就非常简单。 Git工作命令汇总（外派百度）：1) iCode 上新建分支，并拉取iCode分支到本地 git checkout -b branch_name origin/branch_name（注：iCode新建分支后，一定要git pull 更新本地分支，才能把新建分支更新到本地） 2）本地开发完成，合并到分支123git add filenamegit commit -m &quot;remark&quot;git push origin HEAD:refs/for/branch_name 3）特殊操作：① 撤销修改：在工作区：git checkout -- file 可以丢弃工作区的修改在暂存区：（已经 git add了）git reset HEAD file 可以把暂存区的修改撤销掉，重新放回到工作区在本地版本中：只能版本回退了② 版本回退：git log 查看需要回退的 commit_idgit reflog 查看未来版本的 commit_idgit reset --hard commit_id（注：这个回退可以回到以前版本，也可回到未来版本，只要知道commit_id即可）③ 冲突解决：① 手动修改冲突文件并再次提交]]></content>
      <categories>
        <category>版本控制</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 详解之客户端安装（三）]]></title>
    <url>%2F2018%2F03%2F14%2FGit%E8%AF%A6%E8%A7%A3%E4%B9%8B%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%89%E8%A3%85%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、Linux下安装首先，你可以试着输入git，看看系统有没有安装Git：123$ gitThe program &apos;git&apos; is currently not installed. You can install it by typing:sudo apt-get install git 像上面的命令，有很多Linux会友好地告诉你Git没有安装，还会告诉你如何安装Git。如果你碰巧用Debian或Ubuntu Linux，通过一条sudo apt-get install git就可以直接完成Git的安装，非常简单。 如果是其他Linux版本，可以直接通过源码安装。先从Git官网下载源码，然后解压，依次输入：./config，make，sudo make install 这几个命令安装就好了。 二、windows下安装windows下客户端分为两种：（1）Git客户端程序。 Git目前最新版本2.16.2 Git官网下载地址：传送门（2）Git客户端图形化操作程序 TortoiseGit。 TortoiseGit目前最新版本2.6.0 TortoiseGit官网下载地址：传送门 安装过程可以按照程序的默认选项，都选择“下一步”安装完成。 下面以 Git客户端程序为例来安装： 1.双击安装程序“Git-2.10.2-64-bit.exe”2.点击“Next” ，根据自己的情况，选择程序的安装目录。3.继续点击“Next”，显示截图如下：说明：（1）图标组件(Addition icons) : 选择是否创建桌面快捷方式。（2）桌面浏览(Windows Explorer integration) : 浏览源码的方法，使用bash 或者 使用Git GUI工具。（3）关联配置文件 : 是否关联 git 配置文件, 该配置文件主要显示文本编辑器的样式。（4）关联shell脚本文件 : 是否关联Bash命令行执行的脚本文件。（5）使用TrueType编码 : 在命令行中是否使用TruthType编码, 该编码是微软和苹果公司制定的通用编码。4.选择完之后，点击“Next”，开始菜单快捷方式目录：设置开始菜单中快捷方式的目录名称, 也可以选择不在开始菜单中创建快捷方式。5.点击“Next”，显示截图如下：设置环境变量选择使用什么样的命令行工具，一般情况下我们默认使用Git Bash即可：（1）Git自带：使用Git自带的Git Bash命令行工具。（2）系统自带CMD：使用Windows系统的命令行工具。（3）二者都有：上面二者同时配置，但是注意，这样会将windows中的find.exe 和 sort.exe工具覆盖，如果不懂这些尽量不要选择。6.选择之后，继续点击“Next”，显示如下：选择提交的时候换行格式（1）检查出windows格式转换为unix格式：将windows格式的换行转为unix格式的换行再进行提交。（2）检查出原来格式转为unix格式：不管什么格式的，一律转为unix格式的换行再进行提交。（3）不进行格式转换 : 不进行转换，检查出什么，就提交什么。7.选择之后，点击“Next”8.选择之后，点击“Next”9.选择之后，点击“Install”，开始安装 这样，我们的Git客户端就安装完成了。 安装完成后，在开始菜单里面能够找到 “Git –&gt; Git Bash”,如下：]]></content>
      <categories>
        <category>版本控制</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 详解之服务器搭建（二）]]></title>
    <url>%2F2018%2F03%2F14%2FGit%E8%AF%A6%E8%A7%A3%E4%B9%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[服务器环境：CentOS7 + Git(version 1.8.3.1) 1、安装Git yum install -y git安装完后，查看 Git 版本12[root@VM_0_7_centos ~]# git versiongit version 1.8.3.1 2、新建Git账号，用来管理Git服务1234[root@VM_0_7_centos ~]# id gitid: git：无此用户[root@VM_0_7_centos ~]# useradd git[root@VM_0_7_centos ~]# passwd git 3、服务器端创建Git仓库设置 /data/git/gittest.git 为 Git 仓库然后把 Git 仓库的 owner 修改为 git12345[root@VM_0_7_centos ~]# mkdir -p /data/git/gittest.git[root@VM_0_7_centos ~]# git init --bare /data/git/gittest.gitInitialized empty Git repository in /data/git/gittest.git/[root@VM_0_7_centos ~]# cd /data/git/[root@VM_0_7_centos git]# chown -R git:git gittest.git/ 4、客户端首次 clone 远程仓库进入 Git Bash 命令行客户端，创建项目地址（设置在 /d/MyProjects/gittest.git）并进入；然后从 Linux Git 服务器上 clone 项目：$ git clone git@118.24.8.229:/data/git/gittest.git 如果SSH用的不是默认的22端口，则需要使用以下的命令（假设SSH端口号是7700）：$ git clone ssh://git@118.24.8.229:7700/home/data/gittest.git 当第一次连接到目标 Git 服务器时会得到一个提示：1234Cloning into &apos;gittest&apos;...The authenticity of host &apos;118.24.8.229 (118.24.8.229)&apos; can&apos;t be established.ECDSA key fingerprint is SHA256:witKSLOMzfgWBcs78t5LfqmSJ+JEm2/PBEaR0d9mnqA.Are you sure you want to continue connecting (yes/no)? yes 选择 yes：123Warning: Permanently added &apos;118.24.8.229&apos; (ECDSA) to the list of known hosts.git@118.24.8.229&apos;s password:warning: You appear to have cloned an empty repository. 后面提示要输入密码，可以采用 SSH 公钥来进行验证（下面会进行说明）。 5、服务器端Git打开RSA认证进入 /etc/ssh 目录，编辑 sshd_config，打开以下三个配置的注释：123#RSAAuthentication yes#PubkeyAuthentication yes#AuthorizedKeysFile .ssh/authorized_keys ##RSAAuthentication用来设置是否开启RSA密钥验证，只针对SSH1（注：对于SSH2来说，没有此项配置） ##PubkeyAuthentication用来设置是否开启公钥验证，如果使用公钥验证的方式登录时，则设置为yes ##AuthorizedKeysFile用来设置公钥验证文件的路径，与PubkeyAuthentication配合使用,默认值是”.ssh/authorized_keys”。 保存并重启 sshd 服务：[root@VM_0_7_centos git]# systemctl restart sshd 由AuthorizedKeysFile 得知公钥的存放路径是 .ssh/authorized_keys，实际上是 $Home/.ssh/authorized_keys，由于管理 Git 服务的用户是 git，所以实际存放公钥的路径是 /home/git/.ssh/authorized_keys。 6、客户端创建SSH公钥和私钥并将公钥导入服务器端$ ssh-keygen -t rsa此时 C:\Users\用户名.ssh 下会多出两个文件 id_rsa 和 id_rsa.pub id_rsa 是私钥id_rsa.pub 是公钥 回到 Git Bash下，导入文件：（注：需要输入服务器端 git 用户的密码）1234567891011$ ssh git@118.24.8.229git@118.24.8.229&apos;s password:Last login: Sun Mar 18 09:43:25 2018 from 113.46.182.66[git@VM_0_7_centos ~]# ll -h .ssh/authorized_keys-rw-rw-r-- 1 git git 0 Mar 18 09:37 .ssh/authorized_keys[git@VM_0_7_centos ~]$ cat &gt;&gt; ~/.ssh/authorized_keys &lt; ~/.ssh/id_rsa.pub[git@VM_0_7_centos ~]# ll -h .ssh/authorized_keys-rw-rw-r-- 1 git git 399 Mar 18 09:44 .ssh/authorized_keys[git@VM_0_7_centos ~]$ exitlogoutConnection to 118.24.8.229 closed. 由上面可知道 公钥已经被导入到 git用户的认证文件中。 &gt;重要（对于服务器端）：修改 .ssh 目录的权限为 700修改 .ssh/authorized_keys 文件的权限为 600 7、客户端再次 clone 远程仓库12git clone git@118.24.8.229:/data/git/gittest.gitfatal: destination path &apos;gittest&apos; already exists and is not an empty directory. 项目已经 clone 了。（注：已经不需要输入密码了） 8、禁止 Git 用户 SSH 登录服务器编辑 /etc/passwd找到：git:x:502:504::/home/git:/bin/bash修改为git:x:502:504::/home/git:/bin/git-shell此时 git 用户可以正常通过 ssh 使用 git，但无法通过 ssh 登录系统：123456$ ssh git@118.24.8.229git@118.24.8.229&apos;s password:Last login: Sun Mar 18 09:44:11 2018 from 113.46.182.66fatal: Interactive git shell is not enabled.hint: ~/git-shell-commands should exist and have read and execute access.Connection to 118.24.8.229 closed. 问题集锦（1）Git本地公钥导入服务器失败 本地 Git Bash 登陆服务器报错：@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!Someone could be eavesdropping on you right now (man-in-the-middle attack)!It is also possible that the RSA host key has just been changed.The fingerprint for the RSA key sent by the remote host isd0:00:7c:bc:88:5c:dc:de:89:61:44:30:00:60:f9:b2.Please contact your system administrator.Add correct host key in /root/.ssh/known_hosts to get rid of this message.Offending key in /root/.ssh/known_hosts:1RSA host key for 192.168.4.222 has changed and you have requested strict checking.Host key verification failed. 解决方法：先查一下本地家目录的 .ssh/know_hosts 文件中是否存在服务器端的信息12$ cat ~/.ssh/known_hosts118.24.8.229 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBHrKfe2igdpfAMhukTCC0ki3gZEO4bFQviqsO6T+F/8Uqjt+XoVzKs7zmNgMEkodfpkZJ93bZyRMc2JdDRlCp9w= 由此可知，本地已经保存了服务器端的信息，因为我是重装了Git 服务器，所以此时保存的信息已经过时了，才会报次错误。解决方法就是删除已存在信息：由于我本地保存的只有一条服务器端信息，可使用如下命令：$ cat /dev/null &gt; ~/.ssh/known_hosts如果有多条服务器信息，可使用 $ vim ~/.ssh/known_hosts手动删除，或者window下直接打开文件删除对应信息即可。]]></content>
      <categories>
        <category>版本控制</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 详解之协议篇（一）]]></title>
    <url>%2F2018%2F03%2F14%2FGit%E8%AF%A6%E8%A7%A3%E4%B9%8B%E5%8D%8F%E8%AE%AE%E7%AF%87%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[协议Git 可以使用四种主要的协议来传输数据：本地传输，SSH 协议，Git 协议和 HTTP 协议。下面分别介绍一下哪些情形应该使用（或避免使用）这些协议。 值得注意的是，除了 HTTP 协议外，其他所有协议都要求在服务器端安装并运行 Git。 本地协议最基本的就是本地协议（Local protocol），所谓的远程仓库在该协议中的表示，就是硬盘上的另一个目录。这常见于团队每一个成员都对一个共享的文件系统（例如 NFS）拥有访问权，或者比较少见的多人共用同一台电脑的情况。后面一种情况并不安全，因为所有代码仓库实例都储存在同一台电脑里，增加了灾难性数据损失的可能性。 如果你使用一个共享的文件系统，就可以在一个本地文件系统中克隆仓库，推送和获取。克隆的时候只需要将远程仓库的路径作为 URL 使用，比如下面这样： $ git clone /opt/git/project.git 要添加一个本地仓库作为现有 Git 项目的远程仓库，可以这样做： $ git remote add local_proj /opt/git/project.git 然后就可以像在网络上一样向这个远程仓库推送和获取数据了。 SSH 协议（推荐）Git 使用的传输协议中最常见的可能就是 SSH 了。这是因为大多数环境已经支持通过 SSH 对服务器的访问 — 即便还没有，架设起来也很容易。SSH 也是唯一一个同时支持读写操作的网络协议。另外两个网络协议（HTTP 和 Git）通常都是只读的，所以虽然二者对大多数人都可用，但执行写操作时还是需要 SSH。SSH 同时也是一个验证授权的网络协议；而因为其普遍性，一般架设和使用都很容易。 通过 SSH 克隆一个 Git 仓库，你可以像下面这样给出 ssh:// 的 URL： $ git clone ssh://user@server/project.git 或者不指明某个协议 — 这时 Git 会默认使用 SSH ： $ git clone user@server:project.git （注：SSH 的限制在于你不能通过它实现仓库的匿名访问。即使仅为读取数据，人们也必须在能通过 SSH 访问主机的前提下才能访问仓库，这使得 SSH 不利于开源的项目。如果你仅仅在公司网络里使用，SSH 可能是你唯一需要使用的协议。） Git 协议接下来是 Git 协议。这是一个包含在 Git 软件包中的特殊守护进程； 它会监听一个提供类似于 SSH 服务的特定端口（9418），而无需任何授权。打算支持 Git 协议的仓库，需要先创建 git-daemon-export-ok 文件 — 它是协议进程提供仓库服务的必要条件 — 但除此之外该服务没有什么安全措施。要么所有人都能克隆 Git 仓库，要么谁也不能。这也意味着该协议通常不能用来进行推送。你可以允许推送操作；然而由于没有授权机制，一旦允许该操作，网络上任何一个知道项目 URL 的人将都有推送权限。不用说，这是十分罕见的情况。 （注：Git 协议是现存最快的传输协议。如果你在提供一个有很大访问量的公共项目，或者一个不需要对读操作进行授权的庞大项目，架设一个 Git 守护进程来供应仓库是个不错的选择。Git 协议消极的一面是缺少授权机制。用 Git 协议作为访问项目的唯一方法通常是不可取的。一般的做法是，同时提供 SSH 接口，让几个开发者拥有推送（写）权限，其他人通过 git:// 拥有只读权限。 Git 协议可能也是最难架设的协议。） HTTP/S 协议最后还有 HTTP 协议。HTTP 或 HTTPS 协议的优美之处在于架设的简便性。基本上，只需要把 Git 的裸仓库文件放在 HTTP 的根目录下，配置一个特定的 post-update 挂钩（hook）就可以搞定（Git 挂钩的细节见第 7 章）。此后，每个能访问 Git 仓库所在服务器上 web 服务的人都可以进行克隆操作。下面的操作可以允许通过 HTTP 对仓库进行读取： $ cd /var/www/htdocs/$ git clone –bare /path/to/git_project gitproject.git$ cd gitproject.git$ mv hooks/post-update.sample hooks/post-update$ chmod a+x hooks/post-update这样就可以了。Git 附带的 post-update 挂钩会默认运行合适的命令（git update-server-info）来确保通过 HTTP 的获取和克隆正常工作。这条命令在你用 SSH 向仓库推送内容时运行；之后，其他人就可以用下面的命令来克隆仓库： $ git clone http://example.com/gitproject.git （注：使用 HTTP 协议的好处是易于架设。几条必要的命令就可以让全世界读取到仓库的内容。花费不过几分钟。HTTP 协议不会占用过多服务器资源。因为它一般只用到静态的 HTTP 服务提供所有数据。你可以通过 HTTPS 提供只读的仓库，这意味着你可以加密传输内容。但是，HTTP 协议的消极面在于，相对来说客户端效率更低。克隆或者下载仓库内容可能会花费更多时间，而且 HTTP 传输的体积和网络开销比其他任何一个协议都大。）]]></content>
      <categories>
        <category>版本控制</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7默认Python升级]]></title>
    <url>%2F2018%2F03%2F13%2FCentOS7%E9%BB%98%E8%AE%A4Python%E5%8D%87%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[简述CentOS 7 中默认安装了 Python，版本比较低（2.7.5），为了使用新版 3.x，需要对旧版本进行升级。 由于很多基本的命令、软件包都依赖旧版本，比如：yum。所以，在更新 Python 时，建议不要删除旧版本（新旧版本可以共存）。 安装Python3.6.4 （1）下载并解压缩下载地址：传送门此处我选择的版本是Python3.6.4： wget https://www.python.org/ftp/python/3.6.4/Python-3.6.4.tgz解压缩： tar -xvf Python-3.6.4.tgz （2）检测编译环境12cd Python-3.6.4/./configure 执行 ./configure 时，如果报错： configure: error: no acceptable C compiler found in $PATH 说明没有安装合适的编译器。这时，需要安装/升级 gcc 及其它依赖包。 yum install make gcc gcc-c++完成之后，重新执行： ./configure （3）编译 &amp; 安装 make &amp; make install安装中如果报错： zipimport.ZipImportError: can’t decompress data; zlib not available 说明是因为缺少zlib 的相关工具包导致的，知道了问题所在，那么我们只需要安装相关依赖包即可，① 打开终端，输入一下命令安装zlib相关依赖包： yum -y install zlib* 或 yum -y install zlib zlib-devel（未试） ② 进入 python安装包,修改Module路径的setup文件： vim Module/Setup.dist找到以下这行代码，去掉注释： #zlib zlibmodule.c -I$(prefix)/include -L$(exec_prefix)/lib -lz 然后再次执行编译 &amp; 安装。 设置Python默认版本（1）查看Python版本1234[root@VM_0_7_centos ~]# python -VPython 2.7.5[root@VM_0_7_centos ~]# python3 -VPython 3.6.4 查看Python命令路径：1234[root@VM_0_7_centos ~]# which python/usr/bin/python[root@VM_0_7_centos ~]# which python3/usr/local/bin/python3 （2）设置Python3.x为默认版本查看Python默认版本：12345[root@VM_0_7_centos ~]# ll /usr/bin/ | grep python-rwxr-xr-x. 1 root root 11232 Aug 10 2017 abrt-action-analyze-pythonlrwxrwxrwx. 1 root root 7 Jan 9 18:20 python -&gt; python2lrwxrwxrwx. 1 root root 9 Jan 9 18:20 python2 -&gt; python2.7-rwxr-xr-x. 1 root root 7136 Aug 4 2017 python2.7 更改Python默认版本，将原来 python 的软链接重命名： mv /usr/bin/python /usr/bin/python.bak将 python 链接至 python3： ln -s /usr/local/bin/python3 /usr/bin/python这时，再查看 Python 的版本：12[root@VM_0_7_centos ~]# python -VPython 3.6.4 输出的是 3.x，说明已经使用的是 python3了。 配置yum12345[root@VM_0_7_centos ~]# yum File &quot;/usr/bin/yum&quot;, line 30 except KeyboardInterrupt, e: ^SyntaxError: invalid syntax 升级 Python 之后，由于将默认的 python 指向了 python3，yum 不能正常使用，需要编辑 yum 的配置文件： vi /usr/bin/yum同时修改： vi /usr/libexec/urlgrabber-ext-down将 #!/usr/bin/python 改为 #!/usr/bin/python2.7，保存退出即可。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS 图片居中问题（水平居中和垂直居中）]]></title>
    <url>%2F2018%2F03%2F13%2FCSS%E5%9B%BE%E7%89%87%E5%B1%85%E4%B8%AD%E9%97%AE%E9%A2%98%EF%BC%88%E6%B0%B4%E5%B9%B3%E5%B1%85%E4%B8%AD%E5%92%8C%E5%9E%82%E7%9B%B4%E5%B1%85%E4%B8%AD%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1、水平居中（1）利用margin: 0 auto实现图片水平居中利用margin: 0 auto实现图片居中就是在图片上加上css样式margin: 0 auto 如下：1234&lt;div style=&quot;text-align: center; width: 500px; border: green solid 1px;&quot;&gt; &lt;img alt=&quot;&quot; src=&quot;https://www.baidu.com/img/baidu_jgylogo3.gif&quot; style=&quot;margin: 0 auto;&quot; /&gt;&lt;/div&gt; （2）利用文本的水平居中属性text-align: center1234&lt;div style=&quot;text-align: center; width: 500px; border: green solid 1px;&quot;&gt; &lt;img alt=&quot;&quot; src=&quot;https://www.baidu.com/img/baidu_jgylogo3.gif&quot; style=&quot;display: inline-block;&quot; /&gt;&lt;/div&gt; 2、垂直居中（1）利用高==行高实现图片垂直居中这种方法是要知道高度才可以使用，代码如下：12345&lt;div style=&quot;text-align: center; width: 500px;height:200px; line-height:200px; border: green solid 1px;&quot;&gt; &lt;img alt=&quot;&quot; src=&quot;https://www.baidu.com/img/baidu_jgylogo3.gif&quot; style=&quot;display: inline-block; vertical-align: middle;&quot; /&gt;&lt;/div&gt; （2）利用table实现图片垂直居中利用table的方法是利用了table的垂直居中属性，代码如下：1234567&lt;div style=&quot;text-align: center; width: 500px;height:200px; display: table;border: green solid 1px;&quot;&gt; &lt;span style=&quot;display: table-cell; vertical-align: middle; &quot;&gt; &lt;img alt=&quot;&quot; src=&quot;https://www.baidu.com/img/baidu_jgylogo3.gif&quot; style=&quot;display: inline-block;&quot; /&gt; &lt;/span&gt;&lt;/div&gt; 这里使用display: table;和display: table-cell;来模拟table，这种方法并不兼容IE6/IE7，IE67不支持display: table，如果你不需要支持IE67那就可以用（缺点：当你设置了display: table;可能会改变你的原有布局） （3）利用绝对定位实现图片垂直居中如果已知图片的宽度和高度可以这样，代码如下：12345&lt;div style=&quot;width: 500px;height:200px; position: relative; border: green solid 1px;&quot;&gt; &lt;img alt=&quot;&quot; src=&quot;https://www.baidu.com/img/baidu_jgylogo3.gif&quot; style=&quot;width: 120px; height: 40px;position: absolute; left:50%; top: 50%; margin-left: -60px;margin-top: -20px;&quot; /&gt;&lt;/div&gt; （4）移动端可以利用flex布局实现css图片垂直居中移动端一般浏览器版本都比较高，所以可以大胆的使用flex布局，（flex布局参考css3的flex布局用法）演示代码如下：123456789101112131415161718192021222324252627282930313233343536373839/*css代码：*/&lt;style type=&quot;text/css&quot;&gt;.ui-flex &#123; display: -webkit-box !important; display: -webkit-flex !important; display: -ms-flexbox !important; display: flex !important; -webkit-flex-wrap: wrap; -ms-flex-wrap: wrap; flex-wrap: wrap&#125;.ui-flex, .ui-flex *, .ui-flex :after, .ui-flex :before &#123; box-sizing: border-box&#125;.ui-flex.justify-center &#123; -webkit-box-pack: center; -webkit-justify-content: center; -ms-flex-pack: center; justify-content: center&#125;.ui-flex.center &#123; -webkit-box-pack: center; -webkit-justify-content: center; -ms-flex-pack: center; justify-content: center; -webkit-box-align: center; -webkit-align-items: center; -ms-flex-align: center; align-items: center&#125;&lt;/style&gt;/*html代码：*/&lt;div class=&quot;ui-flex justify-center center&quot; style=&quot;border: green solid 1px; width: 500px; height: 200px;&quot;&gt; &lt;div class=&quot;cell&quot;&gt; &lt;img alt=&quot;&quot; src=&quot;https://www.baidu.com/img/baidu_jgylogo3.gif&quot; style=&quot;&quot; /&gt; &lt;/div&gt;&lt;/div&gt;]]></content>
      <categories>
        <category>CSS</category>
        <category>CSS问题</category>
      </categories>
      <tags>
        <tag>CSS问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS 获取同类中第几个元素]]></title>
    <url>%2F2018%2F03%2F13%2FCSS%E8%8E%B7%E5%8F%96%E5%90%8C%E7%B1%BB%E4%B8%AD%E7%AC%AC%E5%87%A0%E4%B8%AA%E5%85%83%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[在前端开发中，我们可能会碰到这样的需求：想让列表中的第一个部分显示不同的样式，想让列表中的偶数部分显示不同的背景颜色，想让列表中的最后一部分样式不一样……这样的需求，我们怎样来实现？我们还可以通过CSS来实现，CSS给我们提供了几个非常有用的样式参数：first-child、last-child、nth-child(n)。下面，详细看一下它们的使用。 1、first-childfirst-child表示选择列表中的第一个标签。代码如下： li:first-child{background:#090}上面的意思是，li 列表中的 第一个li模块的背景颜色。 2、last-childlast-child表示选择列表中的最后一个标签，代码如下： li:last-child{background:#090} 3、nth-child(3)表示选择列表中的第3个标签，代码如下： li:nth-child(3){background:#090}上面代码中的3也可以改成其它数字，如4、5等。想选择第几个标签，就填写几。 4、nth-child(2n)这个表示选择列表中的偶数标签，即选择 第2、第4、第6…… 标签。 5、nth-child(2n-1)这个表示选择列表中的奇数标签，即选择 第1、第3、第5、第7……标签。 6、nth-child(n+3)这个表示选择列表中的标签从第3个开始到最后。 7、nth-child(-n+3)这个表示选择列表中的标签从0到3，即小于3的标签。 8、nth-last-child(3)这个表示选择列表中的倒数第3个标签。 上面这些CSS样式是非常有用的，在我们的网页开发过程中，会派上非常大的用场，可以给我们的网页带来不一样的风格。]]></content>
      <categories>
        <category>CSS</category>
        <category>CSS问题</category>
      </categories>
      <tags>
        <tag>CSS问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端切图技巧总结]]></title>
    <url>%2F2018%2F03%2F09%2F%E5%89%8D%E7%AB%AF%E5%88%87%E5%9B%BE%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[一、传统切图方法使用范围：需要切多张图，带背景的1.打开下载的psd文件出现下面没关系，直接点确定即可 打开后，因为文件比较长所以看不清细节，所以我们要放大图片到合适的大小。 放大图片：”CTRL” +“+”缩小图片：“CTRL”+ “-” 然后我们来认识下photoshop中的一些工具 2.修改参考线我们可以看到我们的图片中很多条蓝色的参考线，这些线是设计人员用来设计用的，有的文件中的线会直接把我们需要切的东西给包裹出来，但是像我们练习的这个文件中的参考线特别的多，特别的密集，很多我们都不需要，所以我们先把这些参考线都清除掉。 删除参考线：选择移动工具，然后把要删除的参考线往标尺上拖，往上或者往左都可以 没有参考线的可能是参考线被隐藏了，通过“CTRL”+“ALT”+“;”就可以显示出来了。 我们只要把挡住图标的参考线给删除就可以 3.选择要切的图标这里我们就切几个页面上的图标，文字部分我们都可以通过代码来实现，所以就不切了。 ①选择工具栏第一个“移动工具”②然后查看上面选项栏“自动选择”有没有被选中，没有被选中点击选中，将它旁边的“组”改为“图层”，这样我们点击图标的时候，右下角的图层面板就能自动的选中对应的图层③点击我们要切的图标，然后到右下角的图层面板，点击当前选中的这个图层旁边的小眼睛来隐藏当前图层，通过显隐来确定是不是我们要切的图标④确认了我们要切的图标后，分别从水平标尺和竖直标尺的地方拉取参考线来把图标给包裹住。因为我们当前选中的就是我们要切的图层，所以当参考线拖过去的时候会自动吸附到图标的边缘。⑤用同样的方法把图标都用参考线包裹出来，最下面一排，要把图标切成一样高的才好，所以以第一个的上边界和下边界为基准。 4.切片需要的图标现在，我们已经用参考线把我们的图标给包裹出来了，下面，我们要用切片工具来切出我们的图标 ①选择切片工具，左边工具栏从上往下数第五个，然后右键就可以找到，然后选中②然后找到我们刚才用参考线包裹的图标，从左上角一直拖到右下角，因为有参考线的帮助，所以软件能够自动吸附到参考线上，所以只要大概找准左上角和右下角就可以选取出来③用同样的方法把剩下的都选取出来 5.将切片存储为图片 ①选择 “文件”=》“存储为Web所用格式”②调整缩放比例，让图片能完整的在窗口显示③在窗口中，从左上角一直拖到右下角，选中所有切片，这样我们才能把切片都存储为我们想要的格式④存储为png-24格式，png-24格式的图片质量比较高⑤保存，选择用户所有切片可以只保存我们自己切出来的切片⑥然后查看保存的文件里面就多了一个images文件夹，里面就是我们切出来的图片 （注：但是这样做会发现png格式的背景还是有的，而且如果只是需要一张图的时候，还要这样切就会比较麻烦，所以(二)中会介绍一些切一两个小图标的方法） 二、切单个图标方法之前介绍了传统的切图技巧，主要用于切多张图片的时候，但很多时候我们可能只需要切一两张图标，如果还用传统的方法就会很繁琐，所以这次分享一种导出单个图标的方法。 1.还是之前的文件，这次我们就切一个图标 2.选择移动工具，并在选项中选中“自动选择”和“图层” 3.点击我们要切的图标，这里是点击中间的笔，然后右边的图层面板就会自动聚焦到对应的图层，通过点击左边的小眼睛来确定是不是我们想要的图层。 4.这里我们找到了我们需要的图标有两部分组成，里面的笔和外面的圆，两个图层被放在了一个组里面 5.在icon文件上右键，选择“转换为智能对象” 6.然后，我们会发现，两个图层合并为了一个图层，这个就是我们需要的图标 7.在icon图层上右键，选择“编辑内容”，出现提示框的话直接点击“确定”即可 8.然后，我们的图标就被单独提取出来了 9.选择“文件”=》“存储为”，将我们的图标保存为PNG格式就大功告成了！ 三、自动生成图标方法1.打开给大家准备的psd文件2.选择“编辑”=》“首选项”=》“增效工具” 3.勾选“启用生成器”，选择“确定” 4.重启一下photoshop 5.重启后，重新打开我们的文件，感觉参考线太碍事了，我们用快捷键“CTRL”+“；”把参考线隐藏了。 6.选择“文件”=》“生成”=》“图像资源” 7.用之前一样的方法找到我们要的图标的图层 8.我们需要的图标放在了icon这个分组中，现在我们将icon重命名为icon.png 9.现在我们打开我们存放psd文件的文件夹中，可以发现现在多了一个文件夹 10.打开这个多出来的文件夹就会看到我们的图标躺在那里了，是不是很激动！！！11.我们还可以继续把第二个图标重命名为icon.jpg12.然后会发现刚才那个文件夹中又多了一个icon.jpg文件13.当然还可以把图标改为icon.svg（注：有时候不可以，原因未查）16.如果不想要要这个图标，我们可以把文件的命名重新改回来 这个方法生成图标非常的方便，很适合我们生成svg图片，可以保存到fontello或者iconfont中，强烈建议！]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>切图篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask - 相关知识点总结]]></title>
    <url>%2F2018%2F03%2F06%2FFlask-%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1、Flask入口文件集成 shell每次启动shell 会话都要导入数据库实例和模型，这真是份枯燥的工作。为了避免一直重复导入，我们可以做些配置，让Flask-Script 的shell 命令自动导入特定的对象。 若想把对象添加到导入列表中，我们要为shell 命令注册一个make_context 回调函数，如示例5-7 所示。示例5-7 hello.py：为shell 命令添加一个上下文1234from flask.ext.script import Shelldef make_shell_context(): return dict(app=app, db=db, User=User, Role=Role)manager.add_command(&quot;shell&quot;, Shell(make_context=make_shell_context)) make_shell_context() 函数注册了程序、数据库实例以及模型，因此这些对象能直接导入shell：1234567$ python hello.py shell&gt;&gt;&gt; app&lt;Flask &apos;app&apos;&gt;&gt;&gt;&gt; db&lt;SQLAlchemy engine=&apos;sqlite:////home/flask/flasky/data.sqlite&apos;&gt;&gt;&gt;&gt; User&lt;class &apos;app.User&apos;&gt; 2、Flash消息请求完成后，有时需要让用户知道状态发生了变化。这里可以使用确认消息、警告或者错误提醒。一个典型例子是，用户提交了有一项错误的登录表单后，服务器发回的响应重新渲染了登录表单，并在表单上面显示一个消息，提示用户用户名或密码错误。 这种功能是Flask 的核心特性。如示例所示，flash() 函数可实现这种效果。1234567891011from flask import Flask, render_template, session, redirect, url_for, flash@app.route(&apos;/&apos;, methods=[&apos;GET&apos;, &apos;POST&apos;])def index(): form = NameForm() if form.validate_on_submit(): old_name = session.get(&apos;name&apos;) if old_name is not None and old_name != form.name.data: flash(&apos;Looks like you have changed your name!&apos;) session[&apos;name&apos;] = form.name.data return redirect(url_for(&apos;index&apos;)) return render_template(&apos;index.html&apos;, form = form, name = session.get(&apos;name&apos;)) 在这个示例中，每次提交的名字都会和存储在用户会话中的名字进行比较，而会话中存储的名字是前一次在这个表单中提交的数据。如果两个名字不一样，就会调用flash() 函数，在发给客户端的下一个响应中显示一个消息。 仅调用flash() 函数并不能把消息显示出来，程序使用的模板要渲染这些消息。最好在基模板中渲染Flash 消息，因为这样所有页面都能使用这些消息。Flask 把get_flashed_messages() 函数开放给模板，用来获取并渲染消息，如示例所示。123456789101112&#123;% block content %&#125;&lt;div class=&quot;container&quot;&gt; &#123;% for message in get_flashed_messages() %&#125; &lt;div class=&quot;alert alert-warning&quot;&gt; &lt;button type=&quot;button&quot; class=&quot;close&quot; data-dismiss=&quot;alert&quot;&gt;&amp;times;&lt;/button&gt; &#123;&#123; message &#125;&#125; &lt;/div&gt; &#123;% endfor %&#125; &#123;% block page_content %&#125;&#123;% endblock %&#125;&lt;/div&gt;&#123;% endblock %&#125; 在模板中使用循环是因为在之前的请求循环中每次调用flash() 函数时都会生成一个消息，所以可能有多个消息在排队等待显示。get_flashed_messages() 函数获取的消息在下次调用时不会再次返回，因此Flash 消息只显示一次，然后就消失了。 进阶用法：区别类别分别显示（使用category_filter字段） 首先，在视图文件中使用如下： 1234flash(message, category)# 例如flash(&apos;Change your name success!&apos;, &apos;ok&apos;)flash(&apos;Change your name faild!&apos;, &apos;error&apos;) 其次，在模板中使用如下： 12&#123;% for message in get_flashed_messages(category_filter=[&apos;ok&apos;]) %&#125;&#123;% for message in get_flashed_messages(category_filter=[&apos;error&apos;]) %&#125; 3、使用Flask-Moment进行日期时间的管理Flask-Moment又是一个flask的扩展模块，用来处理时间日期等信息。用这个模块主要是考虑到两点，第一是为了让不同时区的用户看到的都是各自时区的实际时间，而不是服务器所在地的时间。第二是对于一些时间间隔的处理，如果要手动处理很麻烦，如果有模块就很好了。 1、安装该模块pip install flask-moment 2、初始化框架的时候引入app文件 app/__init__.py：12345678from flask_moment import Moment...# 实例化moment = Moment()# 传入app进行初始化moment.init_app(app) （注：因为我使用了Flask-Scripts来管理项目，所以此处分为两步引入框架。） 3、在base.html模板中引入1234&#123;% block scripts %&#125;&#123;&#123; super() &#125;&#125;&#123;&#123; moment.include_moment() &#125;&#125;&#123;% endblock %&#125; 4、Flask 前后台分离登录问题使用的扩展模块：Flask-Login说明：前台用户表：User; 后台管理员表：Manager;（注：前后台是一个app应用）代码实现，如下：12345@login_manager.user_loaderdef load_user(user_id): if Manager.query.get(int(user_id)): return Manager.query.get(int(user_id)) return User.query.get(int(user_id)) 注释：load_user(user_id),这个方法只负责加载用户对象，所以这里只要通过user_id能够返回正确的用户对象即可。实现效果：前后台同时在一个浏览器中只能登录一个。因为同时都使用了Flask-Login，同一个session，相同的KEY，所以在同一浏览器中只能实现其中一个。相对的，可以在两个浏览器中同时分别登录前后台，使用的cookie就是不同的，可实现前后台同时使用Flask-Login验证的效果。（注：为实现效果，前后台不能同时登录的问题。） 5、Flask 中 url_for() 函数中参数问题此问题是我在分页需要传递参数中遇到的，在此记录一下。按照正常理解，url_for() 函数传递多参数，仅仅相应的在参数中增加即可。刚开始按照此思路测试，一直报错，传不到视图函数中。然后，我查找官方文档，如下：flask.url_for(endpoint, **values)由文档可知，url_for() 函数的第一个参数为视图函数地址，第二个参数为关键字参数，即 key=value 键值对。因此，依据我的理解，刚才的想法是正确的，然后继续梳理，最后验证成功。附分页带参数代码如下：分页调用：1&#123;&#123; pg.page(page_data, &quot;home.loginlog&quot;, id=current_user.id) &#125;&#125; 分页函数相关代码：1234567&#123;% macro page(data, url, id=None) %&#125;...&lt;a href=&quot;&#123;&#123; url_for(url, id=id, page=1) &#125;&#125;&quot; aria-label=&quot;First&quot;&gt; &lt;span aria-hidden=&quot;true&quot;&gt;首页&lt;/span&gt;&lt;/a&gt;...（注：其它类似） 小技巧：对 id的初始值设为None，下面使用时不需要再判断是否传递 id，如果没有传递，id=None，下面url_for()函数的参数中 id就不传了。 6、Flask 获取编辑器中的内容包含转义字符问题编辑器可以使用 emotion特效图片，存储到 MySQL 数据表中的数据自动被 jinja2 模板转义了。转义的效果如下：&lt;p&gt;增加图片评论：&amp;nbsp;&lt;img src=&quot;http://img.baidu.com/hi/jx2/j_0062.gif&quot;/&gt;&lt;/p&gt; 但是，当我取出来的时候做显示的时候，仍然是这样。那肯定是不行了。我查看了下取出来的数据类型是字符串。并没有实现反转义。原因应该是 模板文件不认为该数据为安全的，所以没有自动反转义。因此，需要让模板信任该数据，实现转义，做如下操作：即通过 |safe 过滤器来表示字符串是安全的，渲染的时候可进行反转义。 常用技巧篇1、Flask 表单中上传文件的大小获取filesize = len(form.logo.data.read()) 2、Flask 获取用户ipip = request.remote_addr 3、Flask 前后台需要登录的问题]]></content>
      <categories>
        <category>Python</category>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask - 认证扩展]]></title>
    <url>%2F2018%2F03%2F06%2FFlask-%E8%AE%A4%E8%AF%81%E6%89%A9%E5%B1%95%2F</url>
    <content type="text"><![CDATA[Flask-Login：管理已登录用户的用户会话。Werkzeug：计算密码散列值并进行核对。itsdangerous：生成并核对加密安全令牌。 1、密码安全性 - 使用Werkzeug实现密码散列众所周知，大多数用户都在不同的网站中使用相同的密码，因此，即便不保存任何敏感信息，攻击者获得存储在数据库中的密码之后，也能访问用户在其他网站中的账户。若想保证数据库中用户密码的安全，关键在于不能存储密码本身，而要存储密码的散列值。计算密码散列值的函数接收密码作为输入，使用一种或多种加密算法转换密码，最终得到一个和原始密码没有关系的字符序列。核对密码时，密码散列值可代替原始密码，因为计算散列值的函数是可复现的：只要输入一样，结果就一样。 Werkzeug 中的security 模块能够很方便地实现密码散列值的计算。这一功能的实现只需要两个函数，分别用在注册用户和验证用户阶段。 generate_password_hash(password, method=pbkdf2:sha1, salt_length=8)：这个函数将原始密码作为输入，以字符串形式输出密码的散列值，输出的值可保存在用户数据库中。method 和salt_length 的默认值就能满足大多数需求。 check_password_hash(hash, password)：这个函数的参数是从数据库中取回的密码散列值和用户输入的密码。返回值为True 表明密码正确。 示例8-1 展示了创建的User 模型为支持密码散列所做的改动。示例8-1 app/models.py：在User 模型中加入密码散列1234567891011121314151617181920from werkzeug.security import generate_password_hash, check_password_hashclass User(db.Model): __tablename__ == &apos;users&apos; id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(64), unique=True, index=True) password_hash = db.Column(db.String(128)) @property def password(self): raise AttributeError(&apos;password is not a readable attribute&apos;) @password.setter def password(self, password): self.password_hash = generate_password_hash(password) def verify_password(self, password): return check_password_hash(self.password_hash, password) def __repr__(self): return &apos;&lt;User %r&gt;&apos; % self.username 计算密码散列值的函数通过名为password 的只写属性实现。设定这个属性的值时，赋值方法会调用Werkzeug 提供的generate_password_hash() 函数，并把得到的结果赋值给password_hash 字段。如果试图读取password 属性的值，则会返回错误，原因很明显，因为生成散列值后就无法还原成原来的密码了。 verify_password 方法接受一个参数（ 即密码）， 将其传给Werkzeug 提供的check_password_hash() 函数，和存储在User 模型中的密码散列值进行比对。如果这个方法返回True，就表明密码是正确的。 2、使用Flask-Login认证用户用户登录程序后，他们的认证状态要被记录下来，这样浏览不同的页面时才能记住这个状态。Flask-Login 是个非常有用的小型扩展，专门用来管理用户认证系统中的认证状态，且不依赖特定的认证机制。 （1）准备用于登录的用户模型要想使用Flask-Login 扩展，程序的User 模型必须实现几个方法。需要实现的方法如表8-1所示。表8-1 Flask-Login要求实现的用户方法 方 法 说 明 is_authenticated() 如果用户已经登录，必须返回True，否则返回False is_active() 如果允许用户登录，必须返回True，否则返回False。如果要禁用账户，可以返回False is_anonymous() 对普通用户必须返回False get_id() 必须返回用户的唯一标识符，使用Unicode 编码字符串 这4 个方法可以在模型类中作为方法直接实现，不过还有一种更简单的替代方案。Flask-Login 提供了一个UserMixin 类，其中包含这些方法的默认实现，且能满足大多数需求。修改后的User 模型如示例8-6 所示。 示例8-6 app/models.py：修改User 模型，支持用户登录1234567from flask.ext.login import UserMixinclass User(UserMixin, db.Model): __tablename__ = &apos;users&apos; id = db.Column(db.Integer, primary_key = True) email = db.Column(db.String(64), unique=True, index=True) username = db.Column(db.String(64), unique=True, index=True) password_hash = db.Column(db.String(128)) 注意，示例中同时还添加了email 字段。在这个程序中，用户使用电子邮件地址登录，因为相对于用户名而言，用户更不容易忘记自己的电子邮件地址。 Flask-Login 在程序的工厂函数中初始化，如示例8-7 所示。示例8-7 app/init.py：初始化Flask-Login12345678910from flask.ext.login import LoginManagerlogin_manager = LoginManager()login_manager.session_protection = &apos;strong&apos;login_manager.login_view = &apos;auth.login&apos;def create_app(config_name): # ... login_manager.init_app(app) # ... LoginManager 对象的session_protection 属性可以设为None、’basic’ 或’strong’，以提供不同的安全等级防止用户会话遭篡改。设为’strong’ 时，Flask-Login 会记录客户端IP地址和浏览器的用户代理信息，如果发现异动就登出用户。login_view 属性设置登录页面的端点。如果登录路由在蓝本中定义，因此要在前面加上蓝本的名字。 最后，Flask-Login 要求程序实现一个回调函数，使用指定的标识符加载用户。这个函数的定义如示例8-8 所示。 示例8-8 app/models.py：加载用户的回调函数12345from . import login_manager@login_manager.user_loaderdef load_user(user_id): return User.query.get(int(user_id)) 加载用户的回调函数接收以Unicode 字符串形式表示的用户标识符。如果能找到用户，这个函数必须返回用户对象；否则应该返回None。 （2）保护路由为了保护路由只让认证用户访问，Flask-Login 提供了一个login_required 修饰器。用法演示如下：123456from flask.ext.login import login_required@app.route(&apos;/secret&apos;)@login_requireddef secret(): return &apos;Only authenticated users are allowed!&apos; 如果未认证的用户访问这个路由，Flask-Login 会拦截请求，把用户发往登录页面。 3、使用itsdangerous生成确认令牌为验证电子邮件地址，用户注册后，程序会立即发送一封确认邮件。新账户先被标记成待确认状态，用户按照邮件中的说明操作后，才能证明自己可以被联系上。账户确认过程中，往往会要求用户点击一个包含确认令牌的特殊URL 链接。 确认邮件中最简单的确认链接是http://www.example.com/auth/confirm/ 这种形式的URL，其中id 是数据库分配给用户的数字id。用户点击链接后，处理这个路由的视图函数就将收到的用户id 作为参数进行确认，然后将用户状态更新为已确认。 但这种实现方式显然不是很安全，只要用户能判断确认链接的格式，就可以随便指定URL中的数字，从而确认任意账户。解决方法是把URL 中的id 换成将相同信息安全加密后得到的令牌。 下面这个简短的shell 会话显示了如何使用itsdangerous 包生成包含用户id 的安全令牌：12345678910(venv) $ python manage.py shell&gt;&gt;&gt; from manage import app&gt;&gt;&gt; from itsdangerous import TimedJSONWebSignatureSerializer as Serializer&gt;&gt;&gt; s = Serializer(app.config[&apos;SECRET_KEY&apos;], expires_in = 3600)&gt;&gt;&gt; token = s.dumps(&#123; &apos;confirm&apos;: 23 &#125;)&gt;&gt;&gt; token&apos;eyJhbGciOiJIUzI1NiIsImV4cCI6MTM4MTcxODU1OCwiaWF0IjoxMzgxNzE0OTU4fQ.ey ...&apos;&gt;&gt;&gt; data = s.loads(token)&gt;&gt;&gt; data&#123;u&apos;confirm&apos;: 23&#125; itsdangerous 提供了多种生成令牌的方法。其中，TimedJSONWebSignatureSerializer 类生成具有过期时间的JSON Web 签名（JSON Web Signatures，JWS）。这个类的构造函数接收的参数是一个密钥，在Flask 程序中可使用SECRET_KEY 设置。 dumps() 方法为指定的数据生成一个加密签名，然后再对数据和签名进行序列化，生成令牌字符串。expires_in 参数设置令牌的过期时间，单位为秒。 为了解码令牌，序列化对象提供了loads() 方法，其唯一的参数是令牌字符串。这个方法会检验签名和过期时间，如果通过，返回原始数据。如果提供给loads() 方法的令牌不正确或过期了，则抛出异常。]]></content>
      <categories>
        <category>Python</category>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[物联网入门必备]]></title>
    <url>%2F2018%2F03%2F06%2F%E7%89%A9%E8%81%94%E7%BD%91%E5%85%A5%E9%97%A8%E5%BF%85%E5%A4%87%2F</url>
    <content type="text"><![CDATA[什么是物联网？物联网是新一代信息技术的重要组成部分，也是“信息化”时代的重要发展阶段。其英文名称是：“Internet of things（IoT）”。顾名思义，物联网就是物物相连的互联网。这有两层意思：其一，物联网的核心和基础仍然是互联网，是在互联网基础上的延伸和扩展的网络；其二，其用户端延伸和扩展到了任何物品与物品之间，进行信息交换和通信，也就是物物相息。物联网通过智能感知、识别技术与普适计算等通信感知技术，广泛应用于网络的融合中，也因此被称为继计算机、互联网之后世界信息产业发展的第三次浪潮。物联网是互联网的应用拓展，与其说物联网是网络，不如说物联网是业务和应用。因此，应用创新是物联网发展的核心，以用户体验为核心的创新2.0是物联网发展的灵魂。 什么是认知物联网？简单地说，认知物联网是将认知计算技术与互连设备产生的数据和这些设备可以执行的操作结合使用。您可能已经知道物联网，也知道我们所说的传感器和执行器的意思。从认知计算方面讲，物联网又是什么意思呢？当然，认知意味着思考，而计算机不具备普通人类的思考能力，它们现在可以执行一些人类认为是思考 的基本功能。认知涉及三个关键要素： 理解 推理 学习 在计算机中，系统理解意味着，能够读入大量的结构化和非结构化数据，并从中获取其意义，也就是说，建立一个概念、实体和关系的模型。推理 意味着，使用这个模型能够得出答案或解决相关问题，无需专门对答案和解决方案进行编程。而学习 意味着，能够自动地从数据推断出新的知识，这是大规模理解的一个关键组成部分。 构建大规模的概念和关系的复杂模型过于费时，且成本高昂。此外，有许多关系是事先不知道或不明确的，所以只能让机器自动分析大数据集来发现模式，然后才能真正发现这些关系。 会思考的物体认知计算对于物联网非常重要，这有几个关键原因。 生成数据的速率和规模：学习有助于优化流程或系统，在结合有关系统的传感器数据与其他上下文信息的基础上，使其更加高效。从设备生成的数据快速超越了人类通过分析去发现重要模式和学习的能力。应用机器学习对于能够扩展物联网是必不可少的。 计算转移到物理世界：随着越来越多各种年龄和技术技能水平的人在与物联网系统进行交互，我们需要超越当前的机器接口范式，这种范式要求人类学会与机器交互所需的抽象和专用接口。此外，这种转移需要走向更加以人类为中心的接口。换句话说，人们需要能够用自然语言与物联网系统（物体）进行交互。该系统必须开始理解人类。来自麻省理工媒体实验室 (MIT Media Lab) 的作者 David Rose 提出了术语 “魔法对象 (enchanted objects) ” 来描述看似智能的行为，我们可以通过物联网和认知计算将这些行为注入连接的设备。 多种数据源和数据类型的集成：在物联网中，存在许多数据源，可以提供相关的信息或上下文，有助于更好地理解和制定决策。数据有多种不同的类型，包括数字传感器数据、音频、视频、非结构化的文本数据、位置数据等，消化和分析这些数据类型并通过这些数据类型识别相关性和模式的能力，是一项非常强大的功能。通过掌握上下文可以大大提高对人类操作者的意图的理解，这包括物理环境、时间维度，甚至是情绪状况。通过集成多个不同的数据源可以完善推理和决策，例如，关联传感器数据与声学数据。 人类感知设备认知物联网 是下一次飞跃，它通过学习并将更多的人类意识注入到与我们交互的设备和环境中，提高传感器驱动的复杂系统的准确性和效率。这一飞跃可以让我们的物体 用我们的语言（而不是其他方式）来理解我们，并与我们进行交互。]]></content>
      <categories>
        <category>物联网</category>
      </categories>
      <tags>
        <tag>科技前沿</tag>
        <tag>物联网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链入门必备]]></title>
    <url>%2F2018%2F03%2F06%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%E5%BF%85%E5%A4%87%2F</url>
    <content type="text"><![CDATA[1、区块链是什么？区块链是一种防篡改的、共享的数字化账本，用于记录公有或私有对等网络中的交易。账本分发给网络中的所有成员节点，在通过哈希密码算法链接的区块的顺序链中，永久记录网络中的对等节点之间发生的资产交易的历史记录。 所有经过确认和证明的交易都从链的开头一直链接到最新的区块，因此得名区块链。区块链可以充当单一事实来源，而且区块链网络中的成员只能查看与他们相关的交易。 2、区块链网络的工作原理区块链网络中的成员节点不依赖于第三方（比如金融机构）来仲裁交易，它们使用共识协议来协商账本内容，使用哈希加密算法和数字签名来确保交易的完整性。 共识性能确保共享账本是精确副本，并降低了发生交易欺诈的风险，因为篡改需要在许多地方同时执行。哈希加密算法（比如 SHA256 计算算法）能确保对交易输入的任何改动 — 甚至是最细微的改动 — 都会计算出一个不同的哈希值，表明交易输入可能被损坏。数字签名确保交易源自发送方（已使用私钥签名）而不是冒名顶替者。 去中心化对等区块链网络可阻止任何单个或一组参与者控制底层基础架构或破坏整个系统。网络中的参与者是平等的，都遵守相同的协议。它们可以是个人、国家代表、企业或所有这三种参与者的组合。 在其核心，该系统会记录交易的时间顺序，而且所有节点都使用选定的共识模型来协商交易的有效性。这会使交易不可逆并被网络中的所有成员接受。 3、区块链技术的商业优势在传统业务网络中，所有参与者都维护着自己的账本，这些账本之间的重复和差异会导致争议、更长的结算时间，而且因为需要中介，还会导致相关的间接管理成本。但是，通过使用基于区块链的共享账本，交易在通过共识性验证并写入账本后，就不能再更改，这样企业就能节省时间和成本，同时减少风险。 区块链共识机制提供了经过整合的、一致的数据集的优势，减少了错误，拥有近实时的引用数据，而且参与者能够灵活更改其拥有的资产的描述。 因为没有参与成员拥有共享账本中所含信息的来源，所以区块链技术会提高参与成员之间的交易信息流中的可信度和完整性。 区块链技术的不变性机制降低了审计和合规性成本，增加了透明性。而且在使用区块链技术的业务网络上，合约得以智能、自动化执行并最终确认，所以企业会获得更高的执行速度、更低的成本和更少的风险，所有这些使企业能构建新收入流来与客户交互。 4、如何才算是好的区块链用例？要确定您的用例是否适合使用区块链，请询问自己以下问题： 是否涉及业务网络？ 是否使用共识性来验证交易？ 是否需要审计线索或来源？ 交易记录是否必须不可变或防篡改？ 争议的解决方法是否会是最终解决方法？如果第一个问题和其他问题中的至少一个问题的答案为“是”，那么您的用例就会从区块链技术受益。要成为合适的解决方案，区块链必须涉及一个网络，但该网络可以具有多种形式。该网络可位于企业之间，比如供应链，或者该网络可以在一个企业内。例如：在企业内，可以使用区块链网络在部门之间共享参考数据，或者创建审计或合规性网络。该网络也可存在于个人之间，比如需要在区块链上存储数据、数字资产或合约的人。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>科技前沿</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 基础模块 - 大型程序的结构]]></title>
    <url>%2F2018%2F03%2F05%2FFlask%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9D%97-%E5%A4%A7%E5%9E%8B%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[尽管在单一脚本中编写小型Web 程序很方便，但这种方法并不能广泛使用。程序变复杂后，使用单个大型源码文件会导致很多问题。 不同于大多数其他的Web 框架，Flask 并不强制要求大型项目使用特定的组织方式，程序结构的组织方式完全由开发者决定。在本章，我们将介绍一种使用包和模块组织大型程序的方式。本书后续示例都将采用这种结构。 1、项目结构Flask 程序的基本结构如示例7-1 所示。示例7-1 多文件Flask 程序的基本结构1234567891011121314151617181920|-flasky |-app/ |-templates/ |-static/ |-main/ |-__init__.py |-errors.py |-forms.py |-views.py |-__init__.py |-email.py |-models.py |-migrations/ |-tests/ |-__init__.py |-test*.py |-venv/ |-requirements.txt |-config.py |-manage.py 这种结构有4 个顶级文件夹： Flask程序一般都保存在名为app 的包中； 和之前一样，migrations文件夹包含数据库迁移脚本； 单元测试编写在 tests包中； 和之前一样，venv 文件夹包含 Python 虚拟环境。 同时还创建了一些新文件： requirements.txt列出了所有依赖包，便于在其他电脑中重新生成相同的虚拟环境； config.py 存储配置； manage.py用于启动程序以及其他的程序任务。 为了帮助你完全理解这个结构，下面几节讲解把hello.py 程序转换成这种结构的过程。 2、配置选项程序经常需要设定多个配置。这方面最好的例子就是开发、测试和生产环境要使用不同的数据库，这样才不会彼此影响。 我们不再使用hello.py 中简单的字典状结构配置，而使用层次结构的配置类。config.py 文件的内容如示例7-2 所示。 示例7-2 config.py：程序的配置123456789101112131415161718192021222324252627282930313233343536373839import osbasedir = os.path.abspath(os.path.dirname(__file__))class Config: SECRET_KEY = os.environ.get(&apos;SECRET_KEY&apos;) or &apos;hard to guess string&apos; SQLALCHEMY_COMMIT_ON_TEARDOWN = True FLASKY_MAIL_SUBJECT_PREFIX = &apos;[Flasky]&apos; FLASKY_MAIL_SENDER = &apos;Flasky Admin &lt;flasky@example.com&gt;&apos; FLASKY_ADMIN = os.environ.get(&apos;FLASKY_ADMIN&apos;) @staticmethod def init_app(app): passclass DevelopmentConfig(Config): DEBUG = True MAIL_SERVER = &apos;smtp.googlemail.com&apos; MAIL_PORT = 587 MAIL_USE_TLS = True MAIL_USERNAME = os.environ.get(&apos;MAIL_USERNAME&apos;) MAIL_PASSWORD = os.environ.get(&apos;MAIL_PASSWORD&apos;) SQLALCHEMY_DATABASE_URI = os.environ.get(&apos;DEV_DATABASE_URL&apos;) or \ &apos;sqlite:///&apos; + os.path.join(basedir, &apos;data-dev.sqlite&apos;)class TestingConfig(Config): TESTING = True SQLALCHEMY_DATABASE_URI = os.environ.get(&apos;TEST_DATABASE_URL&apos;) or \ &apos;sqlite:///&apos; + os.path.join(basedir, &apos;data-test.sqlite&apos;)class ProductionConfig(Config): SQLALCHEMY_DATABASE_URI = os.environ.get(&apos;DATABASE_URL&apos;) or \ &apos;sqlite:///&apos; + os.path.join(basedir, &apos;data.sqlite&apos;)config = &#123; &apos;development&apos;: DevelopmentConfig, &apos;testing&apos;: TestingConfig, &apos;production&apos;: ProductionConfig, &apos;default&apos;: DevelopmentConfig&#125; 基类Config 中包含通用配置，子类分别定义专用的配置。如果需要，你还可添加其他配置类。 为了让配置方式更灵活且更安全，某些配置可以从环境变量中导入。例如，SECRET_KEY 的值，这是个敏感信息，可以在环境中设定，但系统也提供了一个默认值，以防环境中没有定义。 在3 个子类中，SQLALCHEMY_DATABASE_URI 变量都被指定了不同的值。这样程序就可在不同的配置环境中运行，每个环境都使用不同的数据库。 配置类可以定义init_app() 类方法，其参数是程序实例。在这个方法中，可以执行对当前环境的配置初始化。现在，基类Config 中的init_app() 方法为空。 在这个配置脚本末尾，config 字典中注册了不同的配置环境，而且还注册了一个默认配置。 3、程序包程序包用来保存程序的所有代码、模板和静态文件。我们可以把这个包直接称为app（应用），如果有需求，也可使用一个程序专用名字。templates 和static 文件夹是程序包的一部分，因此这两个文件夹被移到了app 中。数据库模型和电子邮件支持函数也被移到了这个包中，分别保存为app/models.py 和app/email.py。 （1）使用程序工厂函数在单个文件中开发程序很方便，但却有个很大的缺点，因为程序在全局作用域中创建，所以无法动态修改配置。运行脚本时，程序实例已经创建，再修改配置为时已晚。这一点对单元测试尤其重要，因为有时为了提高测试覆盖度，必须在不同的配置环境中运行程序。这个问题的解决方法是延迟创建程序实例，把创建过程移到可显式调用的工厂函数中。这种方法不仅可以给脚本留出配置程序的时间，还能够创建多个程序实例，这些实例有时在测试中非常有用。程序的工厂函数在app 包的构造文件中定义，如示例7-3 所示。 构造文件导入了大多数正在使用的Flask 扩展。由于尚未初始化所需的程序实例，所以没有初始化扩展，创建扩展类时没有向构造函数传入参数。create_app() 函数就是程序的工厂函数，接受一个参数，是程序使用的配置名。配置类在config.py 文件中定义，其中保存的配置可以使用Flask app.config 配置对象提供的from_object() 方法直接导入程序。至于配置对象，则可以通过名字从config 字典中选择。程序创建并配置好后，就能初始化扩展了。在之前创建的扩展对象上调用init_app() 可以完成初始化过程。 示例7-3 app/__init__.py：程序包的构造文件12345678910111213141516171819202122232425from flask import Flask, render_templatefrom flask.ext.bootstrap import Bootstrapfrom flask.ext.mail import Mailfrom flask.ext.moment import Momentfrom flask.ext.sqlalchemy import SQLAlchemyfrom config import configbootstrap = Bootstrap()mail = Mail()moment = Moment()db = SQLAlchemy()def create_app(config_name): app = Flask(__name__) app.config.from_object(config[config_name]) config[config_name].init_app(app) bootstrap.init_app(app) mail.init_app(app) moment.init_app(app) db.init_app(app) # 附加路由和自定义的错误页面 return app 工厂函数返回创建的程序示例，不过要注意，现在工厂函数创建的程序还不完整，因为没有路由和自定义的错误页面处理程序。 （2）在蓝本中实现程序功能转换成程序工厂函数的操作让定义路由变复杂了。在单脚本程序中，程序实例存在于全局作用域中，路由可以直接使用app.route 修饰器定义。但现在程序在运行时创建，只有调用create_app() 之后才能使用app.route 修饰器，这时定义路由就太晚了。和路由一样，自定义的错误页面处理程序也面临相同的困难，因为错误页面处理程序使用app.errorhandler 修饰器定义。 幸好Flask 使用蓝本提供了更好的解决方法。蓝本和程序类似，也可以定义路由。不同的是，在蓝本中定义的路由处于休眠状态，直到蓝本注册到程序上后，路由才真正成为程序的一部分。使用位于全局作用域中的蓝本时，定义路由的方法几乎和单脚本程序一样。 和程序一样，蓝本可以在单个文件中定义，也可使用更结构化的方式在包中的多个模块中创建。为了获得最大的灵活性，程序包中创建了一个子包，用于保存蓝本。示例7-4 是这个子包的构造文件，蓝本就创建于此。 示例7-4 app/main/__init__.py：创建蓝本123from flask import Blueprintmain = Blueprint(&apos;main&apos;, __name__)from . import views, errors 通过实例化一个Blueprint 类对象可以创建蓝本。这个构造函数有两个必须指定的参数：蓝本的名字和蓝本所在的包或模块。和程序一样，大多数情况下第二个参数使用Python 的__name__ 变量即可。 程序的路由保存在包里的app/main/views.py 模块中，而错误处理程序保存在app/main/errors.py 模块中。导入这两个模块就能把路由和错误处理程序与蓝本关联起来。注意，这些模块在app/main/__init__.py 脚本的末尾导入，这是为了避免循环导入依赖，因为在views.py 和errors.py 中还要导入蓝本main。 蓝本在工厂函数create_app() 中注册到程序上，如示例7-5 所示。示例7-5 app/__init__.py：注册蓝本123456def create_app(config_name): # ... from .main import main as main_blueprint app.register_blueprint(main_blueprint) return app 示例7-6 显示了错误处理程序。示例7-6 app/main/errors.py：蓝本中的错误处理程序12345678910from flask import render_templatefrom . import main@main.app_errorhandler(404)def page_not_found(e): return render_template(&apos;404.html&apos;), 404@main.app_errorhandler(500)def internal_server_error(e): return render_template(&apos;500.html&apos;), 500 在蓝本中编写错误处理程序稍有不同，如果使用errorhandler 修饰器，那么只有蓝本中的错误才能触发处理程序。要想注册程序全局的错误处理程序，必须使用app_errorhandler。 在蓝本中定义的程序路由如示例7-7 所示。示例7-7 app/main/views.py：蓝本中定义的程序路由123456789101112131415from datetime import datetimefrom flask import render_template, session, redirect, url_forfrom . import mainfrom .forms import NameFormfrom .. import dbfrom ..models import User@main.route(&apos;/&apos;, methods=[&apos;GET&apos;, &apos;POST&apos;])def index(): form = NameForm() if form.validate_on_submit(): # ... return redirect(url_for(&apos;.index&apos;)) return render_template(&apos;index.html&apos;, form=form, name=session.get(&apos;name&apos;), known=session.get(&apos;known&apos;, False), current_time=datetime.utcnow()) 在蓝本中编写视图函数主要有两点不同：第一，和前面的错误处理程序一样，路由修饰器由蓝本提供；第二，url_for() 函数的用法不同。你可能还记得，url_for() 函数的第一个参数是路由的端点名，在程序的路由中，默认为视图函数的名字。例如，在单脚本程序中，index() 视图函数的URL 可使用url_for(‘index’) 获取。 在蓝本中就不一样了，Flask 会为蓝本中的全部端点加上一个命名空间，这样就可以在不同的蓝本中使用相同的端点名定义视图函数，而不会产生冲突。命名空间就是蓝本的名字（Blueprint 构造函数的第一个参数），所以视图函数index() 注册的端点名是main.index，其URL 使用url_for(‘main.index’) 获取。 url_for() 函数还支持一种简写的端点形式，在蓝本中可以省略蓝本名，例如url_for(‘.index’)。在这种写法中，命名空间是当前请求所在的蓝本。这意味着同一蓝本中的重定向可以使用简写形式，但跨蓝本的重定向必须使用带有命名空间的端点名。 为了完全修改程序的页面，表单对象也要移到蓝本中，保存于app/main/forms.py 模块。 4、启动脚本顶级文件夹中的manage.py 文件用于启动程序。脚本内容如示例7-8 所示。示例7-8 manage.py：启动脚本1234567891011121314151617181920#!/usr/bin/env pythonimport osfrom app import create_app, dbfrom app.models import User, Rolefrom flask.ext.script import Manager, Shellfrom flask.ext.migrate import Migrate, MigrateCommandapp = create_app(os.getenv(&apos;FLASK_CONFIG&apos;) or &apos;default&apos;)manager = Manager(app)migrate = Migrate(app, db)def make_shell_context(): return dict(app=app, db=db, User=User, Role=Role)manager.add_command(&quot;shell&quot;, Shell(make_context=make_shell_context))manager.add_command(&apos;db&apos;, MigrateCommand)if __name__ == &apos;__main__&apos;: manager.run() 这个脚本先创建程序。如果已经定义了环境变量FLASK_CONFIG，则从中读取配置名；否则使用默认配置。然后初始化Flask-Script、Flask-Migrate 和为Python shell 定义的上下文。出于便利，脚本中加入了shebang 声明，所以在基于Unix 的操作系统中可以通过./manage.py 执行脚本，而不用使用复杂的python manage.py。 5、需求文件程序中必须包含一个requirements.txt 文件，用于记录所有依赖包及其精确的版本号。如果要在另一台电脑上重新生成虚拟环境，这个文件的重要性就体现出来了，例如部署程序时使用的电脑。pip 可以使用如下命令自动生成这个文件： (venv) $ pip freeze &gt;requirements.txt 安装或升级包后，最好更新这个文件。需求文件的内容示例如下： Flask==0.10.1Flask-Bootstrap==3.0.3.1Flask-Mail==0.9.0Flask-Migrate==1.1.0Flask-Moment==0.2.0Flask-SQLAlchemy==1.0Flask-Script==0.6.6Flask-WTF==0.9.4Jinja2==2.7.1Mako==0.9.1MarkupSafe==0.18SQLAlchemy==0.8.4WTForms==1.0.5Werkzeug==0.9.4alembic==0.6.2blinker==1.3itsdangerous==0.23 如果你要创建这个虚拟环境的完全副本，可以创建一个新的虚拟环境，并在其上运行以下命令： (venv) $ pip install -r requirements.txt 当你阅读本书时，该示例requirements.txt 文件中的版本号可能已经过期了。如果愿意，你可以试着使用这些包的最新版。如果遇到问题，你可以随时换回这个需求文件中的版本，因为这些版本和程序兼容。 6、单元测试这个程序很小，所以没什么可测试的。不过为了演示，我们可以编写两个简单的测试，如示例7-9 所示。 示例7-9 tests/test_basics.py：单元测试1234567891011121314151617181920import unittestfrom flask import current_appfrom app import create_app, dbclass BasicsTestCase(unittest.TestCase): def setUp(self): self.app = create_app(&apos;testing&apos;) self.app_context = self.app.app_context() self.app_context.push() db.create_all() def tearDown(self): db.session.remove() db.drop_all() self.app_context.pop() def test_app_exists(self): self.assertFalse(current_app is None) def test_app_is_testing(self): self.assertTrue(current_app.config[&apos;TESTING&apos;]) 这个测试使用Python 标准库中的unittest 包编写。setUp() 和tearDown() 方法分别在各测试前后运行，并且名字以test_ 开头的函数都作为测试执行。 setUp() 方法尝试创建一个测试环境，类似于运行中的程序。首先，使用测试配置创建程序，然后激活上下文。这一步的作用是确保能在测试中使用current_app，像普通请求一样。然后创建一个全新的数据库，以备不时之需。数据库和程序上下文在tearDown() 方法中删除。 第一个测试确保程序实例存在。第二个测试确保程序在测试配置中运行。若想把tests 文件夹作为包使用，需要添加tests/__init__.py 文件，不过这个文件可以为空，因为unittest 包会扫描所有模块并查找测试。 为了运行单元测试，你可以在manage.py 脚本中添加一个自定义命令。示例7-10 展示了如何添加test 命令。 示例7-10 manage.py：启动单元测试的命令123456@manager.commanddef test(): &quot;&quot;&quot;Run the unit tests.&quot;&quot;&quot; import unittest tests = unittest.TestLoader().discover(&apos;tests&apos;) unittest.TextTestRunner(verbosity=2).run(tests) manager.command 修饰器让自定义命令变得简单。修饰函数名就是命令名，函数的文档字符串会显示在帮助消息中。test() 函数的定义体中调用了unittest 包提供的测试运行函数。 单元测试可使用下面的命令运行：123456(venv) $ python manage.py testtest_app_exists (test_basics.BasicsTestCase) ... oktest_app_is_testing (test_basics.BasicsTestCase) ... ok.----------------------------------------------------------------------Ran 2 tests in 0.001sOK 7、创建数据库重组后的程序和单脚本版本使用不同的数据库。 首选从环境变量中读取数据库的URL，同时还提供了一个默认的SQLite 数据库做备用。3种配置环境中的环境变量名和SQLite 数据库文件名都不一样。例如，在开发环境中，数据库URL 从环境变量DEV_DATABASE_URL 中读取，如果没有定义这个环境变量，则使用名为data-dev.sqlite 的SQLite 数据库。 不管从哪里获取数据库URL，都要在新数据库中创建数据表。如果使用Flask-Migrate 踪迁移，可使用如下命令创建数据表或者升级到最新修订版本： (venv) $ python manage.py db upgrade]]></content>
      <categories>
        <category>Python</category>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 基础模块 - 数据库之Flask-SQLAlchemy管理数据库]]></title>
    <url>%2F2018%2F03%2F05%2FFlask%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9D%97-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B9%8BFlask-SQLAlchemy%E7%AE%A1%E7%90%86%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[Flask-SQLAlchemy 是一个Flask 扩展，简化了在Flask 程序中使用SQLAlchemy 的操作。SQLAlchemy 是一个很强大的关系型数据库框架，支持多种数据库后台。SQLAlchemy 提供了高层ORM，也提供了使用数据库原生SQL 的低层功能。 和其他大多数扩展一样，Flask-SQLAlchemy 也使用pip 安装： (venv) $ pip install flask-sqlalchemy在Flask-SQLAlchemy 中，数据库使用URL 指定。最流行的数据库引擎采用的数据库URL格式如表5-1 所示。 表5-1 FLask-SQLAlchemy数据库URL 数据库引擎 URL MySQL mysql://username:password@hostname/database Postgres postgresql://username:password@hostname/database SQLite（Unix） sqlite:////absolute/path/to/database SQLite（Windows） sqlite:///c:/absolute/path/to/database 在这些URL 中，hostname 表示MySQL 服务所在的主机，可以是本地主机（localhost），也可以是远程服务器。数据库服务器上可以托管多个数据库，因此database 表示要使用的数据库名。如果数据库需要进行认证，username 和password 表示数据库用户密令。（注：SQLite 数据库不需要使用服务器，因此不用指定hostname、username 和password。URL 中的database 是硬盘上文件的文件名。） 程序使用的数据库URL 必须保存到Flask 配置对象的SQLALCHEMY_DATABASE_URI 键中。配置对象中还有一个很有用的选项，即SQLALCHEMY_COMMIT_ON_TEARDOWN 键，将其设为True时，每次请求结束后都会自动提交数据库中的变动。其他配置选项的作用请参阅Flask-SQLAlchemy 的文档。示例5-1 展示了如何初始化及配置一个简单的SQLite 数据库。 示例5-1 hello.py：配置数据库123456from flask.ext.sqlalchemy import SQLAlchemybasedir = os.path.abspath(os.path.dirname(__file__))app = Flask(__name__)app.config[&apos;SQLALCHEMY_DATABASE_URI&apos;] = &apos;sqlite:///&apos; + os.path.join(basedir, &apos;data.sqlite&apos;)app.config[&apos;SQLALCHEMY_COMMIT_ON_TEARDOWN&apos;] = Truedb = SQLAlchemy(app) db 对象是SQLAlchemy 类的实例，表示程序使用的数据库，同时还获得了Flask-SQLAlchemy提供的所有功能。 1、定义模型模型这个术语表示程序使用的持久化实体。在ORM 中，模型一般是一个Python 类，类中的属性对应数据库表中的列。 Flask-SQLAlchemy 创建的数据库实例为模型提供了一个基类以及一系列辅助类和辅助函数，可用于定义模型的结构。图5-1 中的roles 表和users 表可定义为模型Role 和User，如示例5-2 所示。 示例5-2 hello.py：定义Role 和User 模型123456789101112class Role(db.Model): __tablename__ = &apos;roles&apos; id = db.Column(db.Integer, primary_key=True) name = db.Column(db.String(64), unique=True) def __repr__(self): return &apos;&lt;Role %r&gt;&apos; % self.nameclass User(db.Model): __tablename__ = &apos;users&apos; id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(64), unique=True, index=True) def __repr__(self): return &apos;&lt;User %r&gt;&apos; % self.username 类变量__tablename__ 定义在数据库中使用的表名。如果没有定义__tablename__，Flask-SQLAlchemy 会使用一个默认名字，但默认的表名没有遵守使用复数形式进行命名的约定，所以最好由我们自己来指定表名。其余的类变量都是该模型的属性，被定义为db.Column类的实例。 db.Column 类构造函数的第一个参数是数据库列和模型属性的类型。表5-2 列出了一些可用的列类型以及在模型中使用的Python 类型。 表5-2 最常用的SQLAlchemy列类型 类型名 Python类型 说 明 Integer int 普通整数，一般是32 位 SmallInteger int 取值范围小的整数，一般是16 位 BigInteger int 或long 不限制精度的整数 Float float 浮点数 Numeric decimal.Decimal 定点数 String str 变长字符串 Text str 变长字符串，对较长或不限长度的字符串做了优化 Unicode unicode 变长Unicode 字符串 UnicodeText unicode 变长Unicode 字符串，对较长或不限长度的字符串做了优化 Boolean bool 布尔值 Date datetime.date 日期 Time datetime.time 时间 DateTime datetime.datetime 日期和时间 Interval datetime.timedelta 时间间隔 Enum str 一组字符串 PickleType 任何Python 对象 自动使用Pickle 序列化 LargeBinary str 二进制文件 db.Column 中其余的参数指定属性的配置选项。表5-3 列出了一些可用选项。 表5-3 最常使用的SQLAlchemy列选项 选项名 说 明 primary_key 如果设为True，这列就是表的主键 unique 如果设为True，这列不允许出现重复的值 index 如果设为True，为这列创建索引，提升查询效率 nullable 如果设为True，这列允许使用空值；如果设为False，这列不允许使用空值 default 为这列定义默认值 （注：Flask-SQLAlchemy 要求每个模型都要定义主键，这一列经常命名为id。） 虽然没有强制要求，但这两个模型都定义了__repr()__ 方法，返回一个具有可读性的字符串表示模型，可在调试和测试时使用。 2、关系关系型数据库使用关系把不同表中的行联系起来。图5-1 所示的关系图表示用户和角色之间的一种简单关系。这是角色到用户的一对多关系，因为一个角色可属于多个用户，而每个用户都只能有一个角色。 图5-1 中的一对多关系在模型类中的表示方法如示例5-3 所示。 示例5-3 hello.py：关系1234567class Role(db.Model): # ... users = db.relationship(&apos;User&apos;, backref=&apos;role&apos;)class User(db.Model): # ... role_id = db.Column(db.Integer, db.ForeignKey(&apos;roles.id&apos;)) 如图5-1 所示，关系使用users 表中的外键连接了两行。添加到User 模型中的role_id 列被定义为外键，就是这个外键建立起了关系。传给db.ForeignKey() 的参数’roles.id’ 表明，这列的值是roles 表中行的id 值。 添加到Role 模型中的users 属性代表这个关系的面向对象视角。对于一个Role 类的实例，其users 属性将返回与角色相关联的用户组成的列表。db.relationship() 的第一个参数表明这个关系的另一端是哪个模型。如果模型类尚未定义，可使用字符串形式指定。 db.relationship() 中的backref 参数向User 模型中添加一个role 属性，从而定义反向关系。这一属性可替代role_id 访问Role 模型，此时获取的是模型对象，而不是外键的值。 大多数情况下，db.relationship() 都能自行找到关系中的外键，但有时却无法决定把哪一列作为外键。例如，如果User 模型中有两个或以上的列定义为Role 模型的外键，SQLAlchemy 就不知道该使用哪列。如果无法决定外键，你就要为db.relationship() 提供额外参数，从而确定所用外键。表5-4 列出了定义关系时常用的配置选项。 表5-4 常用的SQLAlchemy关系选项 选项名 说 明 backref 在关系的另一个模型中添加反向引用 primaryjoin 明确指定两个模型之间使用的联结条件。只在模棱两可的关系中需要指定 lazy 指定如何加载相关记录。可选值有select（首次访问时按需加载）、immediate（源对象加载后就加载）、joined（加载记录，但使用联结）、subquery（立即加载，但使用子查询），noload（永不加载）和dynamic（不加载记录，但提供加载记录的查询） uselist 如果设为Fales，不使用列表，而使用标量值 order_by 指定关系中记录的排序方式 secondary 指定多对多关系中关系表的名字 secondaryjoin SQLAlchemy 无法自行决定时，指定多对多关系中的二级联结条件 除了一对多之外，还有几种其他的关系类型。一对一关系可以用前面介绍的一对多关系表示，但调用db.relationship() 时要把uselist 设为False，把“多”变成“一”。多对一关系也可使用一对多表示，对调两个表即可，或者把外键和db.relationship() 都放在“多”这一侧。最复杂的关系类型是多对多，需要用到第三张表，这个表称为关系表。 3、数据库操作（1）创建表首先，我们要让Flask-SQLAlchemy 根据模型类创建数据库。方法是使用db.create_all()函数：123(venv) $ python hello.py shell&gt;&gt;&gt; from hello import db&gt;&gt;&gt; db.create_all() 如果你查看程序目录，会发现新建了一个名为data.sqlite 的文件。这个SQLite 数据库文件的名字就是在配置中指定的。如果数据库表已经存在于数据库中，那么db.create_all()不会重新创建或者更新这个表。如果修改模型后要把改动应用到现有的数据库中，这一特性会带来不便。更新现有数据库表的粗暴方式是先删除旧表再重新创建：12&gt;&gt;&gt; db.drop_all()&gt;&gt;&gt; db.create_all() （注：遗憾的是，这个方法有个我们不想看到的副作用，它把数据库中原有的数据都销毁了。末尾将会介绍一种更好的方式用于更新数据库。） （2）插入行下面这段代码创建了一些角色和用户：1234567&gt;&gt;&gt; from hello import Role, User&gt;&gt;&gt; admin_role = Role(name=&apos;Admin&apos;)&gt;&gt;&gt; mod_role = Role(name=&apos;Moderator&apos;)&gt;&gt;&gt; user_role = Role(name=&apos;User&apos;)&gt;&gt;&gt; user_john = User(username=&apos;john&apos;, role=admin_role)&gt;&gt;&gt; user_susan = User(username=&apos;susan&apos;, role=user_role)&gt;&gt;&gt; user_david = User(username=&apos;david&apos;, role=user_role) 模型的构造函数接受的参数是使用关键字参数指定的模型属性初始值。注意，role 属性也可使用，虽然它不是真正的数据库列，但却是一对多关系的高级表示。这些新建对象的id属性并没有明确设定，因为主键是由Flask-SQLAlchemy 管理的。现在这些对象只存在于Python 中，还未写入数据库。因此id 尚未赋值：123456&gt;&gt;&gt; print(admin_role.id)None&gt;&gt;&gt; print(mod_role.id)None&gt;&gt;&gt; print(user_role.id)None 通过数据库会话管理对数据库所做的改动，在Flask-SQLAlchemy 中，会话由db.session表示。准备把对象写入数据库之前，先要将其添加到会话中：123456&gt;&gt;&gt; db.session.add(admin_role)&gt;&gt;&gt; db.session.add(mod_role)&gt;&gt;&gt; db.session.add(user_role)&gt;&gt;&gt; db.session.add(user_john)&gt;&gt;&gt; db.session.add(user_susan)&gt;&gt;&gt; db.session.add(user_david) 或者简写成：12&gt;&gt;&gt; db.session.add_all([admin_role, mod_role, user_role,... user_john, user_susan, user_david]) 为了把对象写入数据库，我们要调用commit() 方法提交会话：&gt;&gt;&gt; db.session.commit() 再次查看id 属性，现在它们已经赋值了：123456&gt;&gt;&gt; print(admin_role.id)1&gt;&gt;&gt; print(mod_role.id)2&gt;&gt;&gt; print(user_role.id)3 数据库会话能保证数据库的一致性。提交操作使用原子方式把会话中的对象全部写入数据库。如果在写入会话的过程中发生了错误，整个会话都会失效。如果你始终把相关改动放在会话中提交，就能避免因部分更新导致的数据库不一致性。 （3）修改行在数据库会话上调用add() 方法也能更新模型。我们继续在之前的shell 会话中进行操作，下面这个例子把”Admin” 角色重命名为”Administrator”：123&gt;&gt;&gt; admin_role.name = &apos;Administrator&apos;&gt;&gt;&gt; db.session.add(admin_role)&gt;&gt;&gt; db.session.commit() （4）删除行数据库会话还有个delete() 方法。下面这个例子把”Moderator” 角色从数据库中删除：12&gt;&gt;&gt; db.session.delete(mod_role)&gt;&gt;&gt; db.session.commit() 注意，删除与插入和更新一样，提交数据库会话后才会执行。 （5）查询行Flask-SQLAlchemy 为每个模型类都提供了query 对象。最基本的模型查询是取回对应表中的所有记录：1234&gt;&gt;&gt; Role.query.all()[&lt;Role u&apos;Administrator&apos;&gt;, &lt;Role u&apos;User&apos;&gt;]&gt;&gt;&gt; User.query.all()[&lt;User u&apos;john&apos;&gt;, &lt;User u&apos;susan&apos;&gt;, &lt;User u&apos;david&apos;&gt;] 使用过滤器可以配置query 对象进行更精确的数据库查询。下面这个例子查找角色为”User” 的所有用户：12&gt;&gt;&gt; User.query.filter_by(role=user_role).all()[&lt;User u&apos;susan&apos;&gt;, &lt;User u&apos;david&apos;&gt;] 若要查看SQLAlchemy 为查询生成的原生SQL 查询语句，只需把query 对象转换成字符串：123&gt;&gt;&gt; str(User.query.filter_by(role=user_role))&apos;SELECT users.id AS users_id, users.username AS users_username,users.role_id AS users_role_id FROM users WHERE :param_1 = users.role_id&apos; filter_by() 等过滤器在query 对象上调用，返回一个更精确的query 对象。多个过滤器可以一起调用，直到获得所需结果。 表5-5 列出了可在query 对象上调用的常用过滤器。完整的列表参见SQLAlchemy 文档（http://docs.sqlalchemy.org）。 表5-5 常用的SQLAlchemy查询过滤器 过滤器 说 明 filter() 把过滤器添加到原查询上，返回一个新查询 filter_by() 把等值过滤器添加到原查询上，返回一个新查询 limit() 使用指定的值限制原查询返回的结果数量，返回一个新查询 offset() 偏移原查询返回的结果，返回一个新查询 order_by() 根据指定条件对原查询结果进行排序，返回一个新查询 group_by() 根据指定条件对原查询结果进行分组，返回一个新查询 在查询上应用指定的过滤器后，通过调用all() 执行查询，以列表的形式返回结果。除了all() 之外，还有其他方法能触发查询执行。表5-6 列出了执行查询的其他方法。 表5-6 最常使用的SQLAlchemy查询执行函数 方 法 说 明 all() 以列表形式返回查询的所有结果 first() 返回查询的第一个结果，如果没有结果，则返回None first_or_404() 返回查询的第一个结果，如果没有结果，则终止请求，返回404 错误响应 get() 返回指定主键对应的行，如果没有对应的行，则返回None get_or_404() 返回指定主键对应的行，如果没找到指定的主键，则终止请求，返回404 错误响应 count() 返回查询结果的数量 paginate() 返回一个Paginate 对象，它包含指定范围内的结果 关系和查询的处理方式类似。下面这个例子分别从关系的两端查询角色和用户之间的一对多关系：12345&gt;&gt;&gt; users = user_role.users&gt;&gt;&gt; users[&lt;User u&apos;susan&apos;&gt;, &lt;User u&apos;david&apos;&gt;]&gt;&gt;&gt; users[0].role&lt;Role u&apos;User&apos;&gt; 这个例子中的user_role.users 查询有个小问题。执行user_role.users 表达式时，隐含的查询会调用all() 返回一个用户列表。query 对象是隐藏的，因此无法指定更精确的查询过滤器。就这个特定示例而言，返回一个按照字母顺序排序的用户列表可能更好。如果修改了关系的设置，加入了lazy = ‘dynamic’ 参数，从而禁止自动执行查询。 4、使用Flask-Migrate实现数据库迁移在开发程序的过程中，你会发现有时需要修改数据库模型，而且修改之后还需要更新数据库。 仅当数据库表不存在时，Flask-SQLAlchemy 才会根据模型进行创建。因此，更新表的唯一方式就是先删除旧表，不过这样做会丢失数据库中的所有数据。 更新表的更好方法是使用数据库迁移框架。源码版本控制工具可以跟踪源码文件的变化，类似地，数据库迁移框架能跟踪数据库模式的变化，然后增量式的把变化应用到数据库中。 SQLAlchemy 的主力开发人员编写了一个迁移框架，称为Alembic（https://alembic.readthedocs.org/en/latest/index.html）。除了直接使用Alembic 之外，Flask 程序还可使用Flask-Migrate（http://flask-migrate.readthedocs.org/en/latest/）扩展。这个扩展对Alembic 做了轻量级包装，并集成到Flask-Script 中，所有操作都通过Flask-Script 命令完成。 （1）创建迁移仓库首先，我们要在虚拟环境中安装Flask-Migrate： (venv) $ pip install flask-migrate这个扩展的初始化方法如示例5-8 所示。 示例5-8 hello.py：配置Flask-Migrate1234from flask.ext.migrate import Migrate, MigrateCommand# ...migrate = Migrate(app, db)manager.add_command(&apos;db&apos;, MigrateCommand) 为了导出数据库迁移命令，Flask-Migrate 提供了一个MigrateCommand 类，可附加到Flask-Script 的manager 对象上。在这个例子中，MigrateCommand 类使用db 命令附加。 在维护数据库迁移之前，要使用init 子命令创建迁移仓库：12345678910(venv) $ python hello.py db initCreating directory /home/flask/flasky/migrations...doneCreating directory /home/flask/flasky/migrations/versions...doneGenerating /home/flask/flasky/migrations/alembic.ini...doneGenerating /home/flask/flasky/migrations/env.py...doneGenerating /home/flask/flasky/migrations/env.pyc...doneGenerating /home/flask/flasky/migrations/README...doneGenerating /home/flask/flasky/migrations/script.py.mako...donePlease edit configuration/connection/logging settings in&apos;/home/flask/flasky/migrations/alembic.ini&apos; before proceeding. 这个命令会创建migrations 文件夹，所有迁移脚本都存放其中。 （2）创建迁移脚本在Alembic 中，数据库迁移用迁移脚本表示。脚本中有两个函数，分别是upgrade() 和downgrade()。upgrade() 函数把迁移中的改动应用到数据库中，downgrade() 函数则将改动删除。Alembic 具有添加和删除改动的能力，因此数据库可重设到修改历史的任意一点。 我们可以使用revision 命令手动创建Alembic 迁移，也可使用migrate 命令自动创建。手动创建的迁移只是一个骨架，upgrade() 和downgrade() 函数都是空的，开发者要使用Alembic 提供的Operations 对象指令实现具体操作。自动创建的迁移会根据模型定义和数据库当前状态之间的差异生成upgrade() 和downgrade() 函数的内容。 migrate 子命令用来自动创建迁移脚本：123456789(venv) $ python hello.py db migrate -m &quot;initial migration&quot;INFO [alembic.migration] Context impl SQLiteImpl.INFO [alembic.migration] Will assume non-transactional DDL.INFO [alembic.autogenerate] Detected added table &apos;roles&apos;INFO [alembic.autogenerate] Detected added table &apos;users&apos;INFO [alembic.autogenerate.compare] Detected added index&apos;ix_users_username&apos; on &apos;[&apos;username&apos;]&apos;Generating /home/flask/flasky/migrations/versions/1bc594146bb5_initial_migration.py...done （3）更新数据库检查并修正好迁移脚本之后，我们可以使用db upgrade 命令把迁移应用到数据库中：1234(venv) $ python hello.py db upgradeINFO [alembic.migration] Context impl SQLiteImpl.INFO [alembic.migration] Will assume non-transactional DDL.INFO [alembic.migration] Running upgrade None -&gt; 1bc594146bb5, initial migration 对第一个迁移来说， 其作用和调用db.create_all() 方法一样。但在后续的迁移中，upgrade 命令能把改动应用到数据库中，且不影响其中保存的数据。]]></content>
      <categories>
        <category>Python</category>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 基础模块 - 数据库概述]]></title>
    <url>%2F2018%2F03%2F05%2FFlask%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9D%97-%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[数据库按照一定规则保存程序数据，程序再发起查询取回所需的数据。Web 程序最常用基于关系模型的数据库，这种数据库也称为SQL 数据库，因为它们使用结构化查询语言。不过最近几年文档数据库和键值对数据库成了流行的替代选择，这两种数据库合称NoSQL数据库。 1、SQL数据库关系型数据库把数据存储在表中，表模拟程序中不同的实体。例如，订单管理程序的数据库中可能有表customers、products 和orders。表的列数是固定的，行数是可变的。列定义表所表示的实体的数据属性。例如，customers表中可能有name、address、phone 等列。表中的行定义各列对应的真实数据。 表中有个特殊的列，称为主键，其值为表中各行的唯一标识符。表中还可以有称为外键的列，引用同一个表或不同表中某行的主键。行之间的这种联系称为关系，这是关系型数据库模型的基础。 图5-1 展示了一个简单数据库的关系图。这个数据库中有两个表，分别存储用户和用户角色。连接两个表的线代表两个表之间的关系。 在这个数据库关系图中，roles 表存储所有可用的用户角色，每个角色都使用一个唯一的id 值（即表的主键）进行标识。users 表包含用户列表，每个用户也有唯一的id 值。除了id 主键之外，roles 表中还有name 列，users 表中还有username 列和password 列。users表中的role_id 列是外键，引用角色的id，通过这种方式为每个用户指定角色。 从这个例子可以看出，关系型数据库存储数据很高效，而且避免了重复。将这个数据库中的用户角色重命名也很简单，因为角色名只出现在一个地方。一旦在roles 表中修改完角色名，所有通过role_id 引用这个角色的用户都能立即看到更新。 但从另一方面来看，把数据分别存放在多个表中还是很复杂的。生成一个包含角色的用户列表会遇到一个小问题，因为在此之前要分别从两个表中读取用户和用户角色，再将其联结起来。关系型数据库引擎为联结操作提供了必要的支持。 2、NoSQL数据库所有不遵循上节所述的关系模型的数据库统称为NoSQL 数据库。NoSQL 数据库一般使用集合代替表，使用文档代替记录。NoSQL 数据库采用的设计方式使联结变得困难，所以大多数数据库根本不支持这种操作。对于结构如图5-1 所示的NoSQL 数据库，若要列出各用户及其角色，就需要在程序中执行联结操作，即先读取每个用户的role_id，再在roles表中搜索对应的记录。 NoSQL 数据库更适合设计成如图5-2 所示的结构。这是执行反规范化操作得到的结果，它减少了表的数量，却增加了数据重复量。 这种结构的数据库要把角色名存储在每个用户中。如此一来，将角色重命名的操作就变得很耗时，可能需要更新大量文档。 使用NoSQL 数据库当然也有好处。数据重复可以提升查询速度。列出用户及其角色的操作很简单，因为无需联结。 3、使用SQL还是使用NoSQLSQL 数据库擅于用高效且紧凑的形式存储结构化数据。这种数据库需要花费大量精力保证数据的一致性。NoSQL 数据库放宽了对这种一致性的要求，从而获得性能上的优势。 对不同类型数据库的全面分析、对比待续。对中小型程序来说，SQL 和NoSQL数据库都是很好的选择，而且性能相当。 4、Python数据库框架大多数的数据库引擎都有对应的Python 包，包括开源包和商业包。Flask 并不限制你使用何种类型的数据库包，因此可以根据自己的喜好选择使用MySQL、Postgres、SQLite、Redis、MongoDB 或者CouchDB。 如果这些都无法满足需求，还有一些数据库抽象层代码包供选择，例如SQLAlchemy 和MongoEngine。你可以使用这些抽象包直接处理高等级的Python 对象，而不用处理如表、文档或查询语言此类的数据库实体。 选择数据库框架时，你要考虑很多因素。 易用性如果直接比较数据库引擎和数据库抽象层，显然后者取胜。抽象层，也称为对象关系映射（Object-Relational Mapper，ORM） 或对象文档映射（Object-Document Mapper，ODM），在用户不知觉的情况下把高层的面向对象操作转换成低层的数据库指令。 性能ORM 和ODM 把对象业务转换成数据库业务会有一定的损耗。大多数情况下，这种性能的降低微不足道，但也不一定都是如此。一般情况下，ORM 和ODM 对生产率的提升远远超过了这一丁点儿的性能降低，所以性能降低这个理由不足以说服用户完全放弃ORM 和ODM。真正的关键点在于如何选择一个能直接操作低层数据库的抽象层，以防特定的操作需要直接使用数据库原生指令优化。 可移植性选择数据库时，必须考虑其是否能在你的开发平台和生产平台中使用。例如，如果你打算利用云平台托管程序，就要知道这个云服务提供了哪些数据库可供选择。可移植性还针对ORM 和ODM。尽管有些框架只为一种数据库引擎提供抽象层，但其他框架可能做了更高层的抽象，它们支持不同的数据库引擎，而且都使用相同的面向对象接口。SQLAlchemy ORM 就是一个很好的例子，它支持很多关系型数据库引擎，包括流行的MySQL、Postgres 和SQLite。 FLask集成度选择框架时，你不一定非得选择已经集成了Flask 的框架，但选择这些框架可以节省你编写集成代码的时间。使用集成了Flask 的框架可以简化配置和操作，所以专门为Flask 开发的扩展是你的首选。 基于以上因素，选择使用的数据库框架是Flask-SQLAlchemy（http://pythonhosted.org/Flask-SQLAlchemy/），这个Flask 扩展包装了SQLAlchemy（http://www.sqlalchemy.org/）框架。]]></content>
      <categories>
        <category>Python</category>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 基础模块 - 表单(Flask-WTF)]]></title>
    <url>%2F2018%2F03%2F05%2FFlask%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9D%97-%E8%A1%A8%E5%8D%95(Flask-WTF)%2F</url>
    <content type="text"><![CDATA[1、引入模块Flask-WTF（http://pythonhosted.org/Flask-WTF/）扩展可以把处理Web 表单的过程变成一种愉悦的体验。这个扩展对独立的WTForms（http://wtforms.simplecodes.com）包进行了包装，方便集成到Flask 程序中。 Flask-WTF 及其依赖可使用pip 安装： (venv) $ pip install flask-wtf 2、跨站请求伪造保护默认情况下，Flask-WTF 能保护所有表单免受跨站请求伪造（Cross-Site Request Forgery，CSRF）的攻击。恶意网站把请求发送到被攻击者已登录的其他网站时就会引发CSRF 攻击。 为了实现CSRF 保护，Flask-WTF 需要程序设置一个密钥。Flask-WTF 使用这个密钥生成加密令牌，再用令牌验证请求中表单数据的真伪。设置密钥的方法如示例4-1 所示。 示例4-1 hello.py：设置Flask-WTF12app = Flask(__name__)app.config[&apos;SECRET_KEY&apos;] = &apos;hard to guess string&apos; （注：SECRET_KEY 常量一般放在配置文件中，而常量的值一般放在环境变量中，配置文件从环境变量中获取值。防止他人获取配置文件拿到SECRET_KEY值） 3、表单类使用Flask-WTF 时，每个Web 表单都由一个继承自Form 的类表示。这个类定义表单中的一组字段，每个字段都用对象表示。字段对象可附属一个或多个验证函数。验证函数用来验证用户提交的输入值是否符合要求。示例4-2 是一个简单的Web 表单，包含一个文本字段和一个提交按钮。 示例4-2 是一个简单的Web 表单，包含一个文本字段和一个提交按钮。12345678示例4-2 hello.py：定义表单类from flask_wtf import Formfrom wtforms import StringField, SubmitFieldfrom wtforms.validators import Requiredclass NameForm(Form): name = StringField(&apos;What is your name?&apos;, validators=[Required()]) submit = SubmitField(&apos;Submit&apos;) 这个表单中的字段都定义为类变量，类变量的值是相应字段类型的对象。在这个示例中，NameForm 表单中有一个名为name 的文本字段和一个名为submit 的提交按钮。StringField类表示属性为type=”text” 的&lt;input&gt; 元素。SubmitField 类表示属性为type=”submit” 的&lt;input&gt; 元素。字段构造函数的第一个参数是把表单渲染成HTML 时使用的标号。 StringField 构造函数中的可选参数validators 指定一个由验证函数组成的列表，在接受用户提交的数据之前验证数据。验证函数Required() 确保提交的字段不为空。（注：Form 基类由Flask-WTF 扩展定义，所以从flask.ext.wtf 中导入。字段和验证函数却可以直接从WTForms 包中导入。） 表4-1 WTForms支持的HTML标准字段 字段类型 说 明 StringField 文本字段 TextAreaField 多行文本字段 PasswordField 密码文本字段 HiddenField 隐藏文本字段 DateField 文本字段，值为datetime.date 格式 DateTimeField 文本字段，值为datetime.datetime 格式 IntegerField 文本字段，值为整数 DecimalField 文本字段，值为decimal.Decimal FloatField 文本字段，值为浮点数 BooleanField 复选框，值为True 和False RadioField 一组单选框 SelectField 下拉列表 SelectMultipleField 下拉列表，可选择多个值 FileField 文件上传字段 SubmitField 表单提交按钮 FormField 把表单作为字段嵌入另一个表单 FieldList 一组指定类型的字段 表4-2 WTForms 字段支持的属性 属性名称 说 明 label 字段的标签 validators 调用验证时调用的一系列验证器。 filters 由进程在输入数据上运行的一系列过滤器。 description 字段的描述，通常用于帮助文本。 id 用于该字段的id。一个合理的默认设置是由表单设置的，您不需要手动设置它。 default 如果没有提供表单或对象输入，则将默认值分配给该字段。可能是一个可调用的。 widget 如果提供，则重写用于呈现字段的小部件。 render_kw (dict) 如果提供，则提供在呈现时向小部件提供默认关键字的字典。 表4-2 WTForms验证函数 验证函数 说 明 DataRequired(message=None) 确保字段中有数据 Email(message=None) 验证电子邮件地址 EqualTo(fieldname, message=None) 比较两个字段的值；常用于要求输入两次密码进行确认的情况 InputRequired(message=None) Validates that input was provided for this field. IPAddress(ipv4=True, ipv6=False, message=None) 验证IPv4 网络地址 Length(min=-1, max=-1, message=None) 验证输入字符串的长度 MacAddress(message=None) Validates a MAC address. NumberRange(min=None, max=None, message=None) 验证输入的值在数字范围内 Optional(strip_whitespace=True) 无输入值时跳过其他验证函数 Regexp(regex, flags=0, message=None) 使用正则表达式验证输入值 URL(require_tld=True, message=None) 验证URL UUID(message=None) Validates a UUID. AnyOf(values, message=None, values_formatter=None) 确保输入值在可选值列表中 NoneOf(values, message=None, values_formatter=None) 确保输入值不在可选值列表中 4、把表单渲染成HTML（1）手工方式渲染表单字段是可调用的，在模板中调用后会渲染成HTML。假设视图函数把一个NameForm 实例通过参数form 传入模板，在模板中可以生成一个简单的表单，如下所示：12345&lt;form method=&quot;POST&quot;&gt; &#123;&#123; form.hidden_tag() &#125;&#125; &#123;&#123; form.name.label &#125;&#125; &#123;&#123; form.name() &#125;&#125; &#123;&#123; form.submit() &#125;&#125;&lt;/form&gt; 当然，这个表单还很简陋。要想改进表单的外观，可以把参数传入渲染字段的函数，传入的参数会被转换成字段的HTML 属性。例如，可以为字段指定id 或class 属性，然后定义CSS 样式：12345&lt;form method=&quot;POST&quot;&gt; &#123;&#123; form.hidden_tag() &#125;&#125; &#123;&#123; form.name.label &#125;&#125; &#123;&#123; form.name(id=&apos;my-text-field&apos;) &#125;&#125; &#123;&#123; form.submit() &#125;&#125;&lt;/form&gt; 即便能指定HTML 属性，但按照这种方式渲染表单的工作量还是很大，所以在条件允许的情况下最好能使用Bootstrap 中的表单样式。 （2）使用Bootstrap表单样式渲染Flask-Bootstrap 提供了一个非常高端的辅助函数，可以使用Bootstrap 中预先定义好的表单样式渲染整个Flask-WTF 表单，而这些操作只需一次调用即可完成。使用Flask-Bootstrap，上述表单可使用下面的方式渲染：12&#123;% import &quot;bootstrap/wtf.html&quot; as wtf %&#125;&#123;&#123; wtf.quick_form(form) &#125;&#125; import 指令的使用方法和普通Python 代码一样，允许导入模板中的元素并用在多个模板中。导入的bootstrap/wtf.html 文件中定义了一个使用Bootstrap 渲染Falsk-WTF 表单对象的辅助函数。wtf.quick_form() 函数的参数为Flask-WTF 表单对象，使用Bootstrap 的默认样式渲染传入的表单。 5、在视图函数中处理表单123456789示例4-4 hello.py：路由方法@app.route(&apos;/&apos;, methods=[&apos;GET&apos;, &apos;POST&apos;])def index(): name = None form = NameForm() if form.validate_on_submit(): name = form.name.data form.name.data = &apos;&apos; return render_template(&apos;index.html&apos;, form=form, name=name) app.route 修饰器中添加的methods 参数告诉Flask 在URL 映射中把这个视图函数注册为GET 和POST 请求的处理程序。如果没指定methods 参数，就只把视图函数注册为GET 请求的处理程序。 把POST 加入方法列表很有必要，因为将提交表单作为POST 请求进行处理更加便利。表单也可作为GET 请求提交，不过GET 请求没有主体，提交的数据以查询字符串的形式附加到URL 中，可在浏览器的地址栏中看到。基于这个以及其他多个原因，提交表单大都作为POST 请求进行处理。 局部变量name 用来存放表单中输入的有效名字，如果没有输入，其值为None。如上述代码所示，在视图函数中创建一个NameForm 类实例用于表示表单。提交表单后，如果数据能被所有验证函数接受，那么validate_on_submit() 方法的返回值为True，否则返回False。这个函数的返回值决定是重新渲染表单还是处理表单提交的数据。 用户第一次访问程序时，服务器会收到一个没有表单数据的GET 请求，所以validate_on_submit() 将返回False。if 语句的内容将被跳过，通过渲染模板处理请求，并传入表单对象和值为None 的name 变量作为参数。用户会看到浏览器中显示了一个表单。 用户提交表单后，服务器收到一个包含数据的POST 请求。validate_on_submit() 会调用name 字段上附属的Required() 验证函数。如果名字不为空，就能通过验证，validate_on_submit() 返回True。现在，用户输入的名字可通过字段的data 属性获取。在if 语句中，把名字赋值给局部变量name，然后再把data 属性设为空字符串，从而清空表单字段。最后一行调用render_template() 函数渲染模板，但这一次参数name 的值为表单中输入的名字，因此会显示一个针对该用户的欢迎消息。]]></content>
      <categories>
        <category>Python</category>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 基础模块 - 模板续(集成 Twitter Bootstrap)]]></title>
    <url>%2F2018%2F03%2F05%2FFlask%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9D%97-%E6%A8%A1%E6%9D%BF%E7%BB%AD(%E9%9B%86%E6%88%90Twitter-Bootstrap)%2F</url>
    <content type="text"><![CDATA[1、集成 BootstrapBootstrap（http://getbootstrap.com/）是Twitter 开发的一个开源框架，它提供的用户界面组件可用于创建整洁且具有吸引力的网页，而且这些网页还能兼容所有现代Web 浏览器。 Bootstrap 是客户端框架，因此不会直接涉及服务器。服务器需要做的只是提供引用了Bootstrap 层叠样式表（CSS） 和JavaScript 文件的HTML 响应， 并在HTML、CSS 和JavaScript 代码中实例化所需组件。这些操作最理想的执行场所就是模板。 Flask-Bootstrap 使用pip安装：(venv) $ pip install flask-bootstrapFlask 扩展一般都在创建程序实例时初始化。示例3-4 是Flask-Bootstrap 的初始化方法。示例3-4 hello.py：初始化Flask-Bootstrap123from flask_bootstrap import Bootstrap# ...bootstrap = Bootstrap(app) 初始化Flask-Bootstrap 之后，就可以在程序中使用一个包含所有Bootstrap 文件的基模板。这个模板利用Jinja2 的模板继承机制，让程序扩展一个具有基本页面结构的基模板，其中就有用来引入Bootstrap 的元素。 Jinja2 的模板继承机制可以帮助我们解决这一问题。Flask-Bootstrap 提供了一个具有页面基本布局的基模板，同样，程序可以定义一个具有更完整页面布局的基模板，其中包含导航条，而页面内容则可留到衍生模板中定义。示例3-7 展示了templates/base.html 的内容，这是一个继承自bootstrap/base.html 的新模板，其中定义了导航条。这个模板本身也可作为其他模板的基模板，例如templates/user.html、templates/404.html 和templates/500.html。 示例3-7 templates/base.html：包含导航条的程序基模板123456789101112131415161718192021222324252627282930&#123;% extends &quot;bootstrap/base.html&quot; %&#125;&#123;% block title %&#125;Flasky&#123;% endblock %&#125;&#123;% block navbar %&#125;&lt;div class=&quot;navbar navbar-inverse&quot; role=&quot;navigation&quot;&gt; &lt;div class=&quot;container&quot;&gt; &lt;div class=&quot;navbar-header&quot;&gt; &lt;button type=&quot;button&quot; class=&quot;navbar-toggle&quot; data-toggle=&quot;collapse&quot; data-target=&quot;.navbar-collapse&quot;&gt; &lt;span class=&quot;sr-only&quot;&gt;Toggle navigation&lt;/span&gt; &lt;span class=&quot;icon-bar&quot;&gt;&lt;/span&gt; &lt;span class=&quot;icon-bar&quot;&gt;&lt;/span&gt; &lt;span class=&quot;icon-bar&quot;&gt;&lt;/span&gt; &lt;/button&gt; &lt;a class=&quot;navbar-brand&quot; href=&quot;/&quot;&gt;Flasky&lt;/a&gt; &lt;/div&gt; &lt;div class=&quot;navbar-collapse collapse&quot;&gt; &lt;ul class=&quot;nav navbar-nav&quot;&gt; &lt;li&gt;&lt;a href=&quot;/&quot;&gt;Home&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&#123;% endblock %&#125;&#123;% block content %&#125;&lt;div class=&quot;container&quot;&gt; &#123;% block page_content %&#125;&#123;% endblock %&#125;&lt;/div&gt;&#123;% endblock %&#125; 这个模板的content 块中只有一个&lt;div&gt; 容器，其中包含了一个名为page_content 的新的空块，块中的内容由衍生模板定义。 Jinja2 中的extends 指令从Flask-Bootstrap 中导入bootstrap/base.html， 从而实现模板继承。Flask-Bootstrap 中的基模板提供了一个网页框架，引入了Bootstrap 中的所有CSS 和JavaScript 文件。 基模板中定义了可在衍生模板中重定义的块。block 和endblock 指令定义的块中的内容可添加到基模板中。 上面这个base.html 模板定义了3 个块，分别名为title、navbar 和content。这些块都是基模板提供的，可在衍生模板中重新定义。title 块的作用很明显，其中的内容会出现在渲染后的HTML 文档头部，放在&lt;title&gt; 标签中。navbar 和content 这两个块分别表示页面中的导航条和主体内容。 在这个模板中，navbar 块使用Bootstrap 组件定义了一个简单的导航条。content 块中有个&lt;div&gt; 容器，其中包含一个页面头部。之前版本的模板中的欢迎信息，现在就放在这个页面头部。 Flask-Bootstrap 的base.html 模板还定义了很多其他块，都可在衍生模板中使用。表3-2 列出了所有可用的快。表3-2 Flask-Bootstrap基模板中定义的块 块 名 说 明 doc 整个HTML 文档 html_attribs 标签的属性 html 标签中的内容 head 标签中的内容 title 标签中的内容 metas 一组 标签 styles 层叠样式表定义 body_attribs 标签的属性 body 标签中的内容 navbar 用户定义的导航条 content 用户定义的页面内容 scripts 文档底部的JavaScript 声明 表3-2 中的很多块都是Flask-Bootstrap 自用的，如果直接重定义可能会导致一些问题。例如，Bootstrap 所需的文件在styles 和scripts 块中声明。如果程序需要向已经有内容的块中添加新内容，必须使用Jinja2 提供的super() 函数。例如，如果要在衍生模板中添加新的JavaScript 文件，需要这么定义scripts 块：1234&#123;% block scripts %&#125; &#123;&#123; super() &#125;&#125; &lt;script type=&quot;text/javascript&quot; src=&quot;my-script.js&quot;&gt;&lt;/script&gt;&#123;% endblock %&#125; 2、自定义错误页面像常规路由一样，Flask 允许程序使用基于模板的自定义错误页面。最常见的错误代码有两个：404，客户端请求未知页面或路由时显示；500，有未处理的异常时显示。为这两个错误代码指定自定义处理程序的方式如示例3-6 所示。 示例3-6 hello.py：自定义错误页面123456@app.errorhandler(404)def page_not_found(e): return render_template(&apos;404.html&apos;), 404@app.errorhandler(500)def internal_server_error(e): return render_template(&apos;500.html&apos;), 500 和视图函数一样，错误处理程序也会返回响应。它们还返回与该错误对应的数字状态码。 错误处理程序中引用的模板也需要编写。这些模板应该和常规页面使用相同的布局，因此要有一个导航条和显示错误消息的页面头部。 现在，程序使用的模板继承自 templates/base.html 模板，而不直接继承自Flask-Bootstrap 的基模板。通过继承templates/base.html 模板编写自定义的404 错误页面很简单，如示例3-8 所示。 示例3-8 templates/404.html：使用模板继承机制自定义404 错误页面1234567&#123;% extends &quot;base.html&quot; %&#125;&#123;% block title %&#125;Flasky - Page Not Found&#123;% endblock %&#125;&#123;% block page_content %&#125; &lt;div class=&quot;page-header&quot;&gt; &lt;h1&gt;Not Found&lt;/h1&gt; &lt;/div&gt;&#123;% endblock %&#125; templates/user.html 现在可以通过继承这个基模板来简化内容，如示例3-9 所示。示例3-9 templates/user.html：使用模板继承机制简化页面模板1234567&#123;% extends &quot;base.html&quot; %&#125;&#123;% block title %&#125;Flasky&#123;% endblock %&#125;&#123;% block page_content %&#125; &lt;div class=&quot;page-header&quot;&gt; &lt;h1&gt;Hello, &#123;&#123; name &#125;&#125;!&lt;/h1&gt; &lt;/div&gt;&#123;% endblock %&#125; 3、链接在模板中直接编写简单路由的URL 链接不难，但对于包含可变部分的动态路由，在模板中构建正确的URL 就很困难。而且，直接编写URL 会对代码中定义的路由产生不必要的依赖关系。如果重新定义路由，模板中的链接可能会失效。 为了避免这些问题，Flask 提供了url_for() 辅助函数，它可以使用程序URL 映射中保存的信息生成URL。 url_for() 函数最简单的用法是以视图函数名（或者app.add_url_route() 定义路由时使用的端点名）作为参数，返回对应的URL。例如，在当前版本的hello.py 程序中调用url_for(‘index’) 得到的结果是/。调用url_for(‘index’, _external=True) 返回的则是绝对地址，在这个示例中是http://localhost:5000/。 使用url_for() 生成动态地址时， 将动态部分作为关键字参数传入。例如，url_for(‘user’, name=’john’, _external=True) 的返回结果是http://localhost:5000/user/john。传入url_for() 的关键字参数不仅限于动态路由中的参数。函数能将任何额外参数添加到查询字符串中。例如，url_for(‘index’, page=2) 的返回结果是/?page=2。 4、静态文件Web 程序不是仅由Python 代码和模板组成。大多数程序还会使用静态文件，例如HTML代码中引用的图片、JavaScript 源码文件和CSS。 默认设置下，Flask 在程序根目录中名为static 的子目录中寻找静态文件。如果需要，可在static 文件夹中使用子文件夹存放文件。服务器收到前面那个URL 后，会生成一个响应，包含文件系统中static/css/styles.css 文件的内容。 示例3-10 展示了如何在程序的基模板中放置favicon.ico 图标。这个图标会显示在浏览器的地址栏中。示例3-10 templates/base.html：定义收藏夹图标1234567&#123;% block head %&#125;&#123;&#123; super() &#125;&#125;&lt;link rel=&quot;shortcut icon&quot; href=&quot;&#123;&#123; url_for(&apos;static&apos;, filename = &apos;favicon.ico&apos;) &#125;&#125;&quot; type=&quot;image/x-icon&quot;&gt;&lt;link rel=&quot;icon&quot; href=&quot;&#123;&#123; url_for(&apos;static&apos;, filename = &apos;favicon.ico&apos;) &#125;&#125;&quot; type=&quot;image/x-icon&quot;&gt;&#123;% endblock %&#125; 图标的声明会插入head 块的末尾。注意如何使用super() 保留基模板中定义的块的原始内容。 5、使用Flask-Moment 本地化日期和时间服务器需要统一时间单位，这和用户所在的地理位置无关，所以一般使用协调世界时（Coordinated Universal Time，UTC）。不过用户看到UTC 格式的时间会感到困惑，他们更希望看到当地时间，而且采用当地惯用的格式。 要想在服务器上只使用UTC 时间，一个优雅的解决方案是，把时间单位发送给Web 浏览器，转换成当地时间，然后渲染。Web 浏览器可以更好地完成这一任务，因为它能获取用户电脑中的时区和区域设置。 有一个使用JavaScript 开发的优秀客户端开源代码库，名为moment.js（http://momentjs.com/），它可以在浏览器中渲染日期和时间。Flask-Moment 是一个Flask 程序扩展，能把moment.js 集成到Jinja2 模板中。Flask-Moment 可以使用pip 安装：(venv) $ pip install flask-moment 这个扩展的初始化方法如示例3-11 所示。示例3-11 hello.py：初始化Flask-Moment12from flask.ext.moment import Momentmoment = Moment(app) 除了moment.js，Flask-Moment 还依赖jquery.js。要在HTML 文档的某个地方引入这两个库，可以直接引入，这样可以选择使用哪个版本，也可使用扩展提供的辅助函数，从内容分发网络（Content Delivery Network，CDN）中引入通过测试的版本。Bootstrap 已经引入了jquery.js，因此只需引入moment.js 即可。示例3-12 展示了如何在基模板的scripts 块中引入这个库。示例3-12 templates/base.html：引入moment.js 库1234&#123;% block scripts %&#125;&#123;&#123; super() &#125;&#125;&#123;&#123; moment.include_moment() &#125;&#125;&#123;% endblock %&#125; 为了处理时间戳，Flask-Moment 向模板开放了moment 类。示例3-13 中的代码把变量current_time 传入模板进行渲染。 示例3-13 hello.py：加入一个datetime 变量1234from datetime import datetime@app.route(&apos;/&apos;)def index(): return render_template(&apos;index.html&apos;, current_time=datetime.utcnow()) 示例3-14 展示了如何在模板中渲染current_time。代码3-14 templates/index.html：使用Flask-Moment 渲染时间戳12&lt;p&gt;The local date and time is &#123;&#123; moment(current_time).format(&apos;LLL&apos;) &#125;&#125;.&lt;/p&gt;&lt;p&gt;That was &#123;&#123; moment(current_time).fromNow(refresh=True) &#125;&#125;&lt;/p&gt; format(‘LLL’) 根据客户端电脑中的时区和区域设置渲染日期和时间。参数决定了渲染的方式，’L’ 到’LLLL’ 分别对应不同的复杂度。format() 函数还可接受自定义的格式说明符。 第二行中的fromNow() 渲染相对时间戳，而且会随着时间的推移自动刷新显示的时间。这个时间戳最开始显示为“a few seconds ago”，但指定refresh 参数后，其内容会随着时间的推移而更新。如果一直待在这个页面，几分钟后，会看到显示的文本变成“a minuteago”“2 minutes ago”等。 Flask-Moment 实现了moment.js 中的format()、fromNow()、fromTime()、calendar()、valueOf()和unix() 方法。你可查阅文档（http://momentjs.com/docs/#/displaying/）学习moment.js 提供的全部格式化选项。 Flask-Moment 渲染的时间戳可实现多种语言的本地化。语言可在模板中选择，把语言代码传给lang() 函数即可：\{\{ moment.lang(&#39;es&#39;) \}\}]]></content>
      <categories>
        <category>Python</category>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 基础模块 - 模板(Jinjia2)]]></title>
    <url>%2F2018%2F03%2F05%2FFlask%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9D%97-%E6%A8%A1%E6%9D%BF(Jinjia2)%2F</url>
    <content type="text"><![CDATA[1、渲染模板默认情况下，Flask 在程序文件夹中的templates 子文件夹中寻找模板。在下一个hello.py版本中，要把前面定义的模板保存在templates 文件夹中，并分别命名为index.html 和user.html。 示例3-3 hello.py：渲染模板123456789101112from flask import Flask, render_templateapp = Flask(__name__)@app.route(&apos;/&apos;)def index(): return render_template(&apos;index.html&apos;)@app.route(&apos;/user/&lt;name&gt;&apos;)def user(name): return render_template(&apos;user.html&apos;, name=name)if __name__ == &apos;__main__&apos;: app.run() Flask 提供的render_template 函数把Jinja2 模板引擎集成到了程序中。render_template 函数的第一个参数是模板的文件名。随后的参数都是键值对，表示模板中变量对应的真实值。在这段代码中，第二个模板收到一个名为name 的变量。 2、变量在模板中使用的 结构表示一个变量，它是一种特殊的占位符，告诉模板引擎这个位置的值从渲染模板时使用的数据中获取。 Jinja2 能识别所有类型的变量，甚至是一些复杂的类型，例如列表、字典和对象。在模板中使用变量的一些示例如下：1234&lt;p&gt;A value from a dictionary: &#123;&#123; mydict[&apos;key&apos;] &#125;&#125;.&lt;/p&gt;&lt;p&gt;A value from a list: &#123;&#123; mylist[3] &#125;&#125;.&lt;/p&gt;&lt;p&gt;A value from a list, with a variable index: &#123;&#123; mylist[myintvar] &#125;&#125;.&lt;/p&gt;&lt;p&gt;A value from an object&apos;s method: &#123;&#123; myobj.somemethod() &#125;&#125;.&lt;/p&gt; 可以使用过滤器修改变量，过滤器名添加在变量名之后，中间使用竖线分隔。例如，下述模板以首字母大写形式显示变量name 的值： Hello, \{\{ name|capitalize \}\} 表3-1 列出了Jinja2 提供的部分常用过滤器。 过滤器名 说 明 safe 渲染值时不转义 capitalize 把值的首字母转换成大写，其他字母转换成小写 lower 把值转换成小写形式 upper 把值转换成大写形式 title 把值中每个单词的首字母都转换成大写 trim 把值的首尾空格去掉 striptags 渲染之前把值中所有的HTML 标签都删掉 safe 过滤器值得特别说明一下。默认情况下，出于安全考虑，Jinja2 会转义所有变量。例如，如果一个变量的值为&lt;h1&gt;Hello&lt;/h1&gt;，Jinja2 会将其渲染成&amp;lt;h1&amp;gt;Hello&amp;lt;/h1&amp;gt;，浏览器能显示这个h1 元素，但不会进行解释。很多情况下需要显示变量中存储的HTML 代码，这时就可使用safe 过滤器。 完整的过滤器列表可在Jinja2 文档（http://jinja.pocoo.org/docs/templates/#builtin-filters）中查看。 3、控制结构Jinja2 提供了多种控制结构，可用来改变模板的渲染流程。下面这个例子展示了如何在模板中使用条件控制语句：12345&#123;% if user %&#125; Hello, &#123;&#123; user &#125;&#125;!&#123;% else %&#125; Hello, Stranger!&#123;% endif %&#125; 另一种常见需求是在模板中渲染一组元素。下例展示了如何使用for 循环实现这一需求：12345&lt;ul&gt; &#123;% for comment in comments %&#125; &lt;li&gt;&#123;&#123; comment &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125;&lt;/ul&gt; Jinja2 还支持宏。宏类似于Python 代码中的函数。例如：12345678&#123;% macro render_comment(comment) %&#125; &lt;li&gt;&#123;&#123; comment &#125;&#125;&lt;/li&gt;&#123;% endmacro %&#125;&lt;ul&gt; &#123;% for comment in comments %&#125; &#123;&#123; render_comment(comment) &#125;&#125; &#123;% endfor %&#125;&lt;/ul&gt; 为了重复使用宏，我们可以将其保存在单独的文件中，然后在需要使用的模板中导入：123456&#123;% import &apos;macros.html&apos; as macros %&#125;&lt;ul&gt; &#123;% for comment in comments %&#125; &#123;&#123; macros.render_comment(comment) &#125;&#125; &#123;% endfor %&#125;&lt;/ul&gt; 需要在多处重复使用的模板代码片段可以写入单独的文件，再包含在所有模板中，以避免重复：include &quot;comment.html&quot; 另一种重复使用代码的强大方式是模板继承，它类似于Python 代码中的类继承。首先，创建一个名为base.html 的基模板：1234567891011&lt;html&gt;&lt;head&gt; &#123;% block head %&#125; &lt;title&gt;&#123;% block title %&#125;&#123;% endblock %&#125; - My Application&lt;/title&gt; &#123;% endblock %&#125;&lt;/head&gt;&lt;body&gt; &#123;% block body %&#125; &#123;% endblock %&#125;&lt;/body&gt;&lt;/html&gt; block 标签定义的元素可在衍生模板中修改。在本例中，我们定义了名为head、title 和body 的块。注意，title 包含在head 中。下面这个示例是基模板的衍生模板：1234567891011&#123;% extends &quot;base.html&quot; %&#125;&#123;% block title %&#125;Index&#123;% endblock %&#125;&#123;% block head %&#125; &#123;&#123; super() &#125;&#125; &lt;style&gt;&lt;/style&gt;&#123;% endblock %&#125;&#123;% block body %&#125; &lt;h1&gt;Hello, World!&lt;/h1&gt;&#123;% endblock %&#125; extends 指令声明这个模板衍生自base.html。在extends 指令之后，基模板中的3 个块被重新定义，模板引擎会将其插入适当的位置。注意新定义的head 块，在基模板中其内容不是空的，所以使用super() 获取原来的内容。]]></content>
      <categories>
        <category>Python</category>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 整体概述-一个完整的程序]]></title>
    <url>%2F2018%2F03%2F02%2FFlask%E6%95%B4%E4%BD%93%E6%A6%82%E8%BF%B0-%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[一、Flask 简介Flask(http://flask.pocoo.org/)是一种小型框架，小到可以称为“微框架”。但是，小并不意味着它比其他框架的功能少。Flask 自开发伊始就被设计为可扩展的框架，它具有一个包含基本服务的强健核心，其他功能则可通过扩展实现。你可以自己挑选所需的扩展包，组成一个没有附加功能的精益组合，从而完全精确满足自身需求。 Flask 有两个主要依赖：路由、调试和Web 服务器网关接口（Web Server Gateway Interface，WSGI）子系统由Werkzeug（http://werkzeug.pocoo.org/）提供；模板系统由Jinja2（http://jinja.pocoo.org/）提供。Werkzeug 和Jinjia2 都是由Flask 的核心开发者开发而成。 Flask 并不原生支持数据库访问、Web 表单验证和用户认证等高级功能。这些功能以及其他大多数Web 程序中需要的核心服务都以扩展的形式实现，然后再与核心包集成。开发者可以任意挑选符合项目需求的扩展，甚至可以自行开发。这和大型框架的做法相反，大型框架往往已经替你做出了大多数决定，难以（有时甚至不允许）使用替代方案。 Flask与django、tornado的区别： table th:first-of-type { width: 10%; } table th:nth-of-type(2) { width: 30%; } 框架 图标 特点 flask Flask扩展丰富，冗余度小，可自由选择组合各种插件，性能优越，相比其它web框架十分轻量级，其优雅的设计哲学易于学习掌握，小型项目快速开发，大项目毫无压力。Flask灵活开发，Python高手基本都会喜欢Flask django Django是重量级全栈型web框架，虽然功能强大，但冗余度高，自带ORM和模板引擎，灵活和自由度不够高，开发小型项目时显得过于臃肿与庞大。 tornado Tornado是一个强大的、支持协程、高效并发且可扩展的Web服务器，发布于2009年9月，应用于FriendFeed、Facebook等社交网站。Tornado的强项在于可以利用它的异步协程机制开发高并发的服务器系统。 二、安装1、使用虚拟环境安装Flask 最便捷的方式是使用虚拟环境。虚拟环境是Python解释器的一个私有副本，在这个环境中你可以安装私有包，而且不会影响系统中安装的全局Python 解释器。 虚拟环境非常有用，可以在系统的Python 解释器中避免包的混乱和版本的冲突。为每个程序单独创建虚拟环境可以保证程序只能访问虚拟环境中的包，从而保持全局解释器的干净整洁，使其只作为创建（更多）虚拟环境的源。 虚拟环境使用第三方实用工具virtualenv 创建。输入以下命令可以检查系统是否安装了virtualenv： $ virtualenv --version如果结果显示错误，你就需要安装这个工具。 （1）安装virtualenv① 大多数Linux 发行版都提供了virtualenv 包。例如，Ubuntu 用户可以使用下述命令安装它： $ sudo apt-get install python-virtualenv② 如果你的电脑是Mac OS X 系统，就可以使用easy_install 安装virtualenv： $ sudo easy_install virtualenv③ 如果你使用微软的Windows 系统或其他没有官方virtualenv 包的操作系统，那么安装过程要稍微复杂一点。在浏览器中输入网址https://bitbucket.org/pypa/setuptools，回车后会进入setuptools 安装程序的主页。在这个页面中找到下载安装脚本的链接，脚本名为ez_setup.py。把这个文件保存到电脑的一个临时文件夹中，然后在这个文件夹中执行以下命令：12$ python ez_setup.py$ easy_install virtualenv （2）创建虚拟环境下一步是使用virtualenv 命令在flasky 文件夹中创建Python 虚拟环境。这个命令只有一个必需的参数，即虚拟环境的名字。创建虚拟环境后，当前文件夹中会出现一个子文件夹，名字就是上述命令中指定的参数，与虚拟环境相关的文件都保存在这个子文件夹中。按照惯例，一般虚拟环境会被命名为venv：12345$ virtualenv venvNew python executable in venv/bin/python2.7Also creating executable in venv/bin/pythonInstalling setuptools............done.Installing pip...............done. （3）激活虚拟环境现在，flasky 文件夹中就有了一个名为venv 的子文件夹，它保存一个全新的虚拟环境，其中有一个私有的Python 解释器。在使用这个虚拟环境之前，你需要先将其“激活”。如果你使用bash 命令行（Linux 和Mac OS X 用户），可以通过下面的命令激活这个虚拟环境： $ source venv/bin/activate如果使用微软Windows 系统，激活命令是： $ venv\Scripts\activate虚拟环境被激活后，其中Python 解释器的路径就被添加进PATH 中，但这种改变不是永久性的，它只会影响当前的命令行会话。为了提醒你已经激活了虚拟环境，激活虚拟环境的命令会修改命令行提示符，加入环境名： (venv) $当虚拟环境中的工作完成后，如果你想回到全局Python 解释器中，可以在命令行提示符下输入deactivate。 2、使用pip安装Python包（Flask）大多数Python 包都使用pip 实用工具安装，使用virtualenv 创建虚拟环境时会自动安装pip。激活虚拟环境后，pip 所在的路径会被添加进PATH。执行下述命令可在虚拟环境中安装Flask： (venv) $ pip install flask执行上述命令，你就在虚拟环境中安装Flask 及其依赖了。要想验证Flask 是否正确安装，你可以启动Python 解释器，尝试导入Flask：123(venv) $ python&gt;&gt;&gt; import flask&gt;&gt;&gt; 如果没有看到错误提醒，那恭喜你——你已经可以安装完成了。 三、程序的基本结构首先，来梳理一下Flask的工作流程。 1、初始化所有Flask 程序都必须创建一个程序实例。Web 服务器使用一种名为Web 服务器网关接口（Web Server Gateway Interface，WSGI）的协议，把接收自客户端的所有请求都转交给这个对象处理。程序实例是Flask 类的对象，经常使用下述代码创建：12from flask import Flaskapp = Flask(__name__) Flask 类的构造函数只有一个必须指定的参数，即程序主模块或包的名字。在大多数程序中，Python 的name 变量就是所需的值。 2、路由和视图函数客户端（例如Web 浏览器）把请求发送给Web 服务器，Web 服务器再把请求发送给Flask程序实例。程序实例需要知道对每个URL 请求运行哪些代码，所以保存了一个URL 到Python 函数的映射关系。处理URL和函数之间关系的程序称为路由。 在Flask 程序中定义路由的最简便方式，是使用程序实例提供的app.route 修饰器，把修饰的函数注册为路由。下面的例子说明了如何使用这个修饰器声明路由：123@app.route(&apos;/&apos;)def index(): return &apos;&lt;h1&gt;Hello World!&lt;/h1&gt;&apos; （注：修饰器是Python 语言的标准特性，可以使用不同的方式修改函数的行为。惯常用法是使用修饰器把函数注册为事件的处理程序。） 前例把index() 函数注册为程序根地址的处理程序，即将跟地址映射到index() 函数处理程序。在浏览器中访问http://www.example.com 后，会触发服务器执行index() 函数。这个函数的返回值称为响应，是客户端接收到的内容。如果客户端是Web 浏览器，响应就是显示给用户查看的文档。 像index() 这样的函数称为视图函数（view function）。视图函数返回的响应可以是包含HTML 的简单字符串，也可以是复杂的表单，当然也可以是接口所需的数据格式（JSON或XML）。 此处再扩展一下，route 修饰器也支持动态URL，下例定义的路由中就有一部分是动态名字：123@app.route(&apos;/user/&lt;name&gt;&apos;)def user(name): return &apos;&lt;h1&gt;Hello, %s!&lt;/h1&gt;&apos; % name 尖括号中的内容就是动态部分，任何能匹配静态部分的URL 都会映射到这个路由上。调用视图函数时，Flask 会将动态部分作为参数传入函数。在这个视图函数中，参数用于生成针对个人的欢迎消息。 路由中的动态部分默认使用字符串，不过也可使用类型定义。例如，路由/user/int:id只会匹配动态片段id 为整数的URL。Flask 支持在路由中使用int、float 和path 类型。path 类型也是字符串，但不把斜线视作分隔符，而将其当作动态片段的一部分。 3、启动服务器程序实例用run 方法启动Flask 集成的开发Web 服务器：12if __name__ == &apos;__main__&apos;:app.run(debug=True) __name__==’__main__‘ 是Python 的惯常用法，在这里确保直接执行这个脚本时才启动开发Web 服务器。如果这个脚本由其他脚本引入，程序假定父级脚本会启动不同的服务器，因此不会执行app.run()。 服务器启动后，会进入轮询，等待并处理请求。轮询会一直运行，直到程序停止，比如按Ctrl-C 键。 有一些选项参数可被app.run() 函数接受用于设置Web 服务器的操作模式。在开发过程中启用调试模式会带来一些便利，比如说激活调试器和重载程序。要想启用调试模式，我们可以把debug 参数设为True。 4、一个完整的程序1234567891011121314# hello.pyfrom flask import Flaskapp = Flask(__name__)@app.route(&apos;/&apos;)def index(): return &apos;&lt;h1&gt;Hello World!&lt;/h1&gt;&apos;@app.route(&apos;user/&lt;name&gt;&apos;)def user(name): return &apos;&lt;h1&gt;Hello, %s&lt;/h1&gt;&apos; % self.nameif __name__ == &apos;__main__&apos;: app.run() 然后使用下述命令启动程序：123(venv) $ python hello.py* Running on http://127.0.0.1:5000/* Restarting with reloader]]></content>
      <categories>
        <category>Python</category>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 模块之cPickle]]></title>
    <url>%2F2018%2F02%2F05%2FPython%E6%A8%A1%E5%9D%97%E4%B9%8BcPickle%2F</url>
    <content type="text"><![CDATA[在python中，一般可以使用pickle类来进行python对象的序列化，而cPickle提供了一个更快速简单的接口，如python文档所说的：“cPickle – A faster pickle”。使用cPickle模块永久地把这些对象储存在你的硬盘上。 对象持久化：如果希望透明地存储 Python 对象，而不丢失其身份和类型等信息，则需要某种形式的对象序列化：它是一个将任意复杂的对象转成对象的文本或二进制表示的过程。同样，必须能够将对象经过序列化后的形式恢复到原有的对象。在 Python 中，这种序列化过程称为 pickle，可以将对象 pickle 成字符串、磁盘上的文件或者任何类似于文件的对象，也可以将这些字符串、文件或任何类似于文件的对象 unpickle 成原来的对象。 cPickle可以对任意一种类型的python对象进行序列化操作，比如list，dict，甚至是一个类的对象等。 pickle 模块提供了以下函数对： dumps(object) 返回一个字符串，它包含一个 pickle 格式的对象； loads(string) 返回包含在 pickle 字符串中的对象； dump(object, file) 将对象写到文件，这个文件可以是实际的物理文件，但也可以是任何类似于文件的对象，这个对象具有 write() 方法，可以接受单个的字符串参数； load(file) 返回包含在 pickle 文件中的对象。 dump() 和 load()12345678910111213141516171819&gt;&gt;&gt; a1 = &apos;apple&apos;&gt;&gt;&gt; b1 = &#123;1: &apos;One&apos;, 2: &apos;Two&apos;, 3: &apos;Three&apos;&#125;&gt;&gt;&gt; c1 = [&apos;fee&apos;, &apos;fie&apos;, &apos;foe&apos;, &apos;fum&apos;]&gt;&gt;&gt; f1 = file(&apos;temp.pkl&apos;, &apos;wb&apos;)&gt;&gt;&gt; pickle.dump(a1, f1, True)&gt;&gt;&gt; pickle.dump(b1, f1, True)&gt;&gt;&gt; pickle.dump(c1, f1, True)&gt;&gt;&gt; f1.close()&gt;&gt;&gt; f2 = file(&apos;temp.pkl&apos;, &apos;rb&apos;)&gt;&gt;&gt; a2 = pickle.load(f2)&gt;&gt;&gt; a2&apos;apple&apos;&gt;&gt;&gt; b2 = pickle.load(f2)&gt;&gt;&gt; b2&#123;1: &apos;One&apos;, 2: &apos;Two&apos;, 3: &apos;Three&apos;&#125;&gt;&gt;&gt; c2 = pickle.load(f2)&gt;&gt;&gt; c2[&apos;fee&apos;, &apos;fie&apos;, &apos;foe&apos;, &apos;fum&apos;]&gt;&gt;&gt; f2.close() dump： 将python对象序列化保存到本地的文件dump函数需要指定两个参数，第一个是需要序列化的python对象名称，第二个是本地的文件，需要注意的是，在这里需要使用open函数打开一个文件，并指定“写”操作load：载入本地文件，恢复python对象同dump一样，这里需要使用open函数打开本地的一个文件，并指定“读”操作 dumps() 和 loads()12345678910111213141516171819202122232425&gt;&gt;&gt; import cPickle as pickle&gt;&gt;&gt; t1 = (&apos;this is a string&apos;, 42, [1, 2, 3], None)&gt;&gt;&gt; t1(&apos;this is a string&apos;, 42, [1, 2, 3], None)&gt;&gt;&gt; p1 = pickle.dumps(t1)&gt;&gt;&gt; p1&quot;(S&apos;this is a string&apos;/nI42/n(lp1/nI1/naI2/naI3/naNtp2/n.&quot;&gt;&gt;&gt; print p1(S&apos;this is a string&apos;I42(lp1I1aI2aI3aNtp2.&gt;&gt;&gt; t2 = pickle.loads(p1)&gt;&gt;&gt; t2(&apos;this is a string&apos;, 42, [1, 2, 3], None)&gt;&gt;&gt; p2 = pickle.dumps(t1, True)&gt;&gt;&gt; p2&apos;(U/x10this is a stringK*]q/x01(K/x01K/x02K/x03eNtq/x02.&apos;&gt;&gt;&gt; t3 = pickle.loads(p2)&gt;&gt;&gt; t3(&apos;this is a string&apos;, 42, [1, 2, 3], None) dumps：将python对象序列化保存到一个字符串变量中loads：从字符串变量中载入python对象]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 基础小程序实践]]></title>
    <url>%2F2018%2F02%2F05%2FPython%E5%9F%BA%E7%A1%80%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[要求： 创建你自己的命令行 地址簿 程序。在这个程序中，你可以添加、修改、删除和搜索你的联系人（朋友、家人和同事等等）以及它们的信息（诸如电子邮件地址和/或电话号码）。这些详细信息应该被保存下来以便以后提取。 提示： 创建一个类来表示一个人的信息。使用字典储存每个人的对象，把他们的名字作为键。使用cPickle模块永久地把这些对象储存在你的硬盘上。使用字典内建的方法添加、删除和修改人员信息。 一旦你完成了这个程序，你就可以说是一个Python程序员了。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#!/usr/bin/python#coding=utf-8# Filename: address.pyimport cPickle as pclass Person: &apos;&apos;&apos;Person class save information of email,phone,name.&apos;&apos;&apos; contacts = &#123;&#125; contactfile = &apos;contact.data&apos; def __init__(self): f = file(Person.contactfile) Person.contacts = p.load(f) f.close() def add(self, name, email, phone): Person.contacts[name] = &#123;&apos;email&apos;: email, &apos;phone&apos;: phone&#125; f = file(Person.contactfile, &apos;w&apos;) p.dump(Person.contacts, f) f.close() print &apos;optetor success!&apos; def search(self, name): if Person.contacts.has_key(name): print &apos;Name: %s Email: %s Phone: %s&apos; % (name, Person.contacts[name][&apos;email&apos;], Person.contacts[name][&apos;phone&apos;]) else: print &apos;search faild!&apos; def delete(self, name): if Person.contacts.has_key(name): Person.contacts.pop(name) f = file(Person.contactfile, &apos;w&apos;) p.dump(Person.contacts, f) f.close() print &apos;delete success!&apos; else: print &apos;name is not exists!&apos; def modify(self, name, email, phone): if Person.contacts.has_key(name): if email: Person.contacts[name][&apos;email&apos;] = email if phone: Person.contacts[name][&apos;phone&apos;] = phone f = file(Person.contactfile, &apos;w&apos;) p.dump(Person.contacts, f) f.close() print &apos;modify success!&apos; else: print &apos;name is not exists!&apos;def p_add(): print &quot;Pleace input add information&quot; name = raw_input(&apos;name:&apos;) email = raw_input(&apos;email:&apos;) phone = raw_input(&apos;phone:&apos;) person = Person() person.add(name, email, phone)def p_modify(): name = raw_input(&apos;Pleace input modify name:&apos;) email = raw_input(&apos;email:&apos;) phone = raw_input(&apos;phone:&apos;) person = Person() person.modify(name, email, phone)def p_delete(): name = raw_input(&apos;Pleace input search name:&apos;) person = Person() person.delete(name)def p_search(): name = raw_input(&apos;Pleace input search name:&apos;) person = Person() person.search(name)def main(): print(&apos;&apos;&apos;What would you want to do ? a=add, m=modify, d=delete, s=search&apos;&apos;&apos;) optetor = raw_input(&apos;pleace input a character:&apos;) if(optetor == &apos;a&apos;): p_add() elif(optetor == &apos;m&apos;): p_modify() elif(optetor == &apos;d&apos;): p_delete() elif(optetor == &apos;s&apos;): p_search() else: print &apos;input error&apos;if __name__ == &apos;__main__&apos;: try: main() except Exception,e: print e]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 高级整理（Python中文手册）]]></title>
    <url>%2F2018%2F02%2F05%2FPython%E9%AB%98%E7%BA%A7%E6%95%B4%E7%90%86%EF%BC%88Python%E4%B8%AD%E6%96%87%E6%89%8B%E5%86%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[记住，Python把在程序中用到的任何东西都称为 对象 。这是从广义上说的。因此我们不会说“某某 东西 ”，我们说“某个 对象 ”。给面向对象编程用户的注释： 就每一个东西包括数、字符串甚至函数都是对象这一点来说，Python是极其完全地面向对象的。 1、模块（1）简介你已经学习了如何在你的程序中定义一次函数而重用代码。如果你想要在其他程序中重用很多函数，那么你该如何编写程序呢？你可能已经猜到了，答案是使用模块。模块基本上就是一个包含了所有你定义的函数和变量的文件。为了在其他程序中重用模块，模块的文件名必须以.py为扩展名。 模块可以从其他程序 输入 以便利用它的功能。这也是我们使用Python标准库的方法。模块的用处在于它能为你在别的程序中重用它提供的服务和功能。Python附带的标准库就是这样一组模块的例子。 （2）字节编译的.pyc文件输入一个模块相对来说是一个比较费时的事情，所以Python做了一些技巧，以便使输入模块更加快一些。一种方法是创建 字节编译的文件 ，这些文件以.pyc作为扩展名。字节编译的文件与Python变换程序的中间状态有关。当你在下次从别的程序输入这个模块的时候，.pyc文件是十分有用的——它会快得多，因为一部分输入模块所需的处理已经完成了。另外，这些字节编译的文件也是与平台无关的。 （3）from..import 语句如果你想要直接输入argv变量到你的程序中（避免在每次使用它时打sys.），那么你可以使用from sys import argv语句。如果你想要输入所有sys模块使用的名字，那么你可以使用from sys import *语句。这对于所有模块都适用。一般说来，应该避免使用from..import而使用import语句，因为这样可以使你的程序更加易读，也可以避免名称的冲突。 （4）模块的 __name__每个模块都有一个名称，在模块中可以通过语句来找出模块的名称。这在一个场合特别有用——就如前面所提到的，当一个模块被第一次输入的时候，这个模块的主块将被运行。假如我们只想在程序本身被使用的时候运行主块，而在它被别的模块输入的时候不运行主块，我们该怎么做呢？这可以通过模块的__name__属性完成。 例8.2 使用模块的__name__1234567#!/usr/bin/python# Filename: using_name.pyif __name__ == &apos;__main__&apos;: print &apos;This program is being run by itself&apos;else: print &apos;I am being imported from another module&apos; 输出：1234567$ python using_name.pyThis program is being run by itself$ python&gt;&gt;&gt; import using_nameI am being imported from another module&gt;&gt;&gt; 每个Python模块都有它的__name__，如果它是’__main__‘，这说明这个模块被用户单独运行，我们可以进行相应的恰当操作。 （5）dir() 函数你可以使用内建的dir函数来列出模块定义的标识符。标识符有函数、类和变量。当你为dir()提供一个模块名的时候，它返回模块定义的名称列表。如果不提供参数，它返回当前模块中定义的名称列表。 2、面向对象的编程（1）简介在我们的程序中，我们根据操作数据的函数或语句块来设计程序的。这被称为 面向过程的 编程。还有一种把数据和功能结合起来，用称为对象的东西包裹起来组织程序的方法。这种方法称为 面向对象的 编程理念。在大多数时候你可以使用过程性编程，但是有些时候当你想要编写大型程序或是寻求一个更加合适的解决方案的时候，你就得使用面向对象的编程技术。 类和对象是面向对象编程的两个主要方面。类创建一个新类型，而对象这个类的 实例 。这类似于你有一个int类型的变量，这存储整数的变量是int类的实例（对象）。 注意，即便是整数也被作为对象（属于int类）。这和C++、Java（1.5版之前）把整数纯粹作为类型是不同的。通过help(int)了解更多这个类的详情。 对象可以使用普通的 属于 对象的变量存储数据。属于一个对象或类的变量被称为域。对象也可以使用 属于 类的函数来具有功能。这样的函数被称为类的方法。这些术语帮助我们把它们与孤立的函数和变量区分开来。域和方法可以合称为类的属性。 域有两种类型——属于每个实例/类的对象或属于类本身。它们分别被称为实例变量和类变量。 类使用class关键字创建。类的域和方法被列在一个缩进块中。 （2）self类的方法与普通的函数只有一个特别的区别——它们必须有一个额外的第一个参数名称，但是在调用这个方法的时候你不为这个参数赋值，Python会提供这个值。这个特别的变量指对象本身，按照惯例它的名称是self。 虽然你可以给这个参数任何名称，但是 强烈建议 你使用self这个名称——其他名称都是不赞成你使用的。使用一个标准的名称有很多优点——你的程序读者可以迅速识别它，如果使用self的话，还有些IDE（集成开发环境）也可以帮助你。 你一定很奇怪Python如何给self赋值以及为何你不需要给它赋值。举一个例子会使此变得清晰。假如你有一个类称为MyClass和这个类的一个实例MyObject。当你调用这个对象的方法MyObject.method(arg1, arg2)的时候，这会由Python自动转为MyClass.method(MyObject, arg1, arg2)——这就是self的原理了。 （注：这也意味着如果你有一个不需要参数的方法，你还是得给这个方法定义一个self参数。） （3）创建一个类12345678910例11.1 创建一个类#!/usr/bin/python# Filename: simplestclass.pyclass Person: pass # An empty blockp = Person()print p 输出：12$ python simplestclass.py&lt;__main__.Person instance at 0xf6fcb18c&gt; 我们使用class语句后跟类名，创建了一个新的类。这后面跟着一个缩进的语句块形成类体。在这个例子中，我们使用了一个空白块，它由pass语句表示。 接下来，我们使用类名后跟一对圆括号来创建一个对象/实例。为了验证，我们简单地打印了这个变量的类型。它告诉我们我们已经在__main__模块中有了一个Person类的实例。 可以注意到存储对象的计算机内存地址也打印了出来。这个地址在你的计算机上会是另外一个值，因为Python可以在任何空位存储对象。 （4）init方法在Python的类中有很多方法的名字有特殊的重要意义。现在我们将学习__init__方法的意义。 __init__方法在类的一个对象被建立时，马上运行。这个方法可以用来对你的对象做一些你希望的 初始化 。注意，这个名称的开始和结尾都是双下划线。123456789101112131415例11.3 使用__init__方法#!/usr/bin/python# Filename: class_init.pyclass Person: def __init__(self, name): self.name = name def sayHi(self): print &apos;Hello, my name is&apos;, self.namep = Person(&apos;Swaroop&apos;)p.sayHi()# This short example can also be written as Person(&apos;Swaroop&apos;).sayHi() 输出：12$ python class_init.pyHello, my name is Swaroop 这里，我们把__init__方法定义为取一个参数name（以及普通的参数self）。在这个__init__里，我们只是创建一个新的域，也称为name。注意它们是两个不同的变量，尽管它们有相同的名字。点号使我们能够区分它们。 最重要的是，我们没有专门调用__init__方法，只是在创建一个类的新实例的时候，把参数包括在圆括号内跟在类名后面，从而传递给__init__方法。这是这种方法的重要之处。与 construct 的概念类似。 现在，我们能够在我们的方法中使用self.name域。这在sayHi方法中得到了验证。 （5）类与对象的方法我们已经讨论了类与对象的功能部分，现在我们来看一下它的数据部分。事实上，它们只是与类和对象的名称空间 绑定 的普通变量，即这些名称只在这些类与对象的前提下有效。 有两种类型的 域 ——类的变量和对象的变量，它们根据是类还是对象 拥有 这个变量而区分。 类的变量 由一个类的所有对象（实例）共享使用。只有一个类变量的拷贝，所以当某个对象对类的变量做了改动的时候，这个改动会反映到所有其他的实例上。 对象的变量 由类的每个对象/实例拥有。因此每个对象有自己对这个域的一份拷贝，即它们不是共享的，在同一个类的不同实例中，虽然对象的变量有相同的名称，但是是互不相关的。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152例11.4 使用类与对象的变量#!/usr/bin/python# Filename: objvar.pyclass Person: &apos;&apos;&apos;Represents a person.&apos;&apos;&apos; population = 0 def __init__(self, name): &apos;&apos;&apos;Initializes the person&apos;s data.&apos;&apos;&apos; self.name = name print &apos;(Initializing %s)&apos; % self.name # When this person is created, he/she # adds to the population Person.population += 1 def __del__(self): &apos;&apos;&apos;I am dying.&apos;&apos;&apos; print &apos;%s says bye.&apos; % self.name Person.population -= 1 if Person.population == 0: print &apos;I am the last one.&apos; else: print &apos;There are still %d people left.&apos; % Person.population def sayHi(self): &apos;&apos;&apos;Greeting by the person. Really, that&apos;s all it does.&apos;&apos;&apos; print &apos;Hi, my name is %s.&apos; % self.name def howMany(self): &apos;&apos;&apos;Prints the current population.&apos;&apos;&apos; if Person.population == 1: print &apos;I am the only person here.&apos; else: print &apos;We have %d persons here.&apos; % Person.populationswaroop = Person(&apos;Swaroop&apos;)swaroop.sayHi()swaroop.howMany()kalam = Person(&apos;Abdul Kalam&apos;)kalam.sayHi()kalam.howMany()swaroop.sayHi()swaroop.howMany() 输出：12345678910111213$ python objvar.py(Initializing Swaroop)Hi, my name is Swaroop.I am the only person here.(Initializing Abdul Kalam)Hi, my name is Abdul Kalam.We have 2 persons here.Hi, my name is Swaroop.We have 2 persons here.Abdul Kalam says bye.There are still 1 people left.Swaroop says bye.I am the last one. 这是一个很长的例子，但是它有助于说明类与对象的变量的本质。这里，population属于Person类，因此是一个类的变量。name变量属于对象（它使用self赋值）因此是对象的变量。 观察可以发现__init__方法用一个名字来初始化Person实例。在这个方法中，我们让population增加1，这是因为我们增加了一个人。同样可以发现，self.name的值根据每个对象指定，这表明了它作为对象的变量的本质。 记住，你只能使用self变量来参考同一个对象的变量和方法。这被称为 属性参考 。 在这个程序中，我们还看到docstring对于类和方法同样有用。我们可以在运行时使用Person.__doc__和Person.sayHi.__doc__来分别访问类与方法的文档字符串。 就如同__init__方法一样，还有一个特殊的方法__del__，它在对象消逝的时候被调用。对象消逝即对象不再被使用，它所占用的内存将返回给系统作它用。在这个方法里面，我们只是简单地把Person.population减1。 当对象不再被使用时，__del__方法运行，但是很难保证这个方法究竟在 什么时候 运行。如果你想要指明它的运行，你就得使用del语句，就如同我们在以前的例子中使用的那样。 给C++/Java/C#程序员的注释Python中所有的类成员（包括数据成员）都是 公共的 ，所有的方法都是 有效的 。只有一个例外：如果你使用的数据成员名称以 双下划线前缀 比如__privatevar，Python的名称管理体系会有效地把它作为私有变量。这样就有一个惯例，如果某个变量只想在类或对象中使用，就应该以单下划线前缀。而其他的名称都将作为公共的，可以被其他类/对象使用。记住这只是一个惯例，并不是Python所要求的（与双下划线前缀不同）。同样，注意__del__方法与 destructor 的概念类似。 一些特殊的方法： table th:first-of-type { width: 100px; } 名称 说明 init(self,…) 这个方法在新建对象恰好要被返回使用之前被调用。 del(self) 恰好在对象要被删除之前调用。 str(self) 在我们对对象使用print语句或是使用str()的时候调用。 lt(self,other) 当使用 小于 运算符（&lt;）的时候调用。类似地，对于所有的运算符（+，&gt;等等）都有特殊的方法。 getitem(self,key) 使用x[key]索引操作符的时候调用。 len(self) 对序列对象使用内建的len()函数的时候调用。 （6）继承面向对象的编程带来的主要好处之一是代码的重用，实现这种重用的方法之一是通过 继承 机制。继承完全可以理解成类之间的 类型和子类型 关系。 假设你想要写一个程序来记录学校之中的教师和学生情况。他们有一些共同属性，比如姓名、年龄和地址。他们也有专有的属性，比如教师的薪水、课程和假期，学生的成绩和学费。 你可以为教师和学生建立两个独立的类来处理它们，但是这样做的话，如果要增加一个新的共有属性，就意味着要在这两个独立的类中都增加这个属性。这很快就会显得不实用。 一个比较好的方法是创建一个共同的类称为SchoolMember然后让教师和学生的类 继承 这个共同的类。即它们都是这个类型（类）的子类型，然后我们再为这些子类型添加专有的属性。 使用这种方法有很多优点。如果我们增加/改变了SchoolMember中的任何功能，它会自动地反映到子类型之中。例如，你要为教师和学生都增加一个新的身份证域，那么你只需简单地把它加到SchoolMember类中。然而，在一个子类型之中做的改动不会影响到别的子类型。另外一个优点是你可以把教师和学生对象都作为SchoolMember对象来使用，这在某些场合特别有用，比如统计学校成员的人数。一个子类型在任何需要父类型的场合可以被替换成父类型，即对象可以被视作是父类的实例，这种现象被称为多态现象。 另外，我们会发现在 重用 父类的代码的时候，我们无需在不同的类中重复它。而如果我们使用独立的类的话，我们就不得不这么做了。 在上述的场合中，SchoolMember类被称为 基本类 或 超类 。而Teacher和Student类被称为 导出类 或 子类 。12345678910111213141516171819202122232425262728293031323334353637383940414243444546例11.5 使用继承#!/usr/bin/python# Filename: inherit.pyclass SchoolMember: &apos;&apos;&apos;Represents any school member.&apos;&apos;&apos; def __init__(self, name, age): self.name = name self.age = age print &apos;(Initialized SchoolMember: %s)&apos; % self.name def tell(self): &apos;&apos;&apos;Tell my details.&apos;&apos;&apos; print &apos;Name:&quot;%s&quot; Age:&quot;%s&quot;&apos; % (self.name, self.age),class Teacher(SchoolMember): &apos;&apos;&apos;Represents a teacher.&apos;&apos;&apos; def __init__(self, name, age, salary): SchoolMember.__init__(self, name, age) self.salary = salary print &apos;(Initialized Teacher: %s)&apos; % self.name def tell(self): SchoolMember.tell(self) print &apos;Salary: &quot;%d&quot;&apos; % self.salaryclass Student(SchoolMember): &apos;&apos;&apos;Represents a student.&apos;&apos;&apos; def __init__(self, name, age, marks): SchoolMember.__init__(self, name, age) self.marks = marks print &apos;(Initialized Student: %s)&apos; % self.name def tell(self): SchoolMember.tell(self) print &apos;Marks: &quot;%d&quot;&apos; % self.markst = Teacher(&apos;Mrs. Shrividya&apos;, 40, 30000)s = Student(&apos;Swaroop&apos;, 22, 75)print # prints a blank linemembers = [t, s]for member in members: member.tell() # works for both Teachers and Students 输出：12345678$ python inherit.py(Initialized SchoolMember: Mrs. Shrividya)(Initialized Teacher: Mrs. Shrividya)(Initialized SchoolMember: Swaroop)(Initialized Student: Swaroop)Name:&quot;Mrs. Shrividya&quot; Age:&quot;40&quot; Salary: &quot;30000&quot;Name:&quot;Swaroop&quot; Age:&quot;22&quot; Marks: &quot;75&quot; 为了使用继承，我们把基本类的名称作为一个元组跟在定义类时的类名称之后。然后，我们注意到基本类的init方法专门使用self变量调用，这样我们就可以初始化对象的基本类部分。这一点十分重要——Python不会自动调用基本类的constructor，你得亲自专门调用它。 我们还观察到我们在方法调用之前加上类名称前缀，然后把self变量及其他参数传递给它。 注意，在我们使用SchoolMember类的tell方法的时候，我们把Teacher和Student的实例仅仅作为SchoolMember的实例。 另外，在这个例子中，我们调用了子类型的tell方法，而不是SchoolMember类的tell方法。可以这样来理解，Python总是首先查找对应类型的方法，在这个例子中就是如此。如果它不能在导出类中找到对应的方法，它才开始到基本类中逐个查找。基本类是在类定义的时候，在元组之中指明的。 一个术语的注释——如果在继承元组中列了一个以上的类，那么它就被称作 多重继承 。 3、输入/输出在很多时候，你会想要让你的程序与用户（可能是你自己）交互。你会从用户那里得到输入，然后打印一些结果。我们可以分别使用raw_input和print语句来完成这些功能。对于输出，你也可以使用多种多样的str（字符串）类。例如，你能够使用rjust方法来得到一个按一定宽度右对齐的字符串。利用help(str)获得更多详情。 另一个常用的输入/输出类型是处理文件。创建、读和写文件的能力是许多程序所必需的，我们将会在这章探索如何实现这些功能。 （1）文件你可以通过创建一个file类的对象来打开一个文件，分别使用file类的read、readline或write方法来恰当地读写文件。对文件的读写能力依赖于你在打开文件时指定的模式。最后，当你完成对文件的操作的时候，你调用close方法来告诉Python我们完成了对文件的使用。12345678910111213141516171819202122232425例12.1 使用文件#!/usr/bin/python# Filename: using_file.pypoem = &apos;&apos;&apos;\Programming is funWhen the work is doneif you wanna make your work also fun: use Python!&apos;&apos;&apos;f = file(&apos;poem.txt&apos;, &apos;w&apos;) # open for &apos;w&apos;ritingf.write(poem) # write text to filef.close() # close the filef = file(&apos;poem.txt&apos;)# if no mode is specified, &apos;r&apos;ead mode is assumed by defaultwhile True: line = f.readline() if len(line) == 0: # Zero length indicates EOF break print line, # Notice comma to avoid automatic newline added by Pythonf.close() # close the file 输出：12345$ python using_file.pyProgramming is funWhen the work is doneif you wanna make your work also fun: use Python! 首先，我们通过指明我们希望打开的文件和模式来创建一个file类的实例。模式可以为读模式（’r’）、写模式（’w’）或追加模式（’a’）。事实上还有多得多的模式可以使用，你可以使用help(file)来了解它们的详情。 我们首先用写模式打开文件，然后使用file类的write方法来写文件，最后我们用close关闭这个文件。 接下来，我们再一次打开同一个文件来读文件。如果我们没有指定模式，读模式会作为默认的模式。在一个循环中，我们使用readline方法读文件的每一行。这个方法返回包括行末换行符的一个完整行。所以，当一个 空的 字符串被返回的时候，即表示文件末已经到达了，于是我们停止循环。 注意，因为从文件读到的内容已经以换行符结尾，所以我们在print语句上使用逗号来消除自动换行。最后，我们用close关闭这个文件。 现在，来看一下poem.txt文件的内容来验证程序确实工作正常了。 （2）存储器Python提供一个标准的模块，称为pickle。使用它你可以在一个文件中储存任何Python对象，之后你又可以把它完整无缺地取出来。这被称为 持久地 储存对象。 还有另一个模块称为cPickle，它的功能和pickle模块完全相同，只不过它是用C语言编写的，因此要快得多（比pickle快1000倍）。你可以使用它们中的任一个，而我们在这里将使用cPickle模块。记住，我们把这两个模块都简称为pickle模块。123456789101112131415161718192021222324例12.2 储存与取储存#!/usr/bin/python# Filename: pickling.pyimport cPickle as p#import pickle as pshoplistfile = &apos;shoplist.data&apos;# the name of the file where we will store the objectshoplist = [&apos;apple&apos;, &apos;mango&apos;, &apos;carrot&apos;]# Write to the filef = file(shoplistfile, &apos;w&apos;)p.dump(shoplist, f) # dump the object to a filef.close()del shoplist # remove the shoplist# Read back from the storagef = file(shoplistfile)storedlist = p.load(f)print storedlist 输出：12$ python pickling.py[&apos;apple&apos;, &apos;mango&apos;, &apos;carrot&apos;] 首先，请注意我们使用了import..as语法。这是一种便利方法，以便于我们可以使用更短的模块名称。在这个例子中，它还让我们能够通过简单地改变一行就切换到另一个模块（cPickle或者pickle）！在程序的其余部分的时候，我们简单地把这个模块称为p。 为了在文件里储存一个对象，首先以写模式打开一个file对象，然后调用储存器模块的dump函数，把对象储存到打开的文件中。这个过程称为 储存 。 接下来，我们使用pickle模块的load函数的返回来取回对象。这个过程称为 取储存 。 4、异常当你的程序中出现某些 异常的 状况的时候，异常就发生了。例如，当你想要读某个文件的时候，而那个文件不存在。或者在程序运行的时候，你不小心把它删除了。上述这些情况可以使用异常来处理。 假如你的程序中有一些无效的语句，会怎么样呢？Python会引发并告诉你那里有一个错误，从而处理这样的情况。 （1）处理异常我们可以使用try..except语句来处理异常。我们把通常的语句放在try-块中，而把我们的错误处理语句放在except-块中。1234567891011121314151617例13.1 处理异常#!/usr/bin/python# Filename: try_except.pyimport systry: s = raw_input(&apos;Enter something --&gt; &apos;)except EOFError: print &apos;\nWhy did you do an EOF on me?&apos; sys.exit() # exit the programexcept: print &apos;\nSome error/exception occurred.&apos; # here, we are not exiting the programprint &apos;Done&apos; 输出：1234567$ python try_except.pyEnter something --&gt;Why did you do an EOF on me?$ python try_except.pyEnter something --&gt; Python is exceptional!Done 我们把所有可能引发错误的语句放在try块中，然后在except从句/块中处理所有的错误和异常。except从句可以专门处理单一的错误或异常，或者一组包括在圆括号内的错误/异常。如果没有给出错误或异常的名称，它会处理 所有的 错误和异常。对于每个try从句，至少都有一个相关联的except从句。 如果某个错误或异常没有被处理，默认的Python处理器就会被调用。它会终止程序的运行，并且打印一个消息，我们已经看到了这样的处理。 你还可以让try..catch块关联上一个else从句。当没有异常发生的时候，else从句将被执行。 我们还可以得到异常对象，从而获取更多有个这个异常的信息。 （2）引发异常你可以使用raise语句 引发 异常。你还得指明错误/异常的名称和伴随异常 触发的 异常对象。你可以引发的错误或异常应该分别是一个Error或Exception类的直接或间接导出类。123456789101112131415161718192021222324例13.2 如何引发异常#!/usr/bin/python# Filename: raising.pyclass ShortInputException(Exception): &apos;&apos;&apos;A user-defined exception class.&apos;&apos;&apos; def __init__(self, length, atleast): Exception.__init__(self) self.length = length self.atleast = atleasttry: s = raw_input(&apos;Enter something --&gt; &apos;) if len(s) &lt; 3: raise ShortInputException(len(s), 3) # Other work can continue as usual hereexcept EOFError: print &apos;\nWhy did you do an EOF on me?&apos;except ShortInputException, x: print &apos;ShortInputException: The input was of length %d, \ was expecting at least %d&apos; % (x.length, x.atleast)else: print &apos;No exception was raised.&apos; 输出：1234567891011$ python raising.pyEnter something --&gt;Why did you do an EOF on me?$ python raising.pyEnter something --&gt; abShortInputException: The input was of length 2, was expecting at least 3$ python raising.pyEnter something --&gt; abcNo exception was raised. 这里，我们创建了我们自己的异常类型，其实我们可以使用任何预定义的异常/错误。这个新的异常类型是ShortInputException类。它有两个域——length是给定输入的长度，atleast则是程序期望的最小长度。 在except从句中，我们提供了错误类和用来表示错误/异常对象的变量。这与函数调用中的形参和实参概念类似。在这个特别的except从句中，我们使用异常对象的length和atleast域来为用户打印一个恰当的消息。 （3）try..finally假如你在读一个文件的时候，希望在无论异常发生与否的情况下都关闭文件，该怎么做呢？这可以使用finally块来完成。注意，在一个try块下，你可以同时使用except从句和finally块。如果你要同时使用它们的话，需要把一个嵌入另外一个。123456789101112131415161718例13.3 使用finally#!/usr/bin/python# Filename: finally.pyimport timetry: f = file(&apos;poem.txt&apos;) while True: # our usual file-reading idiom line = f.readline() if len(line) == 0: break time.sleep(2) print line,finally: f.close() print &apos;Cleaning up...closed the file&apos; 输出：12345678$ python finally.pyProgramming is funWhen the work is doneCleaning up...closed the fileTraceback (most recent call last): File &quot;finally.py&quot;, line 12, in ? time.sleep(2)KeyboardInterrupt 我们进行通常的读文件工作，但是我有意在每打印一行之前用time.sleep方法暂停2秒钟。这样做的原因是让程序运行得慢一些（Python由于其本质通常运行得很快）。在程序运行的时候，按Ctrl-c中断/取消程序。 我们可以观察到KeyboardInterrupt异常被触发，程序退出。但是在程序退出之前，finally从句仍然被执行，把文件关闭 5、其它知识点（1）在函数中接收元组和列表当要使函数接收元组或字典形式的参数的时候，有一种特殊的方法，它分别使用*和**前缀。这种方法在函数需要获取可变数量的参数的时候特别有用。 由于在args变量前有*前缀，所有多余的函数参数都会作为一个元组存储在args中。如果使用的是**前缀，多余的参数则会被认为是一个字典的键/值对。 （2）lambda形式lambda语句被用来创建新的函数对象，并且在运行时返回它们。123456789101112例15.2 使用lambda形式#!/usr/bin/python# Filename: lambda.pydef make_repeater(n): return lambda s: s*ntwice = make_repeater(2)print twice(&apos;word&apos;)print twice(5) 输出：123$ python lambda.pywordword10 这里，我们使用了make_repeater函数在运行时创建新的函数对象，并且返回它。lambda语句用来创建函数对象。本质上，lambda需要一个参数，后面仅跟单个表达式作为函数体，而表达式的值被这个新建的函数返回。注意，即便是print语句也不能用在lambda形式中，只能使用表达式。 （3）exec 和 eval 语句exec语句用来执行储存在字符串或文件中的Python语句。例如，我们可以在运行时生成一个包含Python代码的字符串，然后使用exec语句执行这些语句。下面是一个简单的例子。12&gt;&gt;&gt; exec &apos;print &quot;Hello World&quot;&apos;Hello World eval语句用来计算存储在字符串中的有效Python表达式。下面是一个简单的例子。12&gt;&gt;&gt; eval(&apos;2*3&apos;)6]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 基础整理（Python中文手册）]]></title>
    <url>%2F2018%2F02%2F02%2FPython%E5%9F%BA%E7%A1%80%E6%95%B4%E7%90%86%EF%BC%88Python%E4%B8%AD%E6%96%87%E6%89%8B%E5%86%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1、Python概述Python语言是少有的一种可以称得上即简单又功能强大的编程语言。你将惊喜地发现Python语言是多么地简单，它注重的是如何解决问题而不是编程语言的语法和结构。 Python的官方介绍是： Python是一种简单易学，功能强大的编程语言，它有高效率的高层数据结构，简单而有效地实现面向对象编程。Python简洁的语法和对动态输入的支持，再加上解释性语言的本质，使得它在大多数平台上的许多领域都是一个理想的脚本语言，特别适用于快速的应用程序开发。 2、基本数据类型：数和字符串（1）数在Python中有4种类型的数——整数、长整数、浮点数和复数。 （2）字符串字符串是 字符的序列 。字符串基本上就是一组单词。 下面有几种方式使用字符串：使用单引号（’） 你可以用单引号指示字符串，就如同’Quote me on this’这样。所有的空白，即空格和制表符都照原样保留。使用双引号（”） 在双引号中的字符串与单引号中的字符串的使用完全相同，例如”What’s your name?”。使用三引号（’’’或”””） 利用三引号，你可以指示一个多行的字符串。你可以在三引号中自由的使用单引号和双引号。例如：‘’’This is a multi-line string. This is the first line.This is the second line.“What’s your name?,” I asked.He said “Bond, James Bond.”‘’’（注意：使用转义符来输出特殊字符。） 自然字符串与Unicode字符串：自然字符串：如果你想要指示某些不需要如转义符那样的特别处理的字符串，那么你需要指定一个自然字符串。自然字符串通过给字符串加上前缀r或R来指定。例如r”Newlines are indicated by \n”。Unicode字符串：在你处理文本文件的时候使用Unicode字符串，特别是当你知道这个文件含有用非英语的语言写的文本。Python允许你处理Unicode文本——你只需要在字符串前加上前缀u或U。例如，u”This is a Unicode string.”。（注：字符串是不可变的，这意味着一旦你创造了一个字符串，你就不能再改变它了。）（注：相邻的字符串会被Python自动级连。） 3、变量变量就是我们想要的东西——它们的值可以变化，即你可以使用变量存储任何东西。变量只是你的计算机中存储信息的一部分内存。（注：使用变量时只需要给它们赋一个值。不需要声明或定义数据类型。）标示符命名：标识符 是用来标识 某样东西 的名字。在命名标识符的时候，你要遵循这些规则： 标示符由字母(大写或小写)、下划线(‘_’)或数字(0-9)组成。 标示符的第一个字符必须是字母或者下划线。 标示符名称是对大小写敏感的。例如，myname和myName不是一个标识符。 4、逻辑行与物理行（1）Python默认是每个物理行对应一个逻辑行。如果你想要在一个物理行中使用多于一个逻辑行，那么你需要使用分号（;）来特别地标明这种用法。分号表示一个逻辑行/语句的结束。（2）如果多个物理行中写一个逻辑行，可以使用反斜杠（’\’）来连接，被称为明确的行连接。有一种暗示的假设，可以使你不需要使用反斜杠。这种情况出现在逻辑行中使用了圆括号、方括号或波形括号的时候。这被称为暗示的行连接。 5、缩进空白在Python中是重要的。事实上行首的空白是重要的。它称为缩进。在逻辑行首的空白（空格和制表符）用来决定逻辑行的缩进层次，从而用来决定语句的分组。这意味着同一层次的语句必须有相同的缩进。每一组这样的语句称为一个块。如何缩进不要混合使用制表符和空格来缩进，因为这在跨越不同的平台的时候，无法正常工作。我 强烈建议 你在每个缩进层次使用 单个制表符 或 两个 或 四个空格 。选择这三种缩进风格之一。更加重要的是，选择一种风格，然后一贯地使用它，即 只 使用这一种风格。 6、运算符与表达式（1）运算符与它们的用法 table th:first-of-type { width: 50px; } table th:nth-of-type(2) { width: 50px; } 运算符 名称 说明 例子 + 加 两个对象相加 3 + 5得到8。’a’ + ‘b’得到’ab’。 - 减 得到负数或是一个数减去另一个数 -5.2得到一个负数。50 - 24得到26。 * 乘 两个数相乘或是返回一个被重复若干次的字符串 2 3得到6。’la’ 3得到’lalala’。 ** 幂 返回x的y次幂 3 * 4得到81（即3 3 3 3） / 除 x除以y 4/3得到1（整数的除法得到整数结果）。4.0/3或4/3.0得到1.3333333333333333 // 取整除 返回商的整数部分 4 // 3.0得到1.0 % 取模 返回除法的余数 8%3得到2。-25.5%2.25得到1.5 &lt;&lt; 左移 把一个数的比特向左移一定数目（每个数在内存中都表示为比特或二进制数字，即0和1） 2 &lt;&lt; 2得到8。——2按比特表示为10 &gt;&gt; 右移 把一个数的比特向右移一定数目 11 &gt;&gt; 1得到5。——11按比特表示为1011，向右移动1比特后得到101，即十进制的5。 &amp; 按位与 数的按位与 5 &amp; 3得到1。 &#124; 按位或 数的按位或 5 &#124; 3得到7。 ^ 按位异或 数的按位异或 5 ^ 3得到6 ~ 按位翻转 x的按位翻转是-(x+1) ~5得到6。 &lt; 小于 返回x是否小于y。所有比较运算符返回1表示真，返回0表示假。这分别与特殊的变量True和False等价。注意，这些变量名的大写。 5 &lt; 3返回0（即False）而3 &lt; 5返回1（即True）。比较可以被任意连接：3 &lt; 5 &lt; 7返回True。 &gt; 大于 返回x是否大于y 5 &gt; 3返回True。如果两个操作数都是数字，它们首先被转换为一个共同的类型。否则，它总是返回False。 &lt;= 小于等于 返回x是否小于等于y x = 3; y = 6; x &lt;= y返回True。 &gt;= 大于等于 返回x是否大于等于y x = 4; y = 3; x &gt;= y返回True。 == 等于 比较对象是否相等 x = 2; y = 2; x == y返回True。x = ‘str’; y = ‘stR’; x == y返回False。x = ‘str’; y = ‘str’; x == y返回True。 != 不等于 比较两个对象是否不相等 x = 2; y = 3; x != y返回True。 not 布尔“非” 如果x为True，返回False。如果x为False，它返回True。 x = True; not y返回False。 and 布尔“与” 如果x为False，x and y返回False，否则它返回y的计算值。 x = False; y = True; x and y，由于x是False，返回False。在这里，Python不会计算y，因为它知道这个表达式的值肯定是False（因为x是False）。这个现象称为短路计算。 or 布尔“或” 如果x是True，它返回True，否则它返回y的计算值。 x = True; y = False; x or y返回True。短路计算在这里也适用。 （2）下面这个表给出Python的运算符优先级，从最低的优先级（最松散地结合）到最高的优先级（最紧密地结合）。 运算符 描述 lambda Lambda表达式 or 布尔“或” and 布尔“与” not x 布尔“非” in，not in 成员测试 is，is not 同一性测试 &lt;，&lt;=，&gt;，&gt;=，!=，== 比较 &#124; 按位或 ^ 按位异或 &amp; 按位与 &lt;&lt;，&gt;&gt; 移位 +，- 加法与减法 *，/，% 乘法、除法与取余 +x，-x 正负号 ~x 按位翻转 ** 指数 x.attribute 属性参考 x[index] 下标 x[index:index] 寻址段 f(arguments…) 函数调用 (experession,…) 绑定或元组显示 [expression,…] 列表显示 {key:datum,…} 字典显示 ‘expression,…’ 字符串转换 （3）一个表达式可以分解为运算符和操作数。运算符 的功能是完成某件事，它们由如+这样的符号或者其他特定的关键字表示。运算符需要数据来进行运算，这样的数据被称为 操作数 。 7、控制流Python中的控制语句与PHP中的控制语句大概意义上都是一致的，不过要注意Python中没有switch语句结构，而且for语句结构有一些区别，如果你想要写for (int i = 0; i &lt; 5; i++)，那么用Python，你写成for i in range(0,5)。还有一点要注意的是，依据Python语言的特性，每个控制语句的结尾处都要包含一个冒号————我们通过它告诉Python下面跟着一个语句块。 （1）if 语句if语句用来检验一个条件， 如果 条件为真，我们运行一块语句（称为 if-块 ）， 否则 我们处理另外一块语句（称为 else-块 ）。 else 从句是可选的。（注：在Python中没有switch语句。你可以使用if..elif..else语句来完成同样的工作（在某些场合，使用字典会更加快捷。） ） （2）for 语句for..in是另外一个循环语句，它在一序列的对象上 递归 即逐一使用队列中的每个项目。 （3）while 语句只要在一个条件为真的情况下，while语句允许你重复执行一块语句。while语句是所谓 循环 语句的一个例子。while语句有一个可选的else从句。 （4）break 语句break语句是用来 终止 循环语句的，即哪怕循环条件没有称为False或序列还没有被完全递归，也停止执行循环语句。 一个重要的注释是，如果你从for或while循环中 终止 ，任何对应的循环else块将不执行。 （5）continue 语句continue语句被用来告诉Python跳过当前循环块中的剩余语句，然后 继续 进行下一轮循环。 8、函数（1）简介函数是重用的程序段。它们允许你给一块语句一个名称，然后你可以在你的程序的任何地方使用这个名称任意多次地运行这个语句块。这被称为 调用 函数。我们已经使用了许多内建的函数，比如len和range。 函数通过def关键字定义。def关键字后跟一个函数的 标识符 名称，然后跟一对圆括号。圆括号之中可以包括一些变量名，该行以冒号结尾。接下来是一块语句，它们是函数体。 下面这个例子（定义函数）将说明这事实上是十分简单的：1234567#!/usr/bin/python# Filename: function1.pydef sayHello(): print &apos;Hello World!&apos; # block belonging to the functionsayHello() # call the function 我们使用上面解释的语法定义了一个称为sayHello的函数。这个函数不使用任何参数，因此在圆括号中没有声明任何变量。参数对于函数而言，只是给函数的输入，以便于我们可以传递不同的值给函数，然后得到相应的结果。 （2）函数的形参函数取得的参数是你提供给函数的值，这样函数就可以利用这些值 做 一些事情。这些参数就像变量一样，只不过它们的值是在我们调用函数的时候定义的，而非在函数本身内赋值。 参数在函数定义的圆括号对内指定，用逗号分割。当我们调用函数的时候，我们以同样的方式提供值。注意我们使用过的术语——函数中的参数名称为 形参 而你提供给函数调用的值称为 实参 。 （3）变量作用域局部变量：当你在函数定义内声明变量的时候，它们与函数外具有相同名称的其他变量没有任何关系，即变量名称对于函数来说是 局部 的。这称为变量的 作用域 。所有变量的作用域是它们被定义的块，从它们的名称被定义的那点开始。 全局变量：如果你想要为一个定义在函数外的变量赋值，那么你就得告诉Python这个变量名不是局部的，而是 全局 的。我们使用global语句完成这一功能。没有global语句，是不可能为定义在函数外的变量赋值的。你可以使用定义在函数外的变量的值（假设在函数内没有同名的变量）。然而，我并不鼓励你这样做，并且你应该尽量避免这样做，因为这使得程序的读者会不清楚这个变量是在哪里定义的。使用global语句可以清楚地表明变量是在外面的块定义的。 （4）函数的参数类型默认参数：对于一些函数，你可能希望它的一些参数是 可选 的，如果用户不想要为这些参数提供值的话，这些参数就使用默认值。这个功能借助于默认参数值完成。你可以在函数定义的形参名后加上赋值运算符（=）和默认值，从而给形参指定默认参数值。（注意，默认参数值应该是一个参数。更加准确的说，默认参数值应该是不可变的。）（重要只有在形参表末尾的那些参数可以有默认参数值，即你不能在声明函数形参的时候，先声明有默认值的形参而后声明没有默认值的形参。这是因为赋给形参的值是根据位置而赋值的。例如，def func(a, b=5)是有效的，但是def func(a=5, b)是 无效 的。） 关键参数：如果你的某个函数有许多参数，而你只想指定其中的一部分，那么你可以通过命名来为这些参数赋值——这被称作 关键参数 ——我们使用名字（关键字）而不是位置（我们前面所一直使用的方法）来给函数指定实参。 这样做有两个 优势 ——一，由于我们不必担心参数的顺序，使用函数变得更加简单了。二、假设其他参数都有默认值，我们可以只给我们想要的那些参数赋值。 （5）return 语句return语句用来从一个函数 返回 即跳出函数。我们也可选从函数 返回一个值 。（注：pass语句在Python中表示一个空的语句块。） （6）DocStringsPython有一个很奇妙的特性，称为 文档字符串 ，它通常被简称为 docstrings 。DocStrings是一个重要的工具，由于它帮助你的程序文档更加简单易懂，你应该尽量使用它。你甚至可以在程序运行的时候，从函数恢复文档字符串！例7.8 使用DocStrings1234567891011121314151617#!/usr/bin/python# Filename: func_doc.pydef printMax(x, y): &apos;&apos;&apos;Prints the maximum of two numbers. The two values must be integers.&apos;&apos;&apos; x = int(x) # convert to integers, if possible y = int(y) if x &gt; y: print x, &apos;is maximum&apos; else: print y, &apos;is maximum&apos;printMax(3, 5)print printMax.__doc__ 输出：12345$ python func_doc.py5 is maximumPrints the maximum of two numbers. The two values must be integers. 在函数的第一个逻辑行的字符串是这个函数的 文档字符串 。注意，DocStrings也适用于模块和类。文档字符串的惯例是一个多行字符串，它的首行以大写字母开始，句号结尾。第二行是空行，从第三行开始是详细的描述。 强烈建议 你在你的函数中使用文档字符串时遵循这个惯例。 你可以使用__doc__（注意双下划线）调用printMax函数的文档字符串属性（属于函数的名称）。请记住Python把 每一样东西 都作为对象，包括这个函数。 如果你已经在Python中使用过help()，那么你已经看到过DocStings的使用了！它所做的只是抓取函数的__doc__属性，然后整洁地展示给你。你可以对上面这个函数尝试一下——只是在你的程序中包括help(printMax)。记住按q退出help。 自动化工具也可以以同样的方式从你的程序中提取文档。因此，我 强烈建议 你对你所写的任何正式函数编写文档字符串。随你的Python发行版附带的pydoc命令，与help()类似地使用DocStrings。 9、数据结构数据结构基本上就是——它们是可以处理一些 数据 的 结构 。或者说，它们是用来存储一组相关数据的。在Python中有三种内建的数据结构——列表、元组和字典。 （1）列表list是处理一组有序项目的数据结构，即你可以在一个列表中存储一个 序列 的项目（列表也是一个序列）。假想你有一个购物列表，上面记载着你要买的东西，你就容易理解列表了。只不过在你的购物表上，可能每样东西都独自占有一行，而在Python中，你在每个项目之间用逗号分割。 列表中的项目应该包括在方括号中，这样Python就知道你是在指明一个列表。一旦你创建了一个列表，你可以添加、删除或是搜索列表中的项目。由于你可以增加或删除项目，我们说列表是 可变的 数据类型，即这种类型是可以被改变的。（注：我们使用列表的sort方法来对列表排序。需要理解的是，这个方法影响列表本身，而不是返回一个修改后的列表——这与字符串工作的方法不同。这就是我们所说的列表是 可变的 而字符串是 不可变的。） （2）元组元组和列表十分类似，只不过元组和字符串一样是 不可变的 即你不能修改元组。元组通过圆括号中用逗号分割的项目定义。元组通常用在使语句或用户定义的函数能够安全地采用一组值的时候，即被使用的元组的值不会改变。（注：变量zoo是一个元组，我们看到len函数可以用来获取元组的长度。这也表明元组也是一个序列。） 含有0个或1个项目的元组。一个空的元组由一对空的圆括号组成，如myempty = ()。然而，含有单个元素的元组就不那么简单了。你必须在第一个（唯一一个）项目后跟一个逗号，这样Python才能区分元组和表达式中一个带圆括号的对象。即如果你想要的是一个包含项目2的元组的时候，你应该指明singleton = (2 , )。注：元组最通常的用法是用在打印语句中，例如：print ‘%s is %d years old’ % (name, age) （3）序列列表、元组和字符串都是序列，但是序列是什么，它们为什么如此特别呢？序列的两个主要特点是索引操作符和切片操作符。索引操作符让我们可以从序列中抓取一个特定项目。切片操作符让我们能够获取序列的一个切片，即一部分序列。123456789101112131415161718192021222324252627例9.5 使用序列#!/usr/bin/python# Filename: seq.pyshoplist = [&apos;apple&apos;, &apos;mango&apos;, &apos;carrot&apos;, &apos;banana&apos;]# Indexing or &apos;Subscription&apos; operationprint &apos;Item 0 is&apos;, shoplist[0]print &apos;Item 1 is&apos;, shoplist[1]print &apos;Item 2 is&apos;, shoplist[2]print &apos;Item 3 is&apos;, shoplist[3]print &apos;Item -1 is&apos;, shoplist[-1]print &apos;Item -2 is&apos;, shoplist[-2]# Slicing on a listprint &apos;Item 1 to 3 is&apos;, shoplist[1:3]print &apos;Item 2 to end is&apos;, shoplist[2:]print &apos;Item 1 to -1 is&apos;, shoplist[1:-1]print &apos;Item start to end is&apos;, shoplist[:]# Slicing on a stringname = &apos;swaroop&apos;print &apos;characters 1 to 3 is&apos;, name[1:3]print &apos;characters 2 to end is&apos;, name[2:]print &apos;characters 1 to -1 is&apos;, name[1:-1]print &apos;characters start to end is&apos;, name[:] 输出：123456789101112131415$ python seq.pyItem 0 is appleItem 1 is mangoItem 2 is carrotItem 3 is bananaItem -1 is bananaItem -2 is carrotItem 1 to 3 is [&apos;mango&apos;, &apos;carrot&apos;]Item 2 to end is [&apos;carrot&apos;, &apos;banana&apos;]Item 1 to -1 is [&apos;mango&apos;, &apos;carrot&apos;]Item start to end is [&apos;apple&apos;, &apos;mango&apos;, &apos;carrot&apos;, &apos;banana&apos;]characters 1 to 3 is wacharacters 2 to end is aroopcharacters 1 to -1 is waroocharacters start to end is swaroop 首先，我们来学习如何使用索引来取得序列中的单个项目。这也被称作是下标操作。每当你用方括号中的一个数来指定一个序列的时候，Python会为你抓取序列中对应位置的项目。记住，Python从0开始计数。因此，shoplist[0]抓取第一个项目，shoplist[3]抓取shoplist序列中的第四个元素。 索引同样可以是负数，在那样的情况下，位置是从序列尾开始计算的。因此，shoplist[-1]表示序列的最后一个元素而shoplist[-2]抓取序列的倒数第二个项目。 切片操作符是序列名后跟一个方括号，方括号中有一对可选的数字，并用冒号分割。注意这与你使用的索引操作符十分相似。记住数是可选的，而冒号是必须的。 切片操作符中的第一个数（冒号之前）表示切片开始的位置，第二个数（冒号之后）表示切片到哪里结束。如果不指定第一个数，Python就从序列首开始。如果没有指定第二个数，则Python会停止在序列尾。注意，返回的序列从开始位置 开始 ，刚好在 结束 位置之前结束。即开始位置是包含在序列切片中的，而结束位置被排斥在切片外。 这样，shoplist[1:3]返回从位置1开始，包括位置2，但是停止在位置3的一个序列切片，因此返回一个含有两个项目的切片。类似地，shoplist[:]返回整个序列的拷贝。 你可以用负数做切片。负数用在从序列尾开始计算的位置。例如，shoplist[:-1]会返回除了最后一个项目外包含所有项目的序列切片。 使用Python解释器交互地尝试不同切片指定组合，即在提示符下你能够马上看到结果。序列的神奇之处在于你可以用相同的方法访问元组、列表和字符串。 （4）字典字典类似于你通过联系人名字查找地址和联系人详细情况的地址簿，即，我们把键（名字）和值（详细情况）联系在一起。注意，键必须是唯一的，就像如果有两个人恰巧同名的话，你无法找到正确的信息。 注意，你只能使用不可变的对象（比如字符串）来作为字典的键，但是你可以不可变或可变的对象作为字典的值。基本说来就是，你应该只使用简单的对象作为键。 键值对在字典中以这样的方式标记：d = {key1 : value1, key2 : value2 }。注意它们的键/值对用冒号分割，而各个对用逗号分割，所有这些都包括在花括号中。 记住字典中的键/值对是没有顺序的。如果你想要一个特定的顺序，那么你应该在使用前自己对它们排序。 字典是dict类的实例/对象。 123456789101112131415161718192021222324252627例9.4 使用字典#!/usr/bin/python# Filename: using_dict.py# &apos;ab&apos; is short for &apos;a&apos;ddress&apos;b&apos;ookab = &#123; &apos;Swaroop&apos; : &apos;swaroopch@byteofpython.info&apos;, &apos;Larry&apos; : &apos;larry@wall.org&apos;, &apos;Matsumoto&apos; : &apos;matz@ruby-lang.org&apos;, &apos;Spammer&apos; : &apos;spammer@hotmail.com&apos; &#125;print &quot;Swaroop&apos;s address is %s&quot; % ab[&apos;Swaroop&apos;]# Adding a key/value pairab[&apos;Guido&apos;] = &apos;guido@python.org&apos;# Deleting a key/value pairdel ab[&apos;Spammer&apos;]print &apos;\nThere are %d contacts in the address-book\n&apos; % len(ab)for name, address in ab.items(): print &apos;Contact %s at %s&apos; % (name, address)if &apos;Guido&apos; in ab: # OR ab.has_key(&apos;Guido&apos;) print &quot;\nGuido&apos;s address is %s&quot; % ab[&apos;Guido&apos;] 输出：1234567891011$ python using_dict.pySwaroop&apos;s address is swaroopch@byteofpython.infoThere are 4 contacts in the address-bookContact Swaroop at swaroopch@byteofpython.infoContact Matsumoto at matz@ruby-lang.orgContact Larry at larry@wall.orgContact Guido at guido@python.orgGuido&apos;s address is guido@python.org 我们使用已经介绍过的标记创建了字典ab。然后我们使用在列表和元组章节中已经讨论过的索引操作符来指定键，从而使用键/值对。我们可以看到字典的语法同样十分简单。 我们可以使用索引操作符来寻址一个键并为它赋值，这样就增加了一个新的键/值对，就像在上面的例子中我们对Guido所做的一样。 我们可以使用我们的老朋友——del语句来删除键/值对。我们只需要指明字典和用索引操作符指明要删除的键，然后把它们传递给del语句就可以了。执行这个操作的时候，我们无需知道那个键所对应的值。 接下来，我们使用字典的items方法，来使用字典中的每个键/值对。这会返回一个元组的列表，其中每个元组都包含一对项目——键与对应的值。我们抓取这个对，然后分别赋给for..in循环中的变量name和address然后在for－块中打印这些值。 我们可以使用in操作符来检验一个键/值对是否存在，或者使用dict类的has_key方法。你可以使用help(dict)来查看dict类的完整方法列表。 关键字参数与字典。如果换一个角度看待你在函数中使用的关键字参数的话，你已经使用了字典了！只需想一下——你在函数定义的参数列表中使用的键/值对。当你在函数中使用变量的时候，它只不过是使用一个字典的键（这在编译器设计的术语中被称作 符号表 ）。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP预定义变量梳理（PHP手册版）]]></title>
    <url>%2F2018%2F01%2F31%2FPHP%E9%A2%84%E5%AE%9A%E4%B9%89%E5%8F%98%E9%87%8F%E6%A2%B3%E7%90%86%EF%BC%88PHP%E6%89%8B%E5%86%8C%E7%89%88%EF%BC%89%2F</url>
    <content type="text"><![CDATA[预定义变量超全局数组：$GLOBALS服务器和执行环境信息：$_SERVER （详解）URL参数获取变量数组：$_GET表单参数获取变量数组：$_POST上传数据获取：$_FILES$_REQUEST（包含$_GET、$_POST和$_COOKIE）SESSION变量的数组：$_SESSION环境变量数组：$_ENVCOOKIE变量的数组：$_COOKIE下面只对$_SERVER这个变量进行详细说明其参数及其意义：$_SERVER 是一个包含了诸如头信息(header)、路径(path)、以及脚本位置(script locations)等等信息的数组。这个数组中的项目由 Web 服务器创建。不能保证每个服务器都提供全部项目；服务器可能会忽略一些，或者提供一些没有在这里列举出来的项目。Note: PHP 5.4.0 之前，$HTTP_SERVER_VARS 包含着相同的信息，但它不是一个超全局变量。 (注意 $HTTP_SERVER_VARS 与 $_SERVER 是不同的变量，PHP处理它们的方式不同) 在 $_SERVER 中，你也许能够，也许不能够找到下面的这些元素。注意，如果以命令行方式运行 PHP，下面列出的元素几乎没有有效的(或是没有任何实际意义的)。 ‘PHP_SELF’当前执行脚本的文件名，与 document root 有关。例如，在地址为 http://example.com/foo/bar.php 的脚本中使用 $_SERVER[‘PHP_SELF’] 将得到 /foo/bar.php。FILE 常量包含当前(例如包含)文件的完整路径和文件名。 从 PHP 4.3.0 版本开始，如果 PHP 以命令行模式运行，这个变量将包含脚本名。之前的版本该变量不可用。 ‘argv’传递给该脚本的参数的数组。当脚本以命令行方式运行时，argv 变量传递给程序 C 语言样式的命令行参数。当通过 GET 方式调用时，该变量包含query string。 ‘argc’包含命令行模式下传递给该脚本的参数的数目(如果运行在命令行模式下)。 ‘GATEWAY_INTERFACE’服务器使用的 CGI 规范的版本；例如，”CGI/1.1”。 ‘SERVER_ADDR’当前运行脚本所在的服务器的 IP 地址。 ‘SERVER_NAME’当前运行脚本所在的服务器的主机名。如果脚本运行于虚拟主机中，该名称是由那个虚拟主机所设置的值决定。 Note: 在 Apache 2 里，必须设置 UseCanonicalName = On 和 ServerName。 否则该值会由客户端提供，就有可能被伪造。 上下文有安全性要求的环境里，不应该依赖此值。 ‘SERVER_SOFTWARE’服务器标识字符串，在响应请求时的头信息中给出。 ‘SERVER_PROTOCOL’请求页面时通信协议的名称和版本。例如，”HTTP/1.0”。 ‘REQUEST_METHOD’访问页面使用的请求方法；例如，”GET”, “HEAD”，”POST”，”PUT”。 Note: 如果请求方法为 HEAD，PHP 脚本将在发送 Header 头信息之后终止(这意味着在产生任何输出后，不再有输出缓冲)。 ‘REQUEST_TIME’请求开始时的时间戳。从 PHP 5.1.0 起可用。 ‘REQUEST_TIME_FLOAT’请求开始时的时间戳，微秒级别的精准度。 自 PHP 5.4.0 开始生效。 ‘QUERY_STRING’query string（查询字符串），如果有的话，通过它进行页面访问。 ‘DOCUMENT_ROOT’当前运行脚本所在的文档根目录。在服务器配置文件中定义。 ‘HTTP_ACCEPT’当前请求头中 Accept: 项的内容，如果存在的话。 ‘HTTP_ACCEPT_CHARSET’当前请求头中 Accept-Charset: 项的内容，如果存在的话。例如：”iso-8859-1,*,utf-8”。 ‘HTTP_ACCEPT_ENCODING’当前请求头中 Accept-Encoding: 项的内容，如果存在的话。例如：”gzip”。 ‘HTTP_ACCEPT_LANGUAGE’当前请求头中 Accept-Language: 项的内容，如果存在的话。例如：”en”。 ‘HTTP_CONNECTION’当前请求头中 Connection: 项的内容，如果存在的话。例如：”Keep-Alive”。 ‘HTTP_HOST’当前请求头中 Host: 项的内容，如果存在的话。 ‘HTTP_REFERER’引导用户代理到当前页的前一页的地址（如果存在）。由 user agent 设置决定。并不是所有的用户代理都会设置该项，有的还提供了修改 HTTP_REFERER 的功能。简言之，该值并不可信。 ‘HTTP_USER_AGENT’当前请求头中 User-Agent: 项的内容，如果存在的话。该字符串表明了访问该页面的用户代理的信息。一个典型的例子是：Mozilla/4.5 [en] (X11; U; Linux 2.2.9 i586)。除此之外，你可以通过 get_browser() 来使用该值，从而定制页面输出以便适应用户代理的性能。 ‘HTTPS’如果脚本是通过 HTTPS 协议被访问，则被设为一个非空的值。 Note: 注意当使用 IIS 上的 ISAPI 方式时，如果不是通过 HTTPS 协议被访问，这个值将为 off。 ‘REMOTE_ADDR’浏览当前页面的用户的 IP 地址。 ‘REMOTE_HOST’浏览当前页面的用户的主机名。DNS 反向解析不依赖于用户的 REMOTE_ADDR。 Note: 你的服务器必须被配置以便产生这个变量。例如在 Apache 中，你需要在 httpd.conf 中设置 HostnameLookups On 来产生它。参见 gethostbyaddr()。 ‘REMOTE_PORT’用户机器上连接到 Web 服务器所使用的端口号。 ‘REMOTE_USER’经验证的用户 ‘REDIRECT_REMOTE_USER’验证的用户，如果请求已在内部重定向。 ‘SCRIPT_FILENAME’当前执行脚本的绝对路径。 Note: 如果在命令行界面（Command Line Interface, CLI）使用相对路径执行脚本，例如 file.php 或 ../file.php，那么 $_SERVER[‘SCRIPT_FILENAME’] 将包含用户指定的相对路径。 ‘SERVER_ADMIN’该值指明了 Apache 服务器配置文件中的 SERVER_ADMIN 参数。如果脚本运行在一个虚拟主机上，则该值是那个虚拟主机的值。 ‘SERVER_PORT’Web 服务器使用的端口。默认值为 “80”。如果使用 SSL 安全连接，则这个值为用户设置的 HTTP 端口。 Note: 在 Apache 2 里，为了获取真实物理端口，必须设置 UseCanonicalName = On 以及 UseCanonicalPhysicalPort = On。 否则此值可能被伪造，不一定会返回真实端口值。 上下文有安全性要求的环境里，不应该依赖此值。 ‘SERVER_SIGNATURE’包含了服务器版本和虚拟主机名的字符串。 ‘PATH_TRANSLATED’当前脚本所在文件系统（非文档根目录）的基本路径。这是在服务器进行虚拟到真实路径的映像后的结果。 Note: 自 PHP 4.3.2 起，PATH_TRANSLATED 在 Apache 2 SAPI 模式下不再和 Apache 1 一样隐含赋值，而是若 Apache 不生成此值，PHP 便自己生成并将其值放入 SCRIPT_FILENAME 服务器常量中。这个修改遵守了 CGI 规范，PATH_TRANSLATED 仅在 PATH_INFO 被定义的条件下才存在。 Apache 2 用户可以在 httpd.conf 中设置 AcceptPathInfo = On 来定义 PATH_INFO。 ‘SCRIPT_NAME’包含当前脚本的路径。这在页面需要指向自己时非常有用。FILE 常量包含当前脚本(例如包含文件)的完整路径和文件名。 ‘REQUEST_URI’URI 用来指定要访问的页面。例如 “/index.html”。 ‘PHP_AUTH_DIGEST’当作为 Apache 模块运行时，进行 HTTP Digest 认证的过程中，此变量被设置成客户端发送的”Authorization” HTTP 头内容（以便作进一步的认证操作）。 ‘PHP_AUTH_USER’当 PHP 运行在 Apache 或 IIS（PHP 5 是 ISAPI）模块方式下，并且正在使用 HTTP 认证功能，这个变量便是用户输入的用户名。 ‘PHP_AUTH_PW’当 PHP 运行在 Apache 或 IIS（PHP 5 是 ISAPI）模块方式下，并且正在使用 HTTP 认证功能，这个变量便是用户输入的密码。 ‘AUTH_TYPE’当 PHP 运行在 Apache 模块方式下，并且正在使用 HTTP 认证功能，这个变量便是认证的类型。 ‘PATH_INFO’包含由客户端提供的、跟在真实脚本名称之后并且在查询语句（query string）之前的路径信息，如果存在的话。例如，如果当前脚本是通过 URL http://www.example.com/php/path_info.php/some/stuff?foo=bar 被访问，那么 $_SERVER[‘PATH_INFO’] 将包含 /some/stuff。 ‘ORIG_PATH_INFO’在被 PHP 处理之前，”PATH_INFO” 的原始版本。]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>PHP知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP高级整理（PHP手册版）]]></title>
    <url>%2F2018%2F01%2F31%2FPHP%E9%AB%98%E7%BA%A7%E6%95%B4%E7%90%86%EF%BC%88PHP%E6%89%8B%E5%86%8C%E7%89%88%EF%BC%89%2F</url>
    <content type="text"><![CDATA[函数（1）用户自定义函数： （2）函数的参数：值传递（默认情况）引用传递：在该参数前面加上符号 &amp; （3）类型声明在PHP 5中，类型声明也被称为类型提示。类型声明允许函数在调用时要求参数为特定类型。 如果给出的值类型不对，那么将会产生一个错误： 在PHP 5中，这将是一个可恢复的致命错误，而在PHP 7中将会抛出一个TypeError异常。（4）返回值：return （5）匿名函数匿名函数（Anonymous functions），也叫闭包函数（closures），允许 临时创建一个没有指定名称的函数。最经常用作回调函数（callback）参数的值。当然，也有其它应用的情况。 类与对象（1）基本概念每个类的定义都以关键字 class 开头，后面跟着类名，后面跟着一对花括号，里面包含有类的属性与方法的定义。一个类可以包含有属于自己的常量，变量（称为”属性”）以及函数（称为”方法”）。当一个方法在类定义内部被调用时，有一个可用的伪变量 $this。$this 是一个到主叫对象的引用（通常是该方法所从属的对象，但如果是从第二个对象静态调用时也可能是另一个对象）。 new要创建一个类的实例，必须使用 new 关键字。当创建新对象时该对象总是被赋值，除非该对象定义了构造函数并且在出错时抛出了一个异常。类应在被实例化之前定义（某些情况下则必须这样）。如果在 new 之后跟着的是一个包含有类名的字符串，则该类的一个实例被创建。如果该类属于一个名字空间，则必须使用其完整名称。 在类定义内部，可以用 new self 和 new parent 创建新对象。当把一个对象已经创建的实例赋给一个新变量时，新变量会访问同一个实例，就和用该对象赋值一样。此行为和给函数传递入实例时一样。可以用克隆给一个已创建的对象建立一个新实例。 extends一个类可以在声明中用 extends 关键字继承另一个类的方法和属性。PHP不支持多重继承，一个类只能继承一个基类。被继承的方法和属性可以通过用同样的名字重新声明被覆盖。但是如果父类定义方法时使用了 final，则该方法不可被覆盖。可以通过 parent:: 来访问被覆盖的方法或属性。当覆盖方法时，参数必须保持一致否则 PHP 将发出 E_STRICT 级别的错误信息。但构造函数例外，构造函数可在被覆盖时使用不同的参数。 匿名类PHP 7 开始支持匿名类。 匿名类很有用，可以创建一次性的简单对象。 （2）属性类的变量成员叫做”属性”，或者叫”字段”、”特征”，在本文档统一称为”属性”。属性声明是由关键字 public，protected 或者 private 开头，然后跟一个普通的变量声明来组成。属性中的变量可以初始化，但是初始化的值必须是常数，这里的常数是指 PHP 脚本在编译阶段时就可以得到其值，而不依赖于运行时的信息才能求值。 在类的成员方法里面，可以用 -&gt;（对象运算符）：$this-&gt;property（其中 property 是该属性名）这种方式来访问非静态属性。静态属性则是用 ::（双冒号）：self::$property 来访问。 类常量：可以把在类中始终保持不变的值定义为常量。在定义和使用常量的时候不需要使用 $ 符号。常量的值必须是一个定值，不能是变量，类属性，数学运算的结果或函数调用。接口（interface）中也可以定义常量。 （3）构造函数与析构函数PHP 5 允行开发者在一个类中定义一个方法作为构造函数。具有构造函数的类会在每次创建新对象时先调用此方法，所以非常适合在使用对象之前做一些初始化工作。 Note: 如果子类中定义了构造函数则不会隐式调用其父类的构造函数。要执行父类的构造函数，需要在子类的构造函数中调用 parent::__construct()。如果子类没有定义构造函数则会如同一个普通的类方法一样从父类继承（假如没有被定义为 private 的话）。 PHP 5 引入了析构函数的概念，这类似于其它面向对象的语言，如 C++。析构函数会在到某个对象的所有引用都被删除或者当对象被显式销毁时执行。 和构造函数一样，父类的析构函数不会被引擎暗中调用。要执行父类的析构函数，必须在子类的析构函数体中显式调用 parent::__destruct()。此外也和构造函数一样，子类如果自己没有定义析构函数则会继承父类的。析构函数即使在使用 exit() 终c止脚本运行时也会被调用。在析构函数中调用 exit() 将会中止其余关闭操作的运行。 （4）访问控制符对属性或方法的访问控制，是通过在前面添加关键字 public（公有），protected（受保护）或 private（私有）来实现的。被定义为公有的类成员可以在任何地方被访问。被定义为受保护的类成员则可以被其自身以及其子类和父类访问。被定义为私有的类成员则只能被其定义所在的类访问。final关键字：PHP 5 新增了一个 final 关键字。如果父类中的方法被声明为 final，则子类无法覆盖该方法。如果一个类被声明为 final，则不能被继承。 （注：类属性必须定义为公有，受保护，私有之一。如果用 var 定义，则被视为公有。类中的方法可以被定义为公有，私有或受保护。如果没有设置这些关键字，则该方法默认为公有。 ） （5）对象继承继承已为大家所熟知的一个程序设计特性，PHP 的对象模型也使用了继承。继承将会影响到类与类，对象与对象之间的关系。比如，当扩展一个类，子类就会继承父类所有公有的和受保护的方法。除非子类覆盖了父类的方法，被继承的方法都会保留其原有功能。继承对于功能的设计和抽象是非常有用的，而且对于类似的对象增加新功能就无须重新再写这些公用的功能。 （6）静态关键字 static声明类属性或方法为静态，就可以不实例化类而直接访问。静态属性不能通过一个类已实例化的对象来访问（但静态方法可以）。（注：由于静态方法不需要通过对象即可调用，所以伪变量 $this 在静态方法中不可用。）注意事项： 静态属性不可以由对象通过 -&gt; 操作符来访问。 用静态方式调用一个非静态方法会导致一个 E_STRICT 级别的错误。 就像其它所有的 PHP 静态变量一样，静态属性只能被初始化为文字或常量，不能使用表达式。所以可以把静态属性初始化为整数或数组，但不能初始化为另一个变量或函数返回值，也不能指向一个对象。 （7）抽象类PHP 5 支持抽象类和抽象方法。定义为抽象的类不能被实例化。任何一个类，如果它里面至少有一个方法是被声明为抽象的，那么这个类就必须被声明为抽象的。被定义为抽象的方法只是声明了其调用方式（参数），不能定义其具体的功能实现。 继承一个抽象类的时候，子类必须定义父类中的所有抽象方法；另外，这些方法的访问控制必须和父类中一样（或者更为宽松）。例如某个抽象方法被声明为受保护的，那么子类中实现的方法就应该声明为受保护的或者公有的，而不能定义为私有的。此外方法的调用方式必须匹配，即类型和所需参数数量必须一致。例如，子类定义了一个可选参数，而父类抽象方法的声明里没有，则两者的声明并无冲突。 这也适用于 PHP 5.4 起的构造函数。在 PHP 5.4 之前的构造函数声明可以不一样的。 （8）对象接口使用接口（interface），可以指定某个类必须实现哪些方法，但不需要定义这些方法的具体内容。接口是通过 interface 关键字来定义的，就像定义一个标准的类一样，但其中定义所有的方法都是空的。接口中定义的所有方法都必须是公有，这是接口的特性。 实现（implements）要实现一个接口，使用 implements 操作符。类中必须实现接口中定义的所有方法，否则会报一个致命错误。类可以实现多个接口，用逗号来分隔多个接口的名称。 Note: 实现多个接口时，接口中的方法不能有重名。Note: 接口也可以继承，通过使用 extends 操作符。Note: 类要实现接口，必须使用和接口中所定义的方法完全一致的方式。否则会导致致命错误。 常量接口中也可以定义常量。接口常量和类常量的使用完全相同，但是不能被子类或子接口所覆盖。 （9）魔术方法__construct()， __destruct()， __call()， __callStatic()， __get()， __set()， __isset()， __unset()， __sleep()， __wakeup()， __toString()， __invoke()， __set_state()， __clone() 和 __debugInfo() 等方法在 PHP 中被称为”魔术方法”（Magic methods）。在命名自己的类方法时不能使用这些方法名，除非是想使用其魔术功能。 10、命名空间（1）概述什么是命名空间？从广义上来说，命名空间是一种封装事物的方法。在很多地方都可以见到这种抽象概念。例如，在操作系统中目录用来将相关文件分组，对于目录中的文件来说，它就扮演了命名空间的角色。具体举个例子，文件 foo.txt 可以同时在目录/home/greg 和 /home/other 中存在，但在同一个目录中不能存在两个 foo.txt 文件。另外，在目录 /home/greg 外访问 foo.txt 文件时，我们必须将目录名以及目录分隔符放在文件名之前得到 /home/greg/foo.txt。这个原理应用到程序设计领域就是命名空间的概念。 在PHP中，命名空间用来解决在编写类库或应用程序时创建可重用的代码如类或函数时碰到的两类问题： 用户编写的代码与PHP内部的类/函数/常量或第三方类/函数/常量之间的名字冲突。 为很长的标识符名称(通常是为了缓解第一类问题而定义的)创建一个别名（或简短）的名称，提高源代码的可读性。 （2）定义命名空间(PHP 5 &gt;= 5.3.0, PHP 7)虽然任意合法的PHP代码都可以包含在命名空间中，但只有以下类型的代码受命名空间的影响，它们是：类（包括抽象类和traits）、接口、函数和常量。 命名空间通过关键字namespace 来声明。如果一个文件中包含命名空间，它必须在其它所有代码之前声明命名空间，除了一个以外：declare关键字。（注：在声明命名空间之前唯一合法的代码是用于定义源文件编码方式的 declare 语句。另外，所有非 PHP 代码包括空白符都不能出现在命名空间的声明之前） （3）使用命名空间：基础在讨论如何使用命名空间之前，必须了解 PHP 是如何知道要使用哪一个命名空间中的元素的。可以将 PHP 命名空间与文件系统作一个简单的类比。在文件系统中访问一个文件有三种方式： 相对文件名形式如foo.txt。它会被解析为 currentdirectory/foo.txt，其中 currentdirectory 表示当前目录。因此如果当前目录是 /home/foo，则该文件名被解析为/home/foo/foo.txt。 相对路径名形式如subdirectory/foo.txt。它会被解析为 currentdirectory/subdirectory/foo.txt。 绝对路径名形式如/main/foo.txt。它会被解析为/main/foo.txt。 PHP 命名空间中的元素使用同样的原理。例如，类名可以通过三种方式引用： 非限定名称，或不包含前缀的类名称，例如 $a=new foo(); 或 foo::staticmethod();。如果当前命名空间是 currentnamespace，foo 将被解析为 currentnamespace\foo。如果使用 foo 的代码是全局的，不包含在任何命名空间中的代码，则 foo 会被解析为foo。 警告：如果命名空间中的函数或常量未定义，则该非限定的函数名称或常量名称会被解析为全局函数名称或常量名称。详情参见 使用命名空间：后备全局函数名称/常量名称。 限定名称,或包含前缀的名称，例如 $a = new subnamespace\foo(); 或 subnamespace\foo::staticmethod();。如果当前的命名空间是 currentnamespace，则 foo 会被解析为 currentnamespace\subnamespace\foo。如果使用 foo 的代码是全局的，不包含在任何命名空间中的代码，foo 会被解析为subnamespace\foo。 完全限定名称，或包含了全局前缀操作符的名称，例如， $a = new \currentnamespace\foo(); 或 \currentnamespace\foo::staticmethod();。在这种情况下，foo 总是被解析为代码中的文字名(literal name)currentnamespace\foo。 （4）使用命名空间：别名/导入允许通过别名引用或导入外部的完全限定名称，是命名空间的一个重要特征。这有点类似于在类 unix 文件系统中可以创建对其它的文件或目录的符号连接。所有支持命名空间的PHP版本支持三种别名或导入方式：为类名称使用别名、为接口使用别名或为命名空间名称使用别名。PHP 5.6开始允许导入函数或常量或者为它们设置别名。在PHP中，别名是通过操作符 use 来实现的. 下面是一个使用所有可能的五种导入方式的例子：12345678910111213141516171819202122232425262728Example #1 使用use操作符导入/使用别名&lt;?phpnamespace foo;use My\Full\Classname as Another;// 下面的例子与 use My\Full\NSname as NSname 相同use My\Full\NSname;// 导入一个全局类use ArrayObject;// importing a function (PHP 5.6+)use function My\Full\functionName;// aliasing a function (PHP 5.6+)use function My\Full\functionName as func;// importing a constant (PHP 5.6+)use const My\Full\CONSTANT;$obj = new namespace\Another; // 实例化 foo\Another 对象$obj = new Another; // 实例化 My\Full\Classname 对象NSname\subns\func(); // 调用函数 My\Full\NSname\subns\func$a = new ArrayObject(array(1)); // 实例化 ArrayObject 对象// 如果不使用 &quot;use \ArrayObject&quot; ，则实例化一个 foo\ArrayObject 对象func(); // calls function My\Full\functionNameecho CONSTANT; // echoes the value of My\Full\CONSTANT?&gt; 注意对命名空间中的名称（包含命名空间分隔符的完全限定名称如 Foo\Bar以及相对的不包含命名空间分隔符的全局名称如 FooBar）来说，前导的反斜杠是不必要的也不推荐的，因为导入的名称必须是完全限定的，不会根据当前的命名空间作相对解析。 （5）名称解析规则在说明名称解析规则之前，我们先看一些重要的定义： 命名空间名称定义① 非限定名称Unqualified name 名称中不包含命名空间分隔符的标识符，例如 Foo② 限定名称Qualified name 名称中含有命名空间分隔符的标识符，例如 Foo\Bar③ 完全限定名称Fully qualified name 名称中包含命名空间分隔符，并以命名空间分隔符开始的标识符，例如 \Foo\Bar。 namespace\Foo 也是一个完全限定名称。 名称解析遵循下列规则： 对完全限定名称的函数，类和常量的调用在编译时解析。例如 new \A\B 解析为类 A\B。 所有的非限定名称和限定名称（非完全限定名称）根据当前的导入规则在编译时进行转换。例如，如果命名空间 A\B\C 被导入为 C，那么对 C\D\e() 的调用就会被转换为 A\B\C\D\e()。 在命名空间内部，所有的没有根据导入规则转换的限定名称均会在其前面加上当前的命名空间名称。例如，在命名空间 A\B 内部调用 C\D\e()，则 C\D\e() 会被转换为 A\B\C\D\e() 。 非限定类名根据当前的导入规则在编译时转换（用全名代替短的导入名称）。例如，如果命名空间 A\B\C 导入为C，则 new C() 被转换为 new A\B\C() 。 在命名空间内部（例如A\B），对非限定名称的函数调用是在运行时解析的。例如对函数 foo() 的调用是这样解析的： 在当前命名空间中查找名为 A\B\foo() 的函数 尝试查找并调用 全局(global) 空间中的函数 foo()。 在命名空间（例如A\B）内部对非限定名称或限定名称类（非完全限定名称）的调用是在运行时解析的。下面是调用 new C() 及 new D\E() 的解析过程： new C()的解析: 在当前命名空间中查找A\B\C类。 尝试自动装载类A\B\C。new D\E()的解析: 在类名称前面加上当前命名空间名称变成：A\B\D\E，然后查找该类。 尝试自动装载类 A\B\D\E。为了引用全局命名空间中的全局类，必须使用完全限定名称 new \C()。]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>PHP知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP数据类型详解（PHP手册版）]]></title>
    <url>%2F2018%2F01%2F31%2FPHP%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%AF%A6%E8%A7%A3%EF%BC%88PHP%E6%89%8B%E5%86%8C%E7%89%88%EF%BC%89%2F</url>
    <content type="text"><![CDATA[数据类型四种标量类型： boolean（布尔型）、integer（整型）、float（浮点型，也称为double）、string（字符串） 两种复合类型： array（数组）、object（对象） 两种特殊类型： resource（资源）、Null 其它伪类型： mixed（混合类型）、number（数字类型）、callback（回调类型）、void（无类型） 布尔型（boolean）：（1）指定一个布尔值，使用常量 TRUE 或 FALSE。两个都不区分大小写。（2）强制转换为布尔型，用 (bool) 或 (boolean) 来强制转换。但是很多情况下不需要强制转换，会被自动转换。当转换为 boolean 时，以下值被认为是 FALSE： 布尔值 FALSE 本身 整型值 0（零） 浮点型值 0.0（零） 空字符串，以及字符串 “0” 不包括任何元素的数组 特殊类型 NULL（包括尚未赋值的变量） 从空标记生成的 SimpleXML 对象 所有其它值都被认为是 TRUE（包括任何资源 和 NAN）。 整型（interger）：（1）整型值可以使用十进制，十六进制，八进制或二进制表示，前面可以加上可选的符号（- 或者 +）。要使用八进制表达，数字前必须加上 0（零）。要使用十六进制表达，数字前必须加上 0x。要使用二进制表达，数字前必须加上 0b。（2）要明确地将一个值转换为 integer，用 (int) 或 (integer) 强制转换。不过大多数情况下都不需要强制转换，因为当运算符，函数或流程控制需要一个 integer 参数时，值会自动转换。还可以通过函数 intval() 来将一个值转换成整型。当从浮点数转换成整数时，将向下取整。 浮点型（float）：（1）浮点型（也叫浮点数 float，双精度数 double 或实数 real）。（2）浮点数的精度浮点数的精度有限。尽管取决于系统，PHP 通常使用 IEEE 754 双精度格式，则由于取整而导致的最大相对误差为 1.11e-16。非基本数学运算可能会给出更大误差，并且要考虑到进行复合运算时的误差传递。（3）NaN某些数学运算会产生一个由常量 NAN 所代表的结果。此结果代表着一个在浮点数运算中未定义或不可表述的值。任何拿此值与其它任何值（除了 TRUE）进行的松散或严格比较的结果都是 FALSE。由于 NAN 代表着任何不同值，不应拿 NAN 去和其它值进行比较，包括其自身，应该用 is_nan() 来检查。 字符串（string）：（1）一个字符串 string 就是由一系列的字符组成，其中每个字符等同于一个字节。这意味着 PHP 只能支持 256 的字符集，因此不支持 Unicode 。（注：Note: string 最大可以达到 2GB。）（2）一个字符串可以用 4 种方式表达： 单引号 ：定义一个字符串的最简单的方法是用单引号把它包围起来（字符 ‘）。 双引号 ：如果字符串是包围在双引号（”）中， PHP 将对一些特殊的字符进行解析。 heredoc 语法结构 ：第三种表达字符串的方法是用 heredoc 句法结构：&lt;&lt;&lt;。在该运算符之后要提供一个标识符，然后换行。接下来是字符串 string 本身，最后要用前面定义的标识符作为结束标志。结束时所引用的标识符必须在该行的第一列，而且，标识符的命名也要像其它标签一样遵守 PHP 的规则：只能包含字母、数字和下划线，并且必须以字母和下划线作为开头。（Heredoc 结构就象是没有使用双引号的双引号字符串，这就是说在 heredoc 结构中单引号不用被转义，但是上文中列出的转义序列还可以使用。变量将被替换，但在 heredoc 结构中含有复杂的变量时要格外小心。 ） nowdoc 语法结构（自 PHP 5.3.0 起）就象 heredoc 结构类似于双引号字符串，Nowdoc 结构是类似于单引号字符串的。Nowdoc 结构很象 heredoc 结构，但是 nowdoc 中不进行解析操作。这种结构很适合用于嵌入 PHP 代码或其它大段文本而无需对其中的特殊字符进行转义。与 SGML 的 &lt;![CDATA[ ]]&gt; 结构是用来声明大段的不用解析的文本类似，nowdoc 结构也有相同的特征。一个 nowdoc 结构也用和 heredocs 结构一样的标记 &lt;&lt;&lt;， 但是跟在后面的标识符要用单引号括起来，即 &lt;&lt;&lt;’EOT’。Heredoc 结构的所有规则也同样适用于 nowdoc 结构，尤其是结束标识符的规则。 （注：函数、方法、静态类变量和类常量只有在 PHP 5 以后才可在 {$} 中使用。然而，只有在该字符串被定义的命名空间中才可以将其值作为变量名来访问。只单一使用花括号 ({}) 无法处理从函数或方法的返回值或者类常量以及类静态变量的值。 ）（3）字符串可以用 ‘.’（点）运算符连接起来，注意 ‘+’（加号）运算符没有这个功能。对于 string 的操作有很多有用的函数，后面予以单独说明。 数组（array）：PHP 中的数组实际上是一个有序映射。映射是一种把 values 关联到 keys 的类型。此类型在很多方面做了优化，因此可以把它当成真正的数组，或列表（向量），散列表（是映射的一种实现），字典，集合，栈，队列以及更多可能性。由于数组元素的值也可以是另一个数组，树形结构和多维数组也是允许的。（1）语法：1234567定义数组 array()可以用 array() 语言结构来新建一个数组。它接受任意数量用逗号分隔的 键（key） =&gt; 值（value）对。 array( key =&gt; value , ... )// 键（key）可是是一个整数 integer 或字符串 string// 值（value）可以是任意类型的值 最后一个数组单元之后的逗号可以省略。通常用于单行数组定义中，例如常用 array(1, 2) 而不是 array(1, 2, )。对多行数组定义通常保留最后一个逗号，这样要添加一个新单元时更方便。自 5.4 起可以使用短数组定义语法，用 [] 替代 array()。（注：key 可以是 integer 或者 string。value 可以是任意类型。） 此外 key 会有如下的强制转换： 包含有合法整型值的字符串会被转换为整型。例如键名 “8” 实际会被储存为 8。但是 “08” 则不会强制转换，因为其不是一个合法的十进制数值。 浮点数也会被转换为整型，意味着其小数部分会被舍去。例如键名 8.7 实际会被储存为 8。 布尔值也会被转换成整型。即键名 true 实际会被储存为 1 而键名 false 会被储存为 0。 Null 会被转换为空字符串，即键名 null 实际会被储存为 “”。 数组和对象不能被用为键名。坚持这么做会导致警告：Illegal offset type。 如果在数组定义中多个单元都使用了同一个键名，则只使用了最后一个，之前的都被覆盖了。 （2）注意事项： ① 方括号和花括号可以互换使用来访问数组单元（例如 $array[42] 和 $array{42} 在上例中效果相同）。 ② 试图访问一个未定义的数组键名与访问任何未定义变量一样：会导致 E_NOTICE 级别错误信息，其结果为 NULL。 ③ 要修改某个值，通过其键名给该单元赋一个新值。要删除某键值对，对其调用 unset() 函数。 ④ 应该始终在用字符串表示的数组索引上加上引号。例如用 $foo[‘bar’] 而不是 $foo[bar]。原因是此代码中有一个未定义的常量（bar）而不是字符串（’bar’－注意引号），而 PHP 可能会在以后定义此常量，不幸的是你的代码中有同样的名字。它能运行，是因为 PHP 自动将裸字符串（没有引号的字符串且不对应于任何已知符号）转换成一个其值为该裸字符串的正常字符串。例如，如果没有常量定义为 bar，PHP 将把它替代为 ‘bar’ 并使用之。 但这并不意味着总是给键名加上引号。用不着给键名为常量或变量的加上引号，否则会使 PHP 不能解析它们。 对象类型（Object）：要创建一个新的对象 object，使用 new 语句实例化一个类。转换为对象：如果将一个对象转换成对象，它将不会有任何变化。如果其它任何类型的值被转换成对象，将会创建一个内置类 stdClass 的实例。如果该值为 NULL，则新的实例为空。 array 转换成 object 将使键名成为属性名并具有相对应的值，除了数字键，不迭代就无法被访问。 资源类型（resource）：资源 resource 是一种特殊变量，保存了到外部资源的一个引用。资源是通过专门的函数来建立和使用的。转换为资源：由于资源类型变量保存有为打开文件、数据库连接、图形画布区域等的特殊句柄，因此将其它类型的值转换为资源没有意义。 NULL：特殊的 NULL 值表示一个变量没有值。NULL 类型唯一可能的值就是 NULL。（注：NULL 类型只有一个值，就是不区分大小写的常量 NULL。）在下列情况下一个变量被认为是 NULL： 被赋值为 NULL。 尚未被赋值。 被 unset()。 许的强制转换有： (int), (integer) - 转换为整形 integer (bool), (boolean) - 转换为布尔类型 boolean (float), (double), (real) - 转换为浮点型 float (string) - 转换为字符串 string (array) - 转换为数组 array (object) - 转换为对象 object (unset) - 转换为 NULL (PHP 5) (binary) 转换和 b 前缀转换支持为 PHP 5.2.1 新增。 （注意在括号内允许有空格和制表符）]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>PHP知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP基础整理（PHP手册版）]]></title>
    <url>%2F2018%2F01%2F31%2FPHP%E5%9F%BA%E7%A1%80%E6%95%B4%E7%90%86%EF%BC%88PHP%E6%89%8B%E5%86%8C%E7%89%88%EF%BC%89%2F</url>
    <content type="text"><![CDATA[PHP概述：PHP（“PHP: Hypertext Preprocessor”，超文本预处理器的字母缩写）是一种被广泛应用的开放源代码的多用途脚本语言，它可嵌入到 HTML中，尤其适合 web 开发。PHP常用于服务端脚本和命令行脚本。服务端脚本。是 PHP 最传统，也是最主要的目标领域。开展这项工作需要具备以下三点：PHP 解析器（CGI 或者服务器模块）、web 服务器和 web 浏览器。需要在运行 web 服务器时，安装并配置 PHP，然后，可以用 web 浏览器来访问 PHP 程序的输出，即浏览服务端的 PHP 页面。一般做 web 开发和 api 开发。命令行脚本。可以编写一段 PHP 脚本，并且不需要任何服务器或者浏览器来运行它。通过这种方式，仅仅只需要 PHP 解析器来执行。这种用法对于依赖 cron（Unix 或者 Linux 环境）或者 Task Scheduler（Windows 环境）的日常运行的脚本来说是理想的选择。这些脚本也可以用来处理简单的文本。 基础语法：1、PHP标记：普通：&lt;?php ?&gt;短标记：&lt;? ?&gt; 通过 php.ini 配置中的 short_open_tag 开启即可。 2、注释单行：//多行：/ / 3、变量（1）基础PHP 中的变量用一个美元符号后面跟变量名来表示。变量名是区分大小写的。变量名与 PHP 中其它的标签一样遵循相同的规则。一个有效的变量名由字母或者下划线开头，后面跟上任意数量的字母，数字，或者下划线。 变量默认总是传值赋值。那也就是说，当将一个表达式的值赋予一个变量时，整个原始表达式的值被赋值到目标变量。这意味着，例如，当一个变量的值赋予另外一个变量时，改变其中一个变量的值，将不会影响到另外一个变量。PHP 也提供了另外一种方式给变量赋值：引用赋值。这意味着新的变量简单的引用（换言之，”成为其别名” 或者 “指向”）了原始变量。改动新的变量将影响到原始变量，反之亦然。使用引用赋值，简单地将一个 &amp; 符号加到将要赋值的变量前（源变量）。 虽然在 PHP 中并不需要初始化变量，但对变量进行初始化是个好习惯。未初始化的变量具有其类型的默认值 - 布尔类型的变量默认值是 FALSE，整形和浮点型变量默认值是零，字符串型变量（例如用于 echo 中）默认值是空字符串以及数组变量的默认值是空数组。 （2）变量范围：变量的范围即它定义的上下文背景（也就是它的生效范围）。大部分的 PHP 变量只有一个单独的范围。这个单独的范围跨度同样包含了 include 和 require 引入的文件。 局部变量：局部函数内部的变量将被限制在局部函数范围内。全局变量：PHP 中全局变量在函数中使用时必须声明为 global。超全局变量：PHP自定义特殊的数组。$GLOBALS、$_POST、$_GET、$_REQUEST、$_COOKIE。静态变量：静态变量仅在局部函数域中存在，但当程序执行离开此作用域时，其值并不丢失。 （3）可变变量：有时候使用可变变量名是很方便的。就是说，一个变量的变量名可以动态的设置和使用。一个普通的变量通过声明来设置，例如：123&lt;?php$a = &apos;hello&apos;;?&gt; 一个可变变量获取了一个普通变量的值作为这个可变变量的变量名。在上面的例子中 hello 使用了两个美元符号（$）以后，就可以作为一个可变变量的变量了。例如： 123&lt;?php$$a = &apos;world&apos;;?&gt; 这时，两个变量都被定义了：$a 的内容是”hello”并且 $hello 的内容是”world”。 要将可变变量用于数组，必须解决一个模棱两可的问题。这就是当写下 $$a[1] 时，解析器需要知道是想要 $a[1] 作为一个变量呢，还是想要 $$a 作为一个变量并取出该变量中索引为 [1] 的值。解决此问题的语法是，对第一种情况用 ${$a[1]}，对第二种情况用 ${$a}[1]。类的属性也可以通过可变属性名来访问。可变属性名将在该调用所处的范围内被解析。例如，对于 $foo-&gt;$bar 表达式，则会在本地范围来解析 $bar 并且其值将被用于 $foo 的属性名。对于 $bar 是数组单元时也是一样。也可使用花括号来给属性名清晰定界。最有用是在属性位于数组中，或者属性名包含有多个部分或者属性名包含有非法字符时。（注意，在 PHP 的函数和类的方法中，超全局变量不能用作可变变量。$this 变量也是一个特殊变量，不能被动态引用。） 5、常量常量是一个简单值的标识符（名字）。如同其名称所暗示的，在脚本执行期间该值不能改变（除了所谓的魔术常量，它们其实不是常量）。常量默认为大小写敏感。传统上常量标识符总是大写的。常量名和其它任何 PHP 标签遵循同样的命名规则。合法的常量名以字母或下划线开始，后面跟着任何字母，数字或下划线。 （1）语法可以用 define() 函数来定义常量，在 PHP 5.3.0 以后，可以使用 const 关键字在类定义之外定义常量。一个常量一旦被定义，就不能再改变或者取消定义。如果只想检查是否定义了某常量，用 defined() 函数。 常量只能包含标量数据（boolean，integer，float 和 string）。可以定义 resource 常量，但应尽量避免，因为会造成不可预料的结果。 可以简单的通过指定其名字来取得常量的值，与变量不同，不应该在常量前面加上 $ 符号。如果常量名是动态的，也可以用函数 constant() 来获取常量的值。用 get_defined_constants() 可以获得所有已定义的常量列表。（注: 常量和（全局）变量在不同的名字空间中。这意味着例如 TRUE 和 $TRUE 是不同的。） 如果使用了一个未定义的常量，PHP 假定想要的是该常量本身的名字，如同用字符串调用它一样（CONSTANT 对应 “CONSTANT”）。此时将发出一个 E_NOTICE 级的错误。参见手册中为什么 $foo[bar] 是错误的（除非事先用 define() 将 bar 定义为一个常量）。 常量和变量有如下不同： 常量前面没有美元符号（$）； 常量只能用 define() 函数定义，而不能通过赋值语句； 常量可以不用理会变量的作用域而在任何地方定义和访问； 常量一旦定义就不能被重新定义或者取消定义； 常量的值只能是标量。 define() 与 const 区别：和使用 define() 来定义常量相反的是，使用 const 关键字定义常量必须处于最顶端的作用区域，因为用此方法是在编译时定义的。这就意味着不能在函数内，循环内以及 if 语句之内用 const 来定义常量。 （2）魔术常量PHP 向它运行的任何脚本提供了大量的预定义常量。不过很多常量都是由不同的扩展库定义的，只有在加载了这些扩展库时才会出现，或者动态加载后，或者在编译时已经包括进去了。有八个魔术常量它们的值随着它们在代码中的位置改变而改变。例如 LINE 的值就依赖于它在脚本中所处的行来决定。这些特殊的常量不区分大小写，如下： 几个 PHP 的”魔术常量” table th:first-of-type{ width: 100px; } 名称 说明 LINE 文件中的当前行号。 FILE 文件的完整路径和文件名。如果用在被包含文件中，则返回被包含的文件名。自 PHP 4.0.2 起，FILE 总是包含一个绝对路径（如果是符号连接，则是解析后的绝对路径），而在此之前的版本有时会包含一个相对路径。 DIR 文件所在的目录。如果用在被包括文件中，则返回被包括的文件所在的目录。它等价于 dirname(FILE)。除非是根目录，否则目录中名不包括末尾的斜杠。（PHP 5.3.0中新增） = FUNCTION 函数名称（PHP 4.3.0 新加）。自 PHP 5 起本常量返回该函数被定义时的名字（区分大小写）。在 PHP 4 中该值总是小写字母的。 CLASS 类的名称（PHP 4.3.0 新加）。自 PHP 5 起本常量返回该类被定义时的名字（区分大小写）。在 PHP 4 中该值总是小写字母的。类名包括其被声明的作用区域（例如 Foo\Bar）。注意自 PHP 5.4 起 CLASS 对 trait 也起作用。当用在 trait 方法中时，CLASS 是调用 trait 方法的类的名字。 TRAIT Trait 的名字（PHP 5.4.0 新加）。自 PHP 5.4 起此常量返回 trait 被定义时的名字（区分大小写）。Trait 名包括其被声明的作用区域（例如 Foo\Bar）。 METHOD 类的方法名（PHP 5.0.0 新加）。返回该方法被定义时的名字（区分大小写）。 NAMESPACE 当前命名空间的名称（区分大小写）。此常量是在编译时定义的（PHP 5.3.0 新增）。 6、运算符运算符优先级 结合方向 运算符 附加信息 无 clone new clone 和 new 左 [ array() 右 ** 算术运算符 右 ++ – ~ (int) (float) (string) (array) (object) (bool) @ 类型和递增／递减 无 instanceof 类型 右 ! 逻辑运算符 左 * / % 算术运算符 左 + - . 算术运算符和字符串运算符 左 &lt;&lt; &gt;&gt; 位运算符 无 &lt; &lt;= &gt; &gt;= 比较运算符 无 == != === !== &lt;&gt; &lt;=&gt; 比较运算符 左 &amp; 位运算符和引用 左 ^ 位运算符 左 &#124; 位运算符 左 &amp;&amp; 逻辑运算符 左 &#124;&#124; 逻辑运算符 左 ?? 比较运算符 左 ? : ternary 右 = += -= *= **= /= .= %= &amp;= &#124;= ^= &lt;&lt;= &gt;&gt;= 赋值运算符 左 and 逻辑运算符 左 xor 逻辑运算符 左 or 逻辑运算符 7、流程控制（1）条件判断if语句：单分支if语句：if(){}else{}多分支if语句：if(){}elseif(){}else{}分支嵌套语句：if(){if(){}}else{} switch语句：switch(expr){ case 0: statement break; case 1: statement break; default: statement break;} （2）循环判断for循环：for(expr1; expr2; expr3){ } foreach循环：仅能够应用于数组和对象，如果尝试应用于其他数据类型的变量，或者未初始化的变量将发出错误信息。foreach(array_expression as $value){ statement}foreach(array_expression as $key=&gt;$value){ statement} while循环：while(){ } do-while循环：do{ }while(); （3）中止语句跳出循环语句：break;跳出本次循环，执行下一个循环：continue;结束函数的执行并将它的值返回：return; （4）加载语句require 和 include 几乎完全一样，除了处理失败的方式不同之外。require 在出错时产生 E_COMPILE_ERROR 级别的错误。换句话说将导致脚本中止而 include 只产生警告（E_WARNING），脚本会继续运行。require：include：唯一区别是 PHP 会检查该文件是否已经被包含过，如果是则不会再次包含。require_once：include_once：]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>PHP知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux VPS、Linux系统防止DDOS攻击脚本[转]]]></title>
    <url>%2F2018%2F01%2F25%2FLinuxVPS%E3%80%81Linux%E7%B3%BB%E7%BB%9F%E9%98%B2%E6%AD%A2DDOS%E6%94%BB%E5%87%BB%E8%84%9A%E6%9C%AC%E3%80%90%E8%BD%AC%E3%80%91%2F</url>
    <content type="text"><![CDATA[转载地址：http://www.1987.name/33.html 互联网上明争暗斗如同现实社会一样，DDOS攻击屡见不鲜，是很多站长烦恼的事情，尤其是个人站长，资金有限，没有硬件防火墙那只能使用软件。Linux系统上第一个想到的替代软件肯定是iptables，但是它不会智能屏蔽攻击IP，需要手动执行，这样也是不现实。本文主要介绍一款配合iptables来智能抵御DDOS攻击的软件：DDoS Deflate。 关于DDOS Deflate脚本DDOS deflate是一个轻量级的脚本，以协助阻止拒绝服务攻击的过程中的bash shell脚本。它使用下面的命令来创建一个连接到服务器的IP地址列表，以及与它们的连接总数 。这是最简单的安装软件的解决方案之一。我已经使用一年多，抵御一般性的DDOS攻击效果还是不错的。 主要原理是超过了预先配置的连接数的IP地址自动被服务器防火墙（iptables）阻止！netstat -ntu | awk &#39;{print $5}&#39; | cut -d: -f1 | sort | uniq -c | sort -n DDOS Deflate的5个主要功能 可以配置白名单的IP地址文件，配置文件路径：/usr/local/ddos/ignore.ip.list 配置文件简单明了，文件路径：/usr/local/ddos/ddos.conf 可以设置被防火墙（iptables）屏蔽的IP地址封锁时间（默认：600秒后自动解除封锁） 可以修改配置文件，脚本可以定时周期性运行（默认是：1分钟） 当遇到攻击，IP被封锁之后可以为指定的邮箱接收电子邮件警报。 DDOS Deflate的安装方法DDOS Deflate非常简单，下载脚本后，直接执行脚本，结束后会显示安装信息，按ESC退出即可。123wget http://www.inetbase.com/scripts/ddos/install.shchmod +x install.sh./install.sh 安装结束后，配置主配文件ddos.conf123456789101112131415161718192021222324252627282930##### Paths of the script and other filesPROGDIR=&quot;/usr/local/ddos&quot; #软件文件存放位置PROG=&quot;/usr/local/ddos/ddos.sh&quot; #主要功能脚本路径IGNORE_IP_LIST=&quot;/usr/local/ddos/ignore.ip.list&quot; #白名单列表路径CRON=&quot;/etc/cron.d/ddos.cron&quot; #定时任务脚本路径APF=&quot;/etc/apf/apf&quot; #APF路径IPT=&quot;/sbin/iptables&quot; #iptables路径##### frequency in minutes for running the script##### Caution: Every time this setting is changed, run the script with --cron##### option so that the new frequency takes effectFREQ=1 #检查周期时间，默认1分钟##### How many connections define a bad IP? Indicate that below.NO_OF_CONNECTIONS=150 #允许客户端与服务器的最大连接数，超过IP就会被屏蔽，一般保持默认即可##### APF_BAN=1 (Make sure your APF version is atleast 0.96)##### APF_BAN=0 (Uses iptables for banning ips instead of APF)APF_BAN=0 #数字1为使用APF，数字0为使用iptables，这里推荐使用iptables ##### KILL=0 (Bad IPs are&apos;nt banned, good for interactive execution of script)##### KILL=1 (Recommended setting)KILL=1 #是否屏蔽IP，当然是屏蔽，默认即可##### An email is sent to the following address when an IP is banned.##### Blank would suppress sending of mailsEMAIL_TO=&quot;admin@1987.name&quot; #指定一个 电子邮件，用于发送警报 ##### Number of seconds the banned ip should remain in blacklist.BAN_PERIOD=600 #屏蔽时间，这里自由设定 配置文件中提到的APF，它也是linux系统中防火墙之一，这里稍作介绍：APF（Advanced Policy Firewall），是 Rf-x Networks 出品的Linux环境下的软件防火墙。APF采用Linux系统默认的 iptables 规则。APF可以算是Linux中最出名的软件防火墙之一。 为DDOS Deflate开启相关服务开启iptablesservice iptables start开启crontab，定时任务service crond start 如何卸载DDOS Deflate123wget http://www.inetbase.com/scripts/ddos/uninstall.ddoschmod +x uninstall.ddos./uninstall.ddos 希望遇到DDOS攻击的朋友使用此软件能解决头疼问题，也感谢软件作者。]]></content>
  </entry>
  <entry>
    <title><![CDATA[我的vim标配]]></title>
    <url>%2F2018%2F01%2F24%2F%E6%88%91%E7%9A%84vim%E6%A0%87%E9%85%8D%2F</url>
    <content type="text"><![CDATA[在Linux下开发程序，一般都会离不开vi编辑器，而vim是vi的进阶版IDE。目前vim已经取代了vi编辑器，正常使用vi的时候就默认启用的是vim编辑器，不信你可自己查看，在bash命令行下输入”alias”即可看到是否vim已经取代了vi编辑器。但vim的用法与vi编辑器是一模一样，所以，不用担心使用过vi，而不会使用vim，只不过vim有更高级的用法罢了。而vim的配置就是其中一个非常好的用法。一千个vimer就有一千个配置，vim的配置之多让人眩目，但常用的基本配置也就那些——我的vim标配。首先，要知道在什么地方配置vim，一般只修改用户自己的vim配置文件即可(~/.vimrc)。下面给出我的配置清单：1234567set tabstop=4 &quot;设定tab缩进空格数set expandtab &quot;设定tab自动转为空格set number &quot;set nu 显示行号set hlsearch &quot;搜索时高亮显示被找到的文本set syntax=on &quot;自动语法高亮set smartindent &quot;开启新行时使用智能自动缩进set showmatch &quot;自动匹配括号 （注：vim配置注释，使用双引号 “ 注释。） 常用配置：“显示行号set nu/nonu “语法高亮syntax on/off “tab缩进set tabstop=4set shiftwidth=4set expandtabset smarttab “智能缩进set smartindent（表示在换行的时候光标进行智能缩进，触发智能缩进的场景有：以”{“结尾，以C语言关键字开头，可以通过:h cinwords进行查看，包括：if, else, while, do, for, switch，以”}”开头，此情况只有在执行”O”命令时候有效，但是这个配置有一点问题，就是当换行之后本来已经处于缩进状态了，如果输入”#”，那么缩进就会取消，”#”就会被插入到第一列，可是我们在写Python代码的时候，往往不希望这样做，所以我们可以增加配置”:inoremap # X^H”，其中 ^H 是先按CTRL-V再按CTRL-H输入的。） “高亮被搜索的字符set hlsearch “背景色set background=dark “显示匹配（可自动匹配括号）set showmatch 待补录…]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 用户与账号管理]]></title>
    <url>%2F2018%2F01%2F22%2FLinux%E7%94%A8%E6%88%B7%E4%B8%8E%E8%B4%A6%E5%8F%B7%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Linux是个多用户多任务的分时操作系统，所有一个要使用系统资源的用户都必须先向系统管理员申请一个账号，然后以这个账号的身份进入系统。用户的账号一方面能帮助系统管理员对使用系统的用户进行跟踪，并控制他们对系统资源的访问；另一方面也能帮助用户组织文件，并为用户提供安全性保护。每个用户账号都拥有一个惟一的用户名和用户口令。用户在登录时键入正确的用户名和口令后，才能进入系统和自己的主目录。 实现用户账号的管理，要完成的工作主要有如下几个方面：a.用户账号的管理。b.用户口令的管理。c.用户组的管理。d.用户切换 用户账号的管理：添加、删除和修改：1、新增用户：useradd1234567891011121314[root@www ~]# useradd [-u UID] [-g 初始群组] [-G 次要群组] [-mM]\&gt; [-c 说明栏] [-d 家目录绝对路径] [-s shell] 使用者账号名选项与参数： -u ：后面接的是 UID ，是一组数字。直接指定一个特定的 UID 给这个账号； -g ：后面接的那个组名就是用户所属的用户组。该群组的 GID 会被放置到 /etc/passwd 的第四个字段内。 -G ：后面接的组名则是这个账号还可以加入的群组，即附加组。这个选项与参数会修改 /etc/group 内的相关资料！ -M ：强制！不要建立用户家目录！(系统账号默认值) -m ：强制！要建立用户家目录！(一般账号默认值) -c ：这个就是 /etc/passwd 的第五栏的说明内容(comment)！可以随便我们设定 -d ：指定某个目录成为家目录，而不要使用默认值。务必使用绝对路径！如果此目录不存在，则同时使用-m选项，能创建主目录。 -s ：后面接一个 shell ，若没有指定则预设是 /bin/bash -r ：建立一个系统的账号，这个账号的 UID 会有限制 (参考 /etc/login.defs) -e ：后面接一个日期，格式为【YYYY-MM-DD】此项目可写入 shadow 第八字段，亦即账号失效日的设定项目； -f ：后面接 shadow 的第七字段项目，指定密码是否会失效。0 为立刻失效，-1 为永远不失效(密码只会过期而强制于登入时重新设定而已。) 范例一：完全参考默认值建立一个用户，名称为 ben12345678[root@www ~]# useradd ben[root@www ~]# ll -d /home/bendrwx------ 4 ben ben 4096 Feb 25 09:38 /home/ben# 默认会建立用户家目录，且权限为 700 ！这是重点！[root@www ~]# grep ben /etc/passwd /etc/shadow /etc/group/etc/passwd:ben:x:504:505::/home/ben:/bin/bash/etc/shadow:ben:!!:14300:0:99999:7:::/etc/group:ben:x:505: &lt;==预设会建立一个与账号一模一样的群组名 其实系统已经帮我们规定好非常多的默认值了，所以我们可以简单的使用【 useradd 账号 】来建立使用者即可。CentOS 这些默认值主要会帮我们处理几个项目： 在 /etc/passwd 里面建立一行与账号相关的数据，包括建立 UID/GID/ 家目录等； 在 /etc/shadow 里面将此账号的密码相关参数填入，但是尚未有密码； 在 /etc/group 里面加入一个与账号名称一模一样的组名； 在 /home 底下建立一个与账号同名的目录作为用户家目录，且权限为 700 由于在 /etc/shadow 内仅会有密码参数而不会有加密过的密码数据，因此我们在建立使用者账号时，还需要使用【 passwd 账号 】来给予密码才算是完成了用户建立的流程。 2、删除用户：userdel这个功能就太简单了，目的在删除用户的相关数据，而用户的数据有： 用户账号/密码相关参数：/etc/passwd, /etc/shadow 使用者群组相关参数：/etc/group, /etc/gshadow 用户个人档案数据： /home/username, /var/spool/mail/username.. 整个指令的语法非常简单：123[root@www ~]# userdel [-r] username选项与参数： -r ：连同用户的家目录也一起删除 范例一：删除 ben ，连同家目录一起删除[root@www ~]# userdel -r ben 这个指令下达的时候要小心了！通常我们要移除一个账号的时候，你可以手动的将 /etc/passwd 与 /etc/shadow 里头的该账号取消即可！一般而言，如果该账号只是【暂时不启用】的话，那么将 /etc/shadow 里头账号失效日期 (第八字段) 设定为 0 就可以让该账号无法使用，但是所有跟该账号相关的数据都会留下来！使用 userdel 的时机通常是【你真的确定不要让该用户在主机上面使用任何数据了！】 3、修改用户：usermod（注：usermod 就是用来微调 useradd 增加的使用者参数）1234567891011121314[root@www ~]# usermod [-cdegGlsuLU] username选项与参数： -c ：后面接账号的说明，即 /etc/passwd 第五栏的说明栏，可以加入一些账号的说明。 -d ：后面接账号的家目录，即修改 /etc/passwd 的第六栏； -e ：后面接日期，格式是 YYYY-MM-DD 也就是在 /etc/shadow 内的第八个字段数据！ -f ：后面接天数，为 shadow 的第七字段。 -g ：后面接初始群组，修改 /etc/passwd 的第四个字段，亦即是 GID 的字段！ -G ：后面接次要群组，修改这个使用者能够支持的群组，修改的是 /etc/group ！ -a ：与 -G 合用，可【增加次要群组的支持】而非【设定】！ -l ：后面接账号名称。亦即是修改账号名称， /etc/passwd 的第一栏！ -s ：后面接 Shell 的实际档案，例如 /bin/bash 或 /bin/csh 等等。 -u ：后面接 UID 数字！即 /etc/passwd 第三栏的资料； -L ：暂时将用户的密码冻结，让他无法登入。其实仅改 /etc/shadow 的密码栏。 -U ：将 /etc/shadow 密码栏的 ! 拿掉，解冻！ 4、相关用户功能：finger：查阅用户相关的信息1234[root@www ~]# finger [-s] username选项与参数： -s ：仅列出用户的账号、全名、终端机代号与登入时间等等； -m ：列出与后面接的账号相同者，而不是利用部分比对 (包括全名部分) id：查询某人或自己的相关 UID/GID 等信息[root@www ~]# id [username] 用户口令的管理：passwd使用 useradd 建立了账号之后，在预设的情况下，该账号是暂时被封锁的，也就是说，该账号是无法登入的，你可以去瞧一瞧 /etc/shadow 内的第二个字段就知道了。那该如何是好？使用 passwd 设定密码之后就可使用了。123456789101112[root@www ~]# passwd [--sdtin] &lt;==所有人均可使用来改自己的密码[root@www ~]# passwd [-l] [-u] [--sdtin] [-S] \&gt; [-n 日数] [-x 日数] [-w 日数] [-i 日期] 账号 &lt;==root 功能选项与参数： --stdin ：可以透过来自前一个管线的数据，作为密码输入，对 shell script 有帮助！ -l ：是 Lock 的意思，会将 /etc/shadow 第二栏最前面加上 ! 使密码失效； -u ：与 -l 相对，是 Unlock 的意思！ -S ：列出密码相关参数，亦即 shadow 档案内的大部分信息。 -n ：后面接天数，shadow 的第 4 字段，多久不可修改密码天数 -x ：后面接天数，shadow 的第 5 字段，多久内必须要更动密码 -w ：后面接天数，shadow 的第 6 字段，密码过期前的警告天数 -i ：后面接【日期】，shadow 的第 7 字段，密码失效日期 理论上，你的密码最好符合如下要求： 密码不能与账号相同； 密码尽量不要选用字典里面会出现的字符串； 密码需要超过 8 个字符； 密码不要使用个人信息，如身份证、手机号码、其他电话号码等； 密码不要使用简单的关系式，如 1+1=2， Iamben 等； 密码尽量使用大小写字符、数字、特殊字符($,_,-等)的组合。 用户组的管理：新增、删除与修改每个用户都有一个用户组，系统能对一个用户组中的所有用户进行集中管理。不同Linux系统对用户组的规定有所不同，如Linux下的用户属于和他同名的用户组，这个用户组在创建用户时同时创建。用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就对/etc/group文件的更新。用户组（group）就是具有相同特征的用户（user）的集合体；比如有时我们要让多个用户具有相同的权限，比如查看、修改某一文件或执行某个命令，这时我们需要用户组，我们把用户都定义到同一用户组，我们通过修改文件或目录的权限，让用户组具有一定的操作权限，这样用户组下的用户对该文件或目录都具有相同的权限，这是我们通过定义组和修改文件的权限来实现的； 1、新增群组：groupadd1234[root@www ~]# groupadd [-g gid] [-r] 组名选项与参数： -g ：后面接某个特定的 GID ，用来直接给予某个 GID -r ：建立系统群组！与 /etc/login.defs 内的 GID_MIN 有关。 范例一：新建一个群组，名称为 group112345[root@www ~]# groupadd group1[root@www ~]# grep group1 /etc/group /etc/gshadow/etc/group:group1:x:702:/etc/gshadow:group1:!::# 群组的 GID 也是会由 500 以上最大 GID+1 来决定！ 2、修改群组：groupmod跟 usermod 类似的，这个指令仅是在进行 group 相关参数的修改而已。1234[root@www ~]# groupmod [-g gid] [-n group_name] 群组名选项与参数： -g ：修改既有的 GID 数字； -n ：修改既有的组名 范例一：将刚刚上个指令建立的 group1 名称改为 mygroup ， GID 为 2011234[root@www ~]# groupmod -g 201 -n mygroup group1[root@www ~]# grep mygroup /etc/group /etc/gshadow/etc/group:mygroup:x:201:/etc/gshadow:mygroup:!:: 3、删除群组：groupdel[root@www ~]# groupdel [groupname]范例一：将刚刚的 mygroup 删除！[root@www ~]# groupdel mygroup范例二：若要删除 ben1 这个群组的话？12[root@www ~]# groupdel ben1groupdel: cannot remove user&apos;s primary group. 为什么 mygroup 可以删除，但是 ben1 就不能删除呢？原因很简单，【有某个账号 (/etc/passwd)的 initial group 使用该群组！】如果查阅一下，你会发现在 /etc/passwd 内的 vbird1 第四栏的 GID 就是 /etc/group 内的 ben1 那个群组的 GID ，所以，无法删除！否则 ben1 这个用户登入系统后，就会找不到 GID ，那可是会造成很大的麻烦！那么如果硬要要删除 ben1 这个群组呢？你【必须要确认 /etc/passwd 内的账号没有任何人使用该群组作为 initial group 】才行。你可以： 修改 ben1 的 GID ，或者是： 删除 ben1 这个使用者。 用户的身份切换在 Linux 系统当中要作身份的变换？这是为啥？可能有底下几个原因！ 使用一般账号：系统平日操作的好习惯事实上，为了安全的缘故，一些老人家都会建议你，尽量以一般身份使用者来操作 Linux 的日常作业！等到需要设定系统环境时，才变换身份成为 root 来进行系统管理，相对比较安全！避免作错一些严重的指令，例如恐怖的【 rm -rf / 】(千万作不得！)用较低权限启动系统服务相对于系统安全，有的时候，我们必须要以某些系统账号来进行程序的执行。举例来说，Linux 主机上面的一套软件，名称为 apache ，我们可以额外建立一个名为 apache 的用户来启动 apache 软件，如此一来，如果这个程序被攻破，至少系统还不至于就损毁了！软件本身的限制在远古时代的 telnet 程序中，该程序默认是不许使用 root 的身份登入的，telnet 会判断登入者的 UID，若 UID 为 0 的话，那就直接拒绝登入了。所以，你只能使用一般使用者来登入 Linux 服务器。此外，ssh 也可以设定拒绝 root 登入！那如果你有系统设定需求该如何是好？就变换身份！ 由于上述考虑，所以我们都是使用一般账号登入系统的，等有需要进行系统维护或软件更新时才转为 root 的身份来动作。那如何让一般使用者转变身份成为 root 呢？主要有两种方式： 以【 su - 】直接将身份变成 root 即可，但是这个指令即需要 root 的密码，也就是说，如果你要以 su 变成 root 的话，你的一般使用者就必须要有 root 的密码才行； 以【 sudo 指令 】执行 root 的指令串，由于 sudo 需要事先设定妥当，且 sudo 需要输入用户自己的密码，因此多人共管同一部主机时，sudo 要比 su 来的好！至少 root 密码不会流出去！ 1、susu 是最简单的身份切换指令了，他可以进行任何身份的切换！方法如下：1234567[root@www ~]# su [-lm] [-c 指令] [username]选项与参数： - ：单纯使用 - 如【 su - 】代表使用 login-shell 的变量档案读取方式来登入系统； 若使用者名称没有加上去，则代表切换为 root 的身份。 -l ：与 - 类似，但后面需要加欲切换的使用者账号！也是 login-shell 的方式。 -m ：-m 与 -p 是一样的，表示【使用目前的环境设定，而不读取新使用者的配置文件】 -c ：仅进行一次指令，所以 -c 后面可以加上指令！ 2、sudo相对于 su 需要了解新切换的用户密码 (常常是需要 root 的密码)，sudo 的执行则仅需要自己的密码即可！甚至可以设定不需要密码即可执行 sudo ！由于 sudo 可以让你以其他用户的身份执行指令 (通常是使用 root 的身份来执行指令)，因此并非所有人都能够执行 sudo ，而是仅有规范到 /etc/sudoers 内的用户才能够执行 sudo 这个指令！1234[root@www ~]# sudo [-b] [-u 新使用者账号]选项与参数： -b ：将后续的指令放到背景中让系统自行执行，而不与目前的 shell 产生影响 -u ：后面可以接欲切换的使用者，若无此项则代表切换身份为 root 。 范例一：你想要以 sshd 的身份在 /tmp 底下建立一个名为 mysshd 的档案1234[root@www ~]# sudo -u sshd touch /tmp/mysshd[root@www ~]# ll /tmp/mysshd-rw-r--r-- 1 sshd sshd 0 Feb 28 17:42 /tmp/mysshd# 特别留意，这个档案的权限是由 sshd 所建立的情况！ 范例二：你想要以 ben1 的身份建立 ~ben/www 并于其中建立 index.html档案1234567[root@www ~]# sudo -u ben1 sh -c &quot;mkdir ~ben1/www; cd ben1/www; \&gt; echo &apos;This is index.html file&apos; &gt; index.html&quot;[root@www ~]# ll -a ~ben1/wwwdrwxr-xr-x 2 ben1 ben1 4096 Feb 28 17:51 .drwx------ 5 ben1 ben1 4096 Feb 28 17:51 ..-rw-r--r-- 1 ben1 ben1 24 Feb 28 17:51 index.html# 要注意，建立者的身份是 ben1 ，且我们使用 sh -c &quot;一串指令&quot; 来执行的！ sudo 可以让你切换身份来进行某项任务，例如上面的两个范例。 sudo 的执行是这样的流程：1.当用户执行 sudo 时，系统于 /etc/sudoers 档案中搜寻该使用者是否有执行 sudo 的权限；2.若使用者具有可执行 sudo 的权限后，便让使用者【输入用户自己的密码】来确认；3.若密码输入成功，便开始进行 sudo 后续接的指令(但 root 执行 sudo 时，不需要输入密码)；4.若欲切换的身份与执行者身份相同，那也不需要输入密码。 所以说，sudo 执行的重点是：【能否使用 sudo 必须要看 /etc/sudoers 的设定值，而可使用 sudo 者是透过输入用户自己的密码来执行后续的指令串】！由于能否使用与 /etc/sudoers 有关，所以我们当然要去编辑 sudoers 档案！不过，因为该档案的内容是有一定的规范的，因此直接使用 vi 去编辑是不好的。此时，我们得要透过 visudo 去修改这个档案！（需要修改 /etc/sudoers 文件再查找……）]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜基础篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 档案权限与目录配置]]></title>
    <url>%2F2018%2F01%2F22%2FLinux%E6%A1%A3%E6%A1%88%E6%9D%83%E9%99%90%E4%B8%8E%E7%9B%AE%E5%BD%95%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Linux最优秀的地方之一，就在于他的多人多任务环境。而为了让各个使用者具有较保密的档案数据，因此档案的权限管理就变的很重要了。Linux一般将档案可存取的身份分为三个类别，分别是owner/group/others，且三种身份各有read/write/execute等权限 1、身份类别 档案拥有者User 群组概念Group 其他人Others root 对应记录数据的档案： 各种身份的相关信息：/etc/passwd 个人的密码：/etc/shadow 组名:/etc/group 2、权限的意义（1）对档案 r：可读取此档案的实际内容w：可编辑，新增或者修改此档案的内容（但不含删除该档案）x：该档案具有可以被系统执行的权限（Windows底下一个档案是否具有执行的能力是藉由【扩展名】来判断的，例如：.exe, .bat, .com等等，但是在Linux底下，我们的档案是否能被执行，则是藉由是否具有【x】这个权限来决定的） （2）对目录 r：可读取目录结构列表，可查询该目录下的文件名数据，ls命令可显示它们w：可新建目录与档案，可删除已存在的目录与档案，可更改他们的名字，可移动它们的位置x：用户能进入该目录成为工作目录，cd命令可进入它，如果没有x权限，不能执行它下面的任何指令（注：对目录的 w 权限要谨慎给予！） 案例：账户为dmtai的家目录为/home/dmtai，账户dmtai对这个目录有rwx的权限。此目录下有一个档案叫the_root.data，它的权限为：-rwx------ 1 root root 4356 Sep 19 20:20 the_root.data分析：档案对于账户dmtai来说属于others身份，所以这个档案，dmtai无法读取，无法编辑也无法执行。但是dmtai对目录dmtai具有rwx的权限，也就是说dmtai账户可以删除这个档案！！ 3、Linux文件属性123ls -al-rw-r--r-- 1 root root 42304 Sep 4 18:26 install.log[档案类型][档案类型权限][连接数][User][Group][档案容量bytes][档案最后修改时间][档案名] 各栏详解如下： （1）第一栏表示档案类型权限：总共10个字符，第一个字符代表这个档案类型： ① 正规档案[-]：纯文本档（ASCII），二进制文件（binary），数据格式文件（data） ② 目录[d] ③ 连结档[l] ④ 设备与装置文件 1）区块设备档[b]：可随机存取装置（比如硬盘，软盘） 2）字符设备文件[c]：一次性读取装置（比如键盘，鼠标） ⑤ 资料接口文件（sockets）[s]：称为数据接口文件 ⑥ 数据输送文件（FIFO,pipe）[p]：特殊的文件类型接下来9个字符3个一组，三组权限分别对应为：User，Group，Others（2）第二栏表示有多少档名连结到此节点(i-node)：每个档案都会将他的权限与属性记录到文件系统的 i-node 中，不过，我们使用的目录树却是使用文件名来记录，因此每个档名就会连结到一个 i-node ！这个属性记录的，就是有多少不同的档名连结到相同的一个 i-node 号码就是了。（3）第三栏表示这个档案(或目录)的【拥有者账号】（4）第四栏表示这个档案的所属群组（5）第五栏为这个档案的容量大小，默认单位为bytes；（6）第六栏为这个档案的建档日期或者是最近的修改日期：（7）第七栏为这个档案的档名 4、如何改变文件属性与权限（1）chgrp：改变档案所属群组基本语法：chgrp [-R] groupname filename选项与参数： -R：进行递归的持续变更，即目录下的所有档案，目录都更新为属于这个新群组示例：chgrp users install.log #users这个群组必须存在于/etc/group记录的群组里 （2）chown：改变档案拥有者基本语法：chown [-R] ownername[:groupname] filename选项与参数： -R：进行递归的持续变更，即目录下的所有档案，目录都更新为属于这个新拥有者示例：chown bin install.log #bin这个拥有者必须存在于/etc/passwd记录的拥有者里 #应用：cp 源档案 目标档案后，新的使用者可能无法使用copy后的档案，这时就需要赋予新档案一个新的拥有者和新的群组了。 （3）chmod：改变档案的权限，SUID，SGID，SBIT等特性 数字类型改变档案权限各权限的分数如下：r:4 w:2 x:1每种身份各自的权限都是以上3个相加得到。如7=4+2+1,5=4+0+1（不可能是5=2+2+1哦，因为第一个肯定是r，数值要不是4要不是0，不能是2）基本语法：chmod [-R] xyz 档案或目录选项与参数： xyz : 就是刚刚提到的数字类型的权限属性，为 rwx 属性数值的相加。 -R : 进行递归(recursive)的持续变更，亦即连同次目录下的所有档案都会变更示例：chmod 770 .bashrc 符号类型改变档案权限基本语法：chmod ugoa +-= rwx 档案或目录 #a代表all即全部的身份示例：chmod u=rwx,go=rx .bashrc #u=rwx,go=rx是连在一起的，中间没有任何空格符 5、Linux 目录配置（1）目录配置依据：FHS（Filesystem Hierarchy Standard）亊实上，FHS 是根据过去的经验一直再持续的改版的，FHS 依据文件系统使用的频繁与否与是否允许使用者随意更动，而将目录定义成为四种交互作用的形态，用表格来说有点像底下这样： 上表中的目录就是一些代表性的目录，该目录底下所放置的数据在底下会谈到，这里先略过不谈。我们要了解的是，什么是那四个类型？ 可分享的：可以分享给其他系统挂载使用的目录，所以包括执行文件与用户的邮件等数据，是能够分享给网络上其他主机挂载用的目录； 不可分享的：自己机器上面运作的装置档案或者是与程序有关的 socket 档案等，由于仅与自身机器有关，所以当然就不适合分享给其他主机了。 不变的：有些数据是不会经常变动的，跟随着 distribution 而不变动。例如函式库、文件说明文件、系统管理员所管理的主机服务配置文件等等； 可变动的：经常改变的数据，例如登录文件、一般用户可自行收受的新闻组等。 亊实上，FHS 针对目录树架构仅定义出三层目录底下应该放置什么数据而已，分别是底下这三个目录的定义： / (root, 根目录)：与开机系统有关； /usr (unix software resource)：与软件安装/执行有关； /var (variable)：与系统运作过程有关。 （2）根目录 (/) 的意义与内容：根目录是整个系统最重要的一个目录，因为不但所有的目录都是由根目录衍生出来的，同时根目录也与开机/还原/系统修复等动作有关。由于系统开机时需要特定的开机软件、核心档案、开机所需程序、函式库等等档案数据，若系统出现错误时，根目录也必项要包含有能够修复文件系统的程序才行。因为根目录是这么的重要，所以在 FHS 的要求方面，他希服根目录不要放在非常大的分割槽内，因为越大的分割槽你会放入越多的数据，如此一来根目录所在分割槽就可能会有较多发生错误的机会。 因此 FHS 标准建议：根目录(/)所在分割槽应该越小越好，且应用程序所安装的软件最好不要与根目录放在同一个分割槽内，保持根目录越小越好。如此不但效能较佳，根目录所在的文件系统也较不容易发生问题。 有鉴于上述的说明，因此 FHS 定义出根目录(/)底下应该要有底下这些次目录的存在才好： 另外要注意的是，因为根目录与开机有关，开机过程中仅有根目录会被挂载，其他分割槽则是在开机完成之后才会持续的进行挂载的行为。就是因为如此，因此根目录下与开机过程有关的目录，就不能够与根目录放到不同的分割槽去！那哪些目录不可与根目录分开呢？有底下这些： /etc：配置文件 /bin：重要执行档 /dev：所需要的装置档案 /lib：执行档所需的函式库不核心所需的模块 /sbin：重要的系统执行文件 （3）/usr 的意义与内容：依据FHS 的基本定义，/usr 里面放置的数据属于可分享的与不可变动的(shareable, static)，如果你知道如何透过网络进行分割槽的挂载，那么 /usr 确实可以分享给局域网络内的其他主机来使用！ 很多读者都会误会 /usr 为 user 的缩写，其实 usr 是 Unix Software Resource 的缩写，也就是【Unix 操作系统软件资源】所放置的目录，而不是用户的数据！这点要注意。FHS 建议所有软件开发者，应该将他们的数据合理的分别放置到这个目录下的次目录，而不要自行建立该软件自己独立的目录。 因为是所有系统默认的软件(distribution 发布者提供的软件)都会放置到 /usr 底下，因此这个目录有点类似 Windows 系统的【C:\Windows\ + C:\Program files\】这两个目录的综合体，系统刚安装完毕时，这个目录会占用最多的硬盘容量。一般来说，/usr 的次目录建议有底下这些： （4）/var 的意义与内容：如果 /usr 是安装时会占用较大硬盘容量的目录，那么 /var 就是在系统运作后才会渐渐占用硬盘容量的目录。因为 /var 目录主要针对常态性变动的档案，包括快取(cache)、登录档(log file)以及某些软件运作所产生的档案，包括程序档案(lock file, run file)，或者例如 MySQL 数据库的档案等等。常见的次目录有：]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜基础篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些常用服务器概述]]></title>
    <url>%2F2018%2F01%2F22%2F%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[NAT服务器NAT服务器可以说为是一个路由器的延伸服务器，简单地说，你可以称他为内部LAN主机的【IP分享器】。NAT 的全名是 Network Address Translation，字面上的意思是【网络地址的转换】。下面来看一下，若内部 LAN 有任何一部主机想要传送封包出去时，那么这个封包要如何透过 Linux 主机而传送出去？他是这样的： 1.先经过 NAT table 的 PREROUTING 链；2.经由路由判断确定这个封包是要进入本机与否，若不进入本机，则下一步；3.再经过 Filter table 的 FORWARD 链；4.通过 NAT table 的 POSTROUTING 链，最后传送出去。 NAT 服务器的重点就在于上面流程的第 1,4 步骤，也就是 NAT table 的两条重要的链：PREROUTING 与 POSTROUTING。那这两条链有什么重要的功能呢？重点在于修改 IP！但是这两条链修改的 IP 是不一样的！POSTROUTING 在修改来源 IP，PREROUTING 则在修改目标 IP。由于修改的 IP 不一样，所以就称为来源 NAT(Source NAT, SNAT) 及目标 NAT(Destination NAT, DNAT)。 DHCP服务器动态主机设定协议( Dynamic Host Configuration Protocol ，DHCP )，这个服务可以自动的分配 IP 与相关的网络参数给客户端，来提供客户端自动以服务器提供的参数来设定他们的网络。使用者只要将自己的笔电设定好经由 DHCP 协议来取得网络参数后，一插上网络线，马上就可以享受 Internet 的服务 要设定好一个网络的环境，使计算机可以顺利的连上 Internet ，那么你的计算机里面一定要有底下几个网络的参数才行，分别是： IP, netmask, network, broadcast, gateway, DNS IP 其中，那个 IP, netmask, network, broadcast 与 gateway 都可以在 /etc/sysconfig/network-scripts/ifcfg-eth[0-n] 这档案里面设定，DNS 服务器的地址则是在 /etc/resolv.conf 里头设定。只要这几个项目设定正确，那么计算机应该就没问题的可以上网了！所以说，你家里面的 3, 4 部计算机，你都可以手动的来设定好你所需要的网络参数，然后利用 NAT 服务器的功能，就可以连上 Internet 了！ 如果是管理的学生计算机大概有 100 部好了，那么你怎么设定好这 100 部的计算机呢？这个就是 DHCP 服务器最主要的工作，就是自动的将网络参数正确的分配给网域中的每部计算机，让客户端的计算机可以在开机的时候就立即自动的设定好网络的参数值，这些参数值可以包括了 IP、netmask、network、gateway 与 DNS 的地址等等。如此一来，身为管理员的你，只要注意到这一部提供网络参数的主机有没有挂掉就好了，其他同学们的个人计算机！你想都不必想要怎么去帮忙！因为 DHCP 主机已经完全都帮你搞定了！ SSH服务器什么是 SSH 呢？它有什么特异功能？简单的来说，SSH 是 Secure SHell protocol 的简写 (安全的壳程序协议)，它可以透过数据封包加密技术，将等待传输的封包加密后再传输到网络上，因此，数据讯息就比较安全！这个 SSH 可以用来取代较不安全的 finger, R Shell (rcp, rlogin, rsh等), talk 及 telnet 等联机模式。特别注意：这个 SSH 协议，在预设的状态中，本身就提供两个服务器功能： 一个就是类似 telnet 的远程联机使用 shell 的服务器，亦即是俗称的 ssh ； 另一个就是类似 FTP 服务的 sftp-server ！提供更安全的 FTP 服务。 FTP服务器FTP (File Transfer Protocol) 可说是最古老的协议之一了，主要是用来进行档案的传输，尤其是大型档案的传输使用 FTP 更是方便！不过，值得注意的是，使用 FTP 来传输时，其实是具有一定程度的【危险性】，因为数据在因特网上面是完全没有受到保护的【明码】传输方式！但是单纯的 FTP 服务还是有其必要性的，例如很多学校就有 FTP 服务器的架设需求等！想要使用更安全的 FTP 协议，就可选择 vsftpd（Very Secure FTP Daemon） 这个软件。 DNS服务器目前的因特网世界使用的是所谓的 TCP/IP 协议，其中 IP 有第四版的 IPv4(32位) 及第六版的 IPv6(128位)。不过，这个 IPv4 为了人脑已经转成四组十进制的数字了，例如12.34.56.78 这样的格式。当我们利用 Internet 传送数据的时候，就需要这个 IP ，否则数据封包就不知道要送到哪里。那么四组十进制的数字仍然不容易记忆，因此就诞生了域名服务系统（Domain Name System,DNS），DNS 的作用就是将数字IP转换为人们容易记忆的域名。 WWW服务器我们最常讲的【架站】其实就是架设一个 Web 网站！那么什么是 Web 呢？那就是全球信息广播的意思 (World Wide Web)，或者也可以称之为互连网吧！这个是我们目前的人类最常使用的 Internet 的协议之一！通常说的上网就是使用 WWW 来查询用户所需要的信息！目前在 Unix-Like 系统中的 WWW 服务器主要就是透过 Apache 这个服务器软件来达成的，而为了动态网站，于是 LAMP (Linux + Apache + MySQL + PHP) 就这么产生了。 Proxy服务器代理服务器 (Proxy Server) 的原理其实很简单！就是以类似代理人的身份去取得用户所需要的数据就是了！但是由于它的【代理】能力，使得我们可以透过代理服务器来达成防火墙功能与用户浏览数据的分析！此外，也可以藉由代理服务器来达成节省带宽的目的（代理服务器缓存常用数据），以及加快内部网络对因特网的 WWW 访问速度！总之，代理服务器对于企业来说，是一个非常不错的工具！（注：代理服务器还能够再次代理服务器，即上层代理。上层代理服务器最大的功能就是分流，内部代理服务器( Local proxy )可代理多个外部上层代理服务器达到分流的目的，实现更快的速度访问 Internat 服务！） 同时，代理服务器可分为正向代理与反向代理。区分很简单，正向代理就是代理服务器位于客户端，用户主机都通过代理服务器访问 Internat 服务，可以实现内部主机的 IP 隐藏的功能，同时，也可起到 Local 端防火墙的目的。反向代理就是代理服务器位于服务端，所有的外部访问都通过代理服务器来向内部服务请求数据，可以实现过滤用户请求，缓存常用数据增加访问速度及降低带宽，隔绝用户直接访问服务器的请求，达到保护服务器的作用，实现服务端的防火墙功能。 Postfix服务器邮件服务器（Postfix Server）即发送与接受邮件的服务器。那么电子邮件是个啥玩意儿？它是利用网络传递一些信息给远程服务器的一种信息传递行为，虽然消息正文是很冷很硬的计算机文字，确实比不上手写信件来的让人觉得温暖，不过，对于具有时效性的信息来说，电子邮件可是起到很重要的作用啊！不管你在任何有网络的地方，连上 Internat 就可收取的你的信件。而相对的，也会有很多问题出现，例如夹带病毒的电子邮件问题、怪客透过邮件程序入侵、广告及垃圾信息和不实的信件内容等。因此，就出现了免费与付费一说，一般免费的即可满足个人使用，付费的可避免一些问题，免遭损失！]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络安全之常见攻击方式及解决方案]]></title>
    <url>%2F2018%2F01%2F18%2F%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E4%B9%8B%E5%B8%B8%E8%A7%81%E6%94%BB%E5%87%BB%E6%96%B9%E5%BC%8F%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[1、网络封包联机进入主机的流程当来自一个网络上的联机要求想进入我们的主机时，这个网络封包在进入主机实际取得数据的整个流程是如何？了解了整个流程之后，你才会发现：了解要如何保护你的主机安全！ 我们要将该流程更细致化说明，因为，透过这个流程分析，你会知道为啥我们的主机需要进行过一些防护之后，系统才能够比较强壮。此外，了解了网络是双向的，那么就能知道服务器与客户端都得要有 IP:port 才能够让彼此的软件互相沟通。现在，假设你的主机是 WWW 服务器，透过底下的图标，网络封包如何进入你的主机呢？ 1. 经过防火墙的分析：Linux 系统有内建的防火墙机制，因此你的联机能不能成功，得要先看防火墙的脸色才行。预设的 Linux 防火墙就有两个机制，这两个机制都是独立存在的，因此我们预设就有两层防火墙。第一层是封包过滤式的 netfilter 防火墙，另一个则是透过软件管控的 TCP Wrappers 防火墙。 封包过滤防火墙：IP Filtering 或 Net Filter 要进入 Linux 本机的封包都会先通过 Linux 核心的预设防火墙，就是称为 netfilter 的咚咚，简单的说，就是 iptables 这个软件所提供的防火墙功能。为何称为封包过滤呢？因为他主要是分析 TCP/IP 的封包表头来进行过滤的机制，主要分析的是 OSI 的第二、三、四层，主要控制的就是 MAC, IP, ICMP, TCP 与 UDP 的端口与状态 (SYN, ACK…) 等。 第二层防火墙：TCP Wrappers 通过 netfilter 之后，网络封包会开始接受 Super daemons 及 TCP_Wrappers 的检验，那个是什么呢？说穿了就是 /etc/hosts.allow 与 /etc/hosts.deny 的配置文件功能。这个功能也是针对 TCP 的 Header 进行再次的分析，同样你可以设定一些机制来抵制某些 IP 或 Port ，好让来源端的封包被丢弃或通过检验； 透过防火墙的管控，我们可以将大部分来自因特网的垃圾联机丢弃，只允许自己开放的服务的联机进入本机而已，可以达到最基础的安全防护。 2. 服务 (daemon) 的基本功能：预设的防火墙是 Linux 的内建功能，但防火墙主要管理的是 MAC, IP, Port 等封包表头方面的信息，如果想要管控某些目录可以进入，某些目录则无法使用的功能，那就得要透过权限以及服务器软件提供的相关功能了。举例来说，你可以在 httpd.conf 这个配置文件之内规范某些 IP 来源不能使用 httpd 这个服务来取得主机的数据，那么即使该 IP 通过前面两层的过滤，他依旧无法取得主机的资源！但要注意的是，如果 httpd 这支程序本来就有问题的话，那么 client 端将可直接利用 httpd 软件的漏洞来入侵主机，而不需要取得主机内 root 的密码！因此，要小心这些启动在因特网上面的软件！ 3. SELinux 对网络服务的细部权限控制：为了避免前面一个步骤的权限误用，或者是程序有问题所造成的资安状况，因此 Security Enhanced Linux (安全强化 Linux) 就来发挥它的功能！简单的说，SELinux 可以针对网络服务的权限来设定一些规则 (policy) ，让程序能够进行的功能有限，因此即使使用者的档案权限设定错误，以及程序有问题时，该程序能够进行的动作还是被限制的，即使该程序使用的是 root 的权限也一样。举例来说，前一个步骤的 httpd 真的被 cracker 攻击而让对方取得 root 的使用权，由于 httpd 已经被 SELinux 控制在 /var/www/html 里面，且能够进行的功能已经被规范住了，因此 cracker 就无法使用该程序来进行系统的进一步破坏。 4. 使用主机的文件系统资源：想一想，你使用浏览器连接到 WWW 主机最主要的目的是什么？当然就是读取主机的 WWW 数据！那 WWW 资料是啥？就是档案啊！所以，最终网络封包其实是要向主机要求文件系统的数据。我们这里假设你要使用 httpd 这支程序来取得系统的档案数据，但 httpd 默认是由一个系统账号名称为 httpd 来启动的，所以：你的网页数据的权限当然就是要让 httpd 这支程序可以读取才行啊！如果你前面三关的设定都 OK ，最终权限设定错误，使用者依旧无法浏览你的网页数据的。 在这些步骤之外，我们的 Linux 以及相关的软件都可能还会支持登录文件记录的功能，为了记录历史历程，以方便管理者在未来的错误查询与入侵检测，良好的分析登录档的习惯是一定要建立的，尤其是 /var/log/messages 与 /var/log/secure 这些个档案！ 2、常见的攻击手法与相关保护上面我们了解到数据传送到本机时所需要经过的几道防线，那个权限是最后的关键！那么 cracker 是如何透过上述的流程还能够攻击你的系统？底下就让我们来分析分析。 1. 取得帐户信息后猜密码由于很多人喜欢用自己的名字来作为帐户信息，因此账号的取得是很容易的！举例来说，如果你的朋友将你的 email address 不小心泄漏出去，例如：dmtsai@your.host.name 之类的样式，那么人家就会知道你有一部主机，名称为 your.host.name，且在这部主机上面会有一个使用者账号，账号名称为 dmtsai ，之后这个坏家伙再利用某些特殊软件例如 nmap 来进行你主机的 port scan 之后，他就可以开始透过你主机有启动的软件功能来猜你这个账号的密码了！ 另外，如果你常常观察你的主机登录文件，那你也会发现如果你的主机有启动 Mail server 的服务时，你的登录档就会常常出现有些怪家伙尝试以一些奇怪的常见账号在试图猜测你的密码，举例来说像：admin, administrator, webmaster …. 之类的账号，尝试来窃取你的私人信件。如果你的主机真的有这类的账号，而且这类的账号还没有良好的密码规划，那就容易【中标】！所以我们常讲，系统账号千万不能给予密码，容易被猜密码啊！ 这种猜密码的攻击方式算是最早期的入侵模式之一了，攻击者知道你的账号，或者是可以猜出来你的系统有哪些账号，欠缺的就只是密码而已，因此他会【很努力的】去猜你的密码，此时，你的密码规划如果不好的话，很容易就被攻击了！主机也很容易被绑架！所以，良好的密码设置习惯是很重要的。 不过这种攻击方式比较费时，因为目前很多软件都有密码输入次数的限制，如果连续输入三次密码还不能成功的登入，那该次联机就会被断线！所以，这种攻击方式日益减少，目前偶而还会看到就是！这也是初级 cracker 会使用的方式之一。那我们要如何保护呢？基本方式是这样的： （1）减少信息的曝光机会：例如不要将 Email Address 随意散布到 Internet 上头；（2）建立较严格的密码设定规则：包括 /etc/shadow, /etc/login.defs 等档案的设定，如果主机够稳定且不会持续加入某些账号时，也可以考虑使用 chattr 来限制账号 (/etc/passwd, /etc/shadow) 的更改；（3）完善的权限设定：由于这类的攻击方式会取得你的某个使用者账号的登入权限，所以如果你的系统权限设定得宜的话，那么攻击者也仅能取得一般使用者的权限而已，对于主机的伤害比较有限！所以说，权限设定是重要的； 2. 利用系统的程序漏洞【主动】攻击由上图中的第二道防线，我们知道如果你的主机有开放网络服务时，就必须有启动某个网络软件！我们也知道由于软件可能撰写方式的问题，可能产生一些会被 cracker 乱用的臭虫程序代码，而这些臭虫程序代码由于产生问题的大小，有分为 bug (臭虫，可能会造成系统的不稳定或当机) 与 Security (安全问题，程序代码撰写方式会导致系统的权限被恶意者所掌握) 等问题。 当程序的问题被公布后，某些较高阶的 cracker 会尝试撰写一些针对这个漏洞的攻击程序代码，并且将这个程序代码放置到 cracker 常去的网站上面，藉以推销自己的【功力】….. 这种攻击模式是目前最常见的，因为攻击者只要拿到攻击程序就可以进行攻击了，【而且由攻击开始到取得你系统的 root 权限不需要猜密码，不需要两分钟，就能够立刻入侵成功】。但这个玩意儿本身是靠【你主机的程序漏洞】来攻击的，所以，如果你的主机随时保持在实时更新的阶段，或者是关闭大部分不需要的程序，那就可以躲避过这个问题。因此，你应该要这样做： （1）关闭不需要的网络服务：开的 port 越少，可以被入侵的管道越少，一部主机负责的服务越单纯，越容易找出问题点。（2）随时保持更新：这个没话讲！一定要进行的！（3）关闭不需要的软件功能：举例来说，后面会提到的远程登录服务器 SSH 可以提供 root 由远程登录，那么危险的事情当然要给他取消啊！ 3. 利用社交工程作欺骗社交工程 (Social Engineering) 指的其实很简单，就是透过人与人的互动来达到【入侵】的目的！在大公司里面，或许你可能会接到这样的电话：【我是人事部门的经理，我的账号为何突然间不能登入了？你给我看一看，恩？干脆直接帮我另建一个账号，我告诉你我要的密码是….】。如果你一时不查给他账号密码的话，你的主机可能就这样被绑走了！ 社交工程的欺骗方法多的是，包括使用【好心的 email 通知】、【警告信函】、【中奖单】等等，样样都是要欺骗你的账号密码，有的则利用钓鱼方式来欺骗你在某些恶意网站上面输入你的账号密码！举例来说，一般 email 会常常收到一些活动的信件，要我们用账号密码登录，这种的就要小心了！那要如何防范呢？ （1）追踪对谈者：不要一味的相信对方，你必须要有信心的向上呈报，不要一时心慌就中了计！（2）不要随意透露账号/密码等信息：最好不要随意在 Internet 上面填写这些数据，真的很危险！因为在 Internet 上面，你永远不知道对方屏幕前面坐着的是谁？ 4. 利用程序功能的【被动】攻击除了主动攻击之外，还有所谓的被动攻击！那就得要由【恶意网站】讲起了。如果你喜欢上网随意浏览的话，那么有的时候可能会连上一些广告很多，或者是一堆弹出式窗口的网站，这些网站有时还会很好心的【提供你很多好用的软件自动下载与安装】的功能，如果该网站是你所信任的，例如 Red Hat, CentOS, Windows 官网的话，那还好，如果是一个你也不清楚他是干嘛的网站，那你是否要同意下载安装该软件？ 如果你常常在注意一些网络危机处理的相关新闻时，常会发现 Windows 的浏览器 (IE) 有问题，有时则是全部的浏览器 (Firefox, Netscap, IE…) 都会出现问题。那你会不会觉得奇怪啊，怎么【浏览器也会有问题？】这是因为很多浏览器会主动的答应对方 WWW 主机所提供的各项程序功能，或者是自动安装来自对方主机的软件，有时浏览器还可能由于程序发生安全问题，让对方 WWW 浏览器得以传送恶意代码给你的主机来执行！ 那你又会想啊，那我干嘛浏览那样的恶意网站？总是会有些粗心大意的时候啊！如果你今天不小心收到一个 email ，里面告诉你你的银行账号有问题，希望你赶紧连上某个网页去看看你的账号是否在有问题的行列中，你会不会去？如果今天有个网络消息说某某网页在提供大特价商品，那你会不会去碰碰运气？都是可能的啊！不过，这也就很容易被对方攻击到了。 那如何防备啊？当然建立良好的习惯最重要了： （1）随时更新主机上的所有软件：如果你的浏览器是没有问题的，那对方传递恶意代码时，你的浏览器就不会执行，那自然安全的多了！（2）较小化软件的功能：举例来说，让你的收信软件不要主动的下载文件，让你的浏览器在安装某些软件时，要通过你的确认后才安装，这样就比较容易克服一些小麻烦；（3）不要连接到不明的主机：因为很多时候我们都用 google 在搜寻问题的解决之道，那你如何知道对方是否是骗人的？所以，前面两点防备还是很重要的！不要以为没有连接上恶意网站就不会有问题！ 5. 蠕虫或木马的 rootkitrootkit 意思是说可以取得 root 权限的一群工具组 (kit)，就如同前面主动攻击程序漏洞的方法一样，rootkit 主要也是透过主机的程序漏洞。不过，rootkit 也会透过社交工程让用户下载、安装 rootkit 软件，结果让 cracker 得以简单的绑架对方主机！ rootkit 除了可以透过上述的方法来进行入侵之外，rootkit 还会伪装或者是进行自我复制，举例来说，很多的 rootkit 本身就是蠕虫或者是木马间谍程序。蠕虫会让你的主机一直发送封包向外攻击，结果会让你的网络带宽被吃光光，例如 2001-2003年间的 Nimda, Code Red 等等；至于木马程序 (Trojan Horse) 则会对你的主机进行开启后门 (开一个 port 来让 cracker 主动的入侵)，结果就是….绑架、绑架、绑架！ rootkit 其实挺不好追踪的，因为很多时候他会主动的去修改系统观察的指令，包括 ls, top, netstat, ps, who, w, last, find 等等，让你看不到某些有问题的程序，如此一来，你的 Linux 主机就很容易被当成是跳板了！有够危险！那如何防备呢？ （1）不要随意安装不明来源的档案或者是不明网站的档案数据；（2）不要让系统有太多危险的指令：例如 SUID/SGID 的程序，这些程序很可能会造成用户不当的使用，而使得木马程序有机可趁！（3）可以定时以 rkhunter 之类的软件来追查：有个网站提供 rootkit 程序的检查，你可以前往下载与分析你的主机：http://www.rootkit.nl/projects/rootkit_hunter.html 6. DDoS 攻击法 (Distributed Denial of Service )这类型的攻击中文翻译成【分布式阻断服务攻击】，从字面上的意义来看，它就是透过分散在各地的僵尸计算机进行攻击，让你的系统所提供的服务被阻断而无法顺利的提供服务给其他用户的方式。这种攻击法也很要命，而且方法有很多，最常见的就属 SYN Flood 攻击法了！当主机接收了一个带有 SYN 的 TCP 封包之后，就会启用对方要求的 port 来等待联机，并且发送出回应封包(带有 SYN/ACK 旗目标 TCP 封包)，并等待 Client 端的再次回应。 好了，在这个步骤当中我们来想一想，如果 cient 端在发送出 SYN 的封包后，却将来自 Server 端的确认封包丢弃，那么你的 Server 端就会一直空等，而且 Client 端可以透过软件功能，在短短的时间内持续发送出这样的 SYN 封包，那么你的 Server 就会持续不断的发送确认封包，并且开启大量的 port 在空等！等到全部主机的 port 都启用完毕，那么…..系统就挂了！ 更可怕的是，通常攻击主机的一方不会只有一部！他会透过 Internet 上面的僵尸网络 (已经成为跳板，但网站主却没有发现的主机) 发动全体攻击，让你的主机在短时间内就立刻挂点。这种 DDoS 的攻击手法比较类似【玉石俱焚】的手段，他不是入侵你的系统，而是要让你的系统无法正常提供服务！最常被用来作为阻断式服务的网络服务就是 WWW 了，因为 WWW 通常得对整个 Internet 开放服务。 这种攻击方法也是最难处理的，因为要嘛就得要系统核心有支持自动抵挡 DDoS 攻击的机制，要嘛你就得要自行撰写侦测软件来判断！而除非你的网站非常大，并且【得罪不少人】，否则应该不会被 DDoS 攻击！ 7. 其他上面提到的都是比较常见的攻击方法，是还有一些高竿的攻击法，不过那些攻击法都需要有比较高的技术水准，例如 IP 欺骗。他可以欺骗你主机告知该封包来源是来自信任网域，而且透过封包传送的机制，由攻击的一方持续的主动发送出确认封包与工作指令。如此一来，你的主机可能就会误判该封包确实有响应，而且是来自内部的主机。 不过我们知道因特网是有路由的，而每部主机在每一个时段的 ACK 确认码都不相同，所以这个方式要达成可以登入，会比较麻烦，所以说，不太容易发生在我们这些小型主机上面！不过你还是得要注意一下说： （1）设定规则完善的防火墙：利用 Linux 内建的防火墙软件 iptables 建立较为完善的防火墙，可以防范部分的攻击行为；（2）核心功能：这部份比较复杂，你必须要对系统核心有很深入的了解，才有办法设定好你的核心网络功能。（3）登录文件与系统监控：你可以透过分析登录文件来了解系统的状况，另外也可以透过类似 MRTG 之类的监控软件来实时了解到系统是否有异常，这些工作都是很好的努力方向！ 8. 小结不要以为你的主机没有啥重要数据，被入侵或被植入木马也没有关系，因为我们的服务器通常会对内部来源的主机规范的较为宽松，如果你的主机在公司内部，但是不小心被入侵的话，那么贵公司的服务器是否就会暴露在危险的环境当中了？ 所以啊，主机防护还是很重要的！不要小看了！提供几个方向给大家思考看看吧： 1.建立完善的登入密码规则限制；2.完善的主机权限设定；3.设定自动升级与修补软件漏洞、及移除危险软件；4.在每项系统服务的设定当中，强化安全设定的项目；5.利用 iptables, TCP_Wrappers 强化网络防火墙；6.利用主机监控软件如 MRTG 与 logwatch 来分析主机状况与登录文件； 3、关于Linux操作系统网络安全的有效应对措施（1）安装防火墙、保证账号安全 在互联网的安全防御中，每台计算机的系统都会自带安全防御工具及防火墙，其主要作用在于提供给用户审核、访问及防病毒等功能的同时，有效防止非授权用户进入系统，获取相关网络资源信息，并通过恰当地配置和调试，达到限制局域网非授权用户的访问，保护系统网络安全的目的，同时，对Linux操作系统用户账号的管理也要注意加密及口令安全的保护。 （2）关闭无需服务、重要文件需加密 Linux操作系统本身会自带很多网络服务，其中有很多是没有必要的服务，用户应当及时进行关闭服务，以免因其造成严重的安全隐患，可以先找到 /etc/services 这个文件，对其所包含的网络服务数据库进行筛选，关闭不必要的网络服务，增强安全性。另外对系统中较为重要的文件也要进行加密，以保证一旦受到攻击可以不受到损害和窃取。 （3）阻止Ping命令请求以及Finger服务 上文提到，TCP/IP 协议由于自身缺陷极易受到黑客的攻击，利用其漏洞从正常数据包通道传送数据，获得有利信息。如果将系统设置为Ping命令请求阻止，便会减少黑客攻击成功的几率，提高网络的安全性。而禁止Finger服务，删除 /usr/bin 中的Finger命令，则可以防止黑客借助此命令远程或本地侵入系统，得到用户的一切相关信息，进而保护了网络的安全。]]></content>
      <categories>
        <category>书籍</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>网络安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 防火墙的设定]]></title>
    <url>%2F2018%2F01%2F17%2FLinux%E9%98%B2%E7%81%AB%E5%A2%99%E7%9A%84%E8%AE%BE%E5%AE%9A%2F</url>
    <content type="text"><![CDATA[Linux 的封包过滤软件：iptables《防火墙的认识》这篇文章说了那么多，主要就是说明防火墙是什么，能做什么，并且知道防火墙不是万能的。下面我说一下都使用什么来设定防火墙。因为不同的核心使用的防火墙机制不同，且支持的软件指令与语法也不相同，所以在 Linux 上头设定属于你自己的防火墙规则时，要注意，先用 uname -r 追踪一下你的核心版本再说，目前大部分的核心都使用 kernel 2.6 版的核心，主要是使用 iptables 这个防火墙机制。 1、封包进入流程：规则顺序的重要性！因为 iptables 是利用封包过滤的机制，所以他会分析封包的表头数据。根据表头数据与定义的【规则】来决定该封包是否可以进入主机或者是被丢弃。意思就是说：【根据封包的分析资料”比对”你预先定义的规则内容，若封包数据与规则内容相同则进行动作，否则就继续下一条规则的比对！】重点在那个【比对与分析顺序】上。 举个简单的例子，假设我预先定义 10 条防火墙规则好了，那么当 Internet 来了一个封包想要进入我的主机，那么防火墙是如何分析这个封包的呢？我们以底下的图示来说明好了： 当一个网络封包要进入到主机之前，会先经由 NetFilter 进行检查，那就是 iptables 的规则了。检查通过则接受 (ACCEPT) 进入本机取得资源，如果检查不通过，则可能予以丢弃 (DROP) ！上图中主要的目的在告知你：【规则是有顺序的】！例如当网络封包进入 Rule 1 的比对时，如果比对结果符合 Rule 1 ，此时这个网络封包就会进行 Action 1 的动作，而不会理会后续的 Rule 2, Rule 3…. 等规则的分析了。而如果这个封包并不符合 Rule 1 的比对，那就会进入 Rule 2 的比对了！如此一个一个规则去进行比对就是了。那如果所有的规则都不符合怎办？此时就会透过预设动作 (封包政策, Policy) 来决定这个封包的去向。所以，当你的规则顺序排列错误时，就会产生很严重的错误了。（类比 rewrite 一致） 2、iptables 的表格 (table) 与链 (chain)事实上，上图 5-1 所列出的规则仅是 iptables 众多表格当中的一个链(chain) 而已。什么是链呢？这得由 iptables 的名称说起。为什么称为 ip”tables”呢？ 因为这个防火墙软件里面有多个表格 (table) ，每个表格都定义出自己的默认政策与规则，且每个表格的用途都不相同。我们可以使用底下这张图来稍微了解一下： 刚刚图 5-1 的规则内容仅只是图 5-3 内的某个 chain 而已！而预设的情况下，咱们 Linux 的 iptables 至少就有三个表格，包括管理本机进出的 filter 、管理后端主机 (防火墙内部的其他计算机) 的 nat 、管理特殊旗标使用的 mangle (较少使用) 。更有甚者，我们还可以自定义额外的链呢！真是很神奇吧！每个表格与其中链的用途分别是这样的： filter (过滤器)：主要跟进入 Linux 本机的封包有关，这个是预设的 table ！ INPUT：主要与想要进入我们 Linux 本机的封包有关； OUTPUT：主要与我们 Linux 本机所要送出的封包有关； FORWARD：这个咚咚与 Linux 本机比较没有关系，他可以【转递封包】到后端的计算机中，与下列 nat table 相关性较高。 nat (地址转换)：是 Network Address Translation 的缩写，这个表格主要在进行来源与目的之 IP 或 port 的转换，与 Linux 本机较无关，主要与 Linux 主机后的局域网络内计算机较有相关。 PREROUTING：在进行路由判断之前所要进行的规则(DNAT/REDIRECT) POSTROUTING：在进行路由判断之后所要进行的规则(SNAT/MASQUERADE) OUTPUT：与发送出去的封包有关 mangle (破坏者)：这个表格主要是与特殊的封包的路由旗标有关，早期仅有 PREROUTING 及 OUTPUT 链，不过从 kernel 2.4.18 之后加入了 INPUT 及FORWARD 链。由于这个表格与特殊旗标相关性较高，所以像咱们这种单纯的环境当中，较少使用 mangle 这个表格。 所以说，如果你的 Linux 是作为 www 服务，那么要开放客户端对你的 www 要求有响应，就得要处理 filter 的 INPUT 链； 而如果你的 Linux 是作为局域网络的路由器，那么就得要分析 nat 的各个链以及 filter 的 FORWARD 链才行。 3、本机的 iptables 语法防火墙的设定主要使用的就是 iptables 这个指令而已。而防火墙是系统管理员的主要任务之一，且对于系统的影响相当的大，因此【只能让 root 使用 iptables 】，不论是设定还是观察防火墙规则！ （1）规则的观察与清除如果你在安装的时候选择没有防火墙的话，那么 iptables 在一开始的时候应该是没有规则的，不过，可能因为你在安装的时候就有选择系统自动帮你建立防火墙机制，那系统就会有默认的防火墙规则了！无论如何，我们先来看看目前本机的防火墙规则是如何吧！123456789101112131415161718192021222324[root@www ~]# iptables [-t tables] [-L] [-nv]选项与参数： -t ：后面接 table ，例如 nat 或 filter ，若省略此项目，则使用默认的filter -L ：列出目前的 table 的规则 -n ：不进行 IP 与 HOSTNAME 的反查，显示讯息的速度会快很多！ -v ：列出更多的信息，包括通过该规则的封包总位数、相关的网络接口等范例：列出 filter table 三条链的规则[root@www ~]# iptables -L -nChain INPUT (policy ACCEPT) &lt;==针对 INPUT 链，且预设政策为可接受target prot opt source destination &lt;==说明栏ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED&lt;==第 1 条规则ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0&lt;==第 2 条规则ACCEPT all -- 0.0.0.0/0 0.0.0.0/0&lt;==第 3 条规则ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 state NEW tcp dpt:22&lt;==以下类推REJECT all -- 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibitedChain FORWARD (policy ACCEPT) &lt;==针对 FORWARD 链，且预设政策为可接受target prot opt source destinationREJECT all -- 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibitedChain OUTPUT (policy ACCEPT) &lt;==针对 OUTPUT 链，且预设政策为可接受target prot opt source destination 在上表中，每一个 Chain 就是前面提到的每个链。Chain 那一行里面括号的 policy 就是预设的政策，那底下的 target, prot 代表什么呢？target：代表进行的动作， ACCEPT 是放行，而 REJECT 则是拒绝，此外，尚有 DROP (丢弃) 的项目！prot：代表使用的封包协议，主要有 tcp, udp 及 icmp 三种封包格式；opt：额外的选项说明source ：代表此规则是针对哪个【来源 IP】进行限制？destination ：代表此规则是针对哪个【目标 IP】进行限制？ 在输出结果中，第一个范例因为没有加上 -t 的选项，所以默认就是 filter 这个表格内的 INPUT, OUTPUT, FORWARD 三条链的规则啰。若针对单机来说，INPUT 与 FORWARD 算是比较重要的管制防火墙链，所以你可以发现最后一条规则的政策是 REJECT (拒绝)！虽然 INPUT 与 FORWARD 的政策是放行 (ACCEPT)，不过在最后一条规则就已经将全部的封包都拒绝了！ 不过这个指令的观察只是作个格式化的查阅，要详细解释每个规则会比较不容易解析。举例来说，我们将 INPUT 的 5 条规则依据输出结果来说明一下，结果会变成： 只要是封包状态为 RELATED,ESTABLISHED 就予以接受 只要封包协议是 icmp 类型的，就予以放行 无论任何来源 (0.0.0.0/0) 且要去任何目标的封包，不论任何封包格式(prot 为 all)，通通都接受 只要是传给 port 22 的主动式联机 tcp 封包就接受 全部的封包信息通通拒绝 最有趣的应该是第 3 条规则了，怎么会所有的封包信息都予以接受？如果都接受的话，那么后续的规则根本就不会有用嘛！其实那条规则是仅针对每部主机都有的内部循环测试网络 (lo) 接口！如果没有列出接口，那么我们就很容易搞错。所以，近来建议使用 iptables-save 这个指令来观察防火墙规则！因为 iptables-save 会列出完整的防火墙规则，只是并没有规格化输出而已。12345678910111213141516[root@www ~]# iptables-save [-t table]选项与参数：-t ：可以仅针对某些表格来输出，例如仅针对 nat 或 filter 等等[root@www ~]# iptables-save# Generated by iptables-save v1.4.7 on Fri Jul 22 15:51:52 2011*filter &lt;==星号开头的指的是表格，这里为 filter:INPUT ACCEPT [0:0] &lt;==冒号开头的指的是链，三条内建的链:FORWARD ACCEPT [0:0] &lt;==三条内建链的政策都是 ACCEPT 啰！:OUTPUT ACCEPT [680:100461]-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT &lt;==针对 INPUT的规则-A INPUT -p icmp -j ACCEPT-A INPUT -i lo -j ACCEPT &lt;==这条很重要！针对本机内部接口开放！-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT-A INPUT -j REJECT --reject-with icmp-host-prohibited-A FORWARD -j REJECT --reject-with icmp-host-prohibited &lt;==针对FORWARD 的规则COMMIT 由上面的输出来看，有底线且内容含有 lo 的那条规则当中，【 -i lo 】指的就是由 lo 适配卡进来的封包！这样看就清楚多了！因为有写到接口的关系啊！不像之前的 iptables -L -n！既然这个规则不是我们想要的，那该如何修改规则呢？建议，先删除规则再慢慢建立各个需要的规则！那如何清除规则？这样做就对了：123456789[root@www ~]# iptables [-t tables] [-FXZ]选项与参数：-F ：清除所有的已订定的规则；-X ：杀掉所有使用者 &quot;自定义&quot; 的 chain (应该说的是 tables ）；-Z ：将所有的 chain 的计数与流量统计都归零范例：清除本机防火墙 (filter) 的所有规则[root@www ~]# iptables -F[root@www ~]# iptables -X[root@www ~]# iptables -Z 由于这三个指令会将本机防火墙的所有规则都清除，但却不会改变预设政策 (policy) ，所以如果你不是在本机下达这三行指令时，很可能你会被自己挡在家门外 (若 INPUT设定为 DROP 时)！要小心啊！ (2）定义预设政策 (policy)清除规则之后，再接下来就是要设定规则的政策！还记得政策指的是什么吗？【 当你的封包不在你设定的规则之内时，则该封包的通过与否，是以 Policy 的设定为准】，在本机方面的预设政策中，假设你对于内部的使用者有信心的话，那么 filter 内的 INPUT 链方面可以定义的比较严格一点，而 FORWARD 与 OUTPUT 则可以订定的松一些！通常都是将 INPUT 的 policy 定义为 DROP ，其他两个则定义为 ACCEPT。至于 nat table 则暂时先不理会他。12345678910111213141516171819[root@www ~]# iptables [-t nat] -P [INPUT,OUTPUT,FORWARD] [ACCEPT,DROP]选项与参数：-P ：定义政策( Policy )。注意，这个 P 为大写啊！ACCEPT ：该封包可接受DROP ：该封包直接丢弃，不会让 client 端知道为何被丢弃。范例：将本机的 INPUT 设定为 DROP ，其他设定为 ACCEPT[root@www ~]# iptables -P INPUT DROP[root@www ~]# iptables -P OUTPUT ACCEPT[root@www ~]# iptables -P FORWARD ACCEPT[root@www ~]# iptables-save# Generated by iptables-save v1.4.7 on Fri Jul 22 15:56:34 2011*filter:INPUT DROP [0:0]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [0:0]COMMIT# Completed on Fri Jul 22 15:56:34 2011# 由于 INPUT 设定为 DROP 而又尚未有任何规则，所以上面的输出结果显示：# 所有的封包都无法进入你的主机！是不通的防火墙设定！(网络联机是双向的) 看到输出的结果了吧？INPUT 被修改了设定！其他的 nat table 三条链的预设政策设定也是一样的方式，例如：【 iptables -t nat -P PREROUTING ACCEPT 】就设定了 nat table 的 PREROUTING 链为可接受的意思！预设政策设定完毕后，来谈一谈关于各规则的封包基础比对设定吧。 （3）封包的基础比对：IP, 网域及接口装置开始来进行防火墙规则的封包比对设定吧！既然是因特网，那么我们就由最基础的 IP, 网域及端口，亦即是 OSI 的第三层谈起，再来谈谈装置 (网络卡) 的限制等等。123456789101112131415161718192021[root@www ~]# iptables [-AI 链名] [-io 网络接口] [-p 协议] \&gt; [-s 来源IP/网域] [-d 目标IP/网域] -j [ACCEPT|DROP|REJECT|LOG]选项与参数：-AI 链名：针对某的链进行规则的 &quot;插入&quot; 或 &quot;累加&quot; -A ：新增加一条规则，该规则增加在原本规则的最后面。例如原本已经有四条规则， 使用 -A 就可以加上第五条规则！ -I ：插入一条规则。如果没有指定此规则的顺序，默认是插入变成第一条规则。例如原本有四条规则， 使用 -I 则该规则变成第一条，而原本四条变成 2~5 号 链 ：有 INPUT, OUTPUT, FORWARD 等，此链名称又与 -io 有关，请看底下。-io 网络接口：设定封包进出的接口规范 -i ：封包所进入的那个网络接口，例如 eth0, lo 等接口。需与 INPUT 链配合； -o ：封包所传出的那个网络接口，需与 OUTPUT 链配合；-p 协定：设定此规则适用于哪种封包格式 主要的封包格式有： tcp, udp, icmp 及 all 。-s 来源 IP/网域：设定此规则之封包的来源项目，可指定单纯的 IP 或包括网域，例如： IP ：192.168.0.100 网域：192.168.0.0/24, 192.168.0.0/255.255.255.0 均可。 若规范为『不许』时，则加上 ! 即可，例如： -s ! 192.168.100.0/24 表示不许 192.168.100.0/24 之封包来源；-d 目标 IP/网域：同 -s ，只不过这里指的是目标的 IP 或网域。-j ：后面接动作，主要的动作有接受(ACCEPT)、丢弃(DROP)、拒绝(REJECT)及记录(LOG) iptables 的基本参数就如同上面所示的，仅只谈到 IP 、网域与装置等等的信息。至于 TCP, UDP 封包特有的端口 (port number) 与状态 (如 SYN 旗标)，下面谈论。 示例：先让我们来看看最基础的几个规则，例如开放 lo 这个本机的接口以及某个 IP 来源吧！12范例：设定 lo 成为受信任的装置，亦即进出 lo 的封包都予以接受[root@www ~]# iptables -A INPUT -i lo -j ACCEPT 仔细看上面并没有列出 -s, -d 等等的规则，这表示：不论封包来自何处或去到哪里，只要是来自 lo 这个界面，就予以接受！这个观念挺重要的，就是【没有指定的项目，则表示该项目完全接受】的意思！例如这个案例当中，关于 -s, -d…等等的参数没有规定时，就代表不论什么值都会被接受。 (4）TCP, UDP 的规则比对：针对端口设定在谈到 TCP 与 UDP 时，比较特殊的就是那个端口 (port)，在 TCP 方面则另外有所谓的联机封包状态，包括最常见的 SYN 主动联机的封包格式。那么如何针对这两种封包格式进行防火墙规则的设定呢？你可以这样看：123456[root@www ~]# iptables [-AI 链] [-io 网络接口] [-p tcp,udp] \&gt; [-s 来源IP/网域] [--sport 埠口范围] \&gt; [-d 目标IP/网域] [--dport 埠口范围] -j [ACCEPT|DROP|REJECT]选项与参数：--sport 埠口范围：限制来源的端口口号码，端口口号码可以是连续的，例如1024:65535--dport 埠口范围：限制目标的端口口号码。 事实上就是多了那个 –sport 及 –dport 这两个玩意儿，重点在那个 port 上面！不过你得要特别注意，因为仅有 tcp 与 udp 封包具有端口，因此你想要使用 –dport,–sport 时，得要加上 -p tcp 或 -p udp 的参数才会成功！底下让我们来进行几个小测试：123456范例：想要联机进入本机 port 21 的封包都抵挡掉：[root@www ~]# iptables -A INPUT -i eth0 -p tcp --dport 21 -j DROP范例：想连到我这部主机的网络 (upd port 137,138 tcp port 139,445) 就放行[root@www ~]# iptables -A INPUT -i eth0 -p udp --dport 137:138 -j ACCEPT[root@www ~]# iptables -A INPUT -i eth0 -p tcp --dport 139 -j ACCEPT[root@www ~]# iptables -A INPUT -i eth0 -p tcp --dport 445 -j ACCEPT 除了端口之外，在 TCP 还有特殊的旗标啊！最常见的就是那个主动联机的 SYN 旗标了。我们在 iptables 里面还支持【 –syn 】的处理方式，我们以底下的例子来说明好了：123范例：将来自任何地方来源 port 1:1023 的主动联机到本机端的 1:1023 联机丢弃[root@www ~]# iptables -A INPUT -i eth0 -p tcp --sport 1:1023 \&gt; --dport 1:1023 --syn -j DROP 一般来说，client 端启用的 port 都是大于 1024 以上的端口，而 server 端则是启用小于 1023 以下的端口在监听的。所以我们可以让来自远程的小于 1023 以下的端口数据的主动联机都给他丢弃！但不适用在 FTP 的主动联机中！ （5）iptables 外挂模块：mac 与 state在 kernel 2.2 以前使用 ipchains 管理防火墙时，通常会让系统管理员相当头痛！因为 ipchains 没有所谓的封包状态模块，因此我们必须要针对封包的进、出方向进行管控。举例来说，如果你想要联机到远程主机的 port 22 时，你必须要针对两条规则来设定： 本机端的 1024:65535 到远程的 port 22 必须要放行 (OUTPUT 链)； 远程主机 port 22 到本机的 1024:65535 必须放行 (INPUT 链)； 这会很麻烦！因为如果你要联机到 10 部主机的 port 22 时，假设 OUTPUT 为预设开启 (ACCEPT)， 你依旧需要填写十行规则，让那十部远程主机的 port 22 可以联机到你的本地端主机上。那如果开启全部的 port 22 呢？又担心某些恶意主机会主动以 port 22 联机到你的机器上！同样的道理，如果你要让本地端主机可以连到外部的 port 80 (WWW 服务)，那就更不得了！这就是网络联机是双向的一个很重要的概念！ 好在我们的 iptables 免除了这个困扰！他可以透过一个状态模块来分析 【这个想要进入的封包是否为刚刚我发出去的响应？】如果是刚刚我发出去的响应，那么就可以予以接受放行！这样就不用管远程主机是否联机进来的问题了！那如何达到呢？看看底下的语法：1234567891011121314[root@www ~]# iptables -A INPUT [-m state] [--state 状态]选项与参数：-m ：一些 iptables 的外挂模块，主要常见的有： state ：状态模块 mac ：网络卡硬件地址 (hardware address)--state ：一些封包的状态，主要有： INVALID ：无效的封包，例如数据破损的封包状态 ESTABLISHED：已经联机成功的联机状态； NEW ：想要新建立联机的封包状态； RELATED ：这个最常用！表示这个封包是与我们主机发送出去的封包有关范例：只要已建立或相关封包就予以通过，只要是不合法封包就丢弃[root@www ~]# iptables -A INPUT -m state \&gt; --state RELATED,ESTABLISHED -j ACCEPT[root@www ~]# iptables -A INPUT -m state --state INVALID -j DROP 如此一来，我们的 iptables 就会主动分析出该封包是否为响应状态，若是的话，就直接予以接受。这样一来你就不需要针对响应的封包来撰写个别的防火墙规则了！这真是太棒了！底下我们继续谈一下 iptables 的另一个外挂，那就是针对网卡来进行放行与防御：1234范例：针对局域网络内的 aa:bb:cc:dd:ee:ff 主机开放其联机[root@www ~]# iptables -A INPUT -m mac --mac-source aa:bb:cc:dd:ee:ff -j ACCEPT选项与参数：--mac-source ：就是来源主机的 MAC ！ 如果你的区网当中有某些网络高手，老是可以透过修改 IP 去尝试透过路由器往外跑，那你该怎么办？难道将整个区网拒绝？并不需要的，你可以透过之前谈到的 ARP 相关概念，去捉到那部主机的 MAC ，然后透过上头的这个机制，将该主机整个 DROP 掉即可。不管他改了什么 IP ，除非他知道你是用网卡的 MAC 来管理，否则他就是出不去！ （6）超阳春客户端防火墙设计与防火墙规则储存经过上述的本机 iptables 语法分析后，接下来我们来想想，如果站在客户端且不提供网络服务的 Linux 本机角色时， 你应该要如何设计你的防火墙呢？老实说，你只要分析过 CentOS 默认的防火墙规则就会知道了，理论上， 应该要有的规则如下： 规则归零：清除所有已经存在的规则 (iptables -F…) 预设政策：除了 INPUT 这个自定义链设为 DROP 外，其他为预设 ACCEPT； 信任本机：由于 lo 对本机来说是相当重要的，因此 lo 必须设定为信任装置； 回应封包：让本机主动向外要求而响应的封包可以进入本机(ESTABLISHED,RELATED) 信任用户：这是非必要的，如果你想要让区网的来源可用你的主机资源时 这就是最最阳春的防火墙，你可以透过第二步骤抵挡所有远程的来源封包，而透过第四步骤让你要求的远程主机响应封包可以进入， 加上让本机的 lo 这个内部循环装置可以放行，嘿嘿！一部 client 专用的防火墙规则就 OK 了！你可以在某个 script 上面这样做即可：123456789101112131415161718[root@www ~]# vim bin/firewall.sh#!/bin/bash# 1. 清除规则iptables -Fiptables -Xiptables -Z# 2. 设定政策iptables -P INPUT DROPiptables -P OUTPUT ACCEPTiptables -P FORWARD ACCEPT# 3~5. 制订各项规则iptables -A INPUT -i eth0 -m state --state RELATED,ESTABLISHED -j ACCEPTiptables -A INPUT -p icmp -j ACCEPTiptables -A INPUT -i lo -j ACCEPTiptables -A INPUT -p tcp --dport 22 -j ACCEPT #必须要写，要不然就登陆不了SSH了。被锁外面了-_-# 6. 写入防火墙规则配置文件并重启service iptables savesystemctl restart iptables.service 其实防火墙也是一个服务，你可以透过【systemctl list-unit-files iptables】去察看就知道了。]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜服务器架设篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>网络安全</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 防火墙的认识]]></title>
    <url>%2F2018%2F01%2F17%2FLinux%E9%98%B2%E7%81%AB%E5%A2%99%E7%9A%84%E8%AE%A4%E8%AF%86%2F</url>
    <content type="text"><![CDATA[1、认识防火墙网络安全除了随时注意相关软件的漏洞以及网络上的安全通报之外，你最好能够依据自己的环境来订定防火墙机制！这样对于你的网络环境，会比较有保障一点！那么什么是防火墙呢？其实防火墙就是透过订定一些有顺序的规则，并管制进入到我们网域内的主机 (或者可以说是网域) 数据封包的一种机制！更广义的来说，只要能够分析与过滤进出我们管理之网域的封包数据，就可以称为防火墙。 防火墙又可以分为硬件防火墙与本机的软件防火墙。硬件防火墙是由厂商设计好的主机硬件，这部硬件防火墙内的操作系统主要以提供封包数据的过滤机制为主，并将其他不必要的功能拿掉。因为单纯作为防火墙功能而已，因此封包过滤的效率较佳。至于软件防火墙呢？那就是我们主要研究的方向！软件防火墙本身就是在保护系统网络安全的一套软件(或称为机制)，例如 Netfilter 与 TCP Wrappers 都可以称为软件防火墙。 2、为何需要防火墙封包进入本机时，会通过防火墙、服务器软件程序、SELinux 与文件系统等。所以基本上，如果你的系统 (1)已经关闭不需要而且危险的服务； (2)已经将整个系统的所有软件都保持在最新的状态； (3)权限设定妥当且定时进行备份工作； (4)已经教育用户具有良好的网络、系统操作习惯。 那么你的系统实际上已经颇为安全了！要不要架设防火墙？那就见仁见智了。 不过，毕竟网络世界是很复杂的，而 Linux 主机也不是一个简单的东西，说不定哪一天你在进行某个软件的测试时，主机突然间就启动了一个网络服务，如果你没有管制该服务的使用范围，那么该服务就等于对所有 Internet 开放，那就麻烦了！因为该服务可能可以允许任何人登入你的系统，那不是挺危险？ 所以，防火墙能作什么呢？防火墙最大的功能就是帮助你【限制某些服务的存取来源】！举例来说： (1)你可以限制文件传输服务 (FTP) 只在子域内的主机才能够使用，而不对整个 Internet 开放； (2)你可以限制整部 Linux 主机仅可以接受客户端的 WWW 要求，其他的服务都关闭； (3)你还可以限制整部主机仅能主动对外联机。 反过来说，若有客户端对我们主机发送主动联机的封包状态(TCP 封包的 SYN flag)就予以抵挡等等。这些就是最主要的防火墙功能了！ 所以，防火墙最重要的任务就是在规划出： 切割被信任(如子域)与不被信任(如 Internet)的网段； 划分出可提供 Internet 的服务与必须受保护的服务； 分析出可接受与不可接受的封包状态； 当然，咱们 Linux 的 iptables 防火墙软件还可以进行更细部深入的 NAT(Network Address Translation) 的设定，并进行更弹性的 IP 封包伪装功能，不过，对于单一主机的防火墙来说，最简单的任务还是上面那三项就是了！所以，你需不需要防火墙呢？理论上，当然需要！而且你必须要知道【你的系统哪些数据与服务需要保护】，针对需要受保护的服务来设定防火墙的规则吧！底下我们先来谈一谈，那在Linux 上头常见的防火墙类型有哪些？ 3、Linux 系统上防火墙的主要类别基本上，依据防火墙管理的范围，我们可以将防火墙区分为网域型与单一主机型的控管。在单一主机型的控管方面，主要的防火墙有封包过滤型的 Netfilter 与依据服务软件程序作为分析的 TCP Wrappers 两种。若以区域型的防火墙而言，由于此类防火墙都是当作路由器角色，因此防火墙类型主要则有封包过滤的 Netfilter 与利用代理服务器 (proxy server) 进行存取代理的方式了。 （1）Netfilter (封包过滤机制)所谓的封包过滤，亦即是分析进入主机的网络封包，将封包的表头数据捉出来进行分析，以决定该联机为放行或抵挡的机制。由于这种方式可以直接分析封包表头数据，所以包括硬件地址(MAC), 软件地址 (IP), TCP, UDP, ICMP 等封包的信息都可以进行过滤分析的功能，因此用途非常的广泛。(其实主要分析的是OSI 七层协议的 2, 3, 4 层) 在 Linux 上面我们使用核心内建的 Netfilter 这个机制，而 Netfilter 提供了 iptables 这个软件来作为防火墙封包过滤的指令。由于 Netfilter 是核心内建的功能，因此他的效率非常的高！非常适合于一般小型环境的设定！Netfilter 利用一些封包过滤的规则设定，来定义出什么资料可以接收，什么资料需要剔除，以达到保护主机的目的！ （2）TCP Wrappers (程序控管)另一种抵挡封包进入的方法，为透过服务器程序的外挂 (tcpd) 来处置的！与封包过滤不同的是，这种机制主要是分析谁对某程序进行存取，然后透过规则去分析该服务器程序谁能够联机、谁不能联机。由于主要是透过分析服务器程序来控管，因此与启动的端口无关，只与程序的名称有关。举例来说，我们知道 FTP 可以启动在非正规的 port 21 进行监听，当你透过 Linux 内建的 TCP wrappers 限制 FTP 时，那么你只要知道 FTP 的软件名称 (vsftpd) ，然后对他作限制，则不管 FTP 启动在哪个端口，都会被该规则管理的。 （3）Proxy (代理服务器)其实代理服务器是一种网络服务，它可以【代理】用户的需求，而代为前往服务器取得相关的资料。就有点像底下这个图示吧： 以上图为例，当 Client 端想要前往 Internet 取得 Google 的数据时，他取得数据的流程是这样的： 1.client 会向 proxy server 要求数据，请 proxy 帮忙处理；2.proxy 可以分析使用者的 IP 来源是否合法？使用者想要去的 Google 服务器是否合法？如果这个 client 的要求都合法的话，那么 proxy 就会主动的帮忙 client 前往 Google 取得资料；3.Google 所回传的数据是传给 proxy server 的，所以 Google 服务器上面看到的是 proxy server 的 IP；4.最后 proxy 将 Google 回传的数据送给 client。 这样了解了吗？没错，client 并没有直接连上 Internet ，所以在实线部分(步骤 1, 4)只要 Proxy 与 Client 可以联机就可以了！此时 client 甚至不需要拥有 public IP ！而当有人想要攻击 client 端的主机时，除非他能够攻破 Proxy server ，否则是无法与 client 联机的！另外，一般 proxy 主机通常仅开放 port 80, 21, 20 等 WWW 与 FTP 的端口而已，而且通常 Proxy 就架设在路由器上面，因此可以完整的掌控局域网络内的对外联机！让你的 LAN 变的更安全啊！ 4、防火墙的使用限制从前面的分析中，我们已经知道过封包滤式防火墙主要在分析 OSI 七层协议当中的 2, 3, 4 层，既然如此的话，Linux 的 Netfilter 机制到底可以做些什么事情呢？其实可以进行的分析工作主要有： （1）拒绝让 Internet 的封包进入主机的某些端口这个应该不难了解。例如你的 port 21 这个 FTP 相关的端口，若只想要开放给内部网络的话，那么当 Internet 来的封包想要进入你的 port 21 时，就可以将该数据封包丢掉！因为我们可以分析的到该封包表头的端口号码！ （2）拒绝让某些来源 IP 的封包进入例如你已经发现某个 IP 主要都是来自攻击行为的主机，那么只要来自该 IP 的数据封包，就将他丢弃！这样也可以达到基础的安全！ （3）拒绝让带有某些特殊旗标 (flag) 的封包进入最常拒绝的就是带有 SYN 的主动联机的旗标了！只要一经发现，你就可以将该封包丢弃！（因为都是一些攻击封包，占着茅坑不拉屎！） （4）分析硬件地址 (MAC) 来决定联机与否如果你的局域网络里面有比较捣蛋的但是又具有比较高强的网络功力的高手时，如果你使用 IP 来抵挡他使用网络的权限，而他却懂得反正换一个 IP 就好了，都在同一个网域内！同样还是在搞破坏！！怎么办？没关系，我们可以死锁他的网络卡硬件地址啊！因为 MAC 是焊在网络卡上面的，所以你只要分析到该使用者所使用的 MAC 之后，可以利用防火墙将该 MAC 锁住，除非他能够一换再换他的网络卡来取得新的 MAC，否则换 IP 是没有用的！ 虽然 Netfilter 防火墙已经可以做到这么多的事情，不过，还是有很多事情没有办法透过 Netfilter 来完成！什么？设定防火墙之后还不安全啊！那当然啦！谁说设定了防火墙之后你的系统就一定安全？防火墙虽然可以防止不受欢迎的封包进入我们的网络当中，不过，某些情况下，他并不能保证我们的网络一定就很安全。举几个例子来谈一谈：（1）防火墙并不能很有效的抵挡病毒或木马程序假设你已经开放了 WWW 的服务，那么你的 WWW 主机上面，防火墙一定得要将 WWW 服务的 port 开放给 Client 端登入才行吧！否则你的 WWW 主机设定了等于没有用对吧！也就是说，只要进入你的主机的封包是要求 WWW 数据的，就可以通过你的防火墙。那好了，【万一你的 WWW 服务器软件有漏洞，或者本身向你要求 WWW 服务的该封包就是病毒在侦测你的系统】时，你的防火墙可是一点办法也没有啊！因为本来设定的规则就是会让他通过。（2）防火墙对于来自内部 LAN 的攻击较无承受力一般来说，我们对于 LAN 里面的主机都没有什么防火墙的设定，因为是我们自己的 LAN ，所以当然就设定为信任网域了！不过，LAN 里面总是可能有些网络小白，虽然他们不是故意要搞破坏，但是他们就是不懂！所以就乱用网络。这个时候就很糟糕，因为防火墙对于内部的规则设定通常比较少，所以就容易造成内部员工对于网络误用或滥用的情况。 所以，在你的 Linux 主机实地上网之前，还是得先： （1）关闭几个不安全的服务； （2）升级几个可能有问题的套件； （3）架设好最起码的安全防护–防火墙–]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜服务器架设篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>网络安全</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础网络概念（三）]]></title>
    <url>%2F2018%2F01%2F16%2F%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C%E6%A6%82%E5%BF%B5%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1、网络网络是由节点和连线构成，表示诸多对象及其相互联系。在数学上，网络是一种图，一般认为专指加权图。网络除了数学定义外，还有具体的物理含义，即网络是从某种相同类型的实际问题中抽象出来的模型。在计算机领域中，网络是信息传输、接收、共享的虚拟平台，通过它把各个点、面、体的信息联系到一起，从而实现这些资源的共享。网络是人类发展史来最重要的发明，提高了科技和人类社会的发展。 2、网络类型：地理位置： 1.局域网（LAN）：一般限定在较小的区域内，小于10km的范围，通常采用有线的方式连接起来。 2.城域网（MAN）：规模局限在一座城市的范围内，10～100km的区域。 3.广域网（WAN）：网络跨越国界、洲界，甚至全球范围。局域网和广域网是网络的热点。局域网是组成其他两种类型网络的基础，城域网一般都加入了广域网。广域网的典型代表是internet网。 4.个人网：个人局域网就是在个人工作地方把属于个人使用的电子设备（如便携电脑等）用无线技术连接起来的网络，因此也常称为无线个人局域网WPAN，其范围大约在10m左右。 传输介质： 1.有线网：采用同轴电缆和双绞线来连接的计算机网络。 2.光纤网：光纤网也是有线网的一种，但由于其特殊性而单独列出，光纤网采用光导纤维作传输介质。光纤传输距离长，传输率高，可达数千兆bps，抗干扰性强，不会受到电子监听设备的监听，是高安全性网络的理想选择。 3.无线网：用电磁波作为载体来传输数据。 3、互联网互联网（英语：Internet），又称网际网络，或音译因特网(Internet)、英特网，互联网始于1969年美国的阿帕网。是网络与网络之间所串连成的庞大网络，这些网络以一组通用的协议相连，形成逻辑上的单一巨大国际网络。通常internet泛指互联网，而Internet则特指因特网。这种将计算机网络互相联接在一起的方法可称作“网络互联”，在这基础上发展出覆盖全世界的全球性互联网络称互联网，即是互相连接一起的网络结构。互联网并不等同万维网，万维网只是一建基于超文本相互链接而成的全球性系统，且是互联网所能提供的服务其中之一。 互联网运行原理：计算机网络是由许多计算机组成的，要实现网络的计算机之间传输数据，必须要做两件事，数据传输目的地址和保证数据迅速可靠传输的措施，这是因为数据在传输 过程中很容易丢失或传错，Internet使用一种专门的计算机语言(协议)，以保证数据安全、可靠地到达指定的目的地，这种语言分两部TCP(Transmission Control Protocol 传输控制协议)和 IP (Internet Protocol网间协议)sure网络营销理论。 TCP/IP协议的数据传输过程：TCP/IP协议所采用的通信方式是分组交换方式。所谓分组交换，简单说就是数据 在传输时分成若干段，每个数据段称为一个数据包，TCP/IP协议的基本传输单位是数据包，TCP/IP协议主要包括两个主要的协议，即TCP协议和IP协议，这两个协议可以 联合使用，也可以与其他协议联合使用，它们在数据传输过程中主要完成以下功能： 1) 首先由TCP协议把数据分成若干数据包，给每个数据包写上序号，以便接收端把数据还原成原来的格式。 2) IP协议给每个数据包写上发送主机和接收主机的地址，一旦写上源地址和目的地址，数据包就可以在物理网上传送数据了。IP协议还具有利用路由算法进行路由选择的功能。 3) 这些数据包可以通过不同的传输途径(路由)进行传输，由于路径不同，加上其它的原因，可能出现顺序颠倒、数据丢失、数据失真甚至重复的现象。这些问题都由TCP协议来处理，它具有检查和处理错误的功能， 必要时还可以请求发送端重发。简言之，IP协议负责数据的传输，而TCP协议负责数据的可靠传输。 4、因特网因特网（Internet）是一组全球信息资源的总汇。有一种粗略的说法，认为INTERNET是由于许多小的网络（子网）互联而成的一个逻辑网，每个子网中连接着若干台计算机（主机）。Internet以相互交流信息资源为目的，基于一些共同的协议，并通过许多路由器和公共互联网而成，它是一个信息资源和资源共享的集合。 5、万维网WWW是环球信息网的缩写，（亦作“Web”、“WWW”、“’W3’”，英文全称为“World Wide Web”），中文名字为“万维网”，”环球网”等，常简称为Web。 分为Web客户端和Web服务器程序。 WWW可以让Web客户端（常用浏览器）访问浏览Web服务器上的页面。 是一个由许多互相链接的超文本组成的系统，通过互联网访问。在这个系统中，每个有用的事物，称为一样“资源”；并且由一个全局“统一资源标识符”（URI）标识；这些资源通过超文本传输协议（Hypertext Transfer Protocol）传送给用户，而后者通过点击链接来获得资源。万维网并不等同互联网，万维网只是互联网所能提供的服务其中之一，是靠着互联网运行的一项服务。 6、三者之间的关系互联网、因特网、万维网三者的关系是：互联网包含因特网，因特网包含万维网，凡是能彼此通信的设备组成的网络就叫互联网。所以，即使仅有两台机器，不论用何种技术使其彼此通信，也叫互联网。国际标准的互联网写法是Internet，因特网是互联网的一种。因特网可不是仅有两台机器组成的互联网，它是由上千万台设备组成的互联网。因特网使用TCP/IP协议让不同的设备可以彼此通信。但使用TCP/IP协议的网络并不一定是因特网，一个局域网也可以使用TCP/IP协议。判断自己是否接入的是因特网，首先是看自己电脑是否安装了 TCP/IP协议，其次看是否拥有一个公网地址（所谓公网地址，就是所有私网地址以外的地址）。因特网是基于TCP/IP协议实现的，TCP/IP协议由很多协议组成，不同类型的协议又被放在不同的层，其中，位于应用层的协议就有很多，比如FTP、SMTP、HTTP。只要应用层使用的是HTTP协议，就称为万维网（World Wide Web）。之所以在浏览器里输入百度网址时，能看见百度网提供的网页，就是因为您的个人浏览器和百度网的服务器之间使用的是HTTP协议在交流。 7、网络预测（1）语义网SirTim Berners-Lee（Web创始者）关于语义网的观点成为人们的重要关注已经很长一段时间了。事实上，它已经象大白鲸一样神乎其神了。总之，语义网关涉到机器之间的对话，它使得网络更加智能化，或者像Berners-Lee描述的那样，计算机“在网络中分析所有的数据—内容，链接以及人机之间的交易处理”。在另一个时候，Berners-Lee把它描述为“为数据设计的似网程序”，如对信息再利用的设计。就象Alex在《通往语义网》中写道，语义网的核心是创建可以处理事物意义的元数据来描述数据，一旦电脑装备上语义网，它将能解决复杂的语义优化问题。因此，什么时候语义网时代才会到来呢？创建语义网的组件已经出现：RDF，OWL，这些微格式只是众多组件之一.但是，Alex在他文章中指出，将需要一些时间来诠释世界的信息，然后再以某种合适的方式来捕获个人信息。一些公司，如Hakia，Powerset以及Alex自己的adaptive blue都正在积极的实现语义网，因此，未来我们将变得关系更亲密，但是我们还得等上好些年，才能看到语义网的设想实现。（2）人工智能人工智能可能会是计算机历史中的一个终极目标。从1950年，阿兰图灵提出的测试机器如人机对话能力的图灵测试开始，人工智能就成为计算机科学家们的梦想，在接下来的网络发展中，人工智能使得机器更加智能化。在这个意义上来看，这和语义网在某些方面有些相同。1997年5月，IBM公司研制的深蓝（DEEP BLUE）计算机战胜了国际象棋大师卡斯帕洛夫（KASPAROV）。大家或许不会注意到，在一些地方计算机帮助人进行其它原来只属于人类的工作，计算机以它的高速和准确为人类发挥着它的作用。人工智能始终是计算机科学的前沿学科，计算机编程语言和其它计算机软件都因为有了人工智能的进展而得以存在。相信以后人工智能会给人类的生活带来更大的改变，期待下一次的变革“智能时代”的到来！]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜服务器架设篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 系统服务之端口（port）]]></title>
    <url>%2F2018%2F01%2F15%2FLinux%E7%B3%BB%E7%BB%9F%E6%9C%8D%E5%8A%A1%E4%B9%8B%E7%AB%AF%E5%8F%A3%EF%BC%88port%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1、什么是 port ？当你启动一个网络服务，这个服务会依据 TCP/IP 的相关通讯协议启动一个端口在进行监听，那就是 TCP/UDP 封包的 port (端口)了。我们知道网络联机是双向的，服务器端得要启动一个监听的端口，客户端得要随机启动一个端口来接收响应的数据才行。那么服务器端的服务是否需要启动在固定的端口？客户端的端口是否又是固定的呢？我们将与 port 有关的资料给她汇整一下，如下： 1、服务器端启动的监听端口所对应的服务是固定的：例如 WWW 服务开启在 port 80 ，FTP 服务开启在 port 21，email 传送开启在port 25 等等，都是通讯协议上面的规范！2、客户端启动程序时，随机启动一个大于 1024 以上的端口：客户端启动的 port 是随机产生的，主要是开启在大于 1024 以上的端口。这个 port 也是由某些软件所产生的，例如浏览器、Filezilla 这个 FTP 客户端程序等等。3、一部服务器可以同时提供多种服务：所谓的【监听】是某个服务程序会一直常驻在内存当中，所以该程序启动的 port 就会一直存在。只要服务器软件激活的端口不同，那就不会造成冲突。当客户端连接到此服务器时，透过不同的端口，就可以取得不同的服务数据。所以，一部主机上面当然可以同时启动很多不同的服务！4、共 65536 个 port：由 TCP/UDP 表头数据中，就知道 port 占用 16 个位，因此一般主机会有 65536 个 port，而这些 port 又分成两个部分，以 port 1024 作区隔： 只有 root 才能启动的保留的 port：在小于 1024 的端口，都是需要以 root 的身份才能启动的，这些 port 主要是用于一些常见的通讯服务，在 Linux 系统下，常见的协议与 port 的对应是记录在 /etc/services 里面的。 大于 1024 用于 client 端的 port：在大于 1024 以上的 port 主要是作为 client 端的软件激活的 port。 5、是否需要三向交握：建立可靠的联机服务需要使用到 TCP 协议，也就需要所谓的三向交握了，如果是非面向连接的服务，例如 DNS 与视讯系统，那只要使用 UDP 协议即可。6、通讯协议可以启用在非正规的 port：我们知道浏览器默认会连接到 WWW 主机的 port 80，那么你的 WWW 是否可以启动在非 80 的其他端口？当然可以！你可以透过 WWW 软件的设定功能将该软件使用的 port 启动在非正规的端口，只是如此一来，您的客户端要连接到你的主机时，就得要在浏览器的地方额外指定你所启用的非正规的端口才行。这个启动在非正规的端口功能，常常被用在一些所谓的地下网站！另外，某些软件默认就启动在大于 1024 以上的端口，如 MySQL 数据库软件就启动在 3306。7、所谓的 port 的安全性：事实上，没有所谓的 port 的安全性！因为【Port 的启用是由服务软件所造成的】，也就是说，真正影响网络安全的并不是 port ，而是启动 port 的那个软件 (程序)！或许你偶而会听到：【没有修补过漏洞的 bind 8.x 版，很容易被黑客所入侵，请尽快升级到 bind 9.x 以后版本】，所以，对安全真正有危害的是【某些不安全的服务】而不是【开了哪些 port 】才是！因此，没有必要的服务就将他关闭吧！尤其某些网络服务还会启动一些 port ！另外，那些已启动的软件也需要持续的保持更新！ 附属：一些系统必备的软件服务说明 服务名称 服务内容 acpid 新版的电源管理模块，通常建议开启，不过，某些笔记本电脑可能不支持此项服务，那就得关闭 atd 在管理单一预约命令执行的服务，应该要启动的 crond 在管理工作排程的重要服务，请务必要启动啊！ haldaemon 作系统硬件变更侦测的服务，与 USB 设备关系很大 iptables Linux 内建的防火墙软件，这个也可以启动！ network 这个重要了吧？要网络就要有他啊！ postfix 系统内部邮件传递服务，不要随便关闭他！ rsyslog 系统的登录文件记录，很重要的，务必启动啊！ sshd 这是系统默认会启动的，可以让你在远程以文字型态的终端机登入！ xinetd 就是那个 super daemon ！所以也要启动！ 上面列出的是主机需要的重点服务，请您不要关闭他！除非你知道作了之后会有什么后果。举例来说，你如果不需要管理电源，那么将 acpid 关闭也没有关系啊！如果你不需要提供远程联机功能，那么 sshd 也可以关闭啊！那其他你不知道的服务怎办？没关系，只要不是网络服务，你都可以保留他！如果是网络服务呢？那…建议你不知道的服务就先关闭他！ 2、端口的观察：netstat, nmap现在，我们知道 port 是什么东西了，再来就是要来了解一下，我们的主机到底是开了多少的 port 呢？由于 port 的启动与服务有关，那么【服务】跟【 port 】对应的档案是哪一个？是【 /etc/services 】！而常用来观察 port 的则有底下两个程序： 1.netstat：在本机上面以自己的程序监测自己的 port； 2.nmap：透过网络的侦测软件辅助，可侦测非本机上的其他网络主机，但有违法之虞（此处略过）。（注：为什么使用nmap会违法？由于 nmap 的功能太强大了，所以很多 cracker 会直接以他来侦测别人的主机，这个时候就可能造成违法） 底下我们来说一说 netstat 命令：在做为服务器的 Linux 系统中，开启的网络服务越少越好！因为较少的服务可以较容易除错 (debug) 与了解安全漏洞，并可避免不必要的入侵管道！所以，这个时候请了解一下您的系统当中有没有哪些服务被开启了呢？要了解自己的系统当中的服务项目，最简便的方法就是使用 netstat ！这个东西不但简单，而且功能也是很不错的。1234567891011121314基本语法：[root@www ~]# netstat -[rn] &lt;==与路由有关的参数[root@www ~]# netstat -[antulpc] &lt;==与网络接口有关的参数选项与参数：与路由 (route) 有关的参数说明： -r ：列出路由表(route table)，功能如同 route 这个指令； -n ：不使用主机名与服务名称，使用 IP 与 port number ，如同 route -n与网络接口有关的参数： -a ：列出所有的联机状态，包括 tcp/udp/unix socket 等； -t ：仅列出 TCP 封包的联机； -u ：仅列出 UDP 封包的联机； -l ：仅列出有在 Listen (监听) 的服务之网络状态； -p ：列出 PID 与 Program 的檔名； -c ：可以设定几秒钟后自动更新一次，例如 -c 5 每五秒更新一次网络状态的显示； （netstat -rn 与 route -n 是相同的。） 我们先来谈一谈关于网络联机状态的输出部分，他主要是分为底下几个大项：Proto：该联机的封包协议，主要为 TCP/UDP 等封包；Recv-Q：非由用户程序连接所复制而来的总 bytes 数；Send-Q：由远程主机所传送而来，但不具有 ACK 标志的总 bytes 数，意指主动联机 SYN 或其他标志的封包所占的 bytes 数；Local Address：本地端的地址，可以是 IP (-n 参数存在时)，也可以是完整的主机名。使用的格是就是『 IP:port 』只是 IP 的格式有 IPv4 及 IPv6 的差异。 如上所示，在 port 22 的接口中，使用的 :::22 就是针对 IPv6 的显示，事实上他就相同于 0.0.0.0:22 的意思。至于 port 25 仅针对 lo 接口开放，意指 Internet 基本上是无法连接到我本机的 25 埠口啦！Foreign Address：远程的主机 IP 与 port numberstat：状态栏，主要的状态含有： ESTABLISED：已建立联机的状态； SYN_SENT：发出主动联机 (SYN 标志) 的联机封包； SYN_RECV：接收到一个要求联机的主动联机封包； FIN_WAIT1：该插槽服务(socket)已中断，该联机正在断线当中； FIN_WAIT2：该联机已挂断，但正在等待对方主机响应断线确认的封包； TIME_WAIT：该联机已挂断，但 socket 还在网络上等待结束； LISTEN：通常用在服务的监听 port ！可使用【 -l 】参数查阅。 基本上，我们常常谈到的 netstat 的功能，就是在观察网络的联机状态了，而网络联机状态中，又以观察【我目前开了多少的 port 在等待客户端的联机】以及 【目前我的网络联机状态中，有多少联机已建立或产生问题】最常见。示例：1.列出在监听的网络服务：1234567[root@www ~]# netstat -tunlctive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 0 0.0.0.0:111 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:22 0.0.0.0:* LISTENtcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN....(底下省略).... 上面说明了我的主机至少有启动 port 111, 22, 25 等，而且观察各联机接口，可发现 25 为 TCP 端口，但只针对 lo 内部循环测试网络提供服务，因特网是连不到该端口的。至于 port 22 则有提供因特网的联机功能。 2.列出已联机的网络联机状态：1234[root@www ~]# netstat -tunActive Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 52 192.168.1.100:22 192.168.1.101:2162 ESTABLISHED 从上面的数据来看，我的本地端服务器 (Local Address, 192.168.1.100) 目前仅有一条已建立的联机，那就是与 192.168.1.101 那部主机连接的联机，并且联机方向是由对方连接到我主机的 port 22 来取用我服务器的服务！ 3.删除已建立或在监听当中的联机：如果想要将已经建立，或者是正在监听当中的网络服务关闭的话，最简单的方法当然就是找出该联机的 PID，然后将他 kill 掉即可！例如下面的范例：1234[root@www ~]# netstat -tunpActive Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/P nametcp 0 52 192.168.1.100:22 192.168.1.101:2162 ESTABLISHED 1342/0 如上面的范例，我们可以找出来该联机是由 sshd 这个程序来启用的，并且他的 PID 是 1342，希望你不要心急的用 killall 这个指令，否则容易删错人 (因为你的主机里面可能会有多个 sshd 存在)，应该要使用 kill 这个指令才对！[root@www ~]# kill -9 1342 3、端口与服务的启动/关闭及开机时状态设定其实 port 是由执行某些软件之后被软件激活的。所以要关闭某些 port 时，那就直接将某个程序给他关闭就是了！关闭的方法你当然可以使用 kill，不过这毕竟不是正统的解决之道，因为 kill 这个指令通常具有强制关闭某些程序的功能，但我们想要正常的关闭该程序！所以，就利用系统给我们的 script 来关闭就好了。一般传统的服务有哪几种类型？ stand alone 与 super daemon 在一般正常的 Linux 系统环境下，服务的启动与管理主要有两种方式： Stand alone顾名思义，stand alone 就是直接执行该服务的执行档，让该执行文件直接加载到内存当中运作，用这种方式来启动可以让该服务具有较快速响应的优点。一般来说，这种服务的启动 script 都会放置到 /etc/init.d/ 这个目录底下，所以你通常可以使用：【 /etc/init.d/sshd restart 】之类的方式来重新启动这种服务； Super daemon用一个超级服务作为总管，来统一管理某些特殊的服务。在 CentOS 6.x 里面使用的则是 xinetd 这个 super daemon ！这种方式启动的网络服务虽然在响应上速度会比较慢，不过，可以透过 super daemon 额外提供一些控管，例如控制何时启动、何时可以进行联机、那个 IP 可以连进来、是否允许同时联机等等。通常个别服务的配置文件放置在 /etc/xinetd.d/ 当中，但设定完毕后需要重新以【 /etc/init.d/xinetd restart 】重新来启动才行！ 例题：我们知道系统的 Telnet 服务通常是以 super daemon 来控管的，请您启动您系统的 telnet 试看看。答：1、要启动 telnet 首先必须要已经安装了 telnet 的服务器才行，所以请先以 rpm 查询看看是否有安装 telnet-server 呢？【rpm -qa | grep telnet-server】如果没有安装的话，请利用原版光盘来安装，或者使用【yum install telnet-server】安装一下先；2、由于是 super daemon 控管，所以请编辑 /etc/xinetd.d/telnet 这个档案，将其中的【disable = yes】改成【disable = no】之后以【/etc/init.d/xinetd restart】重新启动 super daemon 吧！3、利用 netstat -tnlp 察看是否有启动 port 23 呢？]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜服务器架设篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>Linux</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 常用网络指令]]></title>
    <url>%2F2018%2F01%2F12%2FLinux%E5%B8%B8%E7%94%A8%E7%BD%91%E7%BB%9C%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[一、网络参数设定使用的指令ifconfig ：查询、设定网络卡与 IP 网域等相关参数；ifup, ifdown：这两个档案是 script，透过更简单的方式来启动网络接口；route ：查询、设定路由表 (route table)ip ：复合式的指令，可以直接修改上述提到的功能；（先会使用 ifconfig, ifup , ifdown 与 route 即可，等以后有经验了之后，再继续回来玩 ip 这个好玩的指令。） 1、手动/自动设定与启动/关闭 IP 参数： ifconfig, ifup, ifdown12345678910语法如下：[root@www ~]# ifconfig &#123;interface&#125; &#123;up|down&#125; &lt;== 观察与启动接口[root@www ~]# ifconfig interface &#123;options&#125; &lt;== 设定与修改接口选项与参数：interface：网络卡接口代号，包括 eth0, eth1, ppp0 等等options ：可以接的参数，包括如下： up, down ：启动 (up) 或关闭 (down) 该网络接口(不涉及任何参数) mtu ：可以设定不同的 MTU 数值，例如 mtu 1500 (单位为 byte) netmask ：就是子屏蔽网络； broadcast：就是广播地址啊！ 范例一：观察所有的网络接口(直接输入 ifconfig)12345678910111213141516[root@www ~]# ifconfigeth0 Link encap:Ethernet HWaddr FA:16:3E:C9:BF:42 inet addr:10.99.192.224 Bcast:10.99.192.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:671391959 errors:0 dropped:0 overruns:0 frame:0 TX packets:629925596 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:202240418477 (188.3 GiB) TX bytes:161760425688 (150.6 GiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:19514029692 errors:0 dropped:0 overruns:0 frame:0 TX packets:19514029692 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:4097035202575 (3.7 TiB) TX bytes:4097035202575 (3.7 TiB) 至于上表出现的各项数据是这样的(数据排列由上而下、由左而右)：eth0：就是网络卡的代号，也有 lo 这个 loopback ；HWaddr：就是网络卡的硬件地址，俗称的 MAC 是也；inet addr：IPv4 的 IP 地址，后续的 Bcast, Mask 分别代表的是 Broadcast 与 netmask！inet6 addr：是 IPv6 的版本的 IP ，我们没有使用，所以略过；MTU：最大传输单位RX：那一行代表的是网络由启动到目前为止的封包接收情况，packets 代表封包数、errors 代表封包发生错误的数量、dropped 代表封包由于有问题而遭丢弃的数量等等TX：与 RX 相反，为网络由启动到目前为止的传送情况；collisions：代表封包碰撞的情况，如果发生太多次，表示你的网络状况不太好；txqueuelen：代表用来传输数据的缓冲区的储存长度；RX bytes, TX bytes：总接收、发送字节总量 12ifup/ifdown 语法如下：[root@www ~]# ifup &#123;interface&#125; 2、路由修改： route主机之间一定要有路由才能够互通 TCP/IP 的协议，否则就无法进行联机啊！一般来说，只要有网络接口，该接口就会产生一个路由，所以我们安装的主机有一个 eth0 的接口，看起来就会是这样：123456789101112[root@www ~]# route [-nee][root@www ~]# route add [-net|-host] [网域或主机] netmask [mask] [gw|dev][root@www ~]# route del [-net|-host] [网域或主机] netmask [mask] [gw|dev]观察的参数： -n ：不要使用通讯协议或主机名，直接使用 IP 或 port number； -ee ：使用更详细的信息来显示增加 (add) 与删除 (del) 路由的相关参数： -net ：表示后面接的路由为一个网域； -host ：表示后面接的为连接到单部主机的路由； netmask ：与网域有关，可以设定 netmask 决定网域的大小； gw ：gateway 的简写，后续接的是 IP 的数值，与 dev 不同； dev ：如果只是要指定由那一块网络卡联机出去，则使用这个设定，后面接 eth0 等 范例一：单纯的观察路由状态123456[root@www ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface10.99.192.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0169.254.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth00.0.0.0 10.99.192.1 0.0.0.0 UG 0 0 0 eth0 上面的信息需要知道的一些参数含义：Destination, Genmask：这两个玩意儿就是分别是 network 与 netmask ！所以这两个咚咚就组合成为一个完整的网域！Gateway：该网域是通过哪个 gateway 连接出去的？如果显示 0.0.0.0 表示该路由是直接由本机传送，亦即可以透过局域网络的 MAC 直接传讯；如果有显示 IP 的话，表示该路由需要经过路由器 (通讯闸) 的帮忙才能够传送出去。Flags：总共有多个旗标，代表的意义如下： U (route is up)：该路由是启动的； H (target is a host)：目标是一部主机 (IP) 而非网域； G (use gateway)：需要透过外部的主机 (gateway) 来转递封包； R (reinstate route for dynamic routing)：使用动态路由时，恢复路由信息的旗标； D (dynamically installed by daemon or redirect)：已经由服务或转 port 功能设定为动态路由 M (modified from routing daemon or redirect)：路由已经被修改了； ! (reject route)：这个路由将不会被接受(用来抵挡不安全的网域！)Iface：这个路由传递封包的接口。 3、网络参数综合指令： ip他就是整合了 ifconfig 与 route 这两个指令。ifup 就是利用 ip 这个指令来达成的。12345678[root@www ~]# ip [option] [动作] [指令]选项与参数：option ：设定的参数，主要有： -s ：显示出该装置的统计数据(statistics)，例如总接受封包数等；动作：亦即是可以针对哪些网络参数进行动作，包括有： link ：关于装置 (device) 的相关设定，包括 MTU, MAC 地址等等 addr/address ：关于额外的 IP 协议，例如多 IP 的达成等等； route ：与路由有关的相关设定 由上面的语法我们可以知道， ip 除了可以设定一些基本的网络参数之外，还能够进行额外的 IP 协议，包括多 IP 的达成，真是太完美了！底下我们就分三个部分 (link,addr, route) 来介绍这个 ip 指令吧！ （1）关于装置接口 (device) 的相关设定： ip linkip link 可以设定与装置 (device) 有关的相关参数，包括 MTU 以及该网络接口的 MAC 等等，当然也可以启动 (up) 或关闭 (down) 某个网络接口啦！整个语法是这样的：1234567891011[root@www ~]# ip [-s] link show &lt;== 单纯的查阅该装置相关的信息[root@www ~]# ip link set [device] [动作与参数]选项与参数：show：仅显示出这个装置的相关内容，如果加上 -s 会显示更多统计数据；set ：可以开始设定项目， device 指的是 eth0, eth1 等等界面代号；动作与参数：包括有底下的这些动作：up|down ：启动 (up) 或关闭 (down) 某个接口，其他参数使用默认的以太网络；address ：如果这个装置可以更改 MAC 的话，用这个参数修改！name ：给予这个装置一个特殊的名字；mtu ：就是最大传输单元啊！（使用 ip link show 可以显示出整个装置接口的硬件相关信息，如上所示，包括网卡地址(MAC)、MTU 等等） （2）关于额外的 IP 相关设定： ip address如果说 ip link 是与 OSI 七层协定 的第二层资料连阶层有关的话，那么 ip address (ip addr) 就是与第三层网络层有关的参数啦！ 主要是在设定与 IP 有关的各项参数，包括 netmask, broadcast 等等。12345678910111213141516[root@www ~]# ip address show &lt;==就是查阅 IP 参数啊！[root@www ~]# ip address [add|del] [IP 参数] [dev 装置名] [相关参数]选项与参数：show ：单纯的显示出接口的 IP 信息啊；add|del ：进行相关参数的增加 (add) 或删除 (del) 设定，主要有：IP 参数：主要就是网域的设定，例如 192.168.100.100/24 之类的设定；dev ：这个 IP 参数所要设定的接口，例如 eth0, eth1 等等；相关参数：主要有底下这些： broadcast：设定广播地址，如果设定值是 + 表示『让系统自动计算』 label ：亦即是这个装置的别名，例如 eth0:0 就是了！ scope ：这个界面的领域，通常是这几个大类： global ：允许来自所有来源的联机； site ：仅支持 IPv6 ，仅允许本主机的联机； link ：仅允许本装置自我联机； host ：仅允许本主机内部的联机； 所以当然是使用 global ！预设也是 global ！ （3）关于路由的相关设定： ip route这个项目当然就是路由的观察与设定啰！事实上， ip route 的功能几乎与 route 这个指令差不多，但是，他还可以进行额外的参数设计，例如 MTU 的规划等等，相当的强悍啊！123456789[root@www ~]# ip route show &lt;==单纯的显示出路由的设定而已[root@www ~]# ip route [add|del] [IP 或网域] [via gateway] [dev 装置]选项与参数：show ：单纯的显示出路由表，也可以使用 list ；add|del ：增加 (add) 或删除 (del) 路由的意思。IP 或网域：可使用 192.168.50.0/24 之类的网域或者是单纯的 IP ；via ：从那个 gateway 出去，不一定需要；dev ：由那个装置连出去，这就需要了！mtu ：可以额外的设定 MTU 的数值喔！ 二、网络侦错与观察指令1、两部主机两点沟通： ping12345678910[root@www ~]# ping [选项与参数] IP选项与参数：-c 数值：后面接的是执行 ping 的次数，例如 -c 5 ；-n ：在输出数据时不进行 IP 与主机名的反查，直接使用 IP 输出(速度较快)；-s 数值：发送出去的 ICMP 封包大小，预设为 56bytes，不过你可以放大此一数值；-t 数值：TTL 的数值，预设是 255，每经过一个节点就会少一；-W 数值：等待响应对方主机的秒数。-M [do|dont] ：主要在侦测网络的 MTU 数值大小，两个常见的项目是： do ：代表传送一个 DF (Don&apos;t Fragment) 旗标，让封包不能重新拆包与打包； dont：代表不要传送 DF 旗标，表示封包可以在其他主机上拆包与打包 范例一：侦测一下 172.24.170.43 这部 DNS 主机是否存在？123456789[root@www ~]# ping -c 3 172.24.170.43PING 172.24.170.43 (172.24.170.43) 56(84) bytes of data.64 bytes from 172.24.170.43: icmp_seq=0 ttl=118 time=3.59 ms64 bytes from 172.24.170.43: icmp_seq=1 ttl=118 time=4.41 ms64 bytes from 172.24.170.43: icmp_seq=2 ttl=118 time=2.48 ms--- 172.24.170.43 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2002msrtt min/avg/max/mdev = 2.485/3.495/4.411/0.792 ms, pipe 2 ping 最简单的功能就是传送 ICMP 封包去要求对方主机回应是否存在于网络环境中，上面的响应消息当中，几个重要的项目是这样的：64 bytes：表示这次传送的 ICMP 封包大小为 64 bytes 这么大，这是默认值，在某些特殊场合中，例如要搜索整个网络内最大的 MTU 时，可以使用 -s 2000 之类的数值来取代；icmp_seq=0：ICMP 所侦测进行的次数，第一次编号为 0 ；ttl=118：TTL 与 IP 封包内的 TTL 是相同的，每经过一个带有 MAC 的节点 (node) 时，例如 router, bridge 时， TTL 就会减少一，预设的 TTL 为 255 ，你可以透过 -t 150 之类的方法来重新设定预设 TTL 数值；time=3.59 ms：响应时间，单位有 ms(0.001 秒)及 us(0.000001 秒)，一般来说，越小的响应时间，表示两部主机之间的网络联机越良好！（注：如果你忘记加上 -c 3 这样的规定侦测次数，那就得要使用 [ctrl]-c 将他结束掉了！） 用 ping 追踪路径中的最大 MTU 数值现在我们知道网络卡的 MTU 修改可以透过 ifconfig 或者是 ip 等指令来达成，那么追踪整个网络传输的最大 MTU 时，又该如何查询？最简单的方法当然是透过 ping 传送一个大封包， 并且不许中继的路由器或 switch 将该封包重组，那就能够处理啦！没错！可以这样的：范例二：找出最大的 MTU 数值123456789101112131415161718[root@www ~]# ping -c 2 -s 1000 -M do 172.24.170.43PING 172.24.170.43 (172.24.170.43) 1000(1028) bytes of data.1008 bytes from 172.24.170.43: icmp_seq=0 ttl=118 time=2.28 ms1008 bytes from 172.24.170.43: icmp_seq=1 ttl=118 time=3.40 ms--- 172.24.170.43 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1000msrtt min/avg/max/mdev = 2.281/2.843/3.406/0.565 ms, pipe 2# 如果有响应，那就是可以接受这个封包，如果无响应，那就表示这个 MTU 太大了。[root@www ~]# ping -c 2 -s 8000 -M do 172.24.170.43PING 172.24.170.43 (172.24.170.43) 8000(8028) bytes of data.From 10.99.192.224 icmp_seq=0 Frag needed and DF set (mtu = 1500)From 10.99.192.224 icmp_seq=0 Frag needed and DF set (mtu = 1500)--- 172.24.170.43 ping statistics ---0 packets transmitted, 0 received, +2 errors# 这个错误讯息是说，本地端的 MTU 才到 1500 而已，你要侦测 8000 的 MTU# 根本就是无法达成的！那要如何是好？用前一小节介绍的 ip link 来进行MTU 设定吧！ 不过，你需要知道的是，由于 IP 封包表头 (不含 options) 就已经占用了 20bytes ，再加上 ICMP 的表头有 8 bytes ，所以当然你在使用 -s size 的时候，那个封包的大小就得要先扣除 (20+8=28) 的大小了。 因此如果要使用 MTU 为 1500 时，就得要下达【 ping -s 1472 -M do xx.yy.zz.ip 】才行啊！ 另外，由于本地端的网络卡 MTU 也会影响到侦测，所以如果想要侦测整个传输媒体的 MTU 数值，那么每个可以调整的主机就得要先使用 ifcofig 或 ip 先将 MTU 调大，然后再去进行侦测， 否则就会出现像上面提供的案例一样，可能会出现错误讯息的！ 如果是要连上 Internet 的主机，注意不要随便调整 MTU ，因为我们无法知道 Internet 上面的每部机器能够支持的 MTU 到多大，因为……不是我们能够管的到的。 2、两主机间各节点分析： traceroute我们前面谈到的指令大多数都是针对主机的网络参数设定所需要的，而 ping 是两部主机之间的回声与否判断， 那么有没有指令可以追踪两部主机之间通过的各个节点(node) 通讯状况的好坏呢？举例来说，如果我们联机到 yahoo 的速度比平常慢，你觉得是 (1)自己的网络环境有问题？ (2)还是外部的 Internet 有问题？如果是 (1)的话，我们当然需要检查自己的网络环境啊，看看是否又有谁中毒了？但如果是 Internet的问题呢？那只有『等等等』啊！ 判断是 (1) 还是 (2) 就得要使用 traceroute 这个指令！123456789101112[root@www ~]# traceroute [选项与参数] IP选项与参数：-n ：可以不必进行主机的名称解析，单纯用 IP ，速度较快！-U ：使用 UDP 的 port 33434 来进行侦测，这是预设的侦测协议；-I ：使用 ICMP 的方式来进行侦测；-T ：使用 TCP 来进行侦测，一般使用 port 80 测试-w ：若对方主机在几秒钟内没有回声就宣告不治...预设是 5 秒-p 端号：若不想使用 UDP 与 TCP 的预设埠号来侦测，可在此改变埠号。-i 装置：用在比较复杂的环境，如果你的网络接口很多很复杂时，才会用到这个参数； 举例来说，你有两条 ADSL 可以连接到外部，那你的主机会有两个ppp， 你可以使用 -i 来选择是 ppp0 还是 ppp1 -g 路由：与 -i 的参数相仿，只是 -g 后面接的是 gateway 的 IP 就是了。 范例一：侦测本机到 yahoo 去的各节点联机状态123456789101112131415[root@www ~]# traceroute -n tw.yahoo.comtraceroute to tw.yahoo.com (119.160.246.241), 30 hops max, 40 bytepackets1 192.168.1.254 0.279 ms 0.156 ms 0.169 ms2 172.20.168.254 0.430 ms 0.513 ms 0.409 ms3 10.40.1.1 0.996 ms 0.890 ms 1.042 ms4 203.72.191.85 0.942 ms 0.969 ms 0.951 ms5 211.20.206.58 1.360 ms 1.379 ms 1.355 ms6 203.75.72.90 1.123 ms 0.988 ms 1.086 ms7 220.128.24.22 11.238 ms 11.179 ms 11.128 ms8 220.128.1.82 12.456 ms 12.327 ms 12.221 ms9 220.128.3.149 8.062 ms 8.058 ms 7.990 ms10 * * *11 119.160.240.1 10.688 ms 10.590 ms 119.160.240.3 10.047 ms12 * * * &lt;==可能有防火墙装置等情况发生所致 这个 traceroute 挺有意思的，这个指令会针对欲连接的目的地之所有 node 进行 UDP 的逾时等待，例如上面的例子当中，由鸟哥的主机连接到 Yahoo 时，他会经过 12 个节点以上，traceroute 会主动的对这 12 个节点做 UDP 的回声等待，并侦测回复的时间，每节点侦测三次，最终回传像上头显示的结果。 你可以发现每个节点其实回复的时间大约在 50 ms 以内，算是还可以的 Internet 环境了。 比较特殊的算是第 10/12 个，会回传星号的，代表该 node 可能设有某些防护措施，让我们发送的封包信息被丢弃所致。 因为我们是直接透过路由器转递封包，并没有进入路由器去取得路由器的使用资源，所以某些路由器仅支持封包转递，并不会接受来自客户端的各项侦测啦！此时就会出现上述的问题。 3、察看本机的网络联机与后门： netstat如果你觉得你的某个网络服务明明就启动了，但是就是无法造成联机的话，那么应该怎么办？首先你应该要查询一下自己的网络接口所监听的端口 (port) 来看看是否真的有启动，因为有时候屏幕上面显示的 [OK] 并不一定是 OK 啊！1234567891011121314[root@www ~]# netstat -[rn] &lt;==与路由有关的参数[root@www ~]# netstat -[antulpc] &lt;==与网络接口有关的参数选项与参数：与路由 (route) 有关的参数说明：-r ：列出路由表(route table)，功能如同 route 这个指令；-n ：不使用主机名与服务名称，使用 IP 与 port number ，如同 route -n与网络接口有关的参数：-a ：列出所有的联机状态，包括 tcp/udp/unix socket 等；-t ：仅列出 TCP 封包的联机；-u ：仅列出 UDP 封包的联机；-l ：仅列出有在 Listen (监听) 的服务之网络状态；-p ：列出 PID 与 Program 的檔名；-c ：可以设定几秒钟后自动更新一次，例如 -c 5 每五秒更新一次网络状态的显示；（netstat -rn 与 route -n 是相同的。） 我们先来谈一谈关于网络联机状态的输出部分，他主要是分为底下几个大项：Proto：该联机的封包协议，主要为 TCP/UDP 等封包；Recv-Q：非由用户程序连接所复制而来的总 bytes 数；Send-Q：由远程主机所传送而来，但不具有 ACK 标志的总 bytes 数，意指主动联机 SYN 或其他标志的封包所占的 bytes 数；Local Address：本地端的地址，可以是 IP (-n 参数存在时)，也可以是完整的主机名。使用的格是就是『 IP:port 』只是 IP 的格式有 IPv4 及 IPv6 的差异。 如上所示，在 port 22 的接口中，使用的 :::22 就是针对 IPv6 的显示，事实上他就相同于 0.0.0.0:22 的意思。至于 port 25 仅针对 lo 接口开放，意指 Internet 基本上是无法连接到我本机的 25 埠口啦！Foreign Address：远程的主机 IP 与 port numberstat：状态栏，主要的状态含有： ESTABLISED：已建立联机的状态； SYN_SENT：发出主动联机 (SYN 标志) 的联机封包； SYN_RECV：接收到一个要求联机的主动联机封包； FIN_WAIT1：该插槽服务(socket)已中断，该联机正在断线当中； FIN_WAIT2：该联机已挂断，但正在等待对方主机响应断线确认的封包； TIME_WAIT：该联机已挂断，但 socket 还在网络上等待结束； LISTEN：通常用在服务的监听 port ！可使用『 -l 』参数查阅。 基本上，我们常常谈到的 netstat 的功能，就是在观察网络的联机状态了，而网络联机状态中，又以观察【我目前开了多少的 port 在等待客户端的联机】以及 【目前我的网络联机状态中，有多少联机已建立或产生问题】最常见。]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜服务器架设篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础网络概念（二）]]></title>
    <url>%2F2018%2F01%2F10%2F%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C%E6%A6%82%E5%BF%B5%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、TCP/IP 的链结层相关协议TCP/IP 最底层的链结层主要与硬件比较有关系，因此底下我们主要介绍一些 WAN 与 LAN 的硬件。同时会开始介绍那重要的 CSMA/CD 的以太网络协议，以及相关的硬件与 MAC 讯框格式等。 （1）广域网使用的设备广域网使用的设备价格较为低廉。不过广域网使用到的设备非常的多，一般用户通常会接触到的主要是 ADSL 调制解调器或者是光纤到大厦，以及第四台的 Cable 宽带等。在这里我们先介绍一些比较常见的设备，如果以后你有机会接触到其他设备，再请依据需求自行查阅相关书籍！ 1. 传统电话拨接：透过 ppp 协议早期网络大概都只能透过调制解调器加上电话线以及计算机的九针串行端口(以前接鼠标或游戏杆的插孔)，然后透过 Point-to-Point Protocol (PPP 协议)配合拨接程序来取得网络 IP 参数，这样就能够上网了。不过这样的速度非常慢，而且当电话拨接后， 就不能够讲电话了！因为 PPP 支持 TCP/IP, NetBEUI,IPX/SPX 等通讯协议，所以使用度非常广！2. 整合服务数字网络 (Integrated Services Digital Network, ISDN)也是利用现有的电话线路来达成网络联机的目的，只是联机的两端都需要有 ISDN 的调制解调器来提供联机功能。ISDN 的传输有多种通道可供使用，并且可以将多个信道整合应用，因此速度可以成倍成长。基本的 B 信道速度约为 64Kbps， 但如美国规格使用 23 个以上的通道来达成联机，此时速度可达1.5Mbps 左右。3. 非对称数位用路回路 (Asymmetric Digital Subscriber Line, ADSL)：透过 pppoe 协议也是透过电话线来拨接后取得 IP 的一个方法，只不过这个方式使用的是电话的高频部分，与一般讲电话的频率不同。 因此妳可以一边使用 ADSL 上网同时透过同一个电话号码来打电话聊天。在台湾，由于上传/下载的带宽不同， 因此才称为非对称的回路。ADSL 同样使用调制解调器，只是他透过的是 PPPoE (PPP over Ethernet) 的方法！ 将 PPP 仿真在以太网络卡上，因此你的主机需要透过一张网络卡来连接到调制解调器，并透过拨接程序来取得新的接口 (ppp0)！4. 电缆调制解调器 (Cable modem)主要透过有线电视 (台湾所谓的第四台) 使用的缆线作为网络讯号媒体，同样需要具备调制解调器来连接到 ISP，以取得网络参数来上网。 Cable modem 的带宽主要是分享型的，所以通常具有区域性，并不是你想装就能装的哩！ （2）局域网络使用的设备-以太网络在局域网络的环境中，我们最常使用的就是以太网络。当然啦，在某些超高速网络应用的环境中， 还可能会用到价格相当昂贵的光纤信道哩。只是如同前面提到的，以太网络因为已经标准化了，设备设置费用相对低廉， 所以一般你会听到什么网络线或者是网络媒体，几乎都是使用以太网络来架设的环境！ 只是这里还是要提醒您，整个网络世界并非仅有以太网络这个硬件接口！ 1. 以太网络的速度与标准以太网络的流行主要是它成为国际公认的标准所致。早先 IEEE 所制订的以太网络标准为 802.3 的 IEEE 10BASE5 ，这个标准主要的定义是：【10 代表传输速度为10Mbps，BASE 表示采用基频信号来进行传输，至于 5 则是指每个网络节点之间最长可达 500 公尺。】由于网络的传输信息就是 0 与 1 啊，因此，数据传输的单位为每秒多少 bit ，亦即是 M bits/second, Mbps 的意思。那么为何制订成为 10Mbps 呢？ 这是因为早期的网络线压制的方法以及相关的制作方法，还有以太网络卡制作的技术并不是很好，加上当时的数据传输需求并没有像现在这么高，所以 10Mbps 已经可以符合大多数人的需求了。（注：我们看到的网络提供者 (Internet Services Provider, ISP) 所宣称他们的 ADSL 传输速度可以达到 下行/上行 2Mbps/128Kbps(Kbits per second) 时，那个 Kb 指的可不是 bytes 而是 bits！所以 2M/128K 在实际的档案大小传输速度上面，最大理论的传输为 256KBps/16 KBps(KBytes per second)，所以正常下载的速度约在每秒 100~200 KBytes 之间吶！同样的道理，在网络卡或者是一些网络媒体的广告上面，他们都会宣称自己的产品可以自动辨识传输速度为 10/100 Mbps ( Mega-bits per second)，该数值还是得再除以 8 才是我们一般常用的档案容量计算的单位bytes！） 早期的网络线使用的是旧式的同轴电缆线，这种线路在现在几乎已经看不到了。取而代之的是类似传统电话线的双绞线 (Twisted Pair Ethernet) ，IEEE 并将这种线路的以太网络传输方法制订成为 10BASE-T 的标准。 10BASE-T 使用的是 10 Mbps 全速运作且采用无遮蔽式双绞线 (UTP) 的网络线。此外， 10BASE-T 的 UTP 网络线可以使用星形联机(star)， 也就是以一个集线器为中心来串连各网络设备的一个方法。不同于早期以一条同轴电缆线链接所有的计算机的 bus 联机，透过星形联机的帮助，我们可以很简单的加装其他的设备或者是移除其他设备，而不会受到其他装置的影响，这对网络设备的扩充性与除错来说， 都是一项相当棒的设计！也因此 10BASE-T 让以太网络设备的销售额大幅提升！ 后来 IEEE 更制订了 802.3u 这个支持到 100Mbps 传输速度的 100BASE-T 标准，这个标准与 10BASE-T 差异不大， 只是双绞线线材制作需要更精良，同时也已经支持使用了四对绞线的网络线了，也就是目前很常见的八蕊网络线！这种网络线我们常称为等级五 (Category 5, CAT5) 的网络线。 这种传输速度的以太网络就被称为 Fastethernet 。至于目前我们常常听到的 Gigabit 网络速度 1000 Mbps 又是什么吶？那就是 Gigabit ethernet ！只是 Gigabit ethernet 的网络线就需要更加的精良。 为什么每当传输速度增加时，网络线的要求就更严格呢？这是因为当传输速度增加时，线材的电磁效应相互干扰会增强， 因此在网络线的制作时就得需要特别注意线材的质料以及内部线蕊心之间的缠绕情况配置等，以使电子流之间的电磁干扰降到最小，才能使传输速度提升到应有的 Gigabit 。所以说，在以太网络世界当中，如果你想要提升原有的 fast ethernet 到 gigabit ethernet 的话，除了网络卡需要升级之外，主机与主机之间的网络线，以及连接主机线路的集线器/交换器等，都必须要提升到可以支持 gigabit 速度等级的设备才行！ 2.以太网络的网络线接头 (跳线/并行线)前面提到，网络的速度与线材是有一定程度的相关性的，那么线材的接头又是怎样呢？目前在以太网络上最常见到的接头就是 RJ-45 的网络接头，共有八蕊的接头，有点像是胖了的电话线接头， 如下所示：而 RJ-45 接头又因为每条蕊线的对应不同而分为 568A 与 568B 接头，这两款接头内的蕊线对应如下表：事实上，虽然目前的以太网络线有八蕊且两两成对，但实际使用的只有 1,2,3,6 蕊而已，其他的则是某些特殊用途的场合才会使用到。但由于主机与主机的联机以及主机与集线器的联机时，所使用的网络线脚位定义并不相同，因此由于接头的不同网络线又可分为两种： 跳线：一边为 568A 一边为 568B 的接头时称为跳线，用在直接链接两部主机的网络卡。 并行线：两边接头同为 568A 或同为 568B 时称为并行线，用在链接主机网络卡与集线器之间的线材； （3）以太网络的传输协议：CSMA/CD整个以太网络的重心就是以太网络卡！所以说，以太网络的传输主要就是网络卡对网络卡之间的数据传递而已。每张以太网络卡出厂时，就会赋予一个独一无二的卡号，那就是所谓的 MAC (Media Access Control)！理论上，网卡卡号是不能修改的，不过某些笔记本电脑的网卡卡号是能够修改的！那么以太网络的网卡之间数据是如何传输的呢？那就得要谈一下 IEEE 802.3 的标准 CSMA/CD (Carrier SenseMultiple Access with Collision Detection) 了！我们以下图来作为简介，下图内的中心点为集线器，各个主机都是联机到集线器，然后透过集线器的功能向所有主机发起联机的。 集线器是一种网络共享媒体，什么是网络共享媒体啊？想象一下上述的环境就像一个十字路口，而集线器就是那个路口！ 这个路口一次只允许一辆车通过，如果两辆车同时使用这个路口，那么就会发生碰撞的车祸事件啊！那就是所谓的共享媒体。 也就是说，网络共享媒体在单一时间点内， 仅能被一部主机所使用。 理解了共享媒体的意义后，再来，我们就得要讨论，那么以太网络的网卡之间是如何传输的呢？我们以上图中的 A 要发给 D 网卡为例好了，简单的说， CSMA/CD 搭配上述的环境，它的传输情况需要有以下的流程： 1.监听媒体使用情况 (Carrier Sense)：A 主机要发送网络封包前，需要先对网络媒体进行监听，确认没有人在使用后， 才能够发送出讯框；2.多点传输 (Multiple Access)：A 主机所送出的数据会被集线器复制一份，然后传送给所有连接到此集线器的主机！ 也就是说， A 所送出的数据， B, C, D 三部计算机都能够接收的到！但由于目标是 D 主机，因此 B 与 C 会将此讯框数据丢弃，而 D 则会抓下来处理；3.碰撞侦测 (Collision Detection)：该讯框数据附有检测能力，若其他主机例如 B 计算机也刚好在同时间发送讯框数据时， 那么 A 与 B 送出的数据碰撞在一块 (出车祸) ，此时这些讯框就是损毁，那么 A 与 B 就会各自随机等待一个时间， 然后重新透过第一步再传送一次该讯框数据。 了解这个程序很重要吗？我们就来谈谈： 网络忙碌时，集线器灯号闪个不停，但我的主机明明没有使用网络：透过上述的流程我们会知道，不管哪一部主机发送出讯框，所有的计算机都会接收到！因为集线器会复制一份该数据给所有计算机。 因此，虽然只有一部主机在对外联机，但是在集线器上面的所有计算机灯号就都会闪个不停！ 我的计算机明明没有被入侵，为何我的数据会被隔壁的计算机窃取：透过上述的流程，我们只要在 B 计算机上面安装一套监听软件，这套软件将原本要丢弃的讯框数据捉下来分析，并且加以重组， 就能够知道原本 A 所送出的讯息了。这也是为什么我们都建议重要数据在因特网上面得要【加密】后再传输！ 既然共享媒体只有一个主机可以使用，为何大家可以同时上网：这个问题就有趣了，既然共享媒体一次只能被一个主机所使用，那么万一我传输100MB 的档案，集线器就得被我使用 80 秒 (以 10Mbps 传输时)，在这期间其他人都不可以使用吗？不是的，由于标准的讯框数据在网络卡与其他以太网络媒体一次只能传输 1500bytes，因此我的 100MB 档案就得要拆成多个小数据报，然后一个一个的传送，每个数据报传送前都要经过 CSMA/CD 的机制。 所以，这个集线器的使用权是大家抢着用的！即使只有一部主机在使用网络媒体时，那么这部主机在发送每个封包间， 也都是需要等待一段时间的 (96 bit time)！ 讯框要多大比较好？能不能修改讯框？：如上所述，那么讯框的大小能不能改变呢？因为如果讯框的容量能够增大，那么小数据报的数量就会减少， 那每个讯框传送间的等待就可以减少了！是这样没错，但是以太网络标准讯框确实定义在 1500 bytes， 但近来的超高速以太网络媒体有支持 Jumbo frame (巨型讯框,注10) 的话，那么就能够将讯框大小改为9000bytes 哩！但不是很建议大家随便修改！ （4）MAC 的封装格式上面提到的 CSMA/CD 传送出去的讯框数据，其实就是 MAC 啦！MAC 其实就是我们上面一直讲到的讯框 (frame) 啰！ 只是这个讯框上面有两个很重要的数据，就是目标与来源的网卡卡号，因此我们又简称网卡卡号为 MAC 而已。 简单的说，你可以把 MAC想成是一个在网络线上面传递的包裹，而这个包裹是整个网络硬件上面传送数据的最小单位了。 也就是说，网络线可想成是一条【一次仅可通过一个人】的独木桥， 而 MAC就是在这个独木桥上面动的人啦！接下来，来看一看 MAC 这个讯框的内容吧！ 上图中的目的地址与来源地址指的就是网卡卡号 (hardware address, 硬件地址)，我们前面提到，每一张网卡都有一个独一无二的卡号， 那个卡号的目的就在这个讯框的表头数据使用到啦！硬件地址最小由 00:00:00:00:00:00 到 FF:FF:FF:FF:FF:FF(16 进位法)， 这 6 bytes 当中，前 3bytes 为厂商的代码，后 3bytes 则是该厂商自行设定的装置码了。 在 Linux 当中，你可以使用 ifconfig 这个指令来查阅你的网络卡卡号喔！特别注意，在这个 MAC 的传送中，他仅在局域网络内生效，如果跨过不同的网域 (这个后面 IP 的部分时会介绍)，那么来源与目的的硬件地址就会跟着改变了。 这是因为变成不同网络卡之间的交流了嘛！所以卡号当然不同了！如下所示： 例如上面的图标，我的数据要由计算机 A 通过 B 后才送达 C ，而 B 计算机有两块网络卡，其中 MAC-2 与 A 计算机的 MAC-1 互通，至于 MAC-3 则与 C 计算机的MAC-4 互通。但是 MAC-1 不能与 MAC-3 与 MAC-4 互通，为啥？因为 MAC-1 这块网络卡并没有与 MAC-3 及 MAC-4 使用同样的 switch/hub 相接嘛！所以，数据的流通会变成： 1.先由 MAC-1 传送到 MAC-2 ，此时来源是 MAC-1 而目的地是 MAC-2；2.B 计算机接收后，察看该讯框，发现目标其实是 C 计算机，而为了与 C 计算机沟通， 所以他会将讯框内的来源 MAC 改为 MAC-3 ，而目的改为 MAC-4 ，如此就可以直接传送到 C 计算机了。 也就是说，只要透过 B (就是路由器) 才将封包送到另一个网域去的时候，那么讯框内的硬件地址就会被改变，然后才能够在同一个网域里面直接进行讯框的流通！（注：局域网内是mac地址寻址，网域中是通过IP寻址。） （5）集线器、交换器与相关机制共不共享很重要，集线器还是交换器？当一个很忙碌的网络在运作时，集线器 (hub) 这个网络共享媒体就可能会发生碰撞的情况， 这是因为 CSMA/CD 的缘故。那有没有办法避免这种莫名其妙的封包碰撞情况呢？有的，那就使用非共享媒体的交换器即可！ 交换器 (switch) 等级非常多，我们这里仅探讨支持 OSI 第二层的交换器。交换器与集线器最大的差异，在于交换器内有一个特别的内存， 这个内存可以记录每个 switch port 与其连接的 PC 的 MAC 地址，所以，当来自 switch 两端的 PC 要互传数据时，每个讯框将直接透过交换器的内存数据而传送到目标主机上！ 所以 switch 不是共享媒体，且 switch 的每个端口 (port) 都具有独立的带宽！ 二、TCP/IP 的网络层相关封包与数据我们现在知道要有网络的话，必须要有网络相关的硬件，而目前最常见的网络硬件接口为以太网络，包括网络线、网络卡、Hub/Switch 等等。而以太网络上面的传输使用网络卡卡号为基准的 MAC 讯框，配合 CSMA/CD 的标准来传送讯框，这就是硬件部分。在软件部分，我们知道 Internet 其实就是 TCP/IP 这个通讯协议的通称，Internet 是由 InterNIC 所统一管理的， 但其实他仅是负责分配 Internet 上面的 IP 以及提供相关的 TCP/IP 技术文件而已。不过 Internet 最重要的就是 IP ！所以，这个小节就让我们来讲讲网络层的 IP 与路由！ 1、IP 封包的封装目前因特网社会的 IP 有两种版本，一种是目前使用最广泛的 IPv4 (Internet Protocol version 4, 因特网协定第四版)， 一种则是预期未来会热门的 IPv6 。IPv4记录的地址由于仅有 32 位，预计在 2020 年前后就会分发完毕，如此一来，新兴国家或者是新的网络公司，将没有网络可以使用。为了避免这个问题发生，因此就有 IPv6的产生。IPv6 的地址可以达到 128 位，可以多出 2 的 96 次方倍的网址数量，这样的 IP 数量几乎用不完！虽然 IPv6 具有前瞻性，但目前主流媒体大多还是使用IPv4 ，因此本文主要谈到的 IP 都指 IPv4 而言。 我们在前一小节谈到 MAC 的封装，那么 IP 封包的封装也得要来了解一下，才能知道 IP 到底是如何产生的啊！ IP 封包可以达到 65535 bytes 这么大，在比 MAC 大的情况下，我们的操作系统会对 IP 进行拆解的动作。至于 IP 封装的表头资料绘制如下：(下图第一行为每个字段的 bit 数) 在上面的图示中有个地方要注意，那就是【每一行所占用的位数为 32 bits】，各个表头的内容分别介绍如下： Version(版本)宣告这个 IP 封包的版本，例如目前惯用的还是 IPv4 这个版本就在这里宣告。IHL(Internet Header Length, IP 表头的长度)告知这个 IP 封包的表头长度，使用的单位应该是字组 (word) ，一个字组为 4bytes 大小。Type of Service(服务类型)这个项目的内容为【PPPDTRUU】，表示这个 IP 封包的服务类型，主要分为： PPP：表示此 IP 封包的优先度，目前很少使用； D：若为 0 表示一般延迟(delay)，若为 1 表示为低延迟； T：若为 0 表示为一般传输量 (throughput)，若为 1 表示为高传输量； R：若为 0 表示为一般可靠度(reliability)，若为 1 表示高可靠度。 UU：保留尚未被使用。举例来说，gigabit 以太网络的种种相关规格可以让这个 IP 封包加速且降低延迟，某些特殊的标志就是在这里说明的。Total Length(总长度)指这个 IP 封包的总容量，包括表头与内容 (Data) 部分。最大可达 65535bytes。Identification(辨别码)我们前面提到 IP 袋子必须要放在 MAC 袋子当中。不过，如果 IP 袋子太大的话，就得先要将 IP 再重组成较小的袋子然后再放到 MAC 当中。而当 IP 被重组时，每个来自同一个 IP 的小袋子就得要有个标识符以告知接收端这些小袋子其实是来自同一个 IP 封包才行。也就是说，假如 IP 封包其实是 65536 那么大 (前一个 Total Length 有规定)，那么这个 IP 就得要再被分成更小的 IP 分段后才能塞进 MAC 讯框中。那么每个小 IP 分段是否来自同一个 IP 资料，这就是该标识符的作用！Flags(特殊旗标)这个地方的内容为【0DM】，其意义为： D：若为 0 表示可以分段，若为 1 表示不可分段 M：若为 0 表示此 IP 为最后分段，若为 1 表示非最后分段。Fragment Offset(分段偏移)表示目前这个 IP 分段在原始的 IP 封包中所占的位置。就有点像是序号，有这个序号才能将所有的小 IP 分段组合成为原本的 IP 封包大小！透过 Total Length, Identification, Flags 以及这个 Fragment Offset 就能够将小 IP 分段在收受端组合起来了！Time To Live(TTL, 存活时间)表示这个 IP 封包的存活时间，范围为 0-255。当这个 IP 封包通过一个路由器时，TTL 就会减一，当 TTL 为 0 时，这个封包将会被直接丢弃。说实在的，要让 IP 封包通过 255 个路由器，还挺难的！Protocol Number(协定代码)来自传输层与网络层本身的其他数据都是放置在 IP 封包当中的，我们可以在 IP 表头记载这个 IP 封包内的数据是啥，在这个字段就是记载每种数据封包的内容！在这个字段记载的代码与相关的封包协议名称如下所示：Header Checksum(表头检查码)用来检查这个 IP 表头的错误检验之用。Source Address来源的 IP 地址，从这里我们也知道 IP 是 32 位。Destination Address有来源还需要有目标才能传送，这里就是目标的 IP 地址。Options (其他参数)这个是额外的功能，提供包括安全处理机制、路由纪录、时间戳、严格与宽松之来源路由等。Padding(补齐项目)由于 Options 的内容不一定有多大，但是我们知道 IP 每个数据都必须要是 32bits，所以，若 Options 的数据不足 32 bits 时，则由 padding 主动补齐。 你只要知道 IP 表头里面含有：TTL, Protocol, 来源地址与目标地址也就够了！而这个 IP 表头的来源与目标 IP ，以及那个判断通过多少路由器的 TTL ，就能了解到这个 IP 将被如何传送到目的端的。后续各小节我们将介绍 IP 的组成与范围，还有IP 封包如何传送的机制 (路由) 等等。 2、IP 地址的组成与分级现在我们知道 IP (Internet Protocol) 其实是一种网络封包，而这个封包的表头最重要的就是那个 32 位的来源与目标地址！ 为了方便记忆，所以我们也称这个 32bits 的数值为 IP 网络地址就是了。因为网络是人类发明的，所以很多概念与邮务系统类似！ 那这个 IP 其实就类似所谓的【门牌号码】！那么这个 IP 有哪些重要的地方需要了解的呢？底下我们就来谈一谈吧！ 既然 IP 的组成是 32 bits 的数值，也就是由 32 个 0 与 1 组成的一连串数字！那么当我们思考所有跟 IP 有关的参数时，你就应该要将该参数想成是 32 位的数据！不过，因为人类对于二进制实在是不怎么熟悉，所以为了顺应人们对于十进制的依赖性，因此，就将 32 bits 的 IP 分成四小段，每段含有 8 个 bits ，将 8 个 bits 计算成为十进制，并且每一段中间以小数点隔开，那就成了目前大家所熟悉的 IP 的书写模样了。如下所示： 所以 IP 最小可以由 0.0.0.0 一直到 255.255.255.255 ！但在这一串数字中，其实还可以分为两个部分！主要分为 Net_ID (网域号码)与 Host_ID (主机号码) 两部份。我们先以 192.168.0.0 ~ 192.168.0.255 这个 Class C 的网域当作例子来说明好了： 在上面的范例当中，前面三组数字 (192.168.0) 就是网域号码，最后面一组数字则称为主机号码。至于同一个网域的定义是【在同一个物理网段内，主机的 IP 具有相同的 Net_ID ，并且具有独特的 Host_ID】，那么这些 IP 群就是同一个网域内的 IP 网段！（注：什么是物理网段呢？当所有的主机都是使用同一个网络媒体串在一起，这个时候这些主机在实体装置上面其实是联机在一起的，那么就可以称为这些主机在同一个物理网段内了！同时并请注意，同一个物理网段之内，可以依据不同的 IP 的设定，而设定成多个【IP 网段】！）（请注意，同一个 Net_ID 内，不能具有相同的 Host_ID ，否则就会发生IP冲突，可能会造成两部主机都没有办法使用网络的问题！） （1）IP 在同一网域的意义那么同一个网域该怎么设定，与将 IP 设定在同一个网域之内有什么好处呢？ 1.Net_ID 与 Host_ID 的限制：在同一个网段内，Net_ID 是不变的，而 Host_ID 则是不可重复，此外，Host_ID在二进制的表示法当中，不可同时为 0 也不可同时为 1 ，因为全为 0 表示整个网段的地址 (Network IP)，而全为 1 则表示为广播的地址 (Broadcast IP)。2.在区网内透过 IP 广播传递数据在同物理网段的主机如果设定相同的网域 IP 范围 (不可重复)，则这些主机都可以透过 CSMA/CD 的功能直接在区网内用广播进行网络的联机，亦即可以直接网卡对网卡传递数据 (透过 MAC 讯框)；3.设定不同区网在同物理网段的情况在同一个物理网段之内，如果两部主机设定成不同的 IP 网段，则由于广播地址的不同，导致无法透过广播的方式来进行联机。 此时得要透过路由器 (router)来进行沟通才能将两个网域连结在一起。4.网域的大小当 Host_ID 所占用的位越大，亦即 Host_ID 数量越多时，表示同一个网域内可用以设定主机的 IP 数量越多。 （2）IP 的分级为了 IP 管理与发放注册的方便性，InterNIC 将整个 IP 网段分为五种等级， 每种等级的范围主要与 IP 那 32 bits 数值的前面几个位有关，基本定义如下：五种分级在十进制的表示：Class A : 0.xx.xx.xx ~ 127.xx.xx.xxClass B : 128.xx.xx.xx ~ 191.xx.xx.xxClass C : 192.xx.xx.xx ~ 223.xx.xx.xxClass D : 224.xx.xx.xx ~ 239.xx.xx.xxClass E : 240.xx.xx.xx ~ 255.xx.xx.xx 根据上表的说明，我们可以知道，你只要知道 IP 的第一个十进制数，就能够约略了解到该 IP 属于哪一个等级，以及同网域 IP 数量有多少。这也是为啥我们上头选了 192.168.0.0 这一 IP 网段来说明时，会将巷子定义到第三个数字之故。不过，上表中你只要记忆三种等级，亦即是 Class A, B, C 即可，因为 Class D 是用来作为群播(multicast) 的特殊功能之用 (最常用在大批计算机的网络还原)，至于 Class E 则是保留没有使用的网段。因此，能够用来设定在一般系统上面的，就只有 Class A, B, C三种等级的 IP！ （3）IP 的种类与取得方式接下来要跟大家谈一谈也是很容易造成大家困扰的一个部分，那就是 IP 的种类！很多朋友常常听到什么【真实IP, 实体 IP, 虚拟 IP, 假的 IP….】实际上，在 IPv4 里面就只有两种 IP 的类别，分别是： Public IP : 公共 IP ，经由 INTERNIC 所统一规划的 IP，有这种 IP 才可以连上 Internet ； Private IP : 私有 IP 或保留 IP，不能直接连上 Internet 的 IP ，主要用于局域网络内的主机联机规划。 早在 IPv4 规划的时候就担心 IP 会有不足的情况，而且为了应付某些企业内部的网络设定，于是就有了私有 IP (Private IP) 的产生了。私有 IP 也分别在 A, B, C 三个 Class 当中各保留一段作为私有 IP 网段，那就是： Class A：10.0.0.0 - 10.255.255.255 Class B：172.16.0.0 - 172.31.255.255 Class C：192.168.0.0 - 192.168.255.255由于这三段 Class 的 IP 是预留使用的，所以并不能直接作为 Internet 上面的连接之用，不然的话，到处就都有相同的 IP ！那怎么行！网络岂不混乱？所以，这三个 IP 网段就只做为内部私有网域的 IP 沟通之用。简单的说，他有底下的几个限制： 私有 IP 的路由信息不能对外散播 (只能存在内部网络)； 使用私有 IP 作为来源或目的地址的封包，不能透过 Internet 来转送 (不然网络会混乱)； 关于私有 IP 的参考纪录(如 DNS)，只能限于内部网络使用 (一样的原理) 这个私有 IP 有什么好处呢？由于他的私有路由不能对外直接提供信息，所以，你的内部网络将不会直接被 Internet 上面的 Cracker 所攻击！但是，你也就无法以私有 IP 来【直接上网】！因此相当适合一些尚未具有 Public IP 的企业内部用来规划其网络之设定！否则当你随便指定一些可能是 Public IP 的网段来规划你企业内部的网络设定时，万一哪一天真的连上 Internet 了，那么岂不是可能会造成跟 Internet上面的 Public IP 相同了吗？ 此外，在没有可用的公开网络情况下，如果你想要跟同学玩联机游戏怎办？也就是说，在区网内自己玩自己的联机游戏， 此时你只要规范好所有同学在同一段私有 IP 网段中，就能够顺利的玩你的网络啦！就这么简单呢！ 那么万一你又要将这些私有 IP 送上 Internet 呢？这个简单，设定一个简单的防火墙加上 NAT (Network Address Transfer) 服务，你就可以透过 IP 伪装 (不要急，这个在后面也会提到) 来使你的私有 IP 的计算机也可以连上 Internet ！ 特殊的 loopback IP 网段还有一个奇怪的 Class A 的网域，那就是 lo 这个奇怪的网域啦 (注意：是小写的 o 而不是零喔)！这个 lo 的网络是当初被用来作为测试操作系统内部循环所用的一个网域，同时也能够提供给系统内部原本就需要使用网络接口的服务 (daemon) 所使用。 简单的说，如果你没有安装网络卡在的机器上面， 但是你又希望可以测试一下在你的机器上面设定的服务器环境到底可不可以顺利运作，这个时候怎么办，就是利用这个所谓的内部循环网络啦！这个网段在 127.0.0.0/8 这个 Class A，而且默认的主机 (localhost) 的 IP 是 127.0.0.1 ！所以，当你启动了你的 WWW 服务器，然后在你的主机的 X-Window 上面执行 http://localhost 就可以直接看到你的主页！而且不需要安装网络卡！测试很方便的！ IP 的取得方式基本上，主机的 IP 与相关网域的设定方式主要有： 1.直接手动设定(static)： 你可以直接向你的网管询问可用的 IP 相关参数，然后直接编辑配置文件 (或使用某些软件功能) 来设定你的网络。 常见于校园网络的环境中，以及向 ISP 申请固定 IP 的联机环境；2.透过拨接取得： 向你的 ISP 申请注册，取得账号密码后，直接拨接到 ISP ，你的 ISP 会透过他们自己的设定，让你的操作系统取得正确的网络参数。3.自动取得网络参数 (DHCP)： 在局域网络内会有一部主机负责管理所有计算机的网络参数，你的网络启动时就会主动向该服务器要求 IP 参数， 若取得网络相关参数后，你的主机就能够自行设定好所有服务器给你的网络参数了。最常使用于企业内部、IP 分享器后端、 校园网络与宿舍环境，及缆线宽带等联机方式。 不管是使用上面哪种方式取得的 IP ，你的 IP 都只有所谓的【 Public 与Private IP 】而已！而其他什么浮动式、固定制、 动态式等等有的没有的，就只是告诉你这个 IP 取得的方式而已。 （4）Netmask, 子网与 CIDR (Classless Interdomain Routing)前面我们提到 IP 这个 32 位的数值中分为网域号码与主机号码，其中 Class C的网域号码占了 24 位，而其实我们还可以将这样的网域切的更细，就是让第一个Host_ID 被拿来作为 Net_ID ，所以，整个 Net_ID 就有 25 bits ，至于 Host_ID 则减少为 7 bits 。在这样的情况下，原来的一个 Class C 的网域就可以被切分为两个子域，而每个子域就有【 256/2 - 2 = 126 】个可用的 IP 了！这样一来，就能够将原本的一个网域切为两个较细小的网域，方便分门别类的设计。 Netmask, 或称为 Subnet mask (子网掩码)那到底是什么参数来达成子网的切分呢？那就是 Netmask (子网掩码) 的用途！这个 Netmask 是用来定义出网域的最重要的一个参数。Netmask 的表示就成为： 三、TCP/IP 的传输层相关封包与数据网络层的 IP 封包只负责将数据送到正确的目标主机去，但这个封包到底会不会被接受，或者是有没有被正确的接收，那就不是 IP 的任务啦！那是传送层的任务之一。其中连接导向的 TCP 封包与非连接导向的 UDP 封包，这两个封包很重要！资料能不能正确的被送达目的，与这两个封包有关！ 1、可靠联机的 TCP 协议TCP 封包的表头资料：上图就是一个 TCP 封包的表头数据，各个项目以 Source Port, Destination Port及 Code 算是比较重要的项目，底下我们就分别来谈一谈各个表头数据的内容吧！ Source Port &amp; Destination Port (来源端口 &amp; 目标端口)什么是端口(port)？我们知道 IP 封包的传送主要是藉由 IP 地址连接两端，但是到底这个联机的通道是连接到哪里去呢？就是连接到 port 上头！举例来说，有的网站开放 WWW 服务器，这表示主机必须要启动一个可以让 client 端连接的端口，这个端口就是 port 。同样的，客户端想要连接到 WWW 服务器时，就必须要在 client 主机上面启动一个 port ，这样这两个主机才能够利用这条【通道】来传递封包数据！这个目标与来源 port 的纪录，可以说是 TCP 封包上最重要的参数了！Sequence Number (封包序号)由于 TCP 封包必须要带入 IP 封包当中，所以如果 TCP 数据太大时(大于 IP封包的容许程度)，就得要进行分段。这个 Sequence Number 就是记录每个封包的序号，可以让收受端重新将 TCP 的数据组合起来。Acknowledge Number (回应序号)为了确认主机端确实有收到我们 client 端所送出的封包数据，我们 client 端当然希望能够收到主机方面的响应，那就是这个 Acknowledge Number 的用途了。当 client 端收到这个确认码时，就能够确定之前传递的封包已经被正确的收下了。Data Offset (资料补偿)在图中倒数第二行有个 Options 字段！那个 Options 的字段长度是非固定的，而为了要确认整个 TCP 封包的大小，就需要这个标志来说明整个封包区段的起始位置。Reserved (保留)未使用的保留字段。Code (Control Flag, 控制标志码)当我们在进行网络联机的时候，必须要说明这个联机的状态，好让接收端了解这个封包的主要动作。这可是一个非常重要的句柄！这个字段共有 6 个 bits ，分别代表 6 个句柄，若为 1 则为启动。分别说明如下： URG(Urgent)：若为 1 则代表该封包为紧急封包，接收端应该要紧急处理，且图当中的 Urgent Pointer 字段也会被启用。 ACK(Acknowledge)：若为 1 代表这个封包为响应封包，则与上面提到的 Acknowledge Number 有关。 PSH(Push function)：若为 1 时，代表要求对方立即传送缓冲区内的其他对应封包，而无须等待缓冲区满了才送。 RST(Reset)：如果 RST 为 1 的时候，表示联机会被马上结束，而无需等待终止确认手续。这也就是说，这是个强制结束的联机，且发送端已断线。 SYN(Synchronous)：若为 1，表示发送端希望双方建立同步处理，也就是要求建立联机。通常带有 SYN 标志的封包表示【主动】要连接到对方的意思。 FIN(Finish)：若为 1 ，表示传送结束，所以通知对方数据传毕，是否同意断线，只是发送者还在等待对方的响应而已。其实每个项目都很重要，不过我们这里仅对 ACK/SYN 有兴趣而已，这样未来在谈到防火墙的时候，你才会比较清楚为啥每个 TCP 封包都有所谓的【状态】条件！那就是因为联机方向的不同所致啊。Window (滑动窗口)主要是用来控制封包的流量的，可以告知对方目前本身有的缓冲器容量(Receive Buffer) 还可以接收封包。当 Window=0 时，代表缓冲器已经额满，所以应该要暂停传输数据。 Window 的单位是 byte。Checksum(确认检查码)当数据要由发送端送出前，会进行一个检验的动作，并将该动作的检验值标注在这个字段上；而接收者收到这个封包之后，会再次的对封包进行验证，并且比对原发送的 Checksum 值是否相符，如果相符就接受，若不符就会假设该封包已经损毁，进而要求对方重新发送此封包！Urgent Pointer(紧急资料)这个字段是在 Code 字段内的 URG = 1 时才会产生作用。可以告知紧急数据所在的位置。Options(任意资料)目前此字段仅应用于表示接收端可以接收的最大数据区段容量，若此字段不使用，表示可以使用任意数据区段的大小。这个字段较少使用。Padding(补足字段)如同 IP 封包需要有固定的 32bits 表头一样， Options 由于字段为非固定，所以也需要 Padding 字段来加以补齐才行。同样也是 32 bits 的整数。 2、通讯端口谈完了 TCP 表头数据后，再来让我们了解一下这个表头里面最重要的端口信息吧！在上图的 TCP 表头数据中，最重要的就属那 16 位的两个咚咚，亦即来源与目标的端口。由于是 16 位，因此目标与来源端口最大可达 65535 号 (2 的 16 次方)！那这个埠口有什么用途呢？上面稍微提到过，网络是双向的，服务器与客户端要达成联机的话，两边应该要有一个对应的端口来达成联机信道，好让数据可以透过这个信道来进行沟通。 那么这个端口怎么打开呢？就是透过程序的执行！举例来说，网站上必须要启动一个 WWW 服务器软件，这个服务器软件会主动的唤起 port 80 来等待客户端的联机。你想要看我网站上的数据，就得要利用浏览器，填入网址，然后浏览器也会启动一个端口，并将 TCP 的表头填写目标端口为 80 ，而来源端口是你主机随机启动的一个端口，然后将 TCP 封包封装到 IP 后，送出到网络上。等 WWW 网站主机接收到你这个封包后，再依据你的端口给予回应。（注：曾经有一个朋友问过我说：【一部主机上面这么多服务，那我们跟这部主机进行联机时，该主机怎么知道我们要的数据是 WWW 还是FTP 啊？】就是透过埠口啊！因为每种 Client 软件他们所需要的数据都不相同，例如上面提到的浏览器所需要的数据是 WWW ，所以该软件默认就会向服务器的 port 80 索求数据；而如果你是使用 filezilla 来进行与服务器的 FTP 数据索求时， filezilla 当然预设就是向服务器的 FTP 相关埠口 (预设就是 port 21) 进行连接的动作啦！所以当然就可以正确无误的取得 Client 端所需要的数据了） 特权端口 (Privileged Ports)Internet 上面已经有很多规范好的固定 port (well-known port)，这些 port number 通常小于 1024 ，且是提供给许多知名的网络服务软件用的。在我们的 Linux 环境下，各网络服务与 port number 的对应默认给他写在 /etc/services档案内。底下列出几个常见的 port number 与网络服务的对应： 另外一点比较值得注意的是，小于 1024 以下的埠口要启动时， 启动者的身份必须要是 root 才行，所以才叫做特权埠口嘛！这个限制挺重要的，大家不要忘记了！不过如果是 client 端的话，由于 client 端都是主动向 server 端要数据，所以client 端的 port number 就使用随机取一个大于 1024 以上且没有在用的 port number。 Socket Pair由于网络是双向的，要达成联机的话得要服务器与客户端均提供了 IP 与埠口才行。因此，我们常常将这个成对的数据称之为 Socket Pair 了！ 来源 IP + 来源埠口 (Source Address + Source Port) 目的 IP + 目的埠口 (Destination Address + Destination Port) 由于 IP 与埠口常常连在一起说明，因此网络寻址常常使用【 IP:port 】来说明，例如想要连上网站时，正确的鸟哥网站写法应该是：【 linux.vbird.org:80 】才对！ 3、TCP 的三向交握TCP 被称为可靠的联机封包，主要是透过许多机制来达成的，其中最重要的就是三向交握的功能。我们以底下的图示来作为说明。 在上面的封包连接模式当中，在建立联机之前都必须要通过三个确认的动作，所以这种联机方式也就被称为三向交握(Three-way handshake)。 那么我们将整个流程依据上面的 A, B, C, D 四个阶段来说明一下： A:封包发起当客户端想要对服务器端联机时，就必须要送出一个要求联机的封包，此时客户端必须随机取用一个大于 1024 以上的端口口来做为程序沟通的接口。然后在 TCP 的表头当中，必须要带有 SYN 的主动联机(SYN=1)，并且记下发送出联机封包给服务器端的序号 (Sequence number = 10001) 。B:封包接收与确认封包传送当服务器接到这个封包，并且确定要接收这个封包后，就会开始制作一个同时带有 SYN=1, ACK=1 的封包，其中那个 acknowledge 的号码是要给 client 端确认用的，所以该数字会比(A 步骤)里面的 Sequence 号码多一号 (ack = 10001+1= 10002)， 那我们服务器也必须要确认客户端确实可以接收我们的封包才行，所以也会发送出一个 Sequence (seq=20001) 给客户端，并且开始等待客户端给我们服务器端的回应！C:回送确认封包当客户端收到来自服务器端的 ACK 数字后 (10002) 就能够确认之前那个要求封包被正确的收受了， 接下来如果客户端也同意与服务器端建立联机时，就会再次的发送一个确认封包 (ACK=1) 给服务器，亦即是 acknowledge = 20001+1 =20002 。D:取得最后确认若一切都顺利，在服务器端收到带有 ACK=1 且 ack=20002 序号的封包后，就能够建立起这次的联机了。 也就是说，你必须要了解【网络是双向的】这个事实！所以不论是服务器端还是客户端，都必须要透过一次 SYN 与 ACK 来建立联机，所以总共会进行三次的交谈！在设定防火墙或者是追踪网络联机的问题时，这个【双向】的概念最容易被忽略， 而常常导致无法联机成功的问题啊！切记切记！ 四、非连接导向的 UDP 协议UDP 的全名是：【User Datagram Protocol, 用户数据流协议】，UDP 与 TCP 不一样，UDP 不提供可靠的传输模式，因为他不是面向连接的一个机制，这是因为在 UDP的传送过程中，接受端在接受到封包之后，不会回复响应封包 (ACK) 给发送端，所以封包并没有像 TCP 封包有较为严密的检查机制。至于 UDP 的表头资料如下表所示： TCP 封包确实是比较可靠的，因为通过三向交握嘛！不过，也由于三向交握的缘故，TCP 封包的传输速度会较慢。至于 UDP 封包由于不需要确认对方是否有正确的收到数据，故表头数据较少，所以 UDP 就可以在 Data 处填入更多的数据了。同时 UDP 比较适合需要实时反应的一些数据流，例如影像实时传送软件等，就可以使用这类的封包传送。也就是说，UDP 传输协议并不考虑联机要求、联机终止与流量控制等特性，所以使用的时机是当数据的正确性不很重要的情况，例如网络摄影机！ 另外，很多的软件其实是同时提供 TCP 与 UDP 的传输协议的，举例来说，查询主机名的 DNS 服务就同时提供了 UDP/TCP 协议。由于 UDP 较为快速，所以我们 client 端可以先使用 UDP 来与服务器联机。 但是当使用 UDP 联机却还是无法取得正确的数据时，便转换为较为可靠的 TCP 传输协议来进行数据的传输啰。 这样可以同时兼顾快速与可靠的传输说！]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜服务器架设篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础网络概念（一）]]></title>
    <url>%2F2018%2F01%2F10%2F%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C%E6%A6%82%E5%BF%B5%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1、计算机网络组成组件我们以底下这张联机示意图来解释好了：在上图中，我们主要需要注意到的硬件有哪些呢？大致有底下这些啦： 节点 (node)：节点主要是具有网络地址 (IP) 的设备之称，因此上面图示中的一般PC、Linux 服务器、ADSL 调制解调器与网络打印机等，个别都可以称为一个 node！那中间那个集线器 (hub) 是不是节点呢？因为他不具有 IP ，因此 hub 不是节点。 服务器主机 (server)：就网络联机的方向来说，提供数据以【响应】给用户的主机，都可以被称为是一部服务器。举例来说，Yahoo 是个 WWW 服务器，昆山的 FTP (http://ftp.ksu.edu.tw/) 是个文件服务器等等。 工作站 (workstation) 或客户端 (client)：任何可以在计算机网络输入的设备都可以是工作站，若以联机发起的方向来说，主动发起联机去【要求】数据的，就可以称为是客户端 (client)。举例来说，一般 PC 打开浏览器对 Yahoo 要求新闻数据，那一般 PC 就是客户端。 网络卡 (Network Interface Card, NIC)：内建或者是外插在主机上面的一个设备，主要提供网络联机的卡片，目前大都使用具有 RJ-45 接头的以太网络卡。一般 node 上都具有一个以上的网络卡，以达成网络联机的功能。 网络接口：利用软件设计出来的网络接口，主要在提供网络地址 (IP) 的任务。一张网卡至少可以搭配一个以上的网络接口；而每部主机内部其实也都拥有一个内部的网络接口，那就是 loopback (lo) 这个循环测试接口！ 网络形态或拓朴 (topology)：各个节点在网络上面的链接方式，一般讲的是物理连接方式。举例来说，上图中显示的是一种被称为星形联机 (star) 的方式，主要是透过一个中间连接设备，以放射状的方式连接各个节点的一种形态，这就是一种拓朴。 网关 (route) 或通讯闸 (gateway)：具有两个以上的网络接口，可以连接两个以上不同的网段的设备，例如 IP 分享器就是一个常见的网关设备。那上面的 ADSL 调制解调器算不算网关呢？其实不太能算，因为调制解调器通常视为一个在主机内的网卡设备，我们可以在一般 PC 上面透过拨号软件， 将调制解调器仿真成为一张实体网卡 (ppp) ，因此他不太能算是网关设备！ 网络设备其实非常多也非常复杂，不过如果以小型企业角度来看，我们能够了解上述图示内各设备的角色，那应该也足够了！接下来，让我们继续来讨论一下网络范围的大小吧！ 2、计算机网络区域范围由于各个节点的距离不同，联机的线材与方式也有所差异，由于线材的差异也导致网络速度的不同，让网络的应用方向也不一样。 根据这些差异，早期我们习惯将网络的大小范围定义如下： 局域网络 (Local Area Network, LAN)：节点之间的传输距离较近，例如一栋大楼内，或一个学校的校区内。可以使用较为昂贵的联机材料， 例如光纤或是高质量网络线 (CAT 6) 等。网络速度较快，联机质量较佳且可靠，因此可应用于科学运算的丛集式系统、 分布式系统、云端负荷分担系统等。 广域网 (Wide Area Network, WAN)：传输距离较远，例如城市与城市之间的距离，因此使用的联机媒体需要较为便宜的设备，例如经常使用的电话线就是一例。 由于线材质量较差，因此网络速度较慢且可靠性较低一些，网络应用方面大多为类似 email, FTP, WWW 浏览等功能。 除了这两个之外，还有所谓的都会网络 (Metropolitan Area Network, MAN)，不过近来比较少提及，因此你只要知道有 LAN 及 WAN 即可。这两个名词在很多地方你都可以看的到。改天你回家看看你家的 ADSL 调制解调器或 IP 分享器后面的插孔看看，你就能够看到有 WAN 与 LAN 的插孔，现在你就知道为啥有这两个灯号与插孔了吧。 3、计算机网络协议： OSI 七层协定我们可以将整个网络连接过程分成数个阶层 (layer)，每个阶层都有特别的独立的功能，而且每个阶层的程序代码可以独立撰写，因为每个阶层之间的功能并不会互相干扰的。 如此一来，当某个小环节出现问题时，只要将该层级的程序代码重新撰写即可。所以程序撰写也容易，整个网络概念也就更清晰！ 那就是目前你常听到的 OSI 七层协议 (Open System Interconnection) 的概念。依据定义来说，越接近硬件的阶层为底层 (layer 1)，越接近应用程序的则是高层 (layer 7) 。不论是接收端还是发送端，每个一阶层只认识对方的同一阶层数据。 上图中仔细看每个数据报的部分，上层的包裹是放入下层的数据中，而数据前面则是这个数据的表头。其中比较特殊的是第二层，因为第二层 (数据链结层) 主要是位于软件封包 (packet) 以及硬件讯框 (frame) 中间的一个阶层，他必须要将软件包装的包裹放入到硬件能够处理的包裹中，因此这个阶层又分为两个子层在处理相对应的数据。因为比较特殊，所以第二层的数据格式比较不一样，尾端还出现一个检查码.简单的说，每一层负责的任务如下： table th:first-of-type { width: 150px; } 分层 负责任务 Layer 1物理层Physical Layer 由于网络媒体只能传送 0 与 1 这种位串，因此物理层必须定义所使用的媒体设备之电压与讯号等， 同时还必须了解数据讯框转成位串的编码方式，最后连接实体媒体并传送/接收位串。 Layer 2数据链结层Data-Link Layer 这一层是比较特殊的一个阶层，因为底下是实体的定义，而上层则是软件封装的定义。因此第二层又分两个子层在进行数据的转换动作。 在偏硬件媒体部分，主要负责的是 MAC(Media Access Control) ，我们称这个数据报裹为 MAC 讯框 (frame)， MAC 是网络媒体所能处理的主要数据报裹，这也是最终被物理层编码成位串的数据。MAC 必须要经由通讯协议来取得媒体的使用权， 目前最常使用的则是 IEEE802.3 的以太网络协议。详细的 MAC 与以太网络请参考下节说明。至于偏向软件的部分则是由逻辑链接层 (logical linkcontrol, LLC) 所控制，主要在多任务处理来自上层的封包数据 (packet) 并转成 MAC 的格式， 负责的工作包括讯息交换、流量控制、失误问题的处理等等。 Layer 3网络层Network Layer 这一层是我们最感兴趣的啰，因为我们提及的 IP (Internet Protocol) 就是在这一层定义的。 同时也定义出计算机之间的联机建立、终止与维持等，数据封包的传输路径选择等等，因此这个层级当中最重要的除了 IP 之外，就是封包能否到达目的地的路由 (route) 概念了！ Layer 4传送层Transport Layer 这一个分层定义了发送端与接收端的联机技术(如 TCP, UDP技术)， 同时包括该技术的封包格式，数据封包的传送、流程的控制、传输过程的侦测检查与复原重新传送等等， 以确保各个资料封包可以正确无误的到达目的端。 Layer 5会谈层Session Layer 在这个层级当中主要定义了两个地址之间的联机信道之连接与挂断，此外，亦可建立应用程序之对谈、 提供其他加强型服务如网络管理、签到签退、对谈之控制等等。如果说传送层是在判断资料封包是否可以正确的到达目标， 那么会谈层则是在确定网络服务建立联机的确认。 Layer 6表现层Presentation Layer 我们在应用程序上面所制作出来的数据格式不一定符合网络传输的标准编码格式的！ 所以，在这个层级当中，主要的动作就是：将来自本地端应用程序的数据格式转换(或者是重新编码)成为网络的标准格式， 然后再交给底下传送层等的协议来进行处理。所以，在这个层级上面主要定义的是网络服务(或程序)之间的数据格式的转换， 包括数据的加解密也是在这个分层上面处理。 Layer 7应用层Application Layer 应用层本身并不属于应用程序所有，而是在定义应用程序如何进入此层的沟通接口，以将数据接收或传送给应用程序，最终展示给用户。 事实上，OSI 七层协议只是一个参考的模型 (model)，目前的网络社会并没有什么很知名的操作系统在使用 OSI 七层协议的联网程序代码。那…讲这么多干嘛？这是因为 OSI 所定义出来的七层协议在解释网络传输的情况来说，可以解释的非常棒，因此大家都拿 OSI 七层协议来做为网络的教学与概念的理解。至于实际的联网程序代码，那就交给 TCP/IP 了！ 4、计算机网络协议： TCP/IP虽然 OSI 七层协议的架构非常严谨，是学习网络的好材料。但是也就是因为太过严谨了，因此程序撰写相当不容易，所以造成它在发展上面些许的困扰。而由 ARPANET 发展而来的 TCP/IP 又如何呢？其实 TCP/IP 也是使用 OSI 七层协议的观念，所以同样具有分层的架构，只是将它简化为四层，在结构上面比较没有这么严谨，程序撰写会比较容易些。后来在 1990 年代由于 email, WWW 的流行，造成 TCP/IP 这个标准为大家所接受，这也造就目前我们的网络社会！ 既然 TCP/IP 是由 OSI 七层协议简化而来，那么这两者之间有没有什么相关性呢？它们的相关性可以图示如下， 同时这里也列出目前在这架构底下常见的通讯协议、封包格式与相关标准： 从上图中，我们可以发现 TCP/IP 将应用、表现、会谈三层整合成一个应用层，在应用层上面可以实作的程序协议有 HTTP, SMTP, DNS 等等。 传送层则没有变，不过依据传送的可靠性又将封包格式分为连接导向的 TCP 及非连接导向的 UDP 封包格式。网络层也没有变，主要内容是提供了 IP 封包，并可选择最佳路由来到达目标 IP 地址。数据链结层与物理层则整合成为一个链结层，包括定义硬件讯号、 讯框转位串的编码等等，因此主要与硬件 (不论是区网还是广域网) 有关。 那 TCP/IP 是如何运作的呢？我们就拿妳常常连上的 Yahoo 入口网站来做个说明好了，整个联机的状态可以这样看： 1.应用程序阶段：妳打开浏览器，在浏览器上面输入网址列，按下 [Enter]。此时网址列与相关数据会被浏览器包成一个数据， 并向下传给 TCP/IP 的应用层；2.应用层：由应用层提供的 HTTP 通讯协议，将来自浏览器的数据报起来，并给予一个应用层表头，再向传送层丢去；3.传送层：由于 HTTP 为可靠联机，因此将该数据丢入 TCP 封包内，并给予一个 TCP 封包的表头，向网络层丢去；4.网络层：将 TCP 包裹包进 IP 封包内，再给予一个 IP 表头 (主要就是来源与目标的 IP)，向链结层丢去；5.链结层：如果使用以太网络时，此时 IP 会依据 CSMA/CD 的标准，包裹到MAC 讯框中，并给予 MAC 表头，再转成位串后， 利用传输媒体传送到远程主机上。 等到 Yahoo 收到你的包裹后，在依据相反方向拆解开来，然后交给对应的层级进行分析，最后就让 Yahoo 的 WWW 服务器软件得到你所想要的数据，该服务器软件再根据你的要求，取得正确的资料后，又依循上述的流程，一层一层的包装起来， 最后传送到你的手上！就是这样！ 根据这样的流程，我们就得要知道每个分层所需要了解的基础知识，这样才算学习网络基础.所以下面一节我们会依据 TCP/IP 的链结层、网络层、传送层来进行说明，应用层的协议则在以后对应的协定再来谈！ 5、Linux 网络相关配置文件案我们知道 TCP/IP 的重要参数主要是： IP, Netmask, Gateway, DNS IP ，而且千万不要忘记你这部主机也应该要有主机名 (hostname)！此外，我们也知道 IP 的取得有手动设定、DHCP 处理等。那么这些参数主要是写在哪些配置文件？如何对应呢？底下就让我们来处理一番！ 所需网络参数 主要配置文件档名 重要参数 IP、Netmask、DHCP 与 Gateway 等 /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=网卡的代号BOOTPROTO=是否使用 dhcpHWADDR=是否加入网卡卡号(MAC)IPADDR=就是IP 地址NETMASK=指网络屏蔽ONBOOT=要不要默认启动此接口GATEWAY=就是通讯闸NM_CONTROLLED=额外的网管软件 主机名 /etc/sysconfig/network NETWORKING=要不要有网络NETWORKING_IPV6=支援IPv6 否？HOSTNAME=你的主机名 DNS IP /etc/resolv.conf nameserver DNS 的IP 私有IP对应的主机名 /etc/hosts 私有IP 主机名 别名]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜服务器架设篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之定时任务详解]]></title>
    <url>%2F2018%2F01%2F04%2FLinux%E4%B9%8B%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1、Linux 工作排程的种类： at, cron at ：at 是个可以处理仅执行一次就结束排程的指令，不过要执行 at 时，必须要有 atd 这个服务的支援才行。在某些新版的 distributions 中，atd 可能预设并没有启动，那么 at 这个指令就会失效！不过我们的 CentOS 预设是启动的。 crontab ：crontab 这个指令所设定的工作将会循环的一直进行下去！可循环的时间为分钟、小时、每周、每月或每年等。crontab 除了可以使用指令执行外，亦可编辑 /etc/crontab 来支持。至于让 crontab 可以生效的服务则是 crond 这个服务。 2、atd 的启动与 at 运作的方式要使用单一工作排程时，我们的 Linux 系统上面必须要有负责这个排程的服务，那就是 atd ！不过并非所有的 Linux distributions 都预设会把他打开的，所以，某些时刻我们必须要手动将他启用才行。启用的方法很简单，就是这样：12345[root@www ~]# /etc/init.d/atd restart正在停止 atd: [ 确定 ]正在激活 atd: [ 确定 ]# 再设定一下开机时就启动这个服务，免得每次重新启动都得再来一次！[root@www ~]# chkconfig atd on （1）at 的运作方式既然是工作排程，那么应该会有产生工作的方式，并且将这些工作排进行程表中！那么产生工作的方式是怎么进行的？事实上，我们使用 at 这个指令来产生所要运作的工作，并将这个工作以文本文件的方式写入 /var/spool/at/ 目录内，该工作便能等待 atd 这个服务的取用与执行了。就这么简单。不过，并不是所有的人都可以进行 at 工作排程！为什么？因为安全的理由，很多主机被所谓的【绑架】后，最常发现的就是他们的系统当中多了很多的怪客程序 (cracker program)， 这些程序非常可能运用工作排程来执行或搜集系统信息，并定时的回报给怪客团体！所以，除非是你认可的账号，否则先不要让他们使用 at ！那怎么达到使用 at 的列管呢？我们可以利用 /etc/at.allow 与 /etc/at.deny 这两个档案来进行 at 的使用限制！加上这两个档案后，at 的工作情况其实是这样的： 先找寻 /etc/at.allow 这个档案，写在这个档案中的使用者才能使用 at ，没有在这个档案中的使用者则不能使用 at (即使没有写在 at.deny 当中)； 如果 /etc/at.allow 不存在，就寻找 /etc/at.deny 这个档案，若写在这个 at.deny 的使用者则不能使用at ，而没有在这个 at.deny 档案中的使用者，就可以使用 at ； 如果两个档案都不存在，那么只有 root 可以使用 at 这个指令。 透过这个说明，我们知道 /etc/at.allow 是管理较为严格的方式，而 /etc/at.deny 则较为松散 (因为账号没有在该档案中，就能够执行 at 了)。在一般的 distributions 当中，由于假设系统上的所有用户都是可信任的，因此系统通常会保留一个空的 /etc/at.deny 档案，意思是允讲所有人使用 at 指令的意思 (您可以自行检查一下该档案)。不过，万一你不希望有某些使用者使用 at 的话，将那个使用者的账号写入 /etc/at.deny 即可！一个账号写一行。 （2）实际运作单一工作排程基本的语法如下：123456789101112131415161718[root@www ~]# at [-mldv] TIME[root@www ~]# at -c 工作号码选项与参数：-m ：当 at 的工作完成后，即使没有输出讯息，亦以 email 通知使用者该工作已完成。-l ：at -l 相当于 atq，列出目前系统上面的所有该用户的 at 排程；-d ：at -d 相当于 atrm ，可以取消一个在 at 排程中的工作；-v ：可以使用较明显的时间格式栏出 at 排程中的任务栏表；-c ：可以列出后面接的该项工作的实际指令内容。TIME：时间格式，这里可以定义出【什么时候要进行 at 这项工作】的时间，格式有：HH:MM ex&gt; 04:00在今日的 HH:MM 时刻进行，若该时刻已超过，则明天的 HH:MM 进行此工作。HH:MM YYYY-MM-DD ex&gt; 04:00 2009-03-17强制规定在某年某月的某一天的特殊时刻进行该工作！HH:MM[am|pm] [Month] [Date] ex&gt; 04pm March 17也是一样，强制在某年某月某日的某时刻进行！HH:MM[am|pm] + number [minutes|hours|days|weeks]ex&gt; now + 5 minutes ex&gt; 04pm + 3 days就是说，在某个时间点【再加几个时间后】才进行。 老实说，这个 at 指令的下达最重要的地方在于【时间】的指定了！一般使用【 now + … 】的方式来定义现在过多少时间再进行工作，但有时也需要定义特定的时间点来进行！底下的范例先看看：12345678910111213141516171819202122232425262728范例一：再过五分钟后，将 /root/.bashrc 寄给 root 自己[root@www ~]# at now + 5 minutes &lt;==记得单位要加 sat&gt; /bin/mail root -s &quot;testing at job&quot; &lt; /root/.bashrcat&gt; &lt;EOT&gt; &lt;==这里输入 [ctrl] + d 就会出现 &lt;EOF&gt; 的字样！代表结束！job 4 at 2009-03-14 15:38# 上面这行信息在说明，第 4 个 at 工作将在 2009/03/14 的 15:38 进行！# 而执行 at 会进入所谓的 at shell 环境，让你下达多重指令等待运作！范例二：将上述的第 4 项工作内容列出来查阅[root@www ~]# at -c 4#!/bin/sh &lt;==就是透过 bash shell # atrun uid=0 gid=0# mail root 0umask 22....(中间省略许多的环境变量项目)....cd /root || &#123; &lt;==可以看出，会到下达 at 时的工作目录去执行指令echo &apos;Execution directory inaccessible&apos; &gt;&amp;2exit 1&#125;/bin/mail root -s &quot;testing at job&quot; &lt; /root/.bashrc# 你可以看到指令执行的目录 (/root)，还有多个环境变量与实际的指令内容范例三：由于机房预计于 2009/03/18 停电，我想要在 2009/03/17 23:00 关机？[root@www ~]# at 23:00 2009-03-17at&gt; /bin/syncat&gt; /bin/syncat&gt; /sbin/shutdown -h nowat&gt; &lt;EOT&gt;job 5 at 2009-03-17 23:00# at 还可以在一个工作内输入多个指令 事实上，当我们使用 at 时会进入一个 at shell 的环境来让用户下达工作指令，此时，建议你最好使用绝对路径来下达你的指令，比较不会有问题！由于指令的下达与 PATH 变量有关，同时与当时的工作目录也有关连(如果有牵涉到档案的话)，因此使用绝对路径来下达指令，会是比较一劳永逸的方法。 at 有另外一个很棒的优点，那就是【背景执行】的功能！由于 at 工作排程的使用上，系统会将该项 at 工作独立出你的 bash 环境中，直接交给系统的 atd 程序来接管，因此，当你下达了 at 的工作之后就可以立刻脱机了，剩下的工作就完全交给 Linux 管理即可！所以，如果有长时间的网络工作时，使用 at 可以让你免除网络断线后的困扰！ 3、循环执行的例行性工作排程相对于 at 是仅执行一次的工作，循环执行的例行性工作排程则是由 cron (crond) 这个系统服务来控制的。刚刚谈过 Linux 系统上面原本就有非常多的例行性工作，因此这个系统服务是默认启动的。另外，由于使用者自己也可以进行例行性工作排程，所以，Linux 也提供使用者控制例行性工作排程的指令(crontab)。底下我们分别来聊一聊： （1）使用者的设定使用者想要建立循环型工作排程时，使用的是 crontab 这个指令。不过，为了安全性的问题，与 at 同样的，我们可以限制使用 crontab 的使用者账号！使用的限制数据有： /etc/cron.allow：将可以使用 crontab 的账号写入其中，若不在这个档案内的使用者则不可使用 crontab； /etc/cron.deny：将不可以使用 crontab 的账号写入其中，若未记录到这个档案当中的使用者，就可以使用crontab 。 与 at 很像。同样的，以优先级来说，/etc/cron.allow 比 /etc/cron.deny 要优先，而判断上面，这两个档案只选择一个来限制而已，因此，建议你只要保留一个即可，免得影响自己在设定上面的判断！一般来说，系统默认是保留 /etc/cron.deny，你可以将不想让他执行 crontab 的那个使用者写入 /etc/cron.deny 当中，一个账号一行！ 当用户使用 crontab 这个指令来建立工作排程之后，该项工作就会被记录到 /var/spool/cron/ 里面去，而且是以账号来作为判别的。举例来说，dmtsai 使用 crontab 后，他的工作会被记录到 /var/spool/cron/dmtsai 里头去！但请注意，不要使用 vi 直接编辑该档案，因为可能由于输入语法错误，会导致无法执行 cron 。另外，cron 执行的每一项工作都会被记录到 /var/log/cron 这个登录档中，所以，如果你的 Linux 不知道有否被植入木马时，也可以搜寻一下 /var/log/cron 这个登录档！ （2）crontab 的语法：1234567891011[root@www ~]# crontab [-u username] [-l|-e|-r]选项与参数：-u ：只有 root 才能进行这个任务，亦即帮其他使用者建立/移除 crontab 工作排程；-e ：编辑 crontab 的工作内容-l ：查阅 crontab 的工作内容-r ：移除所有的 crontab 的工作内容，若仅要移除一项，请用 -e 去编辑。范例一：用 dmtsai 的身份在每天的 12:00 发信给自己[dmtsai@www ~]$ crontab -e# 此时会进入 vi 的编辑画面让您编辑工作！注意到，每项工作都是一行。0 12 * * * mail dmtsai -s &quot;at 12:00&quot; &lt; /home/dmtsai/.bashrc#分 时 日 月 周 |&lt;==============指令串========================&gt;| 预设情况下，任何使用者只要不被列入 /etc/cron.deny 当中，那么他就可以直接下达【 crontab -e 】去编辑自己的例行性命令。整个过程就如同上面提到的，会进入 vi 的编辑画面，然后以一个工作一行来编辑，编辑完毕之后输入【 :wq 】储存后离开 vi 就可以了。而每项工作 (每行) 的格式都是具有六个字段，这六个字段的意义为：比较有趣的是那个【周】，周的数字为 0 或 7 时，都代表【星期天】的意思！另外，还有一些辅助的字符，大概有底下这些：（注：那个 crontab 每个人都只有一个档案存在，就是在 /var/spool/cron 里面！还有建议：【指令下达时，最好使用绝对路径，这样比较不会找不到执行档。】） （3）系统的配置文件： /etc/crontab如果是【系统的例行性任务】时，你只要编辑 /etc/crontab 这个档案就可以。有一点需要特别注意！那就是 crontab -e 这个 crontab 其实是 /usr/bin/crontab 这个执行档，但是 /etc/crontab 可是一个【纯文本档】。你可以 root 的身份编辑一下这个档案。基本上， cron 这个服务的最低侦测限制是【分钟】，所以【 cron 会每分钟去读取一次 /etc/crontab 与 /var/spool/cron 里面的数据内容 】，因此，只要你编辑完 /etc/crontab 这个档案，并且将他储存之后，那么 cron 的设定就自动的会来执行了。 下面是 /etc/crontab 的内容：1234567891011[root@www ~]# cat /etc/crontabSHELL=/bin/bash &lt;==使用哪种 shell 接口PATH=/sbin:/bin:/usr/sbin:/usr/bin &lt;==执行文件搜寻路径MAILTO=root &lt;==若有额外STDOUT，以 email 将数据送给谁HOME=/ &lt;==默认此 shell 的家目录所在# run-parts01 * * * * root run-parts /etc/cron.hourly &lt;==每小时02 4 * * * root run-parts /etc/cron.daily &lt;==每天22 4 * * 0 root run-parts /etc/cron.weekly &lt;==每周日42 4 1 * * root run-parts /etc/cron.monthly &lt;==每个月 1 号分 时 日 月 周 执行者身份 指令串 看到这个档案的内容你大概就了解了吧。这个档案与将刚刚我们下达 crontab -e 的内容几乎完全一模一样！只是有几个地方不太相同： MAILTO=root： 这个项目是说，当 /etc/crontab 这个档案中的例行性工作的指令发生错误时，或者是该工作的执行结果有 STDOUT/STDERR 时，会将错误讯息或者是屏幕显示的讯息传给谁？默认当然是由系统直接寄发一封 mail 给 root。不过，由于 root 并无法在客户端中以 POP3 之类的软件收信，因此，通常都将这个 e-mail 改成自己的账号，好随时了解系统的状况！例如：MAILTO=dmtsai@my.host.namePATH=….： 在 BASH 当中一直提到的执行文件路径问题。这里就是输入执行文件的搜寻路径！使用默认的路径设定就已经很足够了。01 root run-parts /etc/cron.hourly： 这个 /etc/crontab 里面默认定义出四项工作任务，分别是每小时、每天、每周及每个月分别进行一次的工作！但是在五个字段后面接的并不是挃令，而是一个新的字段，那就是【执行后面那串指令的身份】为何。这不使用者的 crontab -e 不相同。由于使用者自己的 crontab 并不需要指定身份，但 /etc/crontab 里面当然要指定身份。以上表的内容来说，系统默认的例行性工作是以 root 的身份来进行的。 那么后面那串指令是什么呢？你可以使用【 which run-parts 】搜寻看看，其实那是一个 bash script！如果你直接进入 /usr/bin/run-parts 去看看，会发现这支指令会将后面接的【目录】内的所有档案捉出来执行！这也就是说【如果你想让系统每小时主动帮你执行某个指令，将该指令写成 script，并将该档案放置到 /etc/cron.hourly/ 目录下即可】的意思。 现在你知道系统是如何进行他默认的一堆例行性工作排程了吗？如果你下达【 ll /etc/cron.daily 】就可以看到一堆档案，那些档案就是系统提供的 script ，而这堆 scripts 将会在每天的凌晨 4:02 开始运作！这也是为啥如果你是夜猫族，就会发现奇怪的是，Linux 系统为何早上 4:02 开始会很忙碌的发出一些硬盘跑动的声音！因为他必须要进行 makewhatis,pdatedb, rpm rebuild 等等的任务。 由于 CentOS 提供的 run-parts 这个 script 的辅助，因此 /etc/crontab 这个档案里面支持两种下达指令的方式，一种是直接下达指令，一种则是以目录来规划，例如： 指令型态01 * * * * dmtsai mail -s &quot;testing&quot; kiki &lt; /home/dmtsai/test.txt以 dmtsai 这个使用者的身份，在每小时执行一次 mail 指令。目录规划*/5 * * * * root run-parts /root/runcron建立一个 /root/runcron 的目录，将要每隔五分钟执行的【可执行文件】都写到该目录下，就可以让系统每五分钟执行一次该目录下的所有可执行文件。 4、一些注意事项：（1）资源分配不均的问题如果每个流程都在同一个时间启动的话，那么在某个时段时，我的系统会变的相当的繁忙，所以，这个时候就必须要分别设定。我可以这样做：12345[root@www ~]# vi /etc/crontab1,6,11,16,21,26,31,36,41,46,51,56 * * * * root CMD12,7,12,17,22,27,32,37,42,47,52,57 * * * * root CMD23,8,13,18,23,28,33,38,43,48,53,58 * * * * root CMD34,9,14,19,24,29,34,39,44,49,54,59 * * * * root CMD4 看到了没？那个【 , 】分隔的时候，请注意，不要有空格符！（连续的意思）如此一来，则可以将每五分钟工作的流程分别在不同的时刻来工作！则可以让系统的执行较为顺畅！ （2）取消不要的输出项目另外一个困扰发生在【 当有执行成果或者是执行的项目中有输出的数据时，该数据将会 mail 给 MAILTO 设定的账号 】，那么当有一个排程一直出错（例如 DNS 的侦测系统当中，若 DNS 上层主机挂掉，那么你就会一直收到错误讯息！）怎么办？可以使用数据流重导向直接以【命令重导向】将输出的结果输出到 /dev/null 这个垃圾桶当中就好了！ （3）安全的检验很多时候被植入木马都是以例行命令的方式植入的，所以可以藉由检查 /var/log/cron 的内容来视察是否有【非您设定的 cron 被执行了？】这个时候就需要小心一点！ （4）周与日月不可同时并存另一个需要注意的地方在于：【你可以分别以周或者是日月为单位作为循环，但你不可使用「几月几号且为星期几」的模式工作】。这个意思是说，你不可以这样编写一个工作排程： 30 12 11 9 5 root echo “just test” &lt;==这是错诨癿写法本来你以为九月十一号且为星期五才会进行这项工作，无奈的是，系统可能会判定每个星期五作一次，或每年的 9 月 11 号分别进行，如此一来与你当初的规划就不一样了。所以，得要注意这个地方！上述的写法是不对的！]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜基础篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>Linux</tag>
        <tag>Crontab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[认识与学习BASH之Shell Scripts]]></title>
    <url>%2F2018%2F01%2F03%2F%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%AD%A6%E4%B9%A0BASH%E4%B9%8BShell%20Scripts%2F</url>
    <content type="text"><![CDATA[一、什么是Shell Script什么是 shell script (程序化脚本) 呢？就字面上的意义，我们将他分为两部份。在【 shell 】部分，我们在 BASH 当中已经提过了，那是一个文字接口底下让我们与系统沟通的一个工具接口。那么【 script 】是啥？字面上的意义，script 是【脚本、剧本】的意思。整句话是说，shell script 是针对 shell 所写的【剧本！】 什么东西啊？其实，shell script 是利用 shell 的功能所写的一个【程序 (program)】，这个程序是使用纯文本文件，将一些 shell 的语法与指令(含外部指令)写在里面，搭配正规表示法、管线命令与数据流重导向等功能，以达到我们所想要的处理目的。所以，简单的说，shell script 就像是早期 DOS 年代的批处理文件 (.bat) ，最简单的功能就是将许多指令汇整写在一起，让使用者很轻易的就能够 one touch 的方法去处理复杂的动作 (执行一个档案”shell script” ，就能够一次执行多个指令)。而且 shell script 更提供数组、循环、条件与逻辑判断等重要功能，让用户也可以直接以 shell 来撰写程序，而不必使用类似 C 程序语言等传统程序撰写的语法。shell script 可以简单的被看成是批处理文件，也可以被说成是一个程序语言，且这个程序语言由于都是利用 shell 与相关工具指令，所以不需要编译即可执行，且拥有不错的除错 (debug) 工具，所以，他可以帮助系统管理员快速的管理好主机。 shell script 号称是程序 (program) ，但实际上，shell script 处理数据的速度上是不太够的。因为 shell script 用的是外部的指令与 bash shell 的一些默认工具，所以，他常常会去呼叫外部的函式库，因此，指令周期上面当然比不上传统的程序语言。所以，shell script 用在系统管理上面是很好的一项工具，但是用在处理大量数值运算上，就不够好了，因为 Shell scripts 的速度较慢，且使用的 CPU 资源较多，造成主机资源的分配不良。我们通常利用 shell script 来处理服务器的侦测，倒是没有进行大量运算的需求！所以不必担心。 Shell执行及注意事项在 shell script 的撰写中还需要用到底下的注意事项： 指令的执行是从上而下、从左而右的分析与执行； 指令的下达就是： 指令、选项与参数间的多个空白都会被忽略掉； 空白行也将被忽略掉，并且 [tab] 按键所推开的空白同样视为空格键； 如果读取到一个 Enter 符号 (CR) ，就尝试开始执行该行 (或该串) 命令； 至于如果一行的内容太多，则可以使用【 [Enter] 】来延伸至下一行； 【 # 】可做为批注！任何加在 # 后面的资料将全部被视为批注文字而被忽略！ 如此一来，我们在 script 内所撰写的程序，就会被一行一行的执行。现在我们假设你写的这个程序文件名是 /home/dmtsai/shell.sh 好了，那如何执行这个档案？很简单，可以有底下几个方法： 直接指令下达： shell.sh 档案必须要具备可读与可执行 (rx) 的权限，然后： 绝对路径：使用 /home/dmtsai/shell.sh 来下达指令； 相对路径：假设工作目录在 /home/dmtsai/ ，则使用 ./shell.sh 来执行 变量【PATH】功能：将 shell.sh 放在 PATH 指定的目录内，例如： ~/bin/(目录需要自行设定) 。此时，若 shell.sh 在 ~/bin 内且具有 rx 的权限，那就直接输入 shell.sh 即可执行该脚本程序！ 以 bash 程序来执行：透过【 bash shell.sh 】或【 sh shell.sh 】来执行 反正重点就是要让那个 shell.sh 内的指令可以被执行的意思。 二、简单的shell script练习（1）撰写第一支 script：输出Hello World！1234567891011[root@www ~]# mkdir scripts; cd scripts[root@www scripts]# vi sh01.sh#!/bin/bash# Program:# This program shows &quot;Hello World!&quot; in your screen.# History:# 2005/08/23 VBird First releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATHecho -e &quot;Hello World! \a \n&quot;exit 0 在本章当中，请将所有撰写的 script 放置到你家目录的 ~/scripts 这个目录内，未来比较好管理。上面的写法当中，主要将整个程序的撰写分成数段，大致是这样： 1.第一行 #!/bin/bash 在宣告这个 script 使用的 shell 名称：因为我们使用的是 bash ，所以，必须要以【 #!/bin/bash 】来宣告这个档案内的语法使用 bash 的语法！那么当这个程序被执行时，他就能够加载 bash 的相关环境配置文件 (一般来说就是 non-login shell 的 ~/.bashrc)， 并且执行 bash 来使我们底下的指令能够执行！这很重要！(在很多状况中，如果没有设定好这一行，那么该程序很可能会无法执行，因为系统可能无法判断该程序需要使用什么 shell 来执行！)2.程序内容的说明：整个 script 当中，除了第一行的【 #! 】是用来宣告 shell 的之外，其他的 # 都是【批注】用途！所以上面的程序当中，第二行以下就是用来说明整个程序的基本数据。一般来说，建议你一定要养成说明该 script 的：1. 内容与功能； 2. 版本信息； 3. 作者与联绚方式； 4. 建檔日期；5. 历史纪录 等等。这将有助于未来程序的改写与 debug。3.主要环境变量的宣告：建议务必要将一些重要的环境变量设定好，PATH 与 LANG (如果有使用到输出相关的信息时) 是当中最重要的！如此一来，则可让我们这支程序在进行时，可以直接下达一些外部指令，而不必写绝对路径！4.主要程序部分就将主要的程序写好即可！在这个例子当中，就是 echo 那一行！5.执行成果告知 (定义回传值)是否记得我们在讨论一个指令的执行成功与否，可以使用 $? 这个变量来观察。那么我们也可以利用 exit 这个指令来让程序中断，并且回传一个数值给系统。在我们这个例子当中，使用 exit 0 ，这代表离开 script 并且回传一个 0 给系统，所以我执行完这个 script 后，若接着下达 echo $? 则可得到 0 的值！更聪明的读者应该也知道，利用这个 exit n (n 是数字) 的功能，我们还可以自定义错误讯息，让这支程序发得更加的 smart ！接下来透过刚刚上头介绍的执行方法来执行看看结果吧。 （2）简单范例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061（1）对谈式脚本：变量内容由用户决定[root@www scripts]# vi sh02.sh#!/bin/bash# Program:# User inputs his first name and last name. Program shows his fullname.# History:# 2005/08/23 VBird First releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATHread -p &quot;Please input your first name: &quot; firstname # 提示使用者输入read -p &quot;Please input your last name: &quot; lastname # 提示使用者输入echo -e &quot;\nYour full name is: $firstname $lastname&quot; # 结果由屏幕输出（2）随日期变化：利用date进行档案的建立[root@www scripts]# vi sh03.sh#!/bin/bash# Program:# Program creates three files, which named by user&apos;s input# and date command.# History:# 2005/08/23 VBird First releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATH# 1. 让使用者输入文件名，并取得 fileuser 这个变量；echo -e &quot;I will use &apos;touch&apos; command to create 3 files.&quot; # 纯粹显示信息read -p &quot;Please input your filename: &quot; fileuser # 提示使用者输入# 2. 为了避免使用者随意按 Enter ，利用变量功能分析档名是否有设定？filename=$&#123;fileuser:-&quot;filename&quot;&#125; # 开始判断有否配置文件名# 3. 开始利用 date 指令来取得所需要的档名；date1=$(date --date=&apos;2 days ago&apos; +%Y%m%d) # 前两天的日期date2=$(date --date=&apos;1 days ago&apos; +%Y%m%d) # 前一天的日期date3=$(date +%Y%m%d) # 今天的日期file1=$&#123;filename&#125;$&#123;date1&#125; # 底下三行在配置文件名file2=$&#123;filename&#125;$&#123;date2&#125;file3=$&#123;filename&#125;$&#123;date3&#125;# 4. 将档名建立touch &quot;$file1&quot; # 底下三行在建立档案touch &quot;$file2&quot;touch &quot;$file3&quot;（3）数值运算：简单的加减乘除[root@www scripts]# vi sh04.sh#!/bin/bash# Program:# User inputs 2 integer numbers; program will cross these twonumbers.# History:# 2005/08/23 VBird First releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATHecho -e &quot;You SHOULD input 2 numbers, I will cross them! \n&quot;read -p &quot;first number: &quot; firstnuread -p &quot;second number: &quot; secnutotal=$(($firstnu*$secnu))echo -e &quot;\nThe result of $firstnu x $secnu is ==&gt; $total&quot;# 在数值的运算上，我们可以使用【 declare -i total=$firstnu*$secnu 】# 也可以使用上面的方式来进行。基本上，比较建议使用这样的方式来进行运算： var=$((运算内容))# 不但容易记忆，而且也比较方便的多，因为两个小括号内可以加上空格符！# 未来你可以使用这种方式来计算。至于数值运算上的处理，则有：【 +, -, *, /, % 】等等。 三、判断式（1）利用 test 指令的测试功能 OK！现在我们就利用 test 来帮我们写几个简单的例子。首先，判断一下，让使用者输入一个档名，我们判断： 这个档案是否存在，若不存在则给予一个【Filename does not exist】的讯息，并中断程序； 若这个档案存在，则判断他是个档案或目录，结果输出【Filename is regular file】或【Filename is directory】 判断一下，执行者的身份对这个档案或目录所拥有的权限，并输出权限数据！（注意利用 test 与 &amp;&amp; 还有 || 等标志！） 1234567891011121314151617181920212223242526[root@www scripts]# vi sh05.sh#!/bin/bash# Program:# User input a filename, program will check the flowing:# 1.) exist? 2.) file/directory? 3.) file permissions# History:# 2005/08/25 VBird First releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATH# 1. 讥使用者输入档名，幵且刞断使用者是否真的有输入字符串？echo -e &quot;Please input a filename, I will check the filename&apos;s type and \permission. \n\n&quot;read -p &quot;Input a filename : &quot; filenametest -z $filename &amp;&amp; echo &quot;You MUST input a filename.&quot; &amp;&amp; exit 0# 2. 刞断档案是否存在？若丌存在则显示讯息幵结束脚本test ! -e $filename &amp;&amp; echo &quot;The filename &apos;$filename&apos; DO NOT exist&quot; &amp;&amp;exit 0# 3. 开始刞断文件类型不属性test -f $filename &amp;&amp; filetype=&quot;regulare file&quot;test -d $filename &amp;&amp; filetype=&quot;directory&quot;test -r $filename &amp;&amp; perm=&quot;readable&quot;test -w $filename &amp;&amp; perm=&quot;$perm writable&quot;test -x $filename &amp;&amp; perm=&quot;$perm executable&quot;# 4. 开始输出信息！echo &quot;The filename: $filename is a $filetype&quot;echo &quot;And the permissions are : $perm&quot; （2）利用判断符号 [ ]除了我们很喜欢使用的 test 之外，其实，我们还可以利用判断符号【 [ ] 】(就是中括号) 来进行数据的判断。举例来说，如果我想要知道 $HOME 这个变量是否为空的，可以这样做：[root@www ~]# [ -z &quot;$HOME&quot; ] ; echo $?使用中括号必须要特别注意，因为中括号用在很多地方，包括通配符与正规表示法等等，所以如果要在 bash 的语法当中使用中括号作为 shell 的判断式时，必须要注意中括号的两端需要有空格符来分隔。最好要注意： 在中括号 [] 内的每个组件都需要有空格键来分隔； 在中括号内的变数，最好都以双引号括号起来； 在中括号内的常数，最好都以单或双引号括号起来。 举例来说，假如我设定了 name=&quot;VBird Tsai&quot; ，然后这样判定：123[root@www ~]# name=&quot;VBird Tsai&quot;[root@www ~]# [ $name == &quot;VBird&quot; ]bash: [: too many arguments 怎么会发生错误？bash 还跟我说错误是由于【太多参数 (arguments)】所致。为什么呢？因为 $name 如果没有使用双引号刮起来，那么上面的判定式会变成： [ VBird Tsai == &quot;VBird&quot; ]上面肯定不对了。因为一个判断式仅能有两个数据的比对，上面 VBird 与 Tsai 还有 “VBird” 就有三个资料。这不是我们要的，我们要的应该是底下这个样子： [ &quot;VBird Tsai&quot; == &quot;VBird&quot; ]这可是差很多。另外，中括号的使用方法与 test 几乎一模一样。 四、条件判断式（1）利用if…then判断式（1）单层、简单条件判断式12345678if [ 条件判断式 ]; then当条件判断式成立时，可以进行的指令工作内容；fi &lt;==将 if 反过来写，就成为 fi ！结束 if 之意！括号与括号之间，则以 &amp;&amp; 或 || 来隔开，他们的意义是： &amp;&amp; 代表 AND ； || 代表 or ；例如：[ &quot;$yn&quot; == &quot;Y&quot; -o &quot;$yn&quot; == &quot;y&quot; ] 可替换为：[ &quot;$yn&quot; == &quot;Y&quot; ] || [ &quot;$yn&quot; == &quot;y&quot; ] （2）多重、复杂条件判断式123456789101112131415# 一个条件判断，分成功进行与失败进行 (else)if [ 条件判断式 ]; then 当条件判断式成立时，可以进行的指令工作内容；else 当条件判断式不成立时，可以进行的指令工作内容；fi# 多个条件判断 (if ... elif ... elif ... else) 分多种不同情况执行if [ 条件判断式一 ]; then 当条件判断式一成立时，可以进行的指令工作内容；elif [ 条件判断式二 ]; then 当条件判断式二成立时，可以进行的指令工作内容；else 当条件判断式一与二均不成立时，可以进行的指令工作内容；fi （2）利用 case ….. esac 判断1234567891011case $变量名称 in &lt;==关键词为 case ，还有变数前有钱字号&quot;第一个变量内容&quot;) &lt;==每个变量内容建议用双引号括起来，关键词则为小括号 ) 程序段 ;; &lt;==每个类别结尾使用两个连续的分号来处理！&quot;第二个变量内容&quot;) 程序段 ;;*) &lt;==最后一个变量内容都会用 * 来代表所有其他值，不包含第一个变量内容与第二个变量内容的其他程序执行段 exit 1 ;;esac &lt;==最终的 case 结尾！【反过来写】思考一下！ 一般来说，使用【 case $变量 in 】这个语法中，当中的那个【 $变量 】大致有两种取得的方式： 直接下达式：例如上面提到的，利用【 script.sh variable 】的方式来直接给予 $1 这个变量的内容，这也是在 /etc/init.d 目录下大多数程序的设计方式。 交互式：透过 read 这个指令来让用户输入变量的内容。 五、循环（1）while do done, until do done (不定循环)一般来说，不定循环最常见的就是底下这两种状态了：1234while [ condition ] &lt;==中括号内的状态就是判断式do &lt;==do 是循环的开始！ 程序段落done &lt;==done 是循环的结束 while 的中文是【当….时】，所以，这种方式说的是【当 condition 条件成立时，就进行循环，直到condition 的条件不成立才停止】的意思。还有另外一种不定循环的方式：1234until [ condition ]do 程序段落done 这种方式恰恰与 while 相反，它说的是【当 condition 条件成立时，就终止循环，否则就持续进行循环的程序段。】 （2）for…do…done (固定循环)相对于 while, until 的循环方式是必须要【符合某个条件】的状态，for 这种语法，则是【 已经知道要进行几次循环】的状态！他的语法是：1234for var in con1 con2 con3 ...do 程序段done 以上面的例子来说，这个 $var 的变量内容在循环工作时： 1. 第一次循环时， $var 的内容为 con1 ； 2. 第二次循环时， $var 的内容为 con2 ； 3. 第三次循环时， $var 的内容为 con3 ； 4. .... （3）for…do…done 的数值处理除了上述的方法之外，for 循环还有另外一种写法！语法如下：1234for (( 初始值; 限制值; 执行步阶 ))do 程序段done 这种语法适合于数值方式的运算当中，在 for 后面的括号内的三串内容意义为： 初始值：某个变量在循环当中的起始值，直接以类似 i=1 设定好； 限制值：当变量的值在这个限制值的范围内，就继续进行循环。例如 i&lt;=100； 执行步阶：每作一次循环时，变量的变化量。例如 i=i+1。 值得注意的是，在【执行步阶】的设定上，如果每次增加 1 ，则可以使用类似【i++】的方式，亦即是 i 每次循环都会增加一的意思。 六、shell script的追踪与debugscripts 在执行之前，最怕的就是出现语法错误的问题了！那么我们如何 debug 呢？有没有办法不需要透过直接执行该 scripts 就可以来判断是否有问题呢？当然是有的，我们就直接以 bash 的相关参数来进行判断！12345678910[root@www ~]# sh [-nvx] scripts.sh选项与参数：-n ：不要执行 script，仅查询语法的问题；-v ：再执行 sccript 前，先将 scripts 的内容输出到屏幕上；-x ：将使用到的 script 内容显示到屏幕上，这是很有用的参数！范例一：测试 sh16.sh 有无语法的问题？[root@www ~]# sh -n sh16.sh# 若语法没有问题，则不会显示任何信息！范例二：将 sh15.sh 的执行过程全部列出来～[root@www ~]# sh -x sh15.sh 熟悉 sh 的用法，将可以使你在管理 Linux 的过程中得心应手！至于在 Shell scripts 的学习方法上面，需要【多看、多模仿、并加以修改成自己的样式！】是最快的学习手段了！网络上有相当多的朋友在开发一些相当有用的 scripts ，若是你可以将对方的 scripts 拿来，并且改成适合自己主机的样子！那么学习的效果会是最快的！ 另外，我们 Linux 系统本来就有很多的服务启动脚本，如果你想要知道每个 script 所代表的功能是什么？可以直接以 vim 进入该 script 去查阅一下，通常立刻就知道该 script 的目的了。 举例来说，我们之前一直提到的 /etc/init.d/syslog ，这个 script 是干嘛用的？ 利用 vi 去查阅最前面的几行字，他出现如下信息：123456# description: Syslog is the facility by which many daemons use to log \# messages to various system log files. It is a good idea to always \# run syslog.### BEGIN INIT INFO# Provides: $syslog### END INIT INFO 简单的说，这个脚本在启动一个名为 syslog 的常驻程序 (daemon)，这个常驻程序可以帮助很多系统服务记载她们的登录文件 (log file)，我们的 Linux 建议你一直启动 syslog 是个好主意！简单的看看您就知道啥是啥啦！ 附属：Shell Script中的特殊变量：Shell script 的默认变数($0, $1…)一些较为特殊的变量可以在 script 内使用来呼叫这些参数： $# ：代表后接的参数【个数】，以上表为例这里显示为【 4 】； $@ ：代表【 “$1” “$2” “$3” “$4” 】之意，每个变量是独立的(用双引号括起来)； $* ：代表【 “$1c$2c$3c$4” 】，其中 c 为分隔字符，默认为空格键，所以本例中代表【 “$1 $2 $3 $4” 】之意。 $0 ：代表执行的脚本档名 $n ：代表对应输入的变量 常用的转义字符4个最常用的转义字符，如下所示： 反斜杠（\）：使反斜杠后面的一个变量变为单纯的字符串。单引号（’’）：转义其中所有的变量为单纯的字符串。双引号（””）：保留其中的变量属性，不进行转义处理。反引号（）：把其中的命令执行后返回结果。]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜基础篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>Shell</tag>
        <tag>BASH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令之awk]]></title>
    <url>%2F2017%2F12%2F29%2FLinux%E5%91%BD%E4%BB%A4%E4%B9%8Bawk%2F</url>
    <content type="text"><![CDATA[awk：好用的数据处理工具awk 也是一个非常棒的数据处理工具。相较于 sed 常常作用于一整个行的处理，awk 则比较倾向于一行当中分成数个【字段】来处理。因此，awk 相当的适合处理小型的数据数据处理。awk 通常运作的模式是这样的：[root@www ~]# awk &#39;条件类型1{动作1} 条件类型2{动作2} ...&#39; filename awk脚本基本结构awk &#39;BEGIN{ commands } pattern{ commands } END{ commands }&#39; file工作原理：第一步：执行BEGIN{ commands }语句块中的语句；BEGIN语句块在awk开始从输入流中读取行之前被执行，这是一个可选的语句块，比如变量初始化、打印输出表格的表头等语句通常可以写在BEGIN语句块中。第二步：从文件或标准输入(stdin)读取一行，然后执行pattern{ commands }语句块，它逐行扫描文件，从第一行到最后一行重复这个过程，直到文件全部被读取完毕。pattern语句块中的通用命令是最重要的部分，它也是可选的。如果没有提供pattern语句块，则默认执行{ print }，即打印每一个读取到的行，awk读取的每一行都会执行该语句块。第三步：当读至输入流末尾时，执行END{ commands }语句块。END语句块在awk从输入流中读取完所有的行之后即被执行，比如打印所有行的分析结果这类信息汇总都是在END语句块中完成，它也是一个可选语句块。 awk 后面接两个单引号（双引号一般在动作中使用）并加上大括号 {} 来设定想要对数据进行的处理动作。awk 可以处理后续接的档案，也可以读取来自前个指令的 standard output 。但如前面说的，awk 主要是处理【每一行的字段内的数据】，而默认的【字段的分隔符为 “空格键” 或 “[tab]键”】。举例来说，我们用 last 可以将登入者的数据取出来，结果如下所示：123456[root@www ~]# last -n 5 &lt;==仅取出前五行root pts/1 192.168.1.100 Tue Feb 10 11:21 still logged inroot pts/1 192.168.1.100 Tue Feb 10 00:46 - 02:28 (01:41)root pts/1 192.168.1.100 Mon Feb 9 11:41 - 18:30 (06:48)dmtsai pts/1 192.168.1.100 Mon Feb 9 11:41 - 11:41 (00:00)root tty1 Fri Sep 5 14:09 - 14:10 (00:01) 若我想要取出账号与登入者的 IP ，且账号与 IP 之间以 [tab] 隔开，则会变成这样：123456[root@www ~]# last -n 5 | awk &apos;&#123;print $1 &quot;\t&quot; $3&#125;&apos;root 192.168.1.100root 192.168.1.100root 192.168.1.100dmtsai 192.168.1.100root Fri 上表是 awk 最常使用的动作。透过 print 的功能将字段数据列出来，字段的分隔则以空格键或 [tab] 按键来隔开。因为不论哪一行我都要处理，因此，就不需要有 “条件类型” 的限制。我所想要的是第一栏以及第三栏，但是，第五行的内容怪怪的，这是因为数据格式的问题。所以，使用 awk 的时候，请先确认一下你的数据当中，如果是连续性的数据，请不要有空格或 [tab] 在内，否则，就会像这个例子这样，会发生误判。（注：条件类型是可有可无的） 另外，由上面这个例子你也会知道，在每一行的每个字段都是有变量名称的，那就是 $1, $2… 等变量名称。以上面的例子来说，root 是 $1 ，因为他是第一栏。至于 192.168.1.100 是第三栏，所以他就是 $3，后面以此类推。还有个变数，那就是 $0 ，$0 代表【一整列资料】的意思。以上面的例子来说，第一行的 $0 代表的就是【root ….】那一行。 由此可知，刚刚上面五行当中，整个 awk 的处理流程是： 1. 读入第一行，并将第一行的资料填入 $0, $1, $2.... 等变数当中； 2. 依据 &quot;条件类型&quot; 的限制，判断是否需要进行后面的 &quot;动作&quot;； 3. 做完所有的动作与条件类型； 4. 若还有后续的【行】的数据，则重复上面 1~3 的步骤，直到所有的数据都读完为止。 经过这样的步骤，你会晓得，awk 是【以行为一次处理的单位】，而【以字段为最小的处理单位】。那么 awk 怎么知道我到底这个数据有几行？有几栏？这就需要 awk 的内建变量的帮忙了。 变量名称 代表意义 $0 这个变量包含执行过程中当前行的文本内容。 NF 表示字段数，在执行过程中对应于当前的字段数。 NR 表示记录数，在执行过程中对应于当前的行号。 FS 目前的分隔字符，默认是空格键 我们继续以上面 last -n 5 的例子来做说明，如果我想要： 列出每一行的账号(就是 $1)； 列出目前处理的行数(就是 awk 内的 NR 变量) 并且说明，该行有多少字段(就是 awk 内的 NF 变量) （注：awk 后续的所有动作是以单引号【 ‘ 】括住的，由于单引号与双引号都必须是成对的，所以，awk的格式内容如果想要以print打印时，记得非变量的文字部分，包含printf的格式中，都需要使用双引号来定义出来。因为单引号已经是 awk 的指令固定用法了！） 1234567[root@www ~]# last -n 5| awk &apos;&#123;print $1 &quot;\t lines: &quot; NR &quot;\t columes: &quot; NF&#125;&apos;root lines: 1 columes: 10root lines: 2 columes: 10root lines: 3 columes: 10dmtsai lines: 4 columes: 10root lines: 5 columes: 9# 注意，在 awk 内的 NR, NF 等变量要用大写，且不需要有钱字号 $ awk 的逻辑运算字符既然有需要用到 “条件” 的类别，自然就需要一些逻辑运算。例如底下这些： （注：条件类型一般都使用逻辑运算符来进行判断。）值得注意的是那个【 == 】的符号，因为： 逻辑运算上面亦即所谓的大于、小于、等于等判断式上面，习惯上是以【 == 】来表示； 如果是直接给予一个值，例如变量设定时，就直接使用 = 而已。举例来说，在 /etc/passwd 当中是以冒号 “:” 来作为字段的分隔，该档案中第一字段为账号，第三字段则是UID。那假设我要查阅，第三栏小于 10 以下的数据，并且仅列出账号与第三栏，那么可以这样做：12345[root@www ~]# cat /etc/passwd | awk &apos;&#123;FS=&quot;:&quot;&#125; $3 &lt; 10 &#123;print $1 &quot;\t &quot; $3&#125;&apos;root:x:0:0:root:/root:/bin/bashbin 1daemon 2....(以下省略)....（注：分隔符上述不好使可以使用如下形式：awk FS=&#39;:&#39; &#39;{}&#39;） 另外，如果要用 awk 来进行【计算功能】呢？以底下的例子来看，假设我有一个薪资数据表档名为 pay.txt ，内容是这样的：1234Name 1st 2nd 3thVBird 23000 24000 25000DMTsai 21000 20000 23000Bird2 43000 42000 41000 如何帮我计算每个人的总额呢？而且我还想要格式化输出。我们可以这样考虑： 第一行只是说明，所以第一行不要进行加总 (NR==1 时处理)； 第二行以后就会有加总的情况出现 (NR&gt;=2 以后处理) 123456789[root@www ~]# cat pay.txt | \&gt; awk &apos;NR==1&#123;printf&quot;%10s %10s %10s %10s %10s\n&quot;,$1,$2,$3,$4,&quot;Total&quot; &#125;NR&gt;=2&#123;total = $2 + $3 + $4printf &quot;%10s %10d %10d %10d %10.2f\n&quot;, $1, $2, $3, $4, total&#125;&apos;Name 1st 2nd 3th TotalVBird 23000 24000 25000 72000.00DMTsai 21000 20000 23000 64000.00Bird2 43000 42000 41000 126000.00 上面的例子有几个重要事项应该要先说明的： awk 的指令间隔：所有 awk 的动作，亦即在 {} 内的动作，如果有需要多个指令辅助时，可利用分号【;】间隔，或者直接以 [Enter] 按键来隔开每个指令 逻辑运算当中，如果是【等于】的情况，则务必使用两个等号【==】！ 格式化输出时，在 printf 的格式设定当中，务必加上 \n ，才能进行分行！ 与 bash shell 的变量不同，在 awk 当中，变量可以直接使用，不需加上 $ 符号。 利用 awk 这个玩意儿，就可以帮我们处理很多日常工作了呢！真是好用的很。此外，awk 的输出格式当中，常常会以 printf 来辅助，所以，最好你对 printf 也稍微熟悉一下比较好。另外，awk 的动作内 {} 也是支持 if (条件) 的。举例来说，上面的指令可以修订成为这样：12345[root@www ~]# cat pay.txt | \&gt; awk &apos;&#123;if(NR==1) printf&quot;%10s %10s %10s %10s %10s\n&quot;,$1,$2,$3,$4,&quot;Total&quot;&#125;NR&gt;=2&#123;total = $2 + $3 + $4printf &quot;%10s %10d %10d %10d %10.2f\n&quot;, $1, $2, $3, $4, total&#125;&apos; awk示例awk给文本文件每行添加行号假定文本内容如下：123first linesecond linethird line 现在要在每行前面添加行号，格式为：”1 “。可通过如下命令实现：awk &#39;$0=NR&quot; &quot;$0&#39;最后，通过重定向符可导入某个文件中。 awk查看文本文件的某一行数据可使用如下命令实现：awk &#39;NR==num&#39; filename 格式化打印：printf123456789101112131415[root@www ~]# printf &apos;打印格式&apos; 实际内容选项与参数：关于格式方面的几个特殊样式： \a 警告声音输出 \b 退格键(backspace) \f 清除屏幕 (form feed) \n 输出新的一行 \r 亦即 Enter 按键 \t 水平的 [tab] 按键 \v 垂直的 [tab] 按键 \xNN NN 为两位数的数字，可以转换数字成为字符。关于 C 程序语言内，常见的变数格式 %ns 那个 n 是数字， s 代表 string ，亦即多少个字符； %ni 那个 n 是数字， i 代表 integer ，亦即多少整数字数； %N.nf 那个 n 与 N 都是数字， f 代表 floating (浮点)，如果有小数字数，假设我共要十个位数，但小数点有两位，即为 %10.2f （注：printf 不是管线命令）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令之sed]]></title>
    <url>%2F2017%2F12%2F29%2FLinux%E5%91%BD%E4%BB%A4%E4%B9%8Bsed%2F</url>
    <content type="text"><![CDATA[sed 本身也是一个管线命令，可以分析 standard input 的，而且 sed 还可以将数据进行取代、删除、新增、撷取特定行等等的功能。我们先来了解一下 sed 的用法，再来聊他的用途。12345678910111213141516171819[root@www ~]# sed [-nefr] [动作]选项与参数： -n ：使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN 的数据一般都会被列出到屏幕上。 但如果加上 -n 参数后，则只有经过 sed 特殊处理的那一行(或者动作)才会被列出来。 -e ：直接在指令列模式上进行 sed 的动作编辑； -f ：直接将 sed 的动作写在一个档案内，-f filename 则可以执行 filename 内的 sed 动作； -r ：sed 的动作支持的是延伸型正规表示法的语法。(预设是基础正规表示法语法) -i ：直接修改读取的档案内容，而不是由屏幕输出。动作说明： [n1[,n2]]functionn1, n2 ：不见得会存在，一般代表【选择进行动作的行数】， 举例来说，如果我的动作是需要在 10 到 20 行之间进行的，则【 10,20[动作行为] 】function 有底下这些咚咚： a ：新增， a 的后面可以接字符串，而这些字符串会在新的一行出现(目前的下一行) c ：取代， c 的后面可以接字符串，这些字符串可以取代 n1,n2 之间的行！ d ：删除，因为是删除，所以 d 后面通常不接任何咚咚； i ：插入， i 的后面可以接字符串，而这些字符串会在新的一行出现(目前的上一行)； p ：打印，亦即将某个选择的数据打印出。通常 p 会与参数 sed -n 一起运作 s ：取代，可以直接进行取代的工作。通常这个 s 的动作可以搭配正规表示法。例如 1,20s/old/new/g 就是 下面进行一些示例： 1、以行为单位的新增/删除功能范例一：将 /etc/passwd 的内容列出并且打印行号，同时，请将第 2~5 行删除12345[root@www ~]# nl /etc/passwd | sed &apos;2,5d&apos;1 root:x:0:0:root:/root:/bin/bash6 sync:x:5:0:sync:/sbin:/bin/sync7 shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown.....(后面省略)..... sed 的动作为 ‘2,5d’ ，那个 d 就是删除。因为 2-5 行给他删除了，所以显示的数据就没有 2-5 行了。另外，注意一下，原本应该是要下达 sed -e 才对，没有 -e 也行。同时也要注意的是，sed 后面接的动作，请务必以 ‘’ 两个单引号括住。 如果题型变化一下，举例来说，如果只要删除第 2 行，可以使用【 nl /etc/passwd | sed &#39;2d&#39; 】来达成，至于若是要删除第 3 到最后一行，则是【 nl /etc/passwd | sed &#39;3,$d&#39; 】，那个钱字号【 $ 】代表最后一行。 范例二：承上题，在第二行后(亦即是加在第三行)加上【drink tea?】字样123456[root@www ~]# nl /etc/passwd | sed &apos;2a drink tea&apos;1 root:x:0:0:root:/root:/bin/bash2 bin:x:1:1:bin:/bin:/sbin/nologindrink tea3 daemon:x:2:2:daemon:/sbin:/sbin/nologin.....(后面省略)..... 在 a 后面加上的字符串就已将出现在第二行后面。那如果是要在第二行前呢？【 nl /etc/passwd | sed &#39;2i drink tea&#39; 】就对了。就是将【 a 】变成【 i 】即可。增加一行很简单，那如果是要增将两行以上呢？ 范例三：在第二行后面加入两行字，例如【Drink tea or …..】与【drink beer?】12345678[root@www ~]# nl /etc/passwd | sed &apos;2a Drink tea or ......\&gt; drink beer ?&apos;1 root:x:0:0:root:/root:/bin/bash2 bin:x:1:1:bin:/bin:/sbin/nologinDrink tea or ......drink beer ?3 daemon:x:2:2:daemon:/sbin:/sbin/nologin.....(后面省略)..... 这个范例的重点是【我们可以新增不只一行，可以新增好几行】但是每一行之间都必须要以反斜杠【 \ 】来进行新行的增加。所以，上面的例子中，我们可以发现在第一行的最后面就有 \ 存在，那是一定要的。 2、以行为单位的取代与显示功能范例四：我想将第 2-5 行的内容取代成为【No 2-5 number】12345[root@www ~]# nl /etc/passwd | sed &apos;2,5c No 2-5 number&apos;1 root:x:0:0:root:/root:/bin/bashNo 2-5 number6 sync:x:5:0:sync:/sbin:/bin/sync.....(后面省略)..... 透过这个方法我们就能够将数据整行取代了，非常容易吧，sed 还有更好用的。我们以前想要列出第 11~20 行，得要透过【head -n 20 | tail -n 10】之类的方法来处理，很麻烦。sed 则可以简单的直接取出你想要的那几行，是透过行号来的。看看底下的范例： 范例五：仅列出 /etc/passwd 档案内的第 5-7 行1234[root@www ~]# nl /etc/passwd | sed -n &apos;5,7p&apos;5 lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin6 sync:x:5:0:sync:/sbin:/bin/sync7 shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown 上述的指令中有个重要的选项【 -n 】，按照说明文件，这个 -n 代表的是【安静模式】。那么为什么要使用安静模式呢？你可以自行下达 sed ‘5,7p’ 就知道了 (5-7 行会重复输出)。有没有加上 -n 的参数时，输出的数据可是差很多的。你可以透过这个 sed 的以行为单位的显示功能，就能够将某一个档案内的某些行号捉出来查阅。很棒的功能！ 3、部分数据的搜寻并取代的功能除了整行的处理模式之外，sed 还可以用行为单位进行部分数据的搜寻并取代的功能。基本上 sed 的搜寻与取代的与 vi 相当的类似！他有点像这样：sed &#39;s/要被取代的字符串/新的字符串/g&#39; 我们使用底下这个取得 ifconfig 中 IP 数据的范例来了解一下什么是咱们所谓的搜寻并取代：12345678910111213141516171819202122232425262728步骤一：先观察原始讯息，利用 /sbin/ifconfig 查询 IP 为何？[root@www ~]# /sbin/ifconfig eth0eth0 Link encap:Ethernet HWaddr 00:90:CC:A6:34:84inet addr:192.168.1.100 Bcast:192.168.1.255 Mask:255.255.255.0inet6 addr: fe80::290:ccff:fea6:3484/64 Scope:LinkUP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1.....(以下省略).....# 我们的重点在第二行，也就是 192.168.1.100 这一行。先利用关键词捉出那一行！步骤二：利用关键词配合 grep 撷取出关键的一行数据[root@www ~]# /sbin/ifconfig eth0 | grep &apos;inet addr&apos;inet addr:192.168.1.100 Bcast:192.168.1.255 Mask:255.255.255.0# 当场仅剩下一行，接下来，我们要将开始到 addr: 通通删除，就是像底下这样：# inet addr:192.168.1.100 Bcast:192.168.1.255 Mask:255.255.255.0# 上面的删除关键在于【 ^.*inet addr: 】。正规表示法出现！步骤三：将 IP 前面的部分予以删除[root@www ~]# /sbin/ifconfig eth0 | grep &apos;inet addr&apos; | \&gt; sed &apos;s/^.*addr://g&apos;192.168.1.100 Bcast:192.168.1.255 Mask:255.255.255.0# 仔细与上个步骤比较一下，前面的部分不见了！接下来则是删除后续的部分，亦即：# 192.168.1.100 Bcast:192.168.1.255 Mask:255.255.255.0# 此时所需的正规表示法为：【 Bcast.*$ 】步骤四：将 IP 后面的部分予以删除[root@www ~]# /sbin/ifconfig eth0 | grep &apos;inet addr&apos; | \&gt; sed &apos;s/^.*addr://g&apos; | sed &apos;s/Bcast.*$//g&apos;192.168.1.100 4、直接修改档案内容(危险动作)sed 甚至可以直接修改档案的内容。而不必使用管线命令或数据流重导向。不过，由于这个动作会直接修改到原始的档案，所以千万不要随便拿系统配置文件来测试，可以使用测试档案 regular_express.txt 来测试看看。 范例六：利用 sed 将 regular_express.txt 内每一行结尾若为 . 则换成 !123[root@www ~]# sed -i &apos;s/\.$/\!/g&apos; regular_express.txt# 上头的 -i 选项可以让你的 sed 直接去修改后面接的档案内容而不是由屏幕输出# 这个范例是用在取代，可自行 cat 该档案去查阅结果 范例七：利用 sed 直接在 regular_express.txt 最后一行加入【# This is a test】12[root@www ~]# sed -i &apos;$a # This is a test&apos; regular_express.txt# 由于 $ 代表的是最后一行，而 a 的动作是新增，因此该档案是最后新增 sed 的【 -i 】选项可以直接修改档案内容，这功能非常有帮助。举例来说，如果你有一个 100 万行的档案，你要在第 100 行加某些文字，此时使用 vim 可能会疯掉！因为档案太大了！那怎办？就利用 sed 啊！透过 sed 直接修改/取代的功能，你甚至不需要使用 vim 去修订。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[认识与学习BASH之正规表示法与延伸正规表示法]]></title>
    <url>%2F2017%2F12%2F29%2F%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%AD%A6%E4%B9%A0BASH%E4%B9%8B%E6%AD%A3%E8%A7%84%E8%A1%A8%E7%A4%BA%E6%B3%95%E4%B8%8E%E5%BB%B6%E4%BC%B8%E6%AD%A3%E8%A7%84%E8%A1%A8%E7%A4%BA%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、正规表示法正规表示法 (Regular Expression, RE, 或称为常规表示法)是透过一些特殊字符的排列，用以【搜寻/取代/删除】一列或多列文字字符串，简单的说，正规表示法就是用在字符串的处理上面的一项【表示式】。正规表示法并不是一个工具程序，而是一个字符串处理的标准依据，如果您想要以正规表示法的方式处理字符串，就得要使用支持正规表示法的工具程序才行，这类的工具程序很多，例如 vi, sed, awk 等等。 1、什么是正规表示法简单的说，正规表示法就是处理字符串的方法，他是以行为单位来进行字符串的处理行为，正规表示法透过一些特殊符号的辅助，可以让使用者轻易的达到【搜寻/删除/取代】某特定字符串的处理程序。举个系统常见的例子，假如你发现系统在开机的时候，出现一个关于 mail 程序的错误，而开机过程的相关程序都是在 /etc/init.d/ 底下，也就是说，在该目录底下的某个档案内具有 mail 这个关键词，你想要将该档案捉出来进行查询修改的动作。此时你怎么找出来含有这个关键词的档案？你当然可以一个档案一个档案的开启，然后去搜寻 mail 这个关键词，只是…..该目录底下的档案可能不止100 个。如果了解正规表示法的相关技巧，那么只要一行指令就找出来：【grep &#39;mail&#39; /etc/init.d/*】那个 grep 就是支持正规表示法的工具程序之一。 谈到这里就得要进一步说明，正规表示法基本上是一种【表示法】，只要工具程序支持这种表示法，那么该工具程序就可以用来作为正规表示法的字符串处理之用。例如 vi, grep, awk ,sed 等等工具，因为她们有支持正规表示法，所以，这些工具就可以使用正规表示法的特殊字符来进行字符串的处理。但例如 cp, ls 等指令并未支持正规表示法，所以就只能使用 bash 自己本身的通配符而已。 2、正规表示法与Shell在Linux当中的角色定位：我们谈到的这个正规表示法，就有点像是数学的九九表一样，是 Linux 基础当中的基础，虽然也是最难的部分，不过，如果学成了之后，一定是【大大的有帮助】的。这就好像是金庸小说里面的学武难关：任督二脉！ 打通任督二脉之后，武功立刻成倍成长！所以，不论是对于系统的认识与系统的管理部分，他都有很棒的辅助。 3、延伸的正规表示法：正规表示法的字符串表示方式依照不同的严谨度而分为： 基础正规表示法与延伸正规表示法。延伸型正规表示法除了简单的一组字符串处理之外，还可以作群组的字符串处理，例如进行搜寻VBird 或 netman 或 lman 的搜寻，注意，是【或(or)】而不是【和(and)】的处理，此时就需要延伸正规表示法的帮助，藉由特殊的【 ( 】与【 | 】等字符的协助，就能够达到这样的目的。不过，我们在这里主要介绍最基础的基础正规表示法。（注：有一点要向大家报告说明清楚，那就是：【正规表示法与通配符是完全不一样的东西！】这很重要。因为【通配符 (wildcard) 代表的是 bash 操作接口的一个功能】，但正规表示法则是一种字符串处理的表示方式，这两者要分的很清楚才行。) 4、通过grep实例演示来学习正规表示法：新建regular_express.txt文档及测试内容： “Open Source” is a good mechanism to develop programs.apple is my favorite food.Football game is not use feet only.this dress doesn’t fit me.However, this dress is about $ 3183 dollars.GNU is free air not free beer.Her hair is very beauty.I can’t finish the test.Oh! The soup taste good.motorcycle is cheap than car.This window is clear.the symbol ‘*’ is represented as start.Oh! My god!The gd software is a library for drafting programs.You are the best is mean you are the no. 1.The world is the same with “glad”.I like dog.google is the best tools for search keyword.goooooogle yes!go! go! Let’s go.# I am VBird 共有22行，最底下一行为空白行！现在开始我们一个案例一个案例的来介绍。 例题一：搜寻特定字符串假设我们要从刚刚的档案当中取得 the 这个特定字符串，最简单的方式就是这样：123456[root@www ~]# grep -n &apos;the&apos; regular_express.txt8:I can&apos;t finish the test.12:the symbol &apos;*&apos; is represented as start.15:You are the best is mean you are the no. 1.16:The world &lt;Happy&gt; is the same with &quot;glad&quot;.18:google is the best tools for search keyword. 那如果想要【反向选择】呢？也就是说，当该行没有 ‘the’ 这个字符串时才显示在屏幕上，那就直接使用：[root@www ~]# grep -vn &#39;the&#39; regular_express.txt 接下来，如果你想要取得不论大小写的 the 这个字符串，则：[root@www ~]# grep -in &#39;the&#39; regular_express.txt 例题二、利用中括号 [] 来搜寻集合字符如果我想要搜寻 test 或 taste 这两个单字时，可以发现到，其实她们有共通的 ‘t?st’ 存在。这个时候，我可以这样来搜寻：123[root@www ~]# grep -n &apos;t[ae]st&apos; regular_express.txt8:I can&apos;t finish the test.9:Oh! The soup taste good. 其实 [] 里面不论有几个字符，他都谨代表某【一个】字符，所以，上面的例子说明了，我需要的字符串是【tast】或【test】两个字符串而已。 而如果想要搜寻到有 oo 的字符时，则使用：1234567[root@www ~]# grep -n &apos;oo&apos; regular_express.txt1:&quot;Open Source&quot; is a good mechanism to develop programs.2:apple is my favorite food.3:Football game is not use feet only.9:Oh! The soup taste good.18:google is the best tools for search keyword.19:goooooogle yes! 但是，如果我不想要 oo 前面有 g 的话呢？此时，可以利用在集合字符的反向选择 [^] 来达成：12345[root@www ~]# grep -n &apos;[^g]oo&apos; regular_express.txt2:apple is my favorite food.3:Football game is not use feet only.18:google is the best tools for search keyword.19:goooooogle yes! 假设我 oo 前面不想要有小写字符，所以，我可以这样写 [^abcd….z]oo ，但是这样似乎不怎么方便，由于小写字符的 ASCII 上编码的顺序是连续的，因此，我们可以将之简化为底下这样：12[root@www ~]# grep -n &apos;[^a-z]oo&apos; regular_express.txt3:Football game is not use feet only. 也就是说，当我们在一组集合字符中，如果该字符组是连续的，例如大写英文/小写英文/数字等等，就可以使用[a-z],[A-Z],[0-9]等方式来书写，那么如果我们的要求字符串是数字与英文呢？就将他全部写在一起，变成：[a-zA-Z0-9]。 例题三、行首与行尾字符 ^ $我们在例题一当中，可以查询到一行字符串里面有 the 的，那如果我想要让 the 只在行首列出呢？这个时候就得要使用制表符了。我们可以这样做：12[root@www ~]# grep -n &apos;^the&apos; regular_express.txt12:the symbol &apos;*&apos; is represented as start. 那如果我不想要开头是英文字母，则可以是这样：123[root@www ~]# grep -n &apos;^[^a-zA-Z]&apos; regular_express.txt1:&quot;Open Source&quot; is a good mechanism to develop programs.21:# I am VBird 注意到了吧？那个 ^ 符号，在字符集合符号(括号[])之内与之外是不同的！在 [] 内代表【反向选择】，在 [] 之外则代表定位在行首的意义！ 如果我想要找出来，行尾结束为小数点 (.) 的那一行，该如何处理：1234567[root@www ~]# grep -n &apos;\.$&apos; regular_express.txt1:&quot;Open Source&quot; is a good mechanism to develop programs.2:apple is my favorite food.3:Football game is not use feet only.4:this dress doesn&apos;t fit me.10:motorcycle is cheap than car.... 特别注意到，因为小数点具有其他意义(底下会介绍)，所以必须要使用跳脱字符(\)来加以解除其特殊意义！ 那么如果我想要找出来，哪一行是【空白行】，也就是说，该行并没有输入任何数据，该如何搜寻？12[root@www ~]# grep -n &apos;^$&apos; regular_express.txt22: 因为只有行首跟行尾 (^$)，所以，这样就可以找出空白行了。 假设你已经知道在一个程序脚本(shell script)或者是配置文件当中，空白行与开头为 # 的那一行是批注，因此如果你要将资料列出给别人参考时，可以将这些数据省略掉以节省保贵的纸张，那么你可以怎么作呢？我们以/etc/syslog.conf 这个档案来作范例，你可以自行参考一下输出的结果：12345[root@www ~]# cat -n /etc/syslog.conf# 在 CentOS 中，结果可以发现有 26 行的输出，很多空白行与 # 开头[root@www ~]# grep -v &apos;^$&apos; /etc/syslog.conf | grep -v &apos;^#&apos;# 结果仅有 7 行，其中第一个【 -v &apos;^$&apos; 】代表【不要空白行】，# 第二个【 -v &apos;^#&apos; 】代表【不要开头是 # 的那行】 例题四、任意一个字符 . 与重复字符 *我们知道通配符 * 可以用来代表任意(0 或多个)字符，但是正规表示法并不是通配符，两者之间是不相同的。至于正规表示法当中的【 . 】则代表【绝对有一个任意字符】的意思。这两个符号在正规表示法的意义如下： . (小数点)：代表【一定有一个任意字符】的意思； * (星星号)：代表【重复前一个 0 到无穷多次】的意思，为组合形态 我们直接做个练习吧。假设我需要找出 g??d 的字符串，亦即共有四个字符，起头是 g 而结束是 d ，我可以这样做：1234[root@www ~]# grep -n &apos;g..d&apos; regular_express.txt1:&quot;Open Source&quot; is a good mechanism to develop programs.9:Oh! The soup taste good.16:The world &lt;Happy&gt; is the same with &quot;glad&quot;. 因为 * 代表的是【重复 0 个或多个前面的 RE 字符】的意义，因此，【o*】代表的是：【拥有空字符或一个 o 以上的字符】，特别注意，因为允许空字符(就是有没有字符都可以的意思)，因此，【 grep -n &#39;o*&#39; regular_express.txt 】将会把所有的数据都打印出到屏幕上。因此，当我们需要【至少两个 o 以上的字符串】时，就需要 ooo* ，亦即是：1234567[root@www ~]# grep -n &apos;ooo*&apos; regular_express.txt1:&quot;Open Source&quot; is a good mechanism to develop programs.2:apple is my favorite food.3:Football game is not use feet only.9:Oh! The soup taste good.18:google is the best tools for search keyword.19:goooooogle yes! 例题五、限定连续 RE 字符范围 {}在上个例题当中，我们可以利用 . 与 RE 字符及 * 来设定 0 个到无限多个重复字符，那如果我想要限制一个范围区间内的重复字符数呢？举例来说，我想要找出两个到五个 o 的连续字符串，该如何作？这时候就得要使用到限定范围的字符 {} 了。但因为 { 与 } 的符号在 shell 是有特殊意义的，因此，我们必须要使用跳脱字符 \ 来让他失去特殊意义才行。至于 {} 的语法是这样的，假设我要找到两个 o 的字符串，可以是：1234567[root@www ~]# grep -n &apos;o\&#123;2\&#125;&apos; regular_express.txt1:&quot;Open Source&quot; is a good mechanism to develop programs.2:apple is my favorite food.3:Football game is not use feet only.9:Oh! The soup taste good.18:google is the best tools for search keyword.19:goooooogle yes! 这样看似乎与 ooo* 的字符没有什么差异？因为第 19 行有多个 o 依旧也出现了。那么换个搜寻的字符串，假设我们要找出 g 后面接 2 到 5 个 o ，然后再接一个 g 的字符串，他会是这样：12[root@www ~]# grep -n &apos;go\&#123;2,5\&#125;g&apos; regular_express.txt18:google is the best tools for search keyword. 5、基础正规表示法字符汇整 (characters)经过了上面的几个简单的范例，我们可以将基础的正规表示法特殊字符汇整如下： table th:first-of-type { width: 100px; } RE字符 意义与范例 ^word 意义：待搜寻的字符串(word)在行首！ 范例：搜寻行首为 # 开始的那一行，并列出行号 grep -n &#39;^#&#39; regular_express.txt word$ 意义：待搜寻的字符串(word)在行尾！ 范例：将行尾为 ! 的那一行打印出来，并列出行号 grep -n &#39;!$&#39; regular_express.txt . 意义：代表【一定有一个任意字符】的字符！ 范例：搜寻的字符串可以是 (eve) (eae) (eee) (e e)， 但不能仅有 (ee) ！亦即 e 与 e 中间【一定】仅有一个字符，而空格符也是字符！ grep -n &#39;e.e&#39; regular_express.txt \ 意义：跳脱字符，将特殊符号的特殊意义去除！ 范例：搜寻含有单引号 ‘ 的那一行！ grep -n \&#39; regular_express.txt * 意义：重复零个到无穷多个的前一个 RE 字符 范例：找出含有 (es) (ess) (esss) 等等的字符串，注意，因为 可以是 0 个，所以 es 也是符合带搜寻字符串。另外，因为 为重复【前一个 RE 字符】的符号，因此，在 之前必须要紧接着一个 RE 字符。例如任意字符则为【.】！ grep -n &#39;ess*&#39; regular_express.txt [list] 意义：字符集合的 RE 字符，里面列出想要撷取的字符！ 范例：搜寻含有 (gl) 或 (gd) 的那一行，需要特别留意的是，在 [] 当中【谨代表一个待搜寻的字符】，例如【 a[afl]y 】代表搜寻的字符串可以是 aay, afy, aly 即[afl] 代表 a 或 f 或 l 的意思！ grep -n &#39;g[ld]&#39; regular_express.txt [n1-n2] 意义：字符集合的 RE 字符，里面列出想要撷取的字符范围！ 范例：搜寻含有任意数字的那一行！需特别留意，在字符集合 [] 中的减号 - 是有特殊意义的，他代表两个字符之间的所有连续字符！但这个连续与否与 ASCII 编码有关，因此，你的编码需要设定正确(在 bash 当中，需要确定 LANG 与 LANGUAGE 的变量是否正确！) 例如所有大写字符则为 [A-Z] grep -n &#39;[0-9]&#39; regular_express.txt [^list] 意义：字符集合的 RE 字符，里面列出不要的字符串或范围！ 范例：搜寻的字符串可以是 (oog) (ood) 但不能是 (oot) ，那个 ^ 在 [] 内时，代表的意义是【反向选择】的意思。 例如，我不要大写字符，则为 [^A-Z]。但是，需要特别注意的是，如果以 grep -n [^A-Z] regular_express.txt来搜寻，即发现该档案内的所有行都被列出，为什么？因为这个 [^A-Z] 是【非大写字符】的意思，因为每一行均有非大写字符，例如第一行的 “Open Source” 就有 p,e,n,o….等等的小写字 grep -n &#39;oo[^t]&#39; regular_express.txt \{n,m\} 意义：连续 n 到 m 个的【前一个 RE 字符】 意义：若为 \{n\} 则是连续 n 个的前一个 RE 字符， 意义：若是 \{n,\} 则是连续 n 个以上的前一个 RE 字符！ 范例：在 g 与 g 之间有2 个到 3 个的 o 存在的字符串，亦即 (goog)(gooog) grep -n &#39;go\\{2,3\\}g&#39; regular_express.txt 再次强调：【正规表示法的特殊字符】与一般在指令列输入指令的【通配符】并不相同，例如，在通配符当中的 * 代表的是【 0 ~ 无限多个字符】的意思，但是在正规表示法当中， * 则是【重复 0 到无穷多个的前一个 RE 字符】的意思。使用的意义并不相同，不要搞混了。 二、延伸正规表示法事实上，一般读者只要了解基础型的正规表示法大概就已经相当足够了，不过，某些时刻为了要简化整个指令操作，了解一下使用范围更广的延伸型正规表示法的表示式会更方便。举个简单的例子，我们要去除空白行与行首为 # 的行列，使用的是grep -v &#39;^$&#39; regular_express.txt | grep -v &#39;^#&#39;需要使用到管线命令来搜寻两次！那么如果使用延伸型的正规表示法，我们可以简化为：egrep -v &#39;^$|^#&#39; regular_express.txt延伸型正规表示法可以透过群组功能【 | 】来进行一次搜寻！那个在单引号内的管线意义为【或 or】。此外，grep 预设仅支持基础正规表示法，如果要使用延伸型正规表示法，你可以使用 grep -E ，不过更建议直接使用 egrep ！直接区分指令比较好记忆！其实 egrep 与 grep -E 是类似命令别名的关系。 RE字符 意义与范例 + 意义：重复【一个或一个以上】的前一个 RE 字符 范例：搜寻 (god) (good) (goood)… 等等的字符串。那个 o+ 代表【一个以上的 o 】所以，底下的执行成果会将第 1, 9, 13 行列出来。 egrep -n &#39;go+d&#39; regular_express.txt ? 意义：【零个或一个】的前一个 RE 字符 范例：搜寻 (gd) (god) 这两个字符串。 那个 o? 代表【空的或 1 个 o 】所以，上面的执行成果会将第 13, 14 行列出来。有没有发现到，这两个案例( ‘go+d’ 与 ‘go?d’ )的结果集合与 ‘go*d’ 相同？想想看，这是为什么！ egrep -n &#39;go?d&#39; regular_express.txt &#124; 意义：用或( or )的方式找出数个字符串 范例：搜寻 gd 或 good 这两个字符串，注意，是【或】！ 所以，第 1,9,14 这三行都可以被打印出来。那如果还想要找出 dog 呢？ egrep -n &#39;gd&amp;#124;good&#39; regular_express.txt egrep -n &#39;gd&amp;#124;good&amp;#124;dog&#39; regular_express.txt () 意义：找出【群组】字符串 范例：搜寻 (glad) 或 (good) 这两个字符串，因为 g 与 d 是重复的，所以，我就可以将 la 与 oo 列于 ( ) 当中，并以 &#124; 来分隔开来，就可以。 egrep -n &#39;g(la&amp;#124;oo)d&#39; regular_express.txt ()+ 意义：多个重复群组的判别 范例：将【AxyzxyzxyzxyzC】用 echo 叫出，然后再使用如下的方法搜寻一下！ echo ‘AxyzxyzxyzxyzC’ &#124; egrep ‘A(xyz)+C’ 上面的例子意思是说，我要找开头是 A 结尾是 C ，中间有一个以上的 “xyz” 字符串的意思]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜基础篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>Shell</tag>
        <tag>BASH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[认识与学习BASH之管线命令]]></title>
    <url>%2F2017%2F12%2F28%2F%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%AD%A6%E4%B9%A0BASH%E4%B9%8B%E7%AE%A1%E7%BA%BF%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[就如同《数据流导向》所说的，bash 命令执行的时候有输出的数据会出现，那么如果这群数据必需要经过几道手续之后才能得到我们所想要的格式，应该如何来设定？这就牵涉到管线命令的问题(pipe) ，管线命令使用的是【 | 】这个界定符号。另外，管线命令与【连续下达命令】是不一样的。这点底下我们会再说明。底下我们先举一个例子来说明一下简单的管线命令。12示例：显示/etc/地下有多少个档案，并且利用less指令来分页显示[root@www ~]# ls -al /etc | less 其实这个管线命令【 | 】仅能处理经由前面一个指令传来的正确信息，也就是 standard output 的信息，对于 stdandard error 并没有直接处理的能力。那么整体的管线命令可以使用下图表示： 在每个管线后面接的第一个数据必定是【指令】(管线指令)。而且这个指令必须要能够接收 standard input 的数据才行，这样的指令才可以是为【管线命令】，例如 less, more, head, tail 等都是可以接收 standard input 的管线命令。至于例如 ls, cp, mv 等就不是管线命令。因为 ls, cp, mv 并不会接收来自 stdin的数据。也就是说，管线命令主要有两个比较需要注意的地方： 1.管线命令仅会处理 standard output，对于 standard error output 会予以忽略2.管线命令必须要能够接收来自前一个指令的数据成为 standard input 继续处理才行。 1、撷取命令： cut, grep什么是撷取命令？就是将一段数据经过分析后，取出我们所想要的。或者是经由分析关键词，取得我们所想要的那一行。不过，要注意的是，一般来说，撷取讯息通常是针对【一行一行】来分析的，并不是整篇讯息分析的，底下我们介绍两个很常用的讯息撷取命令： cut这个指令可以将一段讯息的某一段给他【切】出来，处理的讯息是以【行】为单位，底下我们就来谈一谈：12345678910111213141516171819202122232425262728293031[root@www ~]# cut -d&apos;分隔字符&apos; -f fields &lt;==用于有特定分隔字符[root@www ~]# cut -c 字符区间 &lt;==用于排列整齐的讯息选项与参数： -d ：后面接分隔字符。与 -f 一起使用； -f ：依据 -d 的分隔字符将一段讯息分割成为数段，用 -f 取出第几段的意思； -c ：以字符 (characters) 的单位取出固定字符区间；范例一：将 PATH 变量取出，我要找出第五个路径。[root@www ~]# echo $PATH/bin:/usr/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/X11R6/bin:/usr/games:# 1 | 2 | 3 | 4 | 5 | 6 | 7[root@www ~]# echo $PATH | cut -d &apos;:&apos; -f 5# 如同上面的数字显示，我们是以【 : 】作为分隔，因此会出现 /usr/local/bin# 那么如果想要列出第 3 与第 5 呢？，就是这样：[root@www ~]# echo $PATH | cut -d &apos;:&apos; -f 3,5范例二：将 export 输出癿讯息，取得第 12 字符以后的所有字符串[root@www ~]# exportdeclare -x HISTSIZE=&quot;1000&quot;declare -x INPUTRC=&quot;/etc/inputrc&quot;declare -x KDEDIR=&quot;/usr&quot;declare -x LANG=&quot;zh_TW.big5&quot;.....(其他省略).....# 注意看，每个数据都是排列整齐的输出！如果我们不想要【declare -x】时，就得这么做：[root@www ~]# export | cut -c 12-HISTSIZE=&quot;1000&quot;INPUTRC=&quot;/etc/inputrc&quot;KDEDIR=&quot;/usr&quot;LANG=&quot;zh_TW.big5&quot;.....(其他省略).....# 知道怎么回事了吧？用 -c 可以处理比较具有格式的输出数据！# 我们还可以指定某个范围的值，例如第 12-20 的字符，就是 cut -c 12-20 等等！ cut 主要的用途在于将【同一行里面的数据进行分解！】最常使用在分析一些数据或文字数据的时候！这是因为有时候我们会以某些字符当作分割的参数，然后来将数据加以切割，以取得我们所需要的数据。 grep刚刚的 cut 是将一行讯息当中，取出某部分我们想要的，而 grep 则是分析一行讯息，若当中有我们所需要的信息，就将该行拿出来，简单的语法是这样的：12345678910111213141516171819202122[root@www ~]# grep [-acinv] [--color=auto] &apos;搜寻字符串&apos; filename选项与参数： -a ：将 binary 档案以 text 档案的方式搜寻数据 -c ：计算找到 &apos;搜寻字符串&apos; 的次数 -i ：忽略大小写的不同，所以大小写视为相同 -n ：顺便输出行号 -v ：反向选择，亦即显示出没有 &apos;搜寻字符串&apos; 内容的那一行！ --color=auto ：可以将找到的关键词部分加上颜色的显示。范例一：将 last 当中，有出现 root 的那一行就取出来；[root@www ~]# last | grep &apos;root&apos;范例二：与范例一相反，只要没有 root 的就取出！[root@www ~]# last | grep -v &apos;root&apos;范例三：在 last 的输出讯息中，只要有 root 就取出，并且仅取第一栏[root@www ~]# last | grep &apos;root&apos; |cut -d &apos; &apos; -f1范例四：取出 /etc/man.config 内含 MANPATH 的那几行[root@www ~]# grep --color=auto &apos;MANPATH&apos; /etc/man.config....(前面省略)....MANPATH_MAP /usr/X11R6/bin /usr/X11R6/manMANPATH_MAP /usr/bin/X11 /usr/X11R6/manMANPATH_MAP /usr/bin/mh /usr/share/man（如果加上 --color=auto 的选项，找到的关键词部分会用特殊颜色显示。） 2、排序命令： sort, wc, uniqsort他可以帮我们进行排序，而且可以依据不同的数据型态来排序。例如数字与文字的排序就不一样。此外，排序的字符与语系的编码有关，因此，如果您需要排序时，建议使用LANG=C来让语系统一，数据排序会比较好一些。12345678910111213141516171819202122232425262728[root@www ~]# sort [-fbMnrtuk] [file or stdin]选项与参数： -f ：忽略大小写的差异，例如 A 与 a 规为编码相同； -b ：忽略最前面的空格符部分； -M ：以月份的名字来排序，例如 JAN, DEC 等等的排序方法； -n ：使用【纯数字】进行排序(默认是以文字型态来排序的)； -r ：反向排序； -u ：就是 uniq ，相同的数据中，仅出现一行代表； -t ：分隔符，预设是用 [tab] 键来分隔； -k ：以那个区间 (field) 来进行排序的意思范例一：个人账号都记录在 /etc/passwd 下，请将账号进行排序。[root@www ~]# cat /etc/passwd | sortadm:x:3:4:adm:/var/adm:/sbin/nologinapache:x:48:48:Apache:/var/www:/sbin/nologinbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologin#由上面的数据看起来，sort 是预设【以第一个】数据来排序，而且默认是以【文字】型态来排序的。所以由a开始排到最后。范例二：/etc/passwd 内容是以 : 来分割的，我想以第三栏来排序，该如何？[root@www ~]# cat /etc/passwd | sort -t &apos;:&apos; -k 3root:x:0:0:root:/root:/bin/bashuucp:x:10:14:uucp:/var/spool/uucp:/sbin/nologinoperator:x:11:0:operator:/root:/sbin/nologinbin:x:1:1:bin:/bin:/sbin/nologingames:x:12:100:games:/usr/games:/sbin/nologin# 如果是以文字型态来排序的，就是会这样，想要使用数字排序：# cat /etc/passwd | sort -t &apos;:&apos; -k 3 -n# 这样才行。用那个 -n 来告知 sort 以数字来排序。 uniq如果我排序完成了，想要将重复的资料仅列出一个显示，可以怎么做呢？12345678910111213[root@www ~]# uniq [-ic]选项与参数： -i ：忽略大小写字符的不同； -c ：进行计数范例一：使用 last 将账号列出，仅取出账号栏，进行排序后仅取出一位；[root@www ~]# last | cut -d &apos; &apos; -f1 | sort | uniq范例二：承上题，如果我还想要知道每个人的登入总次数[root@www ~]# last | cut -d &apos; &apos; -f1 | sort | uniq -c112 reboot41 root1 wtmp# 从上面的结果可以发现 reboot 有 12 次， root 登入则有 41 次。wtmp与第一行的空白都是last的默认字符，那两个可以忽略。 这个指令用来将【重复的行删除掉只显示一个】，举个例子来说，你要知道这个月份登入你主机的用户有谁，而不在乎他的登入次数，那么就使用上面的范例，(1)先将所有的数据列出；(2)再将人名独立出来；(3)经过排序；(4)只显示一个！由于这个指令是在将重复的东西减少，所以当然需要【配合排序过的档案】来处理。 3、统计命令：wc如果我想要知道 /etc/man.config这个档案里面有多少字？多少行？多少字符的话，可以利用wc这个指令来达成，他可以帮我们计算输出的讯息的整体数据。12345[root@www ~]# wc [-lwm]选项与参数： -l ：仅列出行； -w ：仅列出多少字(英文单字)； -m ：多少字符； 当你要知道目前你的账号档案中有多少个账号时，就使用这个方法：【cat /etc/passwd | wc -l】。因为/etc/passwd里头一行代表一个使用者。所以知道行数就晓得有多少账号在里头。而如果要计算一个档案里头有多少个字符时，就使用 wc -c 这个选项。 4、字符转换命令： tr, col, join, paste, expand我们在 vim 程序编辑器当中，提到过 DOS 断行字符与 Unix 断行字符的不同，并且可以使用 dos2unix 与 unix2dos 来完成转换。当然，还有其他的替代方案，底下我们就来介绍一下这些字符转换命令在管线当中的使用方法： trtr 可以用来删除一段讯息当中的文字，或者是进行文字讯息的替换。123456789[root@www ~]# tr [-ds] SET1 ...选项与参数： -d ：删除讯息当中的 SET1 这个字符串； -s ：取代掉重复的字符！范例一：将 last 输出的讯息中，所有的小写变成大写字符：[root@www ~]# last | tr &apos;[a-z]&apos; &apos;[A-Z]&apos;# 事实上，没有加上单引号也是可以执行的，如：【 last | tr [a-z] [A-Z] 】范例二：将 /etc/passwd 输出的讯息中，将冒号 (:) 删除[root@www ~]# cat /etc/passwd | tr -d &apos;:&apos; 相信处理 Linux &amp; Windows 系统中的人们最麻烦的一件事就是 DOS 底下会自动的在每行行尾加入^M 这个断行符号。使用 tr 将 ^M 可以使用 \r 来代替就可以去除 DOS 档案留下来的 ^M 这个断行的符号。这东西相当的有用！ col1234[root@www ~]# col [-xb]选项与参数： -x ：将 tab 键转换成对等的空格键 -b ：在文字内有反斜杠 (/) 时，仅保留反斜杠最后接的那个字符 joinjoin 看字面上的意义 (加入/参加) 就可以知道，他是在处理两个档案之间的数据，而且，主要是在处理【两个档案当中，有”相同数据”的那一行，才将他加在一起】的意思。我们利用底下的简单例子来说明：12345678910111213[root@www ~]# join [-ti12] file1 file2选项与参数： -t ：join 默认以空格符分隔数据，并且比对【第一个字段】的数据，如果两个档案相同，则将两笔数据联成一行，且第一个字段放在第一个 -i ：忽略大小写的差异； -1 ：这个是数字的 1 ，代表【第一个档案要用那个字段来分析】的意思； -2 ：代表【第二个档案要用那个字段来分析】的意思。范例一：用 root 的身份，将 /etc/passwd 与 /etc/shadow 相关数据整合成一栏[root@www ~]# join -t &apos;:&apos; /etc/passwd /etc/shadow# 透过上面这个动作，我们可以将两个档案第一字段相同者整合成一行。第二个档案的相同字段并不会显示范例二：我们知道 /etc/passwd 第四个字段是 GID ，那个 GID 记录在 /etc/group 当中的第三个字段[root@www ~]# join -t &apos;:&apos; -1 4 /etc/passwd -2 3 /etc/group# 同样的，相同的字段部分被移动到最前面了。所以第二个档案的内容就没再显示 paste这个 paste 就要比 join 简单多了。相对于 join 必须要比对两个档案的数据相关性，paste 就直接【将两行贴在一起，且中间以 [tab] 键隔开】而已。简单的使用方法：1234[root@www ~]# paste [-d] file1 file2选项与参数： -d ：后面可以接分隔字符。预设是以 [tab] 来分隔的 - ：如果 file 部分写成 - ，表示来自 standard input 的资料的意思。 expand将 [tab] 按键转成空格键123[root@www ~]# expand [-t] file选项与参数： -t ：后面可以接数字。一般来说，一个 tab 按键可以用 8 个空格键取代。我们也可以自行定义一个 [tab] 按键代表多少个字符 5、分割命令： split12345[root@www ~]# split [-bl] file PREFIX选项与参数： -b ：后面可接欲分割成的档案大小，可加单位，例如 b, k, m 等； -l ：以行数来进行分割。 PREFIX ：代表前导符的意思，可作为分割档案的前导文字。 6、参数代换： xargs1234567[root@www ~]# xargs [-0epn] command选项与参数： -0 ：如果输入的 stdin 含有特殊字符，例如 `, \, 空格键等等字符时，这个 -0 参数可以将他还原成一般字符。这个参数可以用于特殊状态 -e ：这个是 EOF (end of file) 的意思。后面可以接一个字符串，当 xargs 分析到这个字符串时，就会停止继续工作 -p ：在执行每个指令的 argument 时，都会询问使用者的意思； -n ：后面接次数，每次 command 指令执行时，要使用几个参数的意思。当 xargs 后面没有接任何的指令时，默认是以 echo 来进行输出。]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜基础篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>Shell</tag>
        <tag>BASH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[认识与学习BASH之数据流重导向]]></title>
    <url>%2F2017%2F12%2F28%2F%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%AD%A6%E4%B9%A0BASH%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E9%87%8D%E5%AF%BC%E5%90%91%2F</url>
    <content type="text"><![CDATA[数据流重导向就是将某个指令执行后应该要出现在屏幕上的数据，给他传输到其他的地方，例如档案或者是装置(例如打印机之类的)。 1、什么是数据流导向？这得要由指令的执行结果谈起。一般来说，如果你要执行一个指令，通常他会是这样的：我们执行一个指令的时候，这个指令可能会由档案读入资料，经过处理之后，再将数据输出到屏幕上。在上图当中，standard output 与 standard error output 分别代表【标准输出】与【标准错误输出】，这两个默认都是输出到屏幕上面。那么什么是标准输出与标准错误输出呢？ 2、standard output 与 standard error output简单的说，标准输出指的是【指令执行所回传的正确的讯息】，而标准错误输出可理解为【指令执行失败后，所回传的错误讯息】。不管正确或错误的数据都是默认输出到屏幕上，所以屏幕当然是乱的。那能不能透过某些机制将这两股数据分开呢？答案是可以的。那就是数据流重导向的功能，数据流重导向可以将 standard output(简称 stdout) 与 standard error output (简称 stderr)分别传送到其他的档案或装置去，而分别传送所用的特殊字符则如下所示： 1. 标准输入 (stdin) ：代码为 0 ，使用 &lt; 或 &lt;&lt; ； 2. 标准输出 (stdout)：代码为 1 ，使用 &gt; 或 &gt;&gt; ； 3. 标准错误输出(stderr)：代码为 2 ，使用 2&gt; 或 2&gt;&gt; ； （注：一个’&lt;’符号表示覆盖，两个’&lt;&lt;’表示追加。） 对于输入重定向来讲，用到的符号及其作用如下表： table th:first-of-type { width: 25%; } 符号 作用 命令 &lt; 文件 将文件作为命令的标准输入 命令 &lt;&lt; 分界符 从标准输入中读入，直到遇见分界符才停止 命令 &lt; 文件1 &gt; 文件2 将文件1作为命令的标准输入并将标准输出到文件2 对于输出重定向来讲，用到的符号及其作用如下表： 符号 作用 命令 &gt; 文件 将标准输出重定向到一个文件中（清空原有文件的数据） 命令 2&gt; 文件 将错误输出重定向到一个文件中（清空原有文件的数据） 命令 &gt;&gt; 文件 将标准输出重定向到一个文件中（追加到原有内容的后面） 命令 2&gt;&gt; 文件 将错误输出重定向到一个文件中（追加到原有内容的后面） 命令 &gt;&gt; 文件 2&gt;&amp;1或命令 &amp;&gt;&gt; 文件 将标准输出与错误输出共同写入到文件中（追加到原有内容的后面） 3、/dev/null 垃圾桶黑洞装置与特殊写法如果我知道错误讯息会发生，所以要将错误讯息忽略掉而不显示或储存，这个时候黑洞装置 /dev/null 就很重要了。这个 /dev/null 可以吃掉任何导向这个装置的信息。123示例：[dmtsai@www ~]$ find /home -name .bashrc 2&gt; /dev/null/home/dmtsai/.bashrc &lt;==只有 stdout 会显示到屏幕上， stderr 被丢弃了 如果我要将正确与错误数据通通写入同一个档案去，这个时候就得要使用特殊的写法了。我们同样用底下的案例来说明：1234示例：将指令的数据全部写入名为 list 的档案中[dmtsai@www ~]$ find /home -name .bashrc &gt; list 2&gt; list &lt;==错误[dmtsai@www ~]$ find /home -name .bashrc &gt; list 2&gt;&amp;1 &lt;==正确[dmtsai@www ~]$ find /home -name .bashrc &amp;&gt; list &lt;==正确 上述表格第一行错误的原因是，由于两股数据同时写入一个档案，又没有使用特殊的语法，此时两股数据可能会交叉写入该档案内，造成次序的错乱。所以虽然最终 list 档案还是会产生，但是里面的数据排列是乱的，而不是原本屏幕上的输出排序。至于写入同一个档案的特殊语法如上所示，你可以使用 2&gt;&amp;1 也可以使用 &amp;&gt; 。一般来说，比较常见的是 2&gt;&amp;1 的语法。]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜基础篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>Shell</tag>
        <tag>BASH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[认识与学习BASH之变量]]></title>
    <url>%2F2017%2F12%2F28%2F%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%AD%A6%E4%B9%A0BASH%E4%B9%8B%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[1、Shell 的变量变量是 bash 环境中非常重要的一个知识点，我们知道 Linux 是多人多任务的环境，每个人登入系统都能取得一个 bash ，每个人都能够使用 bash 下达 mail 这个指令来收取【自己】的邮件，问题是，bash 是如何得知你的邮件信箱是哪个档案？这就需要【变量】的帮助。底下我们将介绍重要的环境变量、变量的取用与设定等数据。 命令的执行原理：在用户执行了一条命令之后，Linux系统中到底发生了什么事情呢？简单来说，命令在Linux中的执行分为4个步骤。 第1步：判断用户是否以绝对路径或相对路径的方式输入命令（如/bin/ls），如果是的话则直接执行。 第2步：Linux系统检查用户输入的命令是否为“别名命令”，即用一个自定义的命令名称来替换原本的命令名称。可以用alias命令来创建一个属于自己的命令别名，格式为“alias 别名=命令”。若要取消一个命令别名，则是用unalias命令，格式为“unalias 别名”。我们之前在使用rm命令删除文件时，Linux系统都会要求我们再确认是否执行删除操作，其实这就是Linux系统为了防止用户误删除文件而特意设置的rm别名命令，接下来我们把它取消掉： 12345678910[root@linuxprobe ~]# lsanaconda-ks.cfg Documents initial-setup-ks.cfg Pictures TemplatesDesktop Downloads Music Public Videos[root@linuxprobe ~]# rm anaconda-ks.cfg rm: remove regular file ‘anaconda-ks.cfg’? y[root@linuxprobe~]# alias rmalias rm=&apos;rm -i&apos;[root@linuxprobe ~]# unalias rm[root@linuxprobe ~]# rm initial-setup-ks.cfg [root@linuxprobe ~]# 第3步：Bash解释器判断用户输入的是内部命令还是外部命令。内部命令是解释器内部的指令，会被直接执行；而用户在绝大部分时间输入的是外部命令，这些命令交由步骤4继续处理。可以使用“type命令名称”来判断用户输入的命令是内部命令还是外部命令。 第4步：系统在多个路径中查找用户输入的命令文件，而定义这些路径的变量叫作PATH，可以简单地把它理解成是“解释器的小助手”，作用是告诉Bash解释器待执行的命令可能存放的位置，然后Bash解释器就会乖乖地在这些位置中逐个查找。PATH是由多个路径值组成的变量，每个路径值之间用冒号间隔，对这些路径的增加和删除操作将影响到Bash解释器对Linux命令的查找。12345[root@linuxprobe ~]# echo $PATH/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin:/bin:/sbin[root@linuxprobe ~]# PATH=$PATH:/root/bin[root@linuxprobe ~]# echo $PATH/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin:/bin:/sbin:/root/bin 这里有比较经典的问题：“为什么不能将当前目录（.）添加到PATH中呢? ” 原因是，尽管可以将当前目录（.）添加到PATH变量中，从而在某些情况下可以让用户免去输入命令所在路径的麻烦。但是，如果黑客在比较常用的公共目录/tmp中存放了一个与ls或cd命令同名的木马文件，而用户又恰巧在公共目录中执行了这些命令，那么就极有可能中招了。 2、什么是变量简单的说，就是让某一个特定字符串代表不固定的内容。（1）环境变量当我们登录到Linux之后，就会有一个bash的执行程序用来跟Linux沟通，而在进入 shell 之前，由于系统需要一些变量来提供其他数据的存取 (或者是一些环境的设定参数值，例如是否要显示彩色等等)，所以就有一些所谓的【环境变量】需要来读入系统中。这些环境变量例如 PATH、HOME、MAIL、SHELL 等等，都是很重要的，为了区别与自定义变量的不同，环境变量通常以大写字符来表示。 （2）变量的取用与设定：echo, 取消变量设定规则：unset可以利用 echo 这个指令来取用变量，但是，变量在被取用时，前面必须要加上钱字号【$】才行，举例来说，要知道 PATH 的内容，该如何是好？ 变量的取用： [root@www ~]# echo $PATH/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin变量的取用就如同上面的范例，利用 ehco 就能够读出，只是需要在变量名称前面加上 $，或者是以${变量}的方式来取用都可以。 变量的设定规则： 1.变量与变量内容以一个等号【=】来连结，如下所示： 【myname=ben】2.等号两边不能直接接空格符，如下所示为错误： 【myname = ben】或【myname=ben Tsai】3.变量名称只能是英文字母与数字，但是开头字符不能是数字，如下为错误： 【2myname=ben】4.变量内容若有空格符可使用双引号【”】或单引号【’】将变量内容结合起来，但双引号能够识别变量，单引号原样输出。5.可用跳脱字符【\】将特殊符号(如 [Enter], $, \, 空格符, ‘等)变成一般字符；6.在一串指令中，还需要藉由其他的指令提供的信息，可以使用反单引号【指令】或【$(指令)】。 【version=$(uname -r)】再【echo $version】可得【2.6.32_1-16-0-0_virtio】7.若该变量为扩增变量内容时，则可用 “$变量名称” 或 ${变量} 累加内容，如下所示： 【PATH=”$PATH”:/home/bin】8.若该变量需要在其他子程序执行，则需要以 export 来使变量变成环境发量： 【export PATH】9.通常大写字符为系统默认变量，自行设定变量可以使用小写字符，方便判断；10.取消变量的方法为使用 unset ：【unset 变量名称】例如取消 version 的设定： 【unset version】 （3）其它 什么是【子程序】呢？就是说，在我目前这个 shell 的情况下，去启用另一个新的 shell ，新的那个shell 就是子程序。在一般的状态下，父程序的自定义变量是无法在子程序内使用的。但是透过export 将变量变成环境发量后，就能够在子程序底下应用了。 在指令下达的过程中，反单引号( ` )这个符号代表的意义为何？在一串指令中，在 ` 之内的指令将会被先执行，而其执行出来的结果将做为外部的输入信息。 （4）常见引用方式 3、环境变量的功能用 env 观察环境变量与常见环境变量说明。env 是 environment (环境) 的简写，是列出来所有的环境变量的命令（此处就不贴出 env 命令显示数据）。底下我们对一些常见变量来做一个说明： HOME 代表用户的家目录。还记得我们可以使用 cd ~ 去到自己的家目录吗？或者利用 cd 就可以直接回到用户家目录了。那就是取用这个变量。有很多程序都可能会取用到这个变量的值。 SHELL 告知我们，目前这个环境使用的 SHELL 是哪支程序？ Linux 预设使用 /bin/bash 。 HISTSIZE 这个与【历史命令】有关，亦即是，我们曾经下达过的指令可以被系统记录下来，而记录的【笔数】则是由这个值来设定的。 MAIL 当我们使用 mail 这个指令在收信时，系统会去读取的邮件信箱档案 (mailbox)。 PATH 就是执行文件搜寻的路径，目录与目录中间以冒号(:)分隔，由于档案的搜寻是依序由 PATH 的变量内的目录来查询，所以目录的顺序也是重要的。 LANG 这个重要。就是语系数据。很多讯息都会用到他，举例来说，当我们在启动某些 perl 的程序语言档案时，他会主动的去分析语系数据文件，如果发现有他无法解析的编码语系，可能会产生错误。一般来说，我们中文编码通常是 zh_TW.Big5 或者是 zh_TW.UTF-8，这两个编码偏偏不容易被解译出来，所以，有的时候，可能需要修订一下语系数据。 RANDOM 这个就是【随机随机数】的变量。目前大多数的 distributions 都会有随机数生成器，那就是 /dev/random 这个档案。 我们可以透过这个随机数档案相关的变量 ($RANDOM) 来随机取得随机数值。在 BASH 的环境下，这个 RANDOM 变量的内容，介于 0~32767 之间，所以，你只要 echo $RANDOM 时，系统就会主动的随机取出一个介于 0~32767 的数值。万一我想要使用 0~9 之间的数值，利用 declare 宣告数值类型，然后这样做就可以了： declare -i number=$RANDOM*10/32768 ; echo $number ##此时会随机取出 0~9 之间的数值 Linux作为一个多用户多任务的操作系统，能够为每个用户提供独立的、合适的工作运行环境，因此，一个相同的变量会因为用户身份的不同而具有不同的值。 其实变量是由固定的变量名与用户或系统设置的变量值两部分组成的，我们完全可以自行创建变量，来满足工作需求。例如设置一个名称为WORKDIR的变量，方便用户更轻松地进入一个层次较深的目录：12345[root@linuxprobe ~]# mkdir /home/workdir[root@linuxprobe ~]# WORKDIR=/home/workdir[root@linuxprobe ~]# cd $WORKDIR [root@linuxprobe workdir]# pwd/home/workdir 但是，这样的变量不具有全局性，作用范围也有限，默认情况下不能被其他用户使用。如果工作需要，可以使用export命令将其提升为全局变量，这样其他用户也就可以使用它了：123456[root@linuxprobe ~]# export WORKDIR[root@linuxprobe ~]# su linuxprobeLast login: Fri Mar 20 21:52:10 CST 2017 on pts/0[linuxprobe@linuxprobe ~]$ cd $WORKDIR[linuxprobe@linuxprobe workdir]$ pwd/home/workdir 4、变量键盘的读取、数组与宣告（1）要读取来自键盘输入的变量，就是用 read 这个指令。这个指令最常被用在 shell script 的撰写当中，想要跟使用者对谈？用这个指令就对了。 语法：read [-pt] variable 选项与参数： -p ：后面可以接提示字符； -t ：后面可以接等待的【秒数！】这个比较有趣。不会一直等待使用者 read 之后不加任何参数，直接加上变量名称，那么底下就会主动出现一个空白行等待你的输入。如果加上 -t 后面接秒数，那么 t 秒之内没有任何动作时，该指令就会自动略过。如果是加上 -p ，在输入的光标前就会有比较多可以用的提示字符给我们参考。在指令的下达里面，比较美观。12345示例：[root@www ~]# echo $atestThis is a test &lt;==你刚刚输入的数据已经变成一个变量内容[root@www ~]# read -p &quot;Please keyin your name: &quot; -t 30 namedPlease keyin your name: VBird Tsai &lt;==提示使用者 30 秒内输入自己的大名，将该输入字符串作为名为 named的变量内容 （2）declare 或 typeset 是一样的功能，就是在【宣告变量的类型】。如果使用 declare 后面并没有接任何参数，那么 bash 就会主动的将所有的变量名称与内容通通叫出来，就好像使用 set 一样。declare / typeset 语法：declare [-aixr] variable 选项与参数： -a ：将后面名为 variable 的变量定义成为数组 (array) 类型 -i ：将后面名为 variable 的变量定义成为整数数字 (integer) 类型 -x ：用法与 export 一样，就是将后面的 variable 变成环境变量；取消的话将[-x]变为[+x]即为取消。 -r ：将变量设定成为 readonly 类型，该变量不可被更改内容，也不能 unset 5、变量内容的删除、取代与替换 6、用户登入 shell 后读取的两个配置文件： /etc/profile：这是系统整体的设定，你最好不要修改这个档案； ~/.bash_profile 或 ~/.bash_login 或 ~/.profile：属于使用者个人设定，你要改自己的数据，就写入这里。 7、常见的一些特殊变量 table th:first-of-type { width: 100px; } 参数处理 说明 $0 当前脚本的名称 $# 传递到脚本的参数个数 $? 获取上一个命令的执行结果。0表示没有错误，其他任何值表明有错误。 $* 以一个单字符串显示所有向脚本传递的参数。如”$*”用「”」括起来的情况、以”$1 $2 … $n”的形式输出所有参数。 $@ 与$*相同，但是使用时加引号，并在引号中返回每个参数。如”$@”用「”」括起来的情况、以”$1” “$2” … “$n” 的形式输出所有参数。 $$ 脚本运行的当前进程ID号 $! 后台运行的最后一个进程的ID号 $- 显示Shell使用的当前选项，与set命令功能相同。]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜基础篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>Shell</tag>
        <tag>BASH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[认识与学习BASH之BASH概述]]></title>
    <url>%2F2017%2F12%2F28%2F%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%AD%A6%E4%B9%A0BASH%E4%B9%8BBASH%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[为什么BASH叫做壳程序？ 理解这个就理解了BASH与操作系统的关系，以及BASH是什么，有什么用。 1、认识BASH这个Shell管理整个计算机硬件的其实是操作系统的核心 (kernel)，这个核心是需要被保护的。所以我们一般使用者就只能透过 shell 来跟核心沟通，以让核心达到我们所想要达到的工作。那么系统有多少 shell 可用呢？为什么我们要使用 bash啊？底下分别来谈一谈。 2、硬件、核心 与 Shell在认识 Shell 之前，我们先来了解一下计算机的运作状况吧。举个例子来说：当你要计算机传输出来【音乐】的时候，你的计算机需要什么东西呢？ 1. 硬件：当然就是需要你的硬件有【声卡芯片】这个配备，否则怎么会有声音； 2. 核心管理：操作系统的核心可以支持这个芯片组，当然还需要提供芯片的驱动程序； 3. 应用程序：需要使用者 (就是你) 输入发生声音的指令。 这就是基本的一个输出声音所需要的步骤。也就是说，你必须要【输入】一个指令之后，【硬件】才会透过你下达的指令来工作。那么硬件如何知道你下达的指令呢？那就是 kernel (核心) 的控制工作了，也就是说，我们必须要透过【Shell】将我们输入的指令与 Kernel 沟通，好让 Kernel 可以控制硬件来正确无误的工作。 曾经提到过，操作系统其实是一组软件，由于这组软件在控制整个硬件与管理系统的活动监测，如果这组软件能被用户随意的操作，若使用者应用不当，将会使得整个系统崩溃！因为操作系统管理的就是整个硬件功能，所以当然不能够随便被一些没有管理能力的终端用户随意使用。但是我们总是需要让用户操作系统的，所以就有了在操作系统上面发展的应用程序。用户可以透过应用程序来指挥核心，让核心达成我们所需要的硬件任务！ 我们可以发现应用程序其实是在最外局，就如同鸡蛋的外壳一样，因此这个咚咚也就被称呼为壳程序 (shell)。其实壳程序的功能只是提供用户操作系统的一个接口，因此这个壳程序需要可以呼叫其他软件才好。我们知道有很多指令，例如 man, chmod, chown, vi, fdisk, mkfs 等指令，这些指令都是独立的应用程序，但是我们可以透过壳程序 (就是指令列模式)来操作这些应用程序，让这些应用程序呼叫核心来运作所需的工作。这样对于壳程序是否有了一定的概念了。 也就是说，只要能够操作应用程序的接口都能够称为壳程序。狭义的壳程序指的是指令列方面的软件，包括本章要介绍的 bash 等。广义的壳程序则包括图形接口的软件，因为图形接口其实也能够操作各种应用程序来呼叫核心工作。 3、系统合法的shell与/etc/shells功能知道什么是 Shell 之后，那么我们来了解一下 Linux 使用的是哪一个 shell 。由于早年的 Unix 年代，发展者众，所以由于 shell 依据发展者的不同就有很多的版本，例如常听到的 Bourne SHell (sh) 、在 Sun 里头预设的 C SHell、 商业上常用的 K SHell、还有 TCSH 等等，每一种 Shell 都各有其特点。至于 Linux 使用的这一种版本就称为【Bourne Again SHell (简称 bash) 】，这个 Shell 是 Bourne Shell 的增强版本，也是基准于GNU 的架构下发展出来的。 在介绍 shell 的优点之前，先来说一说 shell 的简单历史：第一个流行的 shell 是由 Steven Bourne 发展出来的，为了纪念他所以就称为 Bourne shell ，或直接简称为 sh 。而后来另一个广为流传的 shell 是由柏克莱大学的 Bill Joy 设计依附于 BSD 版的 Unix 系统中的 shell ，这个 shell 的语法有点类似 C 语言，所以才得名为 C shell ，简称为 csh 。由于在学术界 Sun 主机势力相当的庞大，而Sun 主要是 BSD 的分支之一，所以 C shell 也是另一个很重要而且流传很广的 shell 之一 。 由于 Linux 为 C 程序语言撰写的，很多程序设计师使用 C 来开发软件，因此 C shell相对的就很热门了。Sun 公司的创始人就是 Bill Joy，而 BSD 最早就是 Bill Joy 发展出来的。 那么目前我们的 Linux 有多少我们可以使用的 shells 呢？ 你可以检查一下/etc/shells 这个档案，至少就有底下这几个可以用的 shells：123456/bin/sh (已经被 /bin/bash 所取代)/bin/bash (就是 Linux 预设的 shell)/bin/ksh (Kornshell 由 AT&amp;T Bell lab. 发展出来的，兼容于 bash)/bin/tcsh (整合 C Shell ，提供更多的功能)/bin/csh (已经被 /bin/tcsh 所取代)/bin/zsh (基于 ksh 发展出来的，功能更强大的 shell) 虽然各家 shell 的功能都差不多，但是在某些语法的下达方面则有所不同，因此建议你还是选择某一种 shell 来熟悉一下较佳。 Linux 预设就是使用 bash ，所以最初你只要学会 bash 就ok了。为什么我们系统上合法的 shell 要写入 /etc/shells 这个档案呢？ 这是因为系统某些服务在运作过程中，会去检查使用者能够使用的 shells ，而这些 shell 的查询就是藉由 /etc/shells 这个档案。 那么，我这个使用者取得shell工作后，预设会取得哪一个 shell 呢？当我登入的时候，系统就会给我一个 shell 让我来工作了。而这个登入取得的 shell 就记录在 /etc/passwd 这个档案内。这个档案的内容是啥？12345[root@www ~]# cat /etc/passwdroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologin.....(底下省略)..... 如上所示，在每一行的最后一个数据，就是你登入后取得预设的 shell 。 4、Bash shell 的功能既然 /bin/bash 是 Linux 预设的 shell，那么总是得了解一下这个玩意儿。bash 是 GNU 计划中重要的工具软件之一，目前也是 Linux distributions 的标准 shell 。 bash 主要兼容于 sh ，并且依据一些使用者的需求，而加强的 shell 版本。不论你使用的是那个 distribution ，你都难逃需要学习 bash 的宿命。那么这个 shell 有什么好处，干嘛 Linux 要使用他作为预设的 shell 呢？ bash 主要的优点有底下几个： （1）命令编修能力 (history)：就是记忆使用过的指令，只要在指令列按【上下键】就可以找到前/后一个输入的指令。而在很多 distribution 里头，默认的指令记忆功能可以多达 1000 个。也就是说，你曾经下达过的指令几乎都被记录下来了。 这么多的指令记录在哪里呢？在你的家目录内的 .bash_history。不过，需要留意的是，~/.bash_history 记录的是前一次登入以前所执行过的指令，而至于这一次登入所执行的指令都被暂存在内存中，当你成功的注销系统后，该指令记忆才会记录到 .bash_history 当中。 （2）命令与档案补全功能： ([tab] 按键的好处)常常在 bash 环境中使用 [tab] 是个很棒的习惯。因为至少可以让你 1)少打很多字； 2)确定输入的数据是正确的。使用 [tab] 按键的时机依据 [tab] 接在指令后或参数后而有所不同。如下所示： 1. [Tab] 接在一串指令的第一个字的后面，则为命令补全； 2. [Tab] 接在一串指令的第二个字以后时，则为【档案补齐】。 所以说，如果我想要知道我的环境中，所有可以执行的指令有几个？就直接在 bash 的提示字符后面连续按两次 [tab] 按键就能够显示所有的可执行指令了。 那如果想要知道系统当中所有以 c 为开头的指令呢？就按下【c[tab][tab]】就好了。 （3）命令别名设定功能： (alias)假如我需要知道这个目录底下的所有档案 (包含隐藏档) 及所有的文件属性，那么我就必须要下达【ls -al】这样的指令串。如果使用命令别名，就可以自定义命令来取代长指令串命令，也就是说使用 alias 即可。你可以在指令列输入 alias 就可以知道目前的命令别名有哪些了。也可以直接下达命令来设定别名： alias lm=&#39;ls -al&#39; （4）工作控制、前景背景控制： (job control, foreground, background)使用前、背景的控制可以让工作进行的更为顺利。至于工作控制(jobs)的用途则更广，可以让我们随时将工作丢到背景中执行。而不怕不小心使用了[Ctrl] + c 来停掉该程序。此外，也可以在单一登录的环境中，达到多任务的目的。 （5）程序化脚本： (shell scripts)在 Linux 底下的 shell scripts 可以将你平时管理系统常需要下达的连续指令写成一个档案，该档案并且可以透过对谈交互式的方式来进行主机的侦测工作。也可以藉由 shell 提供的环境变量及相关指令来进行设计，整个设计下来几乎就是一个小型的程序语言。（该部分会单独详解） （6）通配符： (Wildcard)除了完整的字符串之外，bash 还支持很多的通配符来帮助用户查询与指令下达。举例来说，想要知道/usr/bin 底下有多少以 X 为开头的档案，使用：【ls -l /usr/bin/X*】就能够知道。 5、Bash shell 的内建命令： type如何知道这个指令是来自于外部指令(指的是其他非 bash 所提供的指令) 或是内建在 bash 当中的，利用 type 这个指令来观察即可。123456789语法：`type [-tpa] name`选项与参数： ：不加任何选项与参数时，type 会显示出 name 是外部指令还是 bash 内建指令 -t ：当加入 -t 参数时，type 会将 name 以底下这些字眼显示出他的意义： file ：表示为外部指令； alias ：表示该指令为命令别名所设定的名称； builtin ：表示该指令为 bash 内建的指令功能； -p ：如果后面接的 name 为外部指令时，才会显示完整文件名； -a ：会由 PATH 变量定义的路径中，将所有含 name 的指令都列出来，包含alias]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜基础篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>Shell</tag>
        <tag>BASH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim程序编辑器]]></title>
    <url>%2F2017%2F12%2F27%2Fvim%E7%A8%8B%E5%BA%8F%E7%BC%96%E8%BE%91%E5%99%A8%2F</url>
    <content type="text"><![CDATA[vi：文本编辑器vim：程序编辑器 为什么要学vi？因为所有的UNIX Like系统都会内建vi文本编辑器，其它的文本编辑器则不一定会存在。 vi与vim的区别？其实你可以将 vim 规作 vi 的进阶版本，vim 可以用颜色或底线等方式来显示一些特殊的信息。举例来说，当你使用 vim 去编辑一个 C 程序语言的档案，或者是我们后续会谈到的shell script 程序时，vim 会依据档案的扩展名或者是档案内的开头信息，判断该档案的内容而自动的呼叫该程序的语法判断式，再以颜色来显示程序代码与一般信息。也就是说，这个 vim 是个【程序编辑器】。甚至一些 Linux 基础配置文件内的语法，都能够用 vim 来检查。首先，先简单的对vi做个介绍，然后再说一下vim的额外功能与用法。 vi的使用：基本上 vi 共分为三种模式，分别是【一般模式】、【编辑模式】与【指令列命令模式】。 这三种模式的作用分别是： 一般模式：以 vi 打开一个档案就直接进入一般模式了(这是默认的模式)。在这个模式中，你可以使用【上下左右】按键来移动光标，你可以使用【删除字符】或【删除整行】来处理档案内容，也可以使用【复制、粘贴】来处理你的文件数据。 编辑模式：在一般模式中可以进行删除、复制、粘贴等等的动作，但是却无法编辑文件内容。要等到你按下【i, I, o, O, a, A, r, R】等任何一个字母之后才会进入编辑模式。注意了！通常在 Linux 中，按下这些按键时，在画面的左下方会出现【INSERT 或 REPLACE】的字样，此时才可以进行编辑。而如果要回到一般模式时，则必须要按下【Esc】这个按键即可退出编辑模式。 指令列命令模式：在一般模式当中，输入【: / ?】三个中的任何一个按钮，就可以将光标移动到最底下那一行。在这个模式当中，可以提供你【搜寻资料】的动作，而读取、存盘、大量取代字符、离开 vi 、显示行号等等的动作则是在此模式中达成的。 简单的说，我们可以将这三个模式想成底下的图标来表示：注意到上面的图标，你会发现一般模式可与编辑模式及指令列模式切换，但编辑模式与指令列模式之间不可互相切换。 按键说明：第一部份：一般模式可用的按钮说明，光标移动、复制粘贴、搜寻取代等12345678910111213替换语法总结：基础语法：:[range]substitute/old/new/[flags]（注：substitute 常用缩写形式“:s”。）range: . : 当前行 n : 对应行数 % : 所有行 $ : 最后一行flags: 空: 将光标所在行第一个 old 替换为 new g : 将光标所在行所有 old 替换为 new c : 表示进行确认 i : 不区分大小写 第二部份：一般模式切换到编辑模式的可用的按钮说明 第三部份：一般模式切换到指令列模式的可用的按钮说明 vim的暂存档、救援恢复与开启时的警告讯息什么是暂存档？当我们在使用 vim 编辑时， vim 会在不被编辑的档案的目录下，再建立一个名为 .filename.swp 的档案。当然，前提是由于各种原因导致的不正常的中断，才会产生该暂存档文件。如果对文件没有修改或正确存储则不会生成该暂存档文件。该暂存档文件的作用就是保留文件修改内容，防止文件因为系统或网络原因等导致丢失。 警告讯息团队工作中，有可能会有多人同时操作同一个文件的情况，这种情况就会产生警告讯息。至于这个发现暂存盘警告讯息的画面中，有出现六个可用按钮，各按钮的说明如下： [O]pen Read-Only：打开此档案成为只读档，可以用在你只是想要查阅该档案内容并不想要进行编辑行为时。一般来说，在上课时，如果你是登入到同学的计算机去看他的配置文件，结果发现其实同学他自己也在编辑时，可以使用这个模式； (E)dit anyway：还是用正常的方式打开你要编辑的那个档案，并不会载入暂存盘的内容。不过很容易出现两个使用者互相改变对方的档案等问题！不好不好！ (R)ecover：就是加载暂存盘的内容，用在你要救回之前未储存的工作。不过当你救回来并且储存离开 vim 后，还是要手动自行删除那个暂存档。 (D)elete it：你确定那个暂存档是无用的。那么开启档案前会先将这个暂存盘删除！这个动作其实是比较常做的！因为你可能不确定这个暂存档是怎么来的，所以就删除掉他吧！ (Q)uit：按下 q 就离开 vim ，不会进行任何动作回到命令提示字符。 (A)bort：忽略这个编辑行为，感觉上与 quit 非常类似，也会送你回到命令提示字符。 vim的额外功能：查看vi是否被vim替代：alias alias vi=&#39;vim&#39; &lt;==重点在这行这表示当你使用 vi 这个指令时，其实就是执行 vim。如果你没有这一行，那么你就必须要使用 vimfilename 来启动 vim 。基本上， vim 的一般用法与 vi 完全一模一样。 1、区块选择(Visual Block)刚刚我们提到的简单的 vi 操作过程中，几乎提到的都是以行为单位的操作。那么如果我想要搞定的是一个区块范围呢？那就使用区块选择 (Visual Block) 吧！当我们按下 v 或者 V 或者 [Ctrl]+v 时，这个时候光标移动过的地方就会开始反白，这三个按键的意义分别是：透过上述的功能，你可以复制一个区块，并且是贴在某个【区块的范围】内，而不是以行为单位来处理你的整份文件。 2、多档案编辑如果我想要将 A 档案内的十条消息『移动』到 B 档案去，通常要开两个 vim 窗口来复制，偏偏每个 vim 都是独立的，因此并没有办法在 A 档案下达【nyy】再跑到 B 档案去【p】。 在这种情况下最常用的方法就是透过鼠标圈选， 复制后粘贴。不过这样一来还是有问题，因为用 [Tab] 按键进行编排对齐动作，透过鼠标却会将 [Tab] 转成空格键，这样内容就不一样了。此时这个多档案编辑就派上用场了！ 我们可以使用 vim 后面同时接好几个档案来同时开启。相关的按键有： 示例：将hosts 内的前四行 IP 资料复制到你的/etc/hosts 档案内，那可以怎么进行呢？可以这样： 1. 透过【vim hosts /etc/hosts】指令来使用一个 vim 开启两个档案； 2. 在 vim 中先使用【:files】察看一下编辑的档案数据有啥？结果如下所示。至于下图的最后一行显示的是【按下任意键】就会回到 vim 的一般模式中； 3. 在第一行输入【4yy】复制前四行； 4. 在 vim 的环境下输入【:n】会来到第二个编辑的档案，亦即 /etc/hosts 内； 5. 在 /etc/hosts 下按【G】到最后一行，再输入【p】粘贴； 6. 按下多次的【u】来还原原本的档案数据； 7. 最终按下【:q】来离开 vim 的多档案编辑。 3、多窗口功能在开始这个小节前，先来想象两个情况： 当我有一个档案非常的大，我查阅到后面的数据时，想要【对照】前面的数据，是否需要使用[ctrl]+f 与 [ctrl]+b (或 pageup, pagedown 功能键) 来跑前跑后查阅？ 我有两个需要对照着看的档案，不想使用前一小节提到的多档案编辑功能； 在一般窗口接口下的编辑软件大多有【分割窗口】或者是【冻结窗口】的功能来将一个档案分割成多个窗口的展现，那么 vim 能不能达到这个功能？可以啊。在指令列模式输入【:sp {filename}】即可！那个 filename 可有可无，如果想要在新窗口启动另一个档案，就加入档名，否则仅输入 :sp 时，出现的则是同一个档案在两个窗口。下面是一些常用命令的说明：12345:sp [filename] &lt;=&gt; :split [filename] #横向切分窗口:vsp [filename] &lt;=&gt; :vsplit [filename] #竖向切分窗口:clo &lt;=&gt; :close #关闭当前窗口:on &lt;=&gt; :only #仅保留当前窗口，其它窗口都关闭:ctrl + w + [h/j/k/l] #切换窗口 这样的话，复制啊、查阅啊等等的，就变的很简单。分割窗口的相关指令功能有很多，不过只要记得这几个就好了： 4、vim 环境设定与记录： ~/.vimrc, ~/.viminfo有没有发现，如果我们以 vim 软件来搜寻一个档案内部的某个字符串时，这个字符串会被反白，而下次我们再次以 vim 编辑这个档案时，该搜寻的字符串反白情况还是存在呢！甚至于在编辑其他档案时，如果其他档案内也存在这个字符串，竟然还是主动反白。另外，当我们重复编辑同一个档案时，当第二次进入该档案时，光标竟然就在上次离开的那一行上头，真是好方便。但是，怎么会这样呢？ 这是因为我们的 vim 会主动的将你曾经做过的行为登录下来，好让你下次可以轻松的作业。那个记录动作的档案就是： ~/.viminfo ！如果你曾经使用过 vim，那你的家目录应该会存在这个档案才对。这个档案是自动产生的，你不必自行建立。而你在 vim 里头所做过的动作，就可以在这个档案内部查询到。 此外，每个 distributions 对 vim 的预设环境都不太相同，举例来说，某些版本在搜寻到关键词时并不会高亮度反白，有些版本则会主动的帮你进行缩排的行为。但这些其实都可以自行设定的，那就是vim 癿环境设定。vim 的环境设定参数有很多，如果你想要知道目前的设定值，可以在一般模式时输入【:set all】 来查阅，不过…..设定项目实在太多了。所以，在这里仅列出一些平时比较常用的一些简单的设定值，提供参考。 总之，这些设定值很有用处的。但是……我是否每次使用 vim 都要重新设定一次各个参数值？这不太合理吧。所以，我们可以透过配置文件来直接规定我们习惯的 vim 操作环境。整体 vim 的设定值一般是放置在 /etc/vimrc 这个档案，不过，不建议你修改该文件，你可以修改 ~/.vimrc 这个档案(预设不存在，可以自行手动建立！)，将你所希望的设定值写入！举例来说，可以是这样的一个档案：12345678910[root@www ~]# vim ~/.vimrc&quot;这个档案的双引号 (&quot;) 是批注set hlsearch &quot;高亮度反白set backspace=2 &quot;可随时用退格键删除set autoindent &quot;自动缩排set ruler &quot;可显示最后一行的状态set showmode &quot;左下角那一行的状态set nu &quot;可以在每一行的最前面显示行号set bg=dark &quot;显示不同的底色色调syntax on &quot;进行语法检验，颜色显示。 在这个档案中，使用【set hlsearch】或【:set hlsearch】，亦即最前面有没有冒号【:】效果都是一样的！至于双引号则是批注符号，不要用错批注符号，否则每次使用 vim 时都会发生警告讯息。建立好这个档案后，当你下次重新以 vim 编辑某个档案时，该档案的预设环境设定就是上头写的。这样，是否很方便你的操作，多多利用 vim 的环境设定功能。]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜基础篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件系统的压缩详解]]></title>
    <url>%2F2017%2F12%2F27%2F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%8E%8B%E7%BC%A9%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[何为压缩？何为压缩比？为什么要压缩，压缩有什么好处？带着这几个问题我们来学习一下。 1、为什么要压缩：如果一个软件档案很多，或者文件档案太大，都会耗掉很多的磁盘空间，如果是网络传输也会耗掉很多带宽和时间，同时也不利于复制与携带。此时就需要【文件压缩】技术了。 2、压缩的好处：透过文件压缩的技术，最大的好处就是压缩过的档案容量变小了， 所以你的硬盘容量无形之中就可以容纳更多的资料。此外，在一些网络数据的传输中，也会由于数据量的降低，好让网络带宽可以用来作更多的工作。目前很多的 WWW 网站也是利用文件压缩的技术来进行数据的传送，好让网站带宽的可利用率上升。 3、压缩的原理：目前我们使用的计算机系统中都是使用所谓的 bytes（字节） 单位来计量的。不过，事实上，计算机最小的计量单位应该是 bits（比特） 才对。此外，我们也知道 1 byte = 8 bits 。但是如果今天我们只是记忆一个数字，亦即是 1 这个数字？他会如何记录？假设一个 byte 可以看成底下的模样： □□□□□□□□（由于 1 byte = 8 bits ，所以每个 byte 当中会有 8 个空格，而每个空格可以是 0,1） 由于我们记录数字是 1 ，考虑计算机所谓的二进制，如此一来， 1 会在最右边占据 1 个 bit ，而其他的 7 个 bits 将会自动的被填上 0 。你看看，其实在这样的例子中，那 7 个 bits 应该是【空的】才对。不过，为了要满足目前我们的操作系统数据的存取（按字存取或按字节存取两种），所以就会将该数据转为 byte 的型态来记录。而一些聪明的计算机工程师就利用一些复杂的计算方式，将这些没有使用到的空间【丢】出去，以让档案占用的空间变小。这就是压缩的技术。 另外一种压缩技术也很有趣，他是将重复的数据进行统计记录的。举例来说，如果你的数据为【111….】共有100 个1 时， 那么压缩技术会记录为【100 个1】而不是真的有100 个1 的位存在！这样也能够精简档案记录的容量。 简单的说，你可以将他想成，其实档案里面有相当多的【空间】存在，并不是完全填满的， 而【压缩】的技术就是将这些【空间】填满，以让整个档案占用的容量下降。不过，这些【压缩过的档案】并无法直接被我们的操作系统所使用，因此， 若要使用这些被压缩过的档案数据，则必项将他【还原】回来未压缩前的模样， 那就是所谓的【解压缩】。而至于压缩前与压缩后的档案所占用的磁盘空间大小，就可以被称为是【压缩比】。 4、压缩文件扩展名：在Linux中，文件的扩展名是没什么作用的，但是为什么不同的压缩文件有不同的扩展名呢？原因就是，因为 Linux 支持的压缩指令非常多，且不同的指令所用的压缩技术并不相同，当然彼此之间可能就无法互通压缩/解压缩文件。所以，当你下载到某个压缩文件时，自然就需要知道该档案是由哪种压缩指令所制作出来的，好用来对照着解压缩。也就是说，虽然 Linux 档案的属性基本上是与文件名没有绝对关系的，但是为了帮助我们人类小小的脑袋瓜子，所以适当的扩展名还是必要的。底下我们就列出几个常见的压缩文件扩展名吧： *.Z compress 程序压缩的档案； *.gz gzip 程序压缩的档案； *.bz2 bzip2 程序压缩的档案； *.tar tar 程序打包的数据，并没有压缩过； *.tar.gz tar 程序打包的档案，其中并且经过 gzip 的压缩 *.tar.bz2 tar 程序打包的档案，其中并且经过 bzip2 的压缩 5、常用压缩命令1. tar 命令：1234567891011121314151617181920212223242526272829303132语法：语法：tar [主选项+辅选项] 文件或者目录参 数：-c: 建立压缩档案-x：解压-t：查看内容-r：向压缩归档文件末尾追加文件 (注：压缩后的档案再次追加文件会导致失败)-u：更新原压缩包中的文件（注：压缩后的档案再次更新会导致失败）这五个是独立的命令，压缩解压都要用到其中一个，可以和别的命令连用但只能用其中一个。下面的参数是根据需要在压缩或解压档案时可选的。-z：有gzip属性的-j：有bz2属性的-Z：有compress属性的-v：显示所有过程-O：将文件解开到标准输出-f：使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。（必须）-C：解压到指定目录特殊使用：① # tar -rf all.tar *.gif这条命令是将所有.gif的文件增加到all.tar的包里面去。-r是表示增加文件的意思。② # tar -uf all.tar logo.gif这条命令是更新原来tar包all.tar中logo.gif文件，-u是表示更新文件的意思。③ # tar -tf all.tar这条命令是列出all.tar包中所有文件，-t是列出文件的意思压缩：① tar -cvf jpg.tar *.jpg //仅打包，不压缩！② tar -zcvf jpg.tar.gz *.jpg //打包后，以 gzip 压缩③ tar -jcvf jpg.tar.bz2 *.jpg //打包后，以 bzip2 压缩解压：① tar -xvf file.tar //解压 tar包② tar -zxvf file.tar.gz //解压tar.gz③ tar -jxvf file.tar.bz2 //解压 tar.bz2④ tar -xvf archive.tar -C /tmp 将压缩包释放到 /tmp目录下 2. zip 命令：12345678910111213141516171819202122232425262728293031323334语 法：zip [-AcdDfFghjJKlLmoqrSTuvVwXyz$][-b &lt;工 作目录&gt;][-ll][-n &lt;字 尾字符串&gt;]\[-t &lt;日 期时间&gt;][-&lt;压 缩效率&gt;][压 缩文件][文件...][-i &lt;范本样式&gt;][-x &lt;范本样式&gt;]参 数：-r 递归处理，将指定目录下的所有文件和子目录一并处理。-d 从压缩文件内删除指定的文件。-D 压缩文件内不建立目录名称。-m 将文件压缩并加入压缩文件后，删除原始文件，即把文件移到压缩文件中。-x&lt;范本样式&gt; 压缩时排除符合条件的文件。-q 不显示指令执行过程。压缩文件：① 将当前目录下的所有文件和文件夹全部压缩成test.zip文件,-r表示递归压缩子目录下所有文件# zip -r test.zip ./*② 删除压缩文件test1.zip中test.MYI文件# zip -d test1.zip test.MYI③ 向压缩文件中test1.zip中添加test. MYI文件# zip -m test1.zip test. MYI④ 压缩文件时排除某个文件# zip test3.zip tests/* -x tests/ln.logunzip 命令：语 法：unzip [-cflptuvz][-agCjLMnoqsVX][-P &lt;密 码&gt;][.zip文 件][文件][-d &lt;目录&gt;][-x &lt;文件&gt;] 或 unzip [-Z]-v 执 行是时显示详细的信息。或查看压缩文件目录，但不解压。-n 解 压缩时不要覆盖原有的文件。-o 不 必先询问用户，unzip执 行后覆盖原有文件。-q 执 行时不显示任何信息。-d&lt;目录&gt; 指 定文件解压缩后所要存储的目录。解压文件：① 将压缩文件text.zip在当前目录下解压缩。# unzip test.zip ② 将压缩文件text.zip在指定目录/tmp下解压缩，如果已有相同的文件存在，要求unzip命令不覆盖原先的文件。# unzip -n test.zip -d /tmp③ 查看压缩文件目录，但不解压。# unzip -v test.zip④ 将压缩文件test.zip在指定目录tmp下解压缩，如果已有相同的文件存在，要求unzip命令覆盖原先的文件。# unzip -o test.zip -d tmp/]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜基础篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>计算机系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令之shutdown]]></title>
    <url>%2F2017%2F12%2F26%2FLinux%E5%91%BD%E4%BB%A4%E4%B9%8Bshutdown%2F</url>
    <content type="text"><![CDATA[正常情况下，windows在令你不爽的时候，按着电源开关4秒就关机了，或者简单粗暴的方法，直接拔电源。但是在Linux下非常不建议这么做。在 Linux 底下，由于每个程序 (或者说是服务) 都是在背景下执行的，因此，在你看不到的屏幕背后其实可能有相当多人同时在你的主机上面工作，若不正常关机，则可能造成文件系统的毁损 （因为来不及将数据回写到档案中，所以有些服务的档案会有问题！）。惯用的关机指令：shutdown shutdown 可以达成如下的工作： 可以自由选择关机模式：是要关机、重新启动或进入单人操作模式均可； 可以设定关机时间: 可以设定成现在立刻关机, 也可以设定某一个特定的时间才关机。 可以自定义关机讯息：在关机之前，可以将自己设定的讯息传送给在线 user 。 可以仅发出警告讯息：有时有可能你要进行一些测试，而不想让其他的使用者干扰， 或者是明白的告诉使用者某段时间要注意一下！这个时候可以使用 shutdown 来吓一吓使用者，但不是真的要关机！ 可以选择是否要 fsck 检查文件系统 。 简单的语法规则为：1234567891011[root@www ~]# /sbin/shutdown [-t 秒] [-arkhncfF] 时间 [警告讯息]选项不参数：-t sec ： -t 后面加秒数，亦即【过几秒后关机】的意思-k ： 不要真的关机，只是发送警告讯息出去！-r ： 在将系统的服务停掉之后就重新启动 (常用)-h ： 将系统的服务停掉后，立即关机。 (常用)-n ： 不经过 init 程序，直接以 shutdown 的功能来关机-f ： 关机并开机之后，强制略过 fsck 的磁盘检查-F ： 系统重新启动之后，强制进行 fsck 的磁盘检查-c ： 取消已经在进行的 shutdown 指令内容。时间 ： 这是一定要加入的参数！指定系统关机的时间！时间的范例底下会说明。 示例：[root@www ~]# /sbin/shutdown -h 10 &#39;I will shutdown after 10 mins&#39;告诉大家，这部机器会在十分钟后关机！并且“警告讯息”会显示在目前登入者的屏幕前方！[root@www ~]# shutdown -h now立刻关机，其中 now 相当于时间为 0 的状态[root@www ~]# shutdown -h 20:25系统在今天的 20:25 分会关机，若在21:25 才下达此指令，则明天才关机[root@www ~]# shutdown -h +10系统再过十分钟后自动关机[root@www ~]# shutdown -r now系统立刻重新启动[root@www ~]# shutdown -r +30 &#39;The system will reboot&#39;再过三十分钟系统会重新启动，并显示后面的讯息给所有在在线的使用者[root@www ~]# shutdown -k now &#39;This system will reboot&#39;仅发出警告信件的参数！系统并不会关机！吓唬人！ 重新启动命令：reboot, halt, poweroff]]></content>
      <categories>
        <category>每日一个Linux命令</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件系统的概述]]></title>
    <url>%2F2017%2F12%2F26%2F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[由此文Get到的知识点：inode 是什么？ 记录档案的号码。inode 有什么作用？ 记录档案的属性，一个档案占用一个inode，同时记录此档案的数据所在的block号码（通过inode可查找block的位置）。通过ls -i指令可查询到档案对应的inode号码。block 是什么？ 存储档案内容的内存块，且每块block都对应一个号码。支持的block大小有1k、2k和4k。block 有什么作用？ 存储档案内容，且如果档案太大，会占用多个block。 一、文件系统特性我们都知道磁盘分区完毕后还需要进行格式化(format)，之后操作系统才能够使用这个分割槽。为什么需要进行【格式化】呢？这是因为每种操作系统所设定的文件属性/权限并不相同， 为了存放这些档案所需的数据，因此就需要将分割槽进行格式化，以成为操作系统能够利用的【文件系统格式(filesystem)】。 传统的磁盘与文件系统在应用中，一个分割槽就是只能够被格式化成为一个文件系统，所以我们可以说一个 filesystem 就是一个 partition。但是由于新技术的利用，例如我们常听到的LVM 与软件磁盘阵列(software raid)， 这些技术可以将一个分割槽格式化为多个文件系统(例如LVM)，也能够将多个分割槽合成一个文件系统(LVM, RAID)！ 所以说，目前我们在格式化时已经不再说成针对 partition 来格式化了， 通常我们可以称呼一个可被挂载的数据为一个文件系统而不是一个分割槽。 由于操作系统的档案数据含有非常多的属性，例如 Linux 操作系统的档案权限(rwx)与文件属性(拥有者、群组、时间参数等)。文件系统通常会将这两部分的数据分别存放在不同的区块，权限与属性放置到 inode中，至于实际数据则放置到data block 区块中。 另外，还有一个超级区块 (superblock) 会记录整个文件系统的整体信息，包括 inode 与 block 的总量、使用量、剩余量等。 每个 inode 与 block 都有编号，至于这三个数据的意义可以简略说明如下： ① superblock：记录此 filesystem 的整体信息，包括inode/block 的总量、使用量、剩余量， 以及文件系统的格式与相关信息等；② inode：记录档案的属性，一个档案占用一个inode，同时记录此档案的数据所在的 block 号码；③ block：实际记录档案的内容，若档案太大时，会占用多个 block 。 由于每个 inode 与 block 都有编号，而每个档案都会占用一个 inode ，inode 内则有档案数据放置的block 号码。 因此，我们可以知道的是，如果能够找到档案的 inode 的话，那么自然就会知道这个档案所放置数据的 block 号码， 当然也就能够读出该档案的实际数据了。 我们将 inode 与 block 区块用图解来说明一下，如下图所示，文件系统先格式化出 inode 与 block 的区块，假设某一个档案的属性与权限数据是放置到 inode 4 号(下图较小方格内)，而这个 inode 记录了档案数据的实际放置点为 2, 7, 13, 15 这四个 block 号码，此时我们的操作系统就能够据此来排列磁盘的阅读顺序，可以一口气将四个 block 内容读出来！ 那么数据的读取就如同下图中的箭头所指定的模样了。 这种数据存取的方法我们称为索引式文件系统(indexed allocation)。 还有其他的惯用文件系统，例如随身碟(闪存)，随身碟使用的文件系统一般为 FAT 格式。FAT这种格式的文件系统并没有 inode 存在，所以 FAT 没有办法将这个档案的所有 block 在一开始就读取出来。每个 block 号码都记录在前一个 block 当中， 他的读取方式有点像底下这样： 上图中我们假设档案的数据依序写入1-&gt;7-&gt;4-&gt;15 号这四个 block 号码中， 但这个文件系统没有办法一口气就知道四个 block 的号码，他得要一个一个的将 block 读出后，才会知道下一个 block 在何处。 如果同一个档案数据写入的 block 分散的太过厉害时，则我们的磁盘读取头将无法在磁盘转一圈就读到所有的数据， 因此磁盘就会多转好几圈才能完整的读取到这个档案的内容！ 常常会听到所谓的【碎片整理】吧？ 需要碎片整理的原因就是档案写入的 block 太过于离散了，此时档案读取的效能将会变的很差。 这个时候可以透过碎片整理将同一个档案所属的 blocks 汇整在一起，这样数据的读取会比较容易。 二、EXT2文件系统：如上所知，inode 的内容在记录档案的权限与相关属性，至于 block 区块则是在记录档案的实际内容。 而且文件系统一开始就将 inode 与 block 规划好了，除非重新格式化(或者利用resize2fs 等指令变更文件系统大小)，否则 inode 与 block 固定后就不再变动。但是如果仔细考虑一下，如果我的文件系统高达数百GB 时，那么将所有的 inode 与 block 通通放置在一起将是很不智的决定，因为 inode 与 block 的数量太庞大，不容易管理。 为此之故，因此 Ext2 文件系统在格式化的时候基本上是区分为多个区块群组 (block group) 的，每个区块群组都有独立的 inode/block/superblock 系统。感觉上就好像我们在当兵时，一个营里面有分成数个连，每个连有自己的联络系统， 但最终都向营部回报连上最正确的信息一般！这样分成一群群的比较好管理！整个来说，Ext2 格式化后有点像底下这样： 在整体的规划当中，文件系统最前面有一个启动扇区(boot sector)，这个启动扇区可以安装开机管理程序， 这是个非常重要的设计，因为如此一来我们就能够将不同的开机管理程序安装到个别的文件系统最前端，而不用覆盖整颗硬盘唯一的 MBR（主要开机区，Master boot record, MBR）， 这样也才能够制作出多重引寻的环境！至于每一个区块群组(block group)的六个主要内容说明如下： （1）data block (资料区块)data block 是用来放置档案内容数据地方，在 Ext2 文件系统中所支持的 block 大小有 1K, 2K 及 4K三种而已。在格式化时 block 的大小就固定了，且每个 block 都有编号，以方便 inode 的记录。 不过要注意的是，由于 block 大小的差异，会寻致该文件系统能够支持的最大磁盘容量与最大单一档案容量并不相同。 因为 block 大小而产生的 Ext2 文件系统限制如下： Block 大小 1KB 2KB 4KB 最大单一档案限制 16GB 256GB 2TB 最大文件系统总容量 2TB 8TB 16TB （注：在进行文件系统的格式化之前，要想好该文件系统预计使用的情况来选择 Block 大小。过大会产生较严重的磁盘容量浪费，较小则大型档案会占用数量更多的block，而inode也要记录更多的block号码，将可能导致文件系统不良的读写效能。） （2）inode table (inode 表格)如前所述 inode 的内容在记录档案的属性以及该档案实际数据是放置在哪几号 block 内。基本上，inode 记录的档案数据至少有以下这些： 该档案的存取模式(read/write/excute)； 该档案的拥有者与群组(owner/group)； 该档案的容量； 该档案建立或状态改变的时间(ctime)； 最近一次的读取时间(atime)； 最近修改的时间(mtime)； 定义档案特性的旗标(flag)，如 SetUID...； 该档案真正内容的指向 (pointer)； inode 的数量与大小也是在格式化时就已经固定了，除此之外 inode 还有些什么特色呢？ 每个 inode 大小均固定为 128 bytes； 每个档案都仅会占用一个 inode 而已； 承上，因此文件系统能够建立的档案数量与 inode 的数量有关； 系统读取档案时需要先找到 inode，并分析 inode 所记录的权限与用户是否符合，若符合才能够开始实际读取 block 的内容。 inode 要记录的数据非常多，但偏偏又叧有 128bytes 而已， 而 inode 记录一个 block 号码要花掉 4byte ，假设我一个档案有 400MB 且每个block 为 4K 时， 那么至少也要十万笔 block 号码的记录呢！inode 哪有这么多可记录的信息？为此我们的系统很聪明的将 inode 记录 block 号码的区域定义为12 个直接，一个间接, 一个双间接与一个三间接记录区。这是啥？我们将 inode 的结构画一下好了。 上图最左边为 inode 本身 (128 bytes)，里面有 12 个直接指向 block 号码的对照，这 12 笔记录就能够直接取得 block 号码。 至于所谓的间接就是再拿一个 block 来当作记录 block 号码的记录区，如果档案太大时， 就会使用间接的 block 来记录编号。如上图，当中间接只是拿一个 block 来记录额外的号码而已。 同理，如果档案持续长大，那么就会利用所谓的双间接，第一个 block 仅再指出下一个记录编号的 block 在哪里， 实际记录的在第二个 block 当中。依此类推，三间接就是利用第三层block 来记录编号。 这样子 inode 能够指定多少个 block 呢？我们以较小的 1K block 来说明好了，可以指定的情况如下： 12 个直接指向： 12*1K=12K 由于是直接指向，所以总共可记录 12 笔记录，因此总额大小为如上所示； 间接： 256*1K=256K 每笔 block 号码的记录会花去 4bytes，因此 1K 的大小能够记录 256 笔记录，因此一个间接可以记录的档案大小如上； 双间接： 256*256*1K=2562K 第一层 block 会指定 256 个第二层，每个第二层可以指定 256 个号码，因此总额大小如上； 三间接： 256*256*256*1K=2563K 第一层 block 会指定 256 个第二层，每个第二层可以指定 256 个第三层，每个第三层可以指定256 个号码，因此总额大小如上； 总额：将直接、间接、双间接、三间接加总，得到 12 + 256 + 256*256 + 256*256*256 (K) =16GB 此时我们知道当文件系统将 block 格式化为 1K 大小时，能够容纳的最大档案为 16GB，比较一下文件系统限制表的结果可发现是一致的！但这个方法不能用在 2K 及 4K block 大小的计算中， 因为大于2K 的 block 将会受到 Ext2 文件系统本身的限制，所以计算的结果会不太符合。 （3）Superblock (超级区块)Superblock 是记录整个 filesystem 相关信息的地方，没有 Superblock ，就没有这个 filesystem了。他记录的信息主要有： block 与 inode 的总量； 未使用与已使用的 inode / block 数量； block 与 inode 的大小 (block 为 1, 2, 4K，inode 为 128 bytes)； filesystem 的挂载时间、最近一次写入数据的时间、最近一次检验磁盘 (fsck) 的时间等文件系统的相关信息； 一个 valid bit 数值，若此文件系统已被挂载，则 valid bit 为 0 ，若未被挂载，则 valid bit 为1 。 Superblock 是非常重要的，因为我们这个文件系统的基本信息都写在这里，因此，如果 superblock死掉了， 你的文件系统可能就需要花费很多时间去挽救。一般来说， superblock 的大小为1024bytes。相关的 superblock 信息可以使用 dumpe2fs 指令来查看。此外，每个 block group 都可能含有 superblock 。但是我们也说一个文件系统应该仅有一个superblock 而已，那是怎么回事？ 事实上除了第一个 block group 内会含有 superblock 之外，后续的 block group 不一定含有 superblock ， 而若含有 superblock 则该 superblock 主要是做为第一个 block group 内 superblock 的备份，这样可以进行 superblock 的救援。 （4）Filesystem Description (文件系统描述说明)这个区段可以描述每个 block group 的开始与结束的 block 号码，以及说明每个区段 (superblock,bitmap, inodemap, data block) 分别介于哪一个 block 号码之间。这部分也能够用 dumpe2fs 来查看。 （5）block bitmap (区块对照表)如果你想要新增档案时会用到 block，那你要使用那个 block 来记录呢？当然是选择【空的block】来记录新档案的数据。 那你怎么知道那个 block 是空的？这就得要透过 block bitmap 的辅助。从 block bitmap 当中可以知道哪些 block 是空的，因此我们的系统就能够很快速的找到可使用的空间来处置新档案。同样的，如果你删除某些档案时，那么那些档案原本占用的 block 号码就得要释放出来， 此时在block bitmpap 当中相对应到该 block 号码的标志就得要修改为【未使用中】。这就是 bitmap的功能。 （6）inode bitmap (inode 对照表)这个其实与 block bitmap 是类似的功能，只是 block bitmap 记录的是使用与未使用的 block 号码，至于 inode bitmap 则是记录使用与未使用的 inode 号码。 每个区段与 superblock 的信息都可以使用 dumpe2fs 这个指令来查询的，查询的方法与实际的观察如下：dumpe2fs [-bh] 装置文件名选项与参数：-b ：列出保留为坏轨的部分(一般用不到！？)-h ：仅列出 superblock 的数据，不会列出其他的区段内容！]]></content>
      <categories>
        <category>书籍</category>
        <category>《鸟哥的Linux私房菜基础篇（第三版）》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>计算机系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令之man]]></title>
    <url>%2F2017%2F12%2F25%2FLinux%E5%91%BD%E4%BB%A4%E4%B9%8Bman%2F</url>
    <content type="text"><![CDATA[在文本模式下，你可以直接按下两个[Tab]按键，可以查看总共有多少指令可以使用。可以看到有将近2千个命令，那如何记忆呢？肯定不可能全部记下来，所以，软件的开发者为了让大家能够了解指令的用法， 都会自行制作很多的文件，而这些文件也可以直接在在线就能够轻易的被使用者查询出来。 1、man page简单记忆就是，不会该指令，就找男人（man）呀！（注：man就是manual的缩写。）示例：man date （结果自己操作，此处不贴图）首先，在上个表格的第一行，你可以看到的是：【DATE(1)】，DATE 我们知道是指令的名称， 那么(1)代表什么呢？他代表的是【一般用户可使用的指令】的意思。常见的几个数字的意义是这样的： 代号 代表内容 1 用户在shell 环境中可以操作的挃令戒可执行文件 2 系统核心可呼叫的凼数不工具等 3 一些常用的凼数(function)不凼式库(library)，大部分为C 的凼式库(libc) 4 装置档案的说明，通常在/dev 下的档案 5 配置文件戒者是某些档案的格式 6 游戏(games) 7 惯例不协议等，例如Linux 文件系统、网绚协议、ASCII code 等等的说明 8 系统管理员可用的管理挃令 9 跟kernel 有关的文件 上述的表格内容可以使用【man 7 man】来更详绅的取得说明。再来，man page 的内容也分成好几个部分来加以介绍该指令呢！就是上头man date 那个表格内，以NAME 作为开始介绍，最后还有个SEE ALSO 来作为结束。基本上，man page 大致分成底下这几个部分： 代号 内容说明 NAME 简短的指令、数据名称说明 SYNOPSIS 简短的指令下达诧法(syntax)简介 DESCRIPTION 较为完整的说明，这部分最好仔细看看！ OPTIONS 针对 SYNOPSIS 部分中，有列丼的所有可用的选项说明 COMMANDS 当这个程序(软件)在执行的时候，可以在此程序(软件)中下达的指令 FILES 这个程序或数据所使用或参考或连结到的某些档案 SEE ALSO 可以参考的，跟这个指令或数据有相关的其他说明！ EXAMPLE 一些可以参考的范例 BUGS 是否有相关的臭虫！ 有时候除了这些外，还可能会看到Authors与Copyright 等，不过也有很多时候仅有NAME与DESCRIPTION 等部分。注：退出按q（相对应的与man -f 等价的是 whatis ） 2、info page （自我感觉不好用，就不做说明了）]]></content>
      <categories>
        <category>每日一个Linux命令</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下文件与权限详解]]></title>
    <url>%2F2017%2F12%2F25%2FLinux%E4%B8%8B%E6%96%87%E4%BB%B6%E4%B8%8E%E6%9D%83%E9%99%90%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[首先，需要知道的是，“Linux下一切皆档案”。且Linux一般将档案可存取的身份分三个类别，分别是 owner/group/others，且三种身份各有 read/write/execute 等权限。（注：对应的档案存储文件为： 用户记录在 /etc/passwd 档案； 用户密码记录在 /etc/shadow 档案；组名记录在 /etc/group 档案。） 所有者（owner）：一般为文件的创建者，谁创建了该文件，就天然的成为该文件的所有者。（使用chown修改文件的所有者） 文件所在组（group）：当某个用户创建了一个文件后，这个文件的所在组就是该用户所在的组。（使用chgrp修改文件所在的组） 其它人（others）：除开文件的所有者和所在组的用户外，系统的其它用户都是文件的其它组 档案信息详解由命令 ls -al查看档案的所有信息：12-rw-r--r-- 1 root root 42304 Dec 25 17:26 install.log &lt;=范例说明处[权限] [链接数] [拥有者] [群组] [档案容量] [修改日期] [档名] 第一栏代表这个档案的类型与权限(permission)：第一个字符代表这个档案是『目录、档案或链接文件等等』： 当为[ d ]则是目彔，例如上表档名为『.gconf』的那一行； 当为[ - ]则是档案，例如上表档名为『install.log』那一行； 若是[ l ]则表示为链接档(link file)； 若是[ b ]则表示为装置文件里面的可供储存的接口设备(可随机存取装置)； 若是[ c ]则表示为装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置)。（其余字符没3个为一组，分别对应所有者、所属组、其他人。） 第二栏表示有多少档名链接到此节点(i-node，即是索引节点)：每个档案都会将他的权限与属性记录到文件系统的i-node 中，不过，我们使用的目录树却是使用文件名来记录， 因此每个档名就会连结到一个i-node！这个属性记录的，就是有多少不同的档名连结到相同的一个i-node 号码去就是了。 第三栏表示这个档案(或目录)的『拥有者账号』 第四栏表示这个档案的所属群组 第五栏为这个档案的容量大小，默认单位为bytes； 第六栏为这个档案的建档日期或者是最近的修改日期： 第七栏为这个档案的档名 改变档案属性与权限命令1、chgrp（change group）：改变档案所属群组语法：chgrp [-R] dirname/filename ...选项与参数：-R : 进行递归(recursive)的持续变更，亦即连同次目录下的所有档案、目录都更新成为这个群组之意。常常用在变更某一目录内所有的档案之情况。范例：[root@www ~]# chgrp users install.log2、chown（change owner）：改变档案拥有者语法：chown [-R] 账号名称[:组名] 档案或目录选项与参数：-R : 进行递归(recursive)的持续变更，亦即连同次目录下的所有档案都变更范例：将install.log 的拥有者改为bin 这个账号：[root@www ~]# chown bin install.log3、chmod ：改变档案的权限, SUID, SGID, SBIT 等等的特性语法：chmod [-R] xyz 档案或目录选项与参数：xyz : 就是刚刚提到的数字类型的权限属性，为 rwx 属性数值的相加。-R : 进行递归(recursive)的持续变更，亦即连同次目录下的所有档案都会变更 数字类型改变档案权限我们可以使用数字来代表各个权限，各权限的分数对照表如下：r:4w:2x:1[root@www ~]# chmod 777 .bashrc 符号类型改变档案权限基本上就九个权限分别是(1)user (2)group (3)others 三种身份。那么我们就可以藉由u, g, o 来代表三种身份的权限！此外，a 则代表 all 亦即全部的身份！那么读写的权限就可以写成r, w, x。假如我们要『限定』一个档案的权限成为『-rwxr-xr-x』时，基本上就是： user (u)：具有可读、可写、可执行的权限； group 不 others (g/o)：具有可读不执行的权限。[root@www ~]# chmod u=rwx,go=rx .bashrc注意：那个 u=rwx,go=rx 是连在一起的，中间并没有任何空格符！ 权限对于文件与目录的作用1、权限对文件的重要性：权限对于档案来说，他的意义是这样的： 1. r (read)：可读取此一档案的实际内容，如读取文本文件的文字内容等； 2. w (write)：可以编辑、新增或者是修改该档案的内容(但不能删除该档案)； 3. x (eXecute)：该档案具有可以被系统执行的权限。 Windows 底下一个档案是否具有执行的能力是藉由『 扩展名 』来判断的， 例如：.exe, .bat, .com 等等，但是在Linux 底下，我们的档案是否能被执行，则是藉由是否具有『x』这个权限来决定的！跟档名是没有绝对的关系的！ 2、权限对目录的重要性：目录主要的内容在记录文件名列表，文件名与目录有强烈的关连！所以如果是针对目录时，那个 r, w, x 对目录意义是这样的： 1. r (read contents in directory)： 表示具有读取目录结构列表的权限，如利用 ls 这个指令将该目录的内容列表显示出来； 2. w (modify contents of directory)： 表示你具有异动该目录结构列表的权限，也就是底下这些权限： * 建立新的档案与目录； * 删除已经存在的档案与目录(不论该档案的权限为何！) * 将已存在的档案或目录进行更名； * 搬移该目录内的档案、目录位置。 3. x (access directory)： 目录只是记录文件名而已，不可以被执行，目彔的x 代表的是用户能否进入该目录成为工作目录的用途。 Linux档案种类与扩展名1、档案种类：我们在刚刚提到使用『ls -l』观察到第一栏那十个字符中，第一个字符为档案的类型。 除了常见的一般档案(-)与；目录档案(d)之外，还有哪些种类的文件类型呢？（1）正规档案(regular file )：在由 ls -al 所显示出来的属性方面，第一个字符为 [-]，例如 [-rwxrwxrwx ]。另外，依照档案的内容，又大略可以分为： 1、纯文本档(ASCII)：这是Linux 系统中最多的一种文件类型， 称为纯文本档是因为内容为我们人类可以直接读到的数据，例如数字、字母等等。2、二进制文件(binary)：你的Linux当中的可执行文件(scripts, 文字型批处理文件不算)就是这种格式。举例来说，刚刚下达的指令cat 就是一个binary file。3、数据格式文件(data)：有些程序在运作的过程当中会读取某些特定格式的档案，那些特定格式的档案可以被称为数据文件 (data file)。 （2）目录(directory)：就是目录，第一个属性为 [ d ]，例如 [drwxrwxrwx]。（3）链接档(link)：就是类似Windows 系统底下的快捷方式，第一个属性为 l ，例如[lrwxrwxrwx] ；（4）设备与装置文件(device)：与不系统周边及储存等相关的一些档案，通常都集中在/dev 这个目录之下！通常又分为两种： 1、区块(block)设备档 ：就是一些储存数据，以提供系统随机存取的接口设备，举例来说，硬盘与软盘等。你可以自行查一下/dev/sda 看看， 会发现第一个属性为[ b ]。2、字符(character)设备文件：亦即是一些串行端口的接口设备，例如键盘、鼠标等等。第一个属性为 [ c ]。 （5）资料接口文件(sockets)：既然被称为数据接口文件，这种类型的档案通常被用在网络上的数据承接。第一个属性为 [ s ]， 最常在/var/run 这个目录中看到这种文件类型了。（6）数据输送文件(FIFO, pipe)：FIFO 也是一种特殊的文件类型，他主要的目的在解决多个程序同时存取一个档案所造成的错误问题。FIFO 是first-in-first-out 的缩写。第一个属性为[p] 。 2、Linux 档案扩展名：基本上，Linux 的档案是没有所谓的『扩展名』的，我们刚刚就谈过，一个Linux 档案能不能被执行，与他的第一栏的十个属性有关， 与文件名根本一点关系也没有。通常我们还是会以适当的扩展名来表示该档案是什么种类的。底下有数种常用的扩展名： 1、*.sh ： 脚本或批处理文件 (scripts)，因为批处理文件为使用shell 写成的，所以扩展名就编成 .sh；2、*Z, *.tar, *.tar.gz, *.zip, *.tgz： 经过打包的压缩文件。这是因为压缩软件分别为 gunzip, tar 等等的，由与不同的压缩软件，而取其相关的扩展名啰！3、*.html, *.php：网页相关档案，分别代表 HTML 语法与 PHP 语法的网页档案。 3、Linux 文件名的限制：由于Linux 在文字接口下的一些指令操作关系，一般来说，你在设定Linux 底下的文件名时， 最好可以避免一些特殊字符比较好！例如底下这些：* ? &gt; &lt; ; &amp; ! [ ] | \ ‘ “ ` ( ) { }因为这些符号在文字接口下，是有特殊意义的。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令之date、cal、bc]]></title>
    <url>%2F2017%2F12%2F22%2FLinux%E5%91%BD%E4%BB%A4%E4%B9%8Bdate%E3%80%81cal%E3%80%81bc%2F</url>
    <content type="text"><![CDATA[1、显示日期的指令：date如果在文字接口中想要知道目前Linux 系统的时间，那举就直接在指令列模式输入date 即可显示：12345678[bluce-ben@www ~]$ dateFri Dec 22 16:06:43 CST 2017上面显示的是：星期五, 12月22日, 16:06 分, 43 秒，在 2017 年的 CST 时区！那如果我想要让这个程序显示出『2017/12/22』这样的日期显示方式呢？ 那就使用date 的格式化输出功能吧！[bluce-ben@www ~]$ date +%Y/%m/%d2017/12/22[bluce-ben@www ~]$ date +%H:%M16:09 2、显示日历的指令：cal123456789[bluce-ben@www ~]$ cal December 2017Su Mo Tu We Th Fr Sa 1 23 4 5 6 7 8 910 11 12 13 14 15 1617 18 19 20 21 22 2324 25 26 27 28 29 3031 cal (calendar)这个指令可以做的事情还很多，例如你可以显示整年的月历情况：cal 2018基本上cal 这个指令可以接的语法为： cal [month] [year] 3、简单好用的计算器：bcLinux提供了一支计算程序，就是bc。你在指令列输入bc 后，屏幕会显示出版本信息， 之后就进入到等待指示的阶段。如下所示：123456[bluce-ben@www ~]$ bcbc 1.06Copyright 1991-1994, 1997, 1998, 2000 Free Software Foundation, Inc.This is free software with ABSOLUTELY NO WARRANTY.For details type `warranty&apos;._ &lt;==这个时候，光标会停留在这里等徃你的输入 常用运算符： + 加法 - 减法 * 乘法 / 除法 ^ 挃数 % 余数 补充一些小Tips： [Tab]：如果在command后按时，代表【命令补全】；如果在第二个字以后按，就变成【档案补齐】的功能了。 [Ctrl] + c：表示中断目前程序的按键。 [Ctrl] + d：表示【键盘输入结束】的意思，可以用来取代exit的输入。 在文本模式下，你可以直接按下两个[Tab]按键，可以查看总共有多少指令可以使用。]]></content>
      <categories>
        <category>每日一个Linux命令</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[道哥自述： 为什么弹性安全网络将诞生最大的人工智能？]]></title>
    <url>%2F2017%2F12%2F21%2F%E9%81%93%E5%93%A5%E8%87%AA%E8%BF%B0%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BC%B9%E6%80%A7%E5%AE%89%E5%85%A8%E7%BD%91%E7%BB%9C%E5%B0%86%E8%AF%9E%E7%94%9F%E6%9C%80%E5%A4%A7%E7%9A%84%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[阿里妹导读：前阵子，阿里科学家王刚、吴翰清同时入选MIT2017年度TR35 开创中国互联网企业先河一文刷爆了朋友圈，阿里巴巴人工智能实验室首席科学家王刚、阿里云首席安全科学家吴翰清同时入选MIT2017。这是自该奖项创立18年以来，第一次中国公司里同时有2人入选榜单。今天，阿里妹分享一篇来自吴翰清（也就是大家熟悉的道哥、小黑）的文章，让我们一起走进道哥的弹性安全网络世界。 前些天得知自己入选了MIT的TR35，非常开心。我想这是中国安全技术在国际上被认可的一次证明。但这个荣誉不仅属于我一个人，更属于我团队中所有为此做出过努力和贡献的人，也属于那些敢于和我们一起尝试最新技术的客户们，因为新技术在诞生之初往往是生涩的，但缺少了孵化过程中的磨难，我们永远见不到美丽绽放的那天。我也非常感谢王坚博士、弓峰敏博士、华先胜老师、Dawn Song教授能够成为我的TR35推荐人，感谢你们对我所从事的工作的认可。 自从参加工作以来，我一直执着于将中国技术推向全球，我认为中国有着最好的安全技术和最好的人，只是缺乏了让他们成长的土壤和展示的舞台。所以我也希望这次MIT对我个人的认可，能够成为一次鼓励中国安全产业的优秀人才和优秀技术成果走向世界的契机。长期以来，我们享受了很多开源技术的红利，但中国技术对世界互联网发展的贡献却非常微薄。我认为这中间有语言的障碍，有文化的障碍，但没有能力的障碍。现在是时候让我们去跨越这些障碍，去解决全球互联网发展过程中遇到的那些问题了。只有中国本土的优秀人才成长起来，中国才会变得更加强大。 回顾我十多年的工作生涯，期间从事和研究过非常多的技术工作，但我认为唯有「弹性安全网络」的研究是最独特的。「弹性安全网络」不是对现有技术的一种应用， 它是真正的发明了一项此前所没有的技术，提出了一种全新的方法，采用了一个全新的角度来看待现有世界。也因此它能跳出现有的技术框架，带来一些突破性的惊喜。这些惊喜，往往连创造者都没有办法在一开始就想清楚。正如从比特币中抽象出了区块链技术一样，最早我们构建的产品「游戏盾」是用来防御超大流量DDoS攻击，最后抽象出来的「弹性安全网络」技术，却让我们看到了构建下一代互联网的可能性。 简单来说，弹性安全网络是将DDoS防御前置到网络边缘处。但是，未来真正要做的事情是通过端到端的连接，通过风险控制技术，重新构建一个干净的、安全的互联网。 前些天《麻省理工学院技术评论》的记者对我做了一次采访，我完整的阐述了一次关于弹性安全网络的构想。我把这次采访的录音放在这里，分享给所有对这项技术感兴趣的人，并附上整理后的文字稿（但依然强烈推荐听录音原文）。未来我希望有更多人参与到对「弹性安全网络」的建设中来。 为什么要做弹性安全网络 互联网的流量就像流淌在管道里的水，但互联网发展到今天，流量里已经掺杂了太多的东西，变得不再纯粹和健康了。比如说，这些流量里面包含了很多攻击请求，也有很多恶意爬虫请求和一些欺诈行为的请求。 理想状况下，我们希望未来的流量是干净、健康的，希望把所有的网络攻击前置到整个网络的边缘处。就是说进入这张网络的时候，流量本身就是干净的。这就是clear traffic的概念。 为了实现这个想法，我们遇到了很多的困难。我们在思考，需要用一个什么样的架构去实现它。刚巧这个时候，我们有一些客户尝试用快速切换的思路来对抗DDoS攻击。这给了我灵感。最终，我把两个东西结合起来，产生了做弹性安全网络的想法。 什么是弹性安全网络 弹性安全网络真正想要去做的，是替换掉整个互联网最核心的心脏，替换掉DNS，从而让网络变得有弹性，能够快速调度资源，形成一个全新的网络架构。 事实上，DNS诞生在互联网早期，是互联网1.0时代的产物，是一个开放的协议。到今天，也没有一个独立的运营商来运营整个互联网的DNS Server。它分散在各家不同的运营商。全球可能有上百家运营商，都在提供自己的DNS服务。运营商跟运营商之间的打通，是通过标准的DNS协议进行数据交换。 这也是为什么这么多年DNS协议都没办法进步的原因，过于碎片化。 目前，DNS有三个显著问题。第一个，是DNS完全解析的时间过长，这是整个DNS使用中遇到的一个非常大的痛点。 比如，对于一个大型网站，要把用户的所有流量指向一个新地址。把DNS的解析修改之后，可能需要花两到三天时间，流量才会百分之百的切到新地址去，不会在旧地址上还有残余流量。 为什么需要两到三天时间？原因是有很多运营商的DNS递归解析服务器，都需要更新自己的数据。而有的运营商还有自己的省级运营商，甚至更下面的地市级的DNS的递归解析。过于碎片化，使得难于进行统一的数据管理，这是今天现实存在的问题。 第二个问题是今天DNS Server软件中的解析数遇到了瓶颈，没有办法一个名字解析到几千个、甚至上万个，甚至未来十几万个不同地址。一个名字可能最多也就解析到十几个或几十个地址就不能再扩大了。这种瓶颈限制了我们的一些能力拓展。 第三个就是，原本可以基于DNS去实现的一些安全机制，比如风险控制，并没有建立起来。其实也比较好理解，在互联网1.0时代并没有如今天这般强大的数据能力和计算能力。 今天，我们要解决这些问题。在整个弹性安全网络的架构下面，我们在构思下一代的互联网应该是什么形态？答案就是通过可靠的快速调度技术把互联网心脏重构掉。 首先，就是它的快速解析的能力，一定要非常实时以及干净。其次，就是它本身支持的调度能力，要能达到上万的这个级别，规模特别的重要，就是一个名字能够解析到上万个地址、甚至是十几万个地址。 我们以防御DDoS攻击为切入点，进行尝试。过去防御DDoS攻击时，必须要做的是储备单点大带宽。因为IP是变不了的（在中国的网络环境下由于政策原因暂不考虑anycast的方案）。所以在DNS架构下，就是去硬抗这个IP遇到的流量攻击。比如说300G的流量打过来，必须要有300G的带宽在这里，才能够扛得住。如果只有100G的带宽，那整个机房就被堵死了，甚至可能会影响到运营商的网络稳定。 这是在过去攻防对抗的思路，就是你攻击打过来多少，我就必须要有多少带宽储备在这儿。这比的是资源，比的是单纯的带宽储备。 我们现在的思路是，你攻击这个IP，我马上就把这个IP拿掉，不要这个IP了，然后启用一个新的地址，并告诉所有客户，你来访问新地址。 当然，这时候攻击者会跟随，但是攻击者跟随是有成本的。一般，攻击者跟随到一个新地址，需要大概10多分钟。 在这个10分钟里，通过数据分析的方式，我们可以分析出攻击者到底是谁，把好人和坏人分离出来，阻止坏人的流量，并同时放干净的流量继续访问，这就是整个弹性安全网络的核心思想。 如何实现弹性安全网络 弹性安全网络的实现，是通过快速完成上万个地址的调度，从根本上改变过去需要在单点储备大带宽的一种防御方式能力。 就是，你不需要在单点储备大带宽了，你需要的更多的地址，更强的数据分析能力。 要知道，单点储备大带宽的价格非常贵。改用这种方式之后，DDoS防御成本可以下降两到三个数量级，因为不需要再单点储备大带宽。 做完这个之后，我们就发现，其实这个事情，最重要的不是多了一种对抗DDoS攻击的方法，而是改变了DNS本身，这是本质的东西。所以，我们是用一种新技术去解决了一个老问题。 弹性安全网络将诞生最大的人工智能 沿着弹性安全网络的思路，我们希望通过风险控制来管理整个互联网的资源。 未来，弹性安全网络将重新定义互联网的入口。通过为每一个访问者建立“足迹库”，分析他是好人还是坏人的概率。一旦判断这次访问请求可能是有风险的，则可以随即让他访问不到这个资源。 所以，未来最大的人工智能应该是诞生在弹性安全网络，因为整个互联网的资源都被管理起来了，而且是基于每一个访问者的行为沉淀，来判断风险。 相当于想要进入这个封闭的网络，每个访客要先过安检。只有通过安检才能访问到这个资源。而且，访客所有的历史行为会被积累下来，为未来的风险判断做储备。而今天互联网的心脏– DNS，由于其开放性和碎片性，已经失去了将所有访问数据统一汇聚后进行分析的可能性。 在一个自成闭环的体系里面，由一家基础设施的提供商，去运营整个网络心脏的这种解析服务。然后也基于这种解析服务，它能够对整个网内的所有访客进行智能分析，最终就能够实现这张网内的所有访客的请求，都是在风险控制之下的，从而构建一个全新的互联网。 弹性安全网络的未来 今天，一些阿里云上的游戏客户，就是通过弹性安全网络的技术，来调度他们所有的游戏资源，同时对所有玩家进行风险控制的。 弹性安全网络自成闭环。也就是说，这些使用弹性安全网络的游戏，已经从我们现在的互联网，也就是今天以DNS为支撑的这个互联网里，消失掉了。 一个玩家，通过DNS，是访问不到弹性安全网络这张网里的所有资源的。未来我们要做的事情就是，不断地去扩大这张网，直到网内可调度的资源覆盖整个互联网的资源。 目前来看，主要机会就是在IoT和移动互联网，因为这两者实际上是没有DNS的需求的。过去，之所以需要DNS，是因为有一个浏览器，浏览器里面有一个地址栏，这个东西必须通过输入一个好记的地址，才能访问到资源。 在移动互联网时代，今天手机不需要浏览器，而是直接打开一个App。那这个App访问的是什么东西，它不一定需要DNS来解析。 这是我们看到今天这个技术有可能走下去的一个非常重要的原因。 延伸出来，在IoT时代，也是不需要有一个浏览器去访问你所需要访问的服务和资源的。 所以这是我看到，这张网在未来有可能升级今天整个互联网最重要的一个原因。 阿里将开放弹性安全网络技术能力 未来，阿里会开放弹性安全网络的技术。 类似DNS，弹性安全网络本身也不涉及任何访问资源，它只是知道你今天到这个地方来了。就像，一个人今天到某个国家去，需要入关和出关，是一个道理。 事实上，在很多关键领域，弹性安全网络非常有价值。 比如，各个国家政府，或者大型企事业单位的专网或内网。如果它是以DNS为核心的话，那这是一个暴露在整张网内的弱点。因为DNS是一个公开的服务。一旦DNS这个单点被瘫痪掉，整张网可能就没法工作了，所以这是非常大的风险。 所以，弹性安全网络技术，不是为某一个客户设计的，它是为整个互联网设计的。]]></content>
      <categories>
        <category>书籍</category>
        <category>《2017阿里技术》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>阿里技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里巴巴CTO张建锋： 下一波创新机会，重点关注这三个领域]]></title>
    <url>%2F2017%2F12%2F21%2F%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4CTO%E5%BC%A0%E5%BB%BA%E9%94%8B%EF%BC%9A%20%E4%B8%8B%E4%B8%80%E6%B3%A2%E5%88%9B%E6%96%B0%E6%9C%BA%E4%BC%9A%EF%BC%8C%E9%87%8D%E7%82%B9%E5%85%B3%E6%B3%A8%E8%BF%99%E4%B8%89%E4%B8%AA%E9%A2%86%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[2017杭州·云栖大会首日，阿里巴巴集团CTO张建锋宣布成立达摩院，将在全球各地建立实验室，并引入更多高校教授参与其中，未来三年投入1000亿元进行基础科学研发。 以下为演讲全文： 大家上午好，我想借这个机会，把阿里巴巴技术的一些想法和大家做一个分享。一年前马老师提出了“五新”战略，其中像“新零售”，我们看到盒马、无人咖啡店，得到了非常快速的发展。今天想首先跟大家分享一下“新技术”，以及“新技术”是怎么跟阿里巴巴的未来结合起来的。 互联网的第三次波浪潮：智联网、人机自然交互、机器智能 阿里巴巴是一家成长于互联网时代、扎根于互联网时代的一家公司，我们对技术有着非常深刻的体会跟理解。 互联网时代第一波浪潮，是把计算机从一个单一的工具变成了一个平台，它把信息都连接在一起了，比较有代表性的一些企业，比如作为搜索的像Google，它把全世界散落在单点的数据、信息都连成一个平台，这是它带来最大的价值。 第二波我认为是移动互联网的发展。移动互联网把信息分享、传递变得更加自然，这一波里面，我觉得最大的贡献就是今天我们在讲的，像社交、应用等等。阿里巴巴在这一波浪潮中做了一件跟大家想象中不一样的事情——我们做电子商务不仅提供一个货架，而是通过互联网这个平台，把消费者跟生产者连接在一起，把品牌跟消费者连接在一起。这个连接其实跟其他人说的做零售、做电子商务是有一个非常本质的区别的——这个连接是双向的，通过这个连接可以诞生出、创造出无数新的可能，而不仅仅是说我通过电子商务、从事互联网来提升零售效率。所以到今天为止，我们走出了一条非常独特的道路。 今天，PC的销量已经连续在下降，萎缩得比较厉害了。现在去电子市场，纯粹卖一台PC，基本上没有什么生存空间了；以手机为代表的无线互联网，手机从2016年开始，基本上稳定在四亿的出货量，在中国，也没有新的增量。手机操作界面已经决定了，这个界面只有一个屏幕，这个屏幕里面可以放的东西是有限的，现在的超级APP基本上占据了手机最主要的入口来源。 下一波机会来自于什么地方，我们思考后觉得有三个领域值得关注： 第一个，是智联网。因为现在还有这么多的设备、这么多的物体没有被连接。以IoT为代表的智联网应该是接下来最需要解决的一个问题。这上面我们也做了非常多的尝试，我们做的城市大脑，希望把城市里面所有的物体连接起来，小到井盖、电线杆，再到马路、到红绿灯，都能够通过物联网连接起来，但我们认为光连接是不够的，因为连接只是把所有的人、物聚在一起，我们还需要去感知，还需要去处理数据，最终我们还要实时做出决策，去控制被连接的主体，这才是有价值的智联网。 第二个，新一代人机自然交互。今天我们有了很多交互手段，包括现在非常热门的自动驾驶。自动驾驶目前要解决的主要是一个人机交互的问题。开车一定要拿一个方向盘吗，可能没有这个必要；控制空调就一定要拿摇控器吗，可能也没有这个必要。因为我们可以有更自然的方式，可能是语音，可能是其他的。以苹果手机为代表，它从原先的键盘式操作，升级到屏幕触摸式的操作，但它只是在一个范围之内的升级。我们希望能够把整个人机交互，从家里的一切应用到驾驶，都有全面的升级。 还有一个，就是机器智能。马老师非常强调我们做的是机器智能。为什么要说我们做的是机器智能，机器智能跟人工智能到底有什么区别？ 我的理解，今天我们很多东西之所以这样做，是因为以前人类就是这么做的——以前的做法都是要人来控制，所以我今天不想让人来控制了，我要机器来控制，所以要模仿人类来控制。举个例子，现在人工智能里面最热门的是做图像识别，我们在交通上也好，在城市管理上也好，装了无数的摄像头，因为我们拍了这么多的照片， 现在我们人看不过来了，所以需要机器来看，所以机器又要模仿人的所有思考方法，重新认识这个图片。但是我们有没有想过，假如这个照片就是用机器来看的，那为什么一定要拍成现在这个照片的样子，它直接可以是机器认识的就可以了，机器可能不一定要4K、8K、高清、彩色，可能是从另外一个角度去理解这个世界。 王坚博士举过一个例子，人的东西一定是最好的吗，狗的嗅觉比人更好，你用机器来模仿，会做得更好吗？ 所以，我们要做的是，把机器变得有智能，而且变成独立的智能，这个智能应该是机器的能力决定的，而不是人类的能力决定的。这也是为什么，我们今天一定要用机器智能这个概念，重新定义我们真正要的智能是怎么样的一个智能。 平台化、实时化的数据是未来世界的血液 我们今天要做这么多事情，要解决这么多连接的问题，不可避免的会产生大量数据。这个世界一定会被数字化的，我们对此深信不疑，因为只有数字化之后，才有自动化的可能，才有智能化的可能。九年前，阿里巴巴第一次提出阿里巴巴是一家大数据公司，数据是能源，但我今天想说的是，数据不仅是能源，如果机器智能、智联网，包括人机自然交互组成一个人体的话，数据就是血液，没有这个血液，所有上面的一切都没有创新的能量来源。数据，我们认为它远远不止于这个资源，它是组成所有未来一切的血液。这是我们怎么来看待未来这个世界一个非常重要的出发点。 今后的数据有两个特点非常重要： 一个是实时性。数据一定要非常实时。以前一个产品要推广，做广告。三个月之后，厂家才知道这个广告做得好不好，这个效果好不好，消费者买不买单，这个时候才能去组织生产、组织安排。现在我们这些数字化的广告，每一分钟都知道我这个效果怎么样。 第二个，数据一定要平台化，一定要融合贯通。阿里巴巴有三件事情是统一的，其中最重要的一件事情就是数据的统一，我们统一定义、清洗、处理。我举个例子，我们跟小黄车它们合作，把小黄车给联网了，我们知道每一个车的运行轨迹，我们也知道它的密度。知道这个小区到哪个小区，或者哪个小区到哪个地方，骑共享单车的人是不是特别多。这个数据拿到之后，一方面可以改进小黄车的运营效率，这个数据如果被公交公司知道了，公交公司可以优化它的公交线路，现在没有这些数据，公交公司说今天班车在开，我一直往前开好了。所以数据一定要平台化，它只有融会贯通之后，才能产生新的生产力，才能有新的创造力。 互联网公司跟传统公司有什么不一样，以前我们都讲互联网思维，互联网思维是一个什么样的思维？对于阿里巴巴来讲，我们觉得互联网思维，第一就是一个数据思维——你必须要有数据，你才能做出一些合理的决策。传统公司的CEO跟互联网公司的CEO有很大的不一样，传统公司的CEO，他做一个决定，他想知道这个决定正确还是错误，可能要验证很久。在互联网公司，逍遥子可能跟我们讨论，这个页面按钮应该是红色还是蓝色，为什么做这个决定，他有这个数据，他知道改了之后，这个数据有变化了，他敢于做这种决定。我觉得这就是互联网公司跟传统公司非常大的不一样。 我们有一个不成文的规定：我们开会，我跟他们讲，第一，你有数据说数据；没有数据，那就说案例；没有案例，就说观点。都没有，那就不要说了，说了也没用。数据是第一位的，有数据，你就跟CEO一样有这个Power，这是互联网思维里面非常重要的一个维度。 汇聚全球智慧，以科技创新世界的阿里巴巴达摩院 今天我们要做这么多的东西，智联网、人机自然交互、机器智能等等等等，我们后面还有非常多的问题要解决。这些问题包括我们的计算能力、计算平台、算法，自然语言的处理、理解，安全，还有更底层的芯片，更底层的操作系统。因为今天对于阿里巴巴这家公司来说，你已经不可能从市面上买到商用的一些产品来支撑我们未来发展需要的技术。所以我们必须要自己去做更深层次、更高维度的研发。 科学是什么，科学是用来发现规律、掌握规律的；技术是什么，技术是来利用这个规律的；而工程是来实现这个规律的。阿里巴巴这么多年来，通过双11积累了非常强的工程技术能力。我们今天把双11这一天的技术保障称为“互联网的超级工程”。很多超级工程，比如造世界第一的高楼大桥。而阿里巴巴的双11技术支撑这套体系，要支撑那么大规模的业务，解决无数的技术问题，它就是一个“超级工程”。但今天我们想更进一步，我们觉得光解决工程技术问题不够，我们还想掌握规律、发现规律，这是我们真正能够引领未来、真正能够定义未来的核心要素。 今天，在这里，我们正式宣布成立阿里巴巴的全球研究院。因为我们需要有更多的人才，一起参与，一起来改变这个世界。我们这个研究院有一个独一无二的名字叫做阿里巴巴达摩院。 我们计划在三年之内，对新技术投资超过1000亿人民币，我们想要在技术上面，真正做一些原创性、根本性的探索。这么多钱干什么，我们想吸引全球一流的人才，我们也始终认为人才是真正的生产力。在阿里巴巴达摩院，不是叫你来做苦行僧的，是叫你来做骑士的，你们是新一代的骑士，你们不是壮士，科学工作者必须得到应得的尊重与荣誉，这就是阿里巴巴达摩院。 阿里巴巴有这么多的技术、这么多的平台，我们还有一个非常重要的思想，我们不仅去探索未来，不仅服务好我们自己的业务，我们还想通过阿里云这个平台去赋能所有创业者。因为我们是这么想的，所以我们八年前就这么做了——我们做了云计算。我们有这个Believe，我们相信这个事情一定会发生，我们才做这个事情，我们并不是像其它云计算公司一样，因为我要转型升级了，是因为这个东西非常流行。我做云计算，我们真的是因为坚信。 整个达摩院由三个部分组成： 第一部分，我们在全球各地建自己的实验室，这是阿里巴巴集团自己投资的。我们在以色列、新加坡、莫斯科、西雅图跟圣马特奥都建立了自己的研究机构。在数据智能、智联网、大数据处理等等方面，做一些前沿性的基础性研究，并且能够快速把这些研究成果变成我们业务上可以用的一些东西，也可以通过阿里云这个平台，变成所有人可以使用的一个技术基础设施。 第二部分，我们是跟高校建立联合研究所。我们跟浙江大学联合成立的前沿技术研究中心运行得非常好，有很多教授、博士在这个平台上工作。为什么吸引他们在这个平台上工作，因为我们有非常大的计算装置，我们有非常多的业务场景。我们采用非常与众不同的方法，别人可能是这样，我有一个项目建好了，然后交给别人来招投标，交给浙大，你来做。我们不是这样——今天这个时代，发现一个问题，跟解决一个问题的难度是一样的。我们在定义未来的世界，发现问题对我们来说也是很大的挑战。我们请他们进来，我们一起来看到底有什么问题，用你们的眼光来看有什么问题，我们一起来解决。我们今天跟浙大、跟伯克利、跟清华大学等都成立了联合实验室，一起来做这个事情。 第三部分，是我们的产学研平台。这个平台非常有意思，我们把要解决的非常多的问题做成一个列表，发给全球的所有高校、机构。高校、机构的教授、学者，对他们感兴趣的研究方向做一个匹配，然后来写他的Proposal，我们看这个Proposal跟我们是否匹配。我们现在有四十多个项目正在开始启动做，而且这个教授、机构，绝大部分来自于海外，国内很多高校也参加了。 最终我们这个达摩院会是三部分：我们自己会建实验室，跟高校做联合实验室，通过产学研平台这个项目，让更多的教授、机构能够参与进来。最终我们希望以科技来创新这个世界，来改变这个世界，这是我们达摩院的愿景。]]></content>
      <categories>
        <category>书籍</category>
        <category>《2017阿里技术》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>阿里技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[达摩院：阿里巴巴的科技雄心]]></title>
    <url>%2F2017%2F12%2F21%2F%E8%BE%BE%E6%91%A9%E9%99%A2%EF%BC%9A%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E7%9A%84%E7%A7%91%E6%8A%80%E9%9B%84%E5%BF%83%2F</url>
    <content type="text"><![CDATA[10月11日，以“飞天·智能”为主题2017杭州·云栖大会在浙江杭州云栖小镇开幕，阿里巴巴董事局主席马云在开幕式上演讲。（本文来源：中国新闻周刊） 四名科学家同时盖上印章后，两名“武者”展开长卷，淡淡的水墨之间，“达摩院”三个大字跃然纸上。 在武侠世界里，“达摩院”代表着武林绝学和至尊，这是阿里巴巴董事局主席马云为新成立的研究院取的名字。就在2017云栖大会前两周，阿里巴巴集团首席人力官童文红给马云打电话，讨论研究院如何命名，马云灵光一闪，“达摩院”三字脱口而出。 而在此前长达半年的酝酿中，阿里巴巴内部都将这个新机构称做“阿里巴巴全球创新研究院”。 “为什么一定要研究院、实验室这样的说法，为什么不能创造一个自己的名字，我觉得达摩院就很好。”马云在给童文红的电话中说，他甚至连英文名都想好了，就是拼音DAMO。 正如在名称上独辟蹊径一样，马云希望达摩院能走出自己的模式，“我们会学习IBM，学习微软，学习贝尔实验室，学习在过去人类历史科技发展过程中取得的巨大的经验和教训，但我们必须走出自己的路。” 按照阿里巴巴在云栖大会上的说法，“达摩院”是探索人类科技未来的实验室，阿里巴巴将在研发投入1000亿元，用于涵盖基础科学和颠覆式技术创新的研究。 在阿里巴巴董事局主席马云看来，这是18岁的阿里巴巴应有的担当精神。他将“达摩院”视为阿里巴巴将留给世界最好的东西之一。有一天即使阿里巴巴不在了，希望“达摩院”还能继续存在。为了做到这一点，“达摩院”必须做到商业与科技、市场与研究的完美结合。 “向技术要红利！” 在十年前，马云可不是这样想的。那时候的他，曾经坚决反对公司有任何研究室、实验室，因为在他看来，当时阿里巴巴还是一个初创公司，在还没有立足之前就考虑研发是大灾难。 在“达摩院”筹备组成员、阿里巴巴技术战略部总监刘湘雯的印象中，早在2008年，阿里巴巴就已经从战略层面考虑，从一家电商公司变成一家数据公司。尽管有这样一个愿景，大家却并不知道如何去做。当时，阿里巴巴的平台沉淀了很多数据，怎样去发挥数据的价值，从技术上怎么做，引发了阿里巴巴高层一系列的思考。 最终，马云选择了相信云计算，成立了阿里云计算有限公司。虽然没有被叫做“研究院”，但在刘湘雯看来，这是阿里巴巴第一次从战略上向科技进行转移。至此，阿里巴巴全面进入云计算，对自身的定位也从一家电商公司变成一家数据驱动的公司。 2014 年，阿里巴巴成立了iDST（Institute of Data Science&amp;Technologies），这是阿里巴巴集团专注于底层数据技术研究的机构。此前，马云已经把下一个时代命名为DT时代，也就是数据科技时代。而iDST以机器学习、深度学习技术为依托，打造图像视频、语音交互、自然语言理解、智能决策等人工智能核心技术， 为阿里巴巴集团的电商、金融、物流、社交、娱乐等业务提供强大的技术后盾。这些AI技术通过阿里云平台对外形成产品化的输出。 用刘湘雯的话说，“达摩院”的成立是一个水到渠成的过程，离不开“母体”阿里巴巴的发展。一个公司只有当业务发展到一定的阶段，有足够复杂的场景，足够多的业务体量，才会有足够多的科技难题出现，才能支撑一群科学家在里面做事情，才会产生一家机构，因此，“达摩院”的出现是阿里巴巴发展的必然。 “如果说阿里云让我们拥有了计算的能力，那么iDST则更多的是提供算法的能力。我们集中建设了这样一批能力，加上本身具有非常丰富的场景跟数据，然后才提出了向更纵深去发展。”刘湘雯解释道。 刘湘雯第一次听到马云谈到关于建立“达摩院”的设想是在今年3月，阿里巴巴内部召开了首次技术大会，会上马云分享了他的科技愿景。马云认为，此前18年，阿里巴巴的商业场景推动了技术升级，面向未来20年，核心技术升级才能推动商业模式创新，必须建立起NASA这样的机构。 “必须向技术要红利！”这句话，阿里巴巴首席技术官张建锋在会上重复了多次。而早在2016年云栖大会上，马云就提出过“五新”战略，即新零售、新金融、新制造、新技术和新能源。截至2017年3月，新零售已经在落实，新金融正在布局，“已经到半路了”，接下来“必须组建阿里的新技术”。 在这次技术大会上，马云动员全球两万多名科学家和工程师投身“新技术战略”，并启动“NASA”计划，“面向机器学习、芯片、物联网、操作系统、生物识别这些核心技术，我们将组建崭新的团队，建立新的机制和方法，全力以赴。”马云强调，“以前我们的技术跟着业务走，是‘兵工厂模式’，但手榴弹造得再好，也造不出导弹来。阿里巴巴必须思考建立导弹的机制，成立新技术研发体系，聚焦核心领域的研究。” 阿里巴巴有着巨大的野心——未来20年，阿里巴巴要构建世界第五大经济体，服务全球20亿消费者，创造1亿就业机会，帮助1000万家企业盈利。因此，“NASA”计划的目标也是面向未来20年，其产品或服务能够覆盖到20亿人。 “NASA”计划 据刘湘雯介绍，“NASA”计划的2万多人，不仅是研究人员，也包括工程技术人员。近3年来，阿里巴巴人才数量年均增长40％以上，目前拥有2.5万名工程师和科学家，500多位博士。在36位合作人中，有9位拥有工程师背景。 同时，阿里巴巴也面向全球网罗顶尖科技人才。今年3月，人工智能专家、前南洋理工大学教授王刚加入阿里人工智能实验室。6月26日，亚马逊最顶级的华人科学家任小枫加盟了阿里，出任iDST副院长。 9月11日，量子技术领域的重量级人物施尧耘加入阿里巴巴，担任阿里云量子技术首席科学家，负责组建并领导阿里云量子计算实验室，同时，施尧耘也在之江实验室担任副主任，该实验室是由浙江省政府、浙江大学、阿里巴巴集团出资成立的混合所有制新型研发机构，9月6日正式挂牌成立。 按照“NASA”计划，如果2万多人才全涌到阿里巴巴所在地杭州，似乎也并不现实，在人才聚集地建立海外实验室成为实现“NASA”计划的更好方式。就在云栖大会当天，阿里巴巴首席技术官张建锋透露，“达摩院”已经开始在全球各地组建前沿科技研究中心，包括亚洲达摩院、美洲达摩院、欧洲达摩院，并在北京、杭州、新加坡、以色列、圣马特奥、贝尔维尤、莫斯科等地设立不同研究方向的实验室，初期计划引入100名顶尖科学家和研究人员。 张建锋表示，选择在何地建实验室主要有两个原则，一是根据当地的人才状况，比如以色列的安全做得很好，美国的一些大数据算法人才比较好等；二是跟业务有一些关系，比如新加坡本身是有产业基础的，更有利于科技成果在当地转化。 据刘湘雯介绍，现在杭州、北京两地的实验室已经在建设中，美洲、欧洲等实验室的人员陆续到位，已经开始做一些事情。新加坡在加快团队建设的速度，可能很短的时间就能到位。俄罗斯和以色列的实验室还处于筹备阶段。 “NASA”计划已见雏形，但究竟研究哪些领域并没有确定。10月10日，就在云栖大会前一天，13位顶级科学家造访阿里巴巴，并与马云举办了一场座谈。 在这13位科学家中，包括中国唯一的图灵奖获得者姚期智院士、中国量子力学第一人潘建伟院士、定义了“计算思维”的哥伦比亚大学教授周以真、全球人脸识别技术“开拓者”和“探路者”汤晓鸥教授等。这些科学家研究领域不同，但都参与了“达摩院”的出谋划策。 数年前，潘建伟曾向阿里巴巴提出，成立一个中科院跟阿里巴巴联合量子计算的实验室。今年7月30日，中国科学院－阿里巴巴量子计算实验室正式成立，实验室将结合阿里云在经典计算算法、架构和云计算方面的技术优势，以及中科院在量子计算和模拟、量子人工智能等方面的优势，颠覆摩尔定律，探索超越经典计算机的下一代超快计算技术。 “当时我说可能15年之内都不会有产出，也不会有回报。没想到阿里巴巴很快参与进来合作。”潘建伟感慨地说。 这也是“达摩院”三大组成部分之一，即自主研究中心、与高校和研究机构建立的联合实验室（研究中心）和全球开放研究项目。 与具有科研优势、地缘优势的著名高校联合建立科研基地是阿里学术合作的主要方式之一。继去年10月成立清华大学－蚂蚁金服金融科技联合实验室，今年1月成立UC Berkeley RISE 实验室之后，“NASA”计划启动以来，5月成立了阿里巴巴－浙江大学前沿技术联合研究中心，阿里巴巴不断探索新的合作模式，汇集诸多技术领域内全球最优秀的学术人才，共同打造高效率的科技创新链条和一体化的创新体系。 学术合作的另一种方式是发布全球开放研究项目，将阿里巴巴遇到的工程和技术挑战和各个实验室里最强的学术大脑进行碰撞，进而实现工业界与学术界科技能力的融合。在此次云栖大会上，公布了“阿里巴巴创新研究计划（AIR）”2017全球课题评选结果，在13个国家和地区的99个高校与科研机构（国内54个，海外45个）提交申请的234份科研项目提案中，有40余个优秀项目最终入选。 AIR 是阿里巴巴集团探索科技创新设立的首个全球性科研项目，聚焦技术驱动未来，致力于推进计算机科学领域基础性、前瞻性、突破性的研究，以校企深度合作的方式引领重大科技创新的实践应用，构建技术生态。以此搭建学术界、工业界的合作平台，联合双方优势共同促进前沿技术的发展。 “一家公司要做长远的科研非常不容易。世界上很少有公司能够做到。阿里巴巴能够有此决心，不只是做跟阿里巴巴商业相关的东西，非常高瞻远瞩。”姚期智对达摩院的雄心表示赞赏。 来势汹汹 “达摩院”在成立之初，便显出凶猛的势头。在马云的演讲中，已经提出“必须要超越英特尔，必须超越微软，必须超越IBM”，而首批公布的学术委员会更是“星光熠熠”，十人中有三位中国两院院士、五位美国科学院院士，包括世界人工智能泰斗Michael I. Jordan、分布式计算大家李凯、人类基因组计划负责人George M. Church等。 这样的登场，使得达摩院从一开始便赚足了眼球。 “科学研究是有其自身规律的，需要大量的资源、资金和人才，而研究的周期和结果更是无法预测，‘达摩院’才刚起步，现在谈发展如何还为时尚早。”一位某知名企业研究机构的工作人员表示。 “达摩院”首批公布的13个研究领域，包括量子计算、机器学习、基础算法、网络安全、视觉计算、自然语言处理、下一代人机交互、芯片技术、传感器技术、嵌入式系统等，涵盖机器智能、智联网、金融科技等多个产业领域。 “这是一个综合决策的过程！”刘湘雯表示，从今3月开始，就在确定研究领域，不光有科研人员，还有从事产业研究的人员，以及公司管理层。 刘湘雯表示，这些领域基本上会基于整个科技发展的规律，一方面是阿里巴巴自身的业务诉求，另外一方面，虽然没有看清楚它对业务有怎样的影响，但从大趋势来看，有可能是颠覆性的，比如量子计算机。 然而，一些基础性或颠覆性的学科可能投入大，而回报慢，作为一家企业所属的研究院，必须考虑不同类别学科的合理组合。因此要放回产业成熟度的链条上，有一些可能三五年能见到成果，有一些可能需要十年以上。“但究竟是怎样一个比例，目前没法给出一个确切的数字。”刘湘雯说。 在中国产学研合作促进会秘书长王建华看来，“达摩院”的建立值得鼓励，要创新一种模式，总需要有人先去探索、实践，“达摩院”正是这样一种探索。 另一个冲击眼球的是阿里巴巴的人才战略，10月16日，“达摩院”宣布，微软亚洲研究院首席研究员聂再清博士、谷歌Tango和DayDream项目技术主管李名杨博士，入职阿里人工智能实验室。 不难发现，“NASA”计划实施以来，加入阿里和“达摩院”的顶尖人才，除了来自高校和科研院所，有相当一部分来自于知名企业研究院，加上马云对几家研究机构的公开“宣战”，很难不让人联想到“挖墙脚”一词。 “达摩院”的成立起到了一种“鲶鱼效应”，相当于整个科研生态里出了一个新物种，一定会打破暂时的平衡，也一定会有人才的流动。但即使达摩院从某处吸引了一个人才，造成该处暂时的空缺，肯定会从另一处补进一个人才，使整个系统领域在某个阶段会快速地进行重新定位，重新达成一种平衡，并且这种平衡往往会比原来更健康。 按照马云对“达摩院”的定位，即“Research for solving the problem with profit and fun（为解决问题研究并带来利润和快乐）”，刘湘雯认为，“达摩院”招揽的人才除了在专业上彼此认可，在时间地点上恰好也适合这些客观条件之外，从软性条件上来说，双方对于愿景的驱动这件事情要有高度的契合。 经营之道 近年来，阿里巴巴不断加大技术上的投入。财报显示，阿里巴巴2017财年技术投入为170亿元，居中国互联网公司之首。 此次“达摩院”的成立，阿里巴巴宣布将在3年内投入1000亿元，相当于每年330多亿元，几乎是2017财年技术投入的一倍。但在马云眼里，这笔钱只是给“达摩院”的创业基金，实验室绝不能等资金，要有挣钱意识，才能活下去。“我希望不仅仅靠论文活下来，90％以上研究的东西，不能只在实验室里面，必须在市场上。只有这样，这个实验室才能走得长。”马云说。 如何挣钱？作为“达摩院”首任院长，张建锋对此回应道，达摩院作为一个依托大数据而建立的新型的研究机构，需要一个产业的支撑。而阿里巴巴拥有诸多业务，有金融科技，有电子商务，有物流等等，都是对研究非常重要的支持。同时，依托于阿里云平台，达摩院可以连接更多的应用场景给客户，使他们通过研究院和阿里云这个平台，能够去做智慧城市，做工业大脑，做医疗大脑，连接更多的行业。“我认为这是现在达摩院最大的价值。” 刘湘雯认为，阿里巴巴企业的本质决定了“达摩院”在做科技成果转化上是有天然优势的。 在她看来，一方面，阿里巴巴有丰富的业务场景，都会来到“达摩院”里找他们能用的东西，而“达摩院”通过设立技术开放日，也可以向业务团队去介绍他们的东西。 另一方面，更大的优势就是阿里云。“阿里云从‘达摩院’这头看，是一个放大器，从客户那头看是一个漏斗。”刘湘雯说。 在旷视研究院院长孙剑看来，企业研究院分为两种，一种是研究的内容和企业没有太大关系，主要只是起到“保险”的作用，确保公司将来在大的方向上不要走错；另一种是研究和企业，和当前产品能够有效结合，为公司赚钱。 “其实如果看过去的这些研究院，当一个公司快速发展的时候，或者公司的赚钱是非常没有问题的时候，这个研究院是会蓬勃发展的。但是公司的经济有问题的话，研究院是第一个会被考虑裁减的，因此在企业快速发展，在形势大好的时候，最需要拿出资源投入对未来的投资。”孙剑表示。 从这一角度，阿里巴巴三年1000亿元的投入，显然能给“达摩院”一个相对宽松的科研环境，但如何运用这笔资金，也是大家关注的焦点。 刘湘雯表示，达摩院实行的是院长负责制，资金大部分可能会花在人才上，因为对于一个研究院来说，最主要的就是人才，此外，还将用于购买必要的科研设施。 接下来，“达摩院”既会建自己的研究院，也会建联合的实验室，还会向学术界开放，资金也会朝这三个方向分配。]]></content>
      <categories>
        <category>书籍</category>
        <category>《2017阿里技术》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>阿里技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[25岁Java工程师如何转型学习人工智能？]]></title>
    <url>%2F2017%2F12%2F20%2F25%E5%B2%81Java%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%A6%82%E4%BD%95%E8%BD%AC%E5%9E%8B%E5%AD%A6%E4%B9%A0%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[“大牛我要问”栏目推出一段时间后，阿里妹收到不少童鞋的来信，其中以职业发展、技术成长的困惑居多。 今天阿里妹选择了一个颇具有代表性的问题：关于目前大热的AI入门学习，希望能帮助有同样问题的童鞋解惑指路。 来信问题：25岁Java工程师如何转型学习AI？ 我是一名25岁的Java开发工程师。本科学习的专业是信息与计算科学（数学专业），因为对计算机方面感兴趣，之后培训学习了Java，所以现在从事Java开发。目前就是在电商公司开发一些系统。 我对人工智能非常感兴趣，对数学的兴趣也从未减弱。人工智能设计的学习材料很多，像我这样的状况，如果想要转型以后从事这方面的工作，具体应该学习些什么？ 阿里技术童鞋“以均”回信： 首先，我想聊聊为何深度学习最近这么火。 外行所见的是2016年AlphaGo 4比1 战胜李世石，掀起了一波AI热潮，DeepMind背后所用的深度学习一时间火得不得了。其实在内行看来，AlphaGo对阵李世石的结果是毫无悬念的，真正的突破在几年前就发生了。 2012年，Gefferey Hinton的学生Alex使用一个特别构造的深度神经网络（后来就叫AlexNet），在图像识别的专业比赛ImageNet中，得到了远超之前最好成绩的结果，那个时候，整个人工智能领域就已经明白，深度学习的革命已经到来了。 果然，之后深度学习在包括语音识别，图像理解，机器翻译等传统的人工智能领域都超越了原先各自领域效果最好的方法。从2015年起，工业界内一些嗅觉灵敏的人士也意识到，一场革命或已到来。 关于基本概念的学习机器学习与深度学习 深度学习是机器学习中的一种技术，机器学习包含深度学习。机器学习还包含其他非深度学习的技术，比如支持向量机，决策树，随机森林，以及关于“学习”的一些基本理论，比如，同样都能描述已知数据的两个不同模型，参数更少的那个对未知数据的预测能力更好（奥卡姆剃刀原理）。 深度学习是一类特定的机器学习技术，主要是深度神经网络学习，在之前经典的多层神经网络的基础上，将网络的层数加深，并辅以更复杂的结构，在有极大量的数据用于训练的情况下，在很多领域得到了比其他方法更好的结果。 机器学习与大数据 大数据：机器学习的基础，但在多数语境下，更侧重于统计学习方法。 机器学习，深度学习，数据挖掘，大数据的关系可以用下图表示 系统学习资料 深度学习火起来之后，网上关于深度学习的资料很多。但是其质量参差不齐。我从2013年开始就关注深度学习，见证了它从一个小圈子的领先技术到一个大众所追捧的热门技术的过程，也看了很多资料。我认为一个高质量的学习资料可以帮助你真正的理解深度学习的本质，并且更好地掌握这项技术，用于实践。 以下是我所推荐的学习资料： 首先是视频课程。 Yaser Abu-Mostafa 加州理工的Yaser Abu-Mostafa教授出品的机器学习网络课程，非常系统地讲解了机器学习背后的原理，以及主要的技术。讲解非常深入浅出，让你不光理解机器学习有哪些技术，还能理解它们背后的思想，为什么要提出这项技术，机器学习的一些通用性问题的解决方法（比如用正则化方法解决过拟合）。强烈推荐。 课程名称：Machine Learning Course - CS 156 视频地址： https://www.youtube.com/watch?v=mbyG85GZ0PI&amp;list=PLD63A284B7615313A Geoffrey Hinton 深度学习最重要的研究者。也是他和另外几个人（Yann LeCun，Yoshua Bengio等）在神经网络被人工智能业界打入冷宫，进入低谷期的时候仍然不放弃研究，最终取得突破，才有了现在的深度学习热潮。 他在Coursera上有一门深度学习的课程，其权威性自不待言，但是课程制作的质量以及易于理解的程度，实际上比不上前面Yaser Mostafa的。当然，因为其实力，课程的干货还是非常多的。 课程名称：Neural Networks for Machine Learning 课程地址：https://www.coursera.org/learn/neural-networks UdaCity Google工程师出品的一个偏重实践的深度学习课程。讲解非常简明扼要，并且注重和实践相结合。推荐。 课程名称：深度学习 课程地址：https://cn.udacity.com/course/deep-learning--ud730 小象学院 国内小象学院出品的一个深度学习课程，理论与实践并重。由纽约城市大学的博士李伟主讲，优点是包含了很多业内最新的主流技术的讲解。值得一看。 课程名称：深度学习（第四期） 课程地址： http://www.chinahadoop.cn/classroom/45/courses 推荐阅读书目 《Deep Learning the Book》 —— 这本书是前面提到的大牛Yoshua Begio的博士生Goodfellow写的。Goodfellow是生成式对抗网络的提出者，生成式对抗网络被Yann LeCun认为是近年最激动人心的深度学习技术想法。这本书比较系统，专业，偏重理论，兼顾实践，是系统学习深度学习不可多得的好教材。 英文版：http://deeplearningthebook.com 目前Github上已经有人翻译出了中文版： https://github.com/exacity/deeplearningbook-chinese 推荐学习路径 不同的人有不同的需求，有些人希望掌握好理论基础，然后进行实践，有些人希望能够快速上手，马上做点东西，有些人希望理论与实践兼顾。下面推荐几条学习路径，照顾到不同的需求。大家可以根据自己的特点进行选择。 Hard way Yaser -&gt; Geoffrey Hinton -&gt; UdaCity -&gt; Good Fellow 特点：理论扎实，步步为营。最完整的学习路径，也是最“难”的。 推荐指数： 4星 Good way Yaser -&gt; UdaCity -&gt; 小象学院-&gt; Good Fellow 特点：理论扎实，紧跟潮流，兼顾实战，最后系统梳理。比较平衡的学习路径。 推荐指数： 5星 “Fast” way UdaCity -&gt; Good Fellow 特点：快速上手，然后完善理论。 推荐指数： 4星 “码农” way UdaCity 特点：快速上手，注重实践。 推荐指数： 3星]]></content>
      <categories>
        <category>书籍</category>
        <category>《2017阿里技术》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>阿里技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[技术变化那么快，程序员如何做到不被淘汰？]]></title>
    <url>%2F2017%2F12%2F20%2F%E6%8A%80%E6%9C%AF%E5%8F%98%E5%8C%96%E9%82%A3%E4%B9%88%E5%BF%AB%EF%BC%8C%E7%A8%8B%E5%BA%8F%E5%91%98%E5%A6%82%E4%BD%95%E5%81%9A%E5%88%B0%E4%B8%8D%E8%A2%AB%E6%B7%98%E6%B1%B0%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[作者：空融 阿里妹导读：写了这么多年的代码，你是否曾经有过这样的迷茫和困惑——技术发展日新月异，奋力追赶的我们，究竟是技术的主人还是技术的奴隶？今天，我们邀请到了蚂蚁金服的技术专家空融，一起来聊聊技术人的软件世界观。 在浩大的软件世界里，作为一名普通程序员，显得十分渺小，甚至会感到迷茫。我们内心崇拜技术，却也对日新月异的技术抱有深深的恐惧。有时候我会思考难道在技术领域内不断紧跟新潮，不断提升技能就是我的价值所在？那么我是技术的主人还是技术的奴隶？人之所以迷茫往往是找不到工作生活的重心，感受不到工作或生活的价值。那么什么是价值呢？说的大一点就是我改变了世界，说的小一点就是我的所作所为改善了某些问题。如果不清楚自己的行为、目标、价值三者的关系，那么又何来重心？又如何能分得清重要性与优先级呢？ 程序员的迷茫不仅仅是面对技术繁杂的无力感，更重要的是因为长期埋没于软件世界的浩大的分工体系中，无法看清从业务到软件架构的价值链条，无法清楚定位自己在分工体系的位置，处理不好自身与技术、业务的关系所致。 很多程序员打心底不喜欢业务，这一点我曾经也经历过，我更宁愿从事框架工具、技术组件研究的相关事情。我有个朋友经常吐槽我说：”你们天天加班加点写了那么多代码，然后呢？有改变什么吗？还不是写出了一堆垃圾。”仔细想想很多时候业务在我们脑海中存留的只是逻辑和流程，我们丢失的是对业务场景的感受，对用户痛点的体会，对业务发展的思考。这些都是与价值紧密相关的部分。我们很自然的用战术的勤快掩盖战略的懒惰！那么这样的后果就是我们把自己限死在流水线的工位上，阉割了自己能够发现业务价值的能力，而过多关注新技术对职场竞争力的价值。这也就是我们面对繁杂技术，而产生技术学习焦虑症的根本原因。 业务、技术与软件系统的价值链 那么什么是业务呢？就是指某种有目的的工作或工作项目，业务的目的就是解决人类社会与吃喝住行息息相关的领域问题，包括物质的需求和精神的需求，使开展业务活动的主体和受众都能得到利益。通俗的讲业务就是用户的痛点，是业务提供方（比如公司）的盈利点。而技术则是解决问题的工具和手段。比如为了解决用户随时随地购物的业务问题时，程序员利用web技术构建电子商务App，而当需求升级为帮助用户快速选购商品时，程序员会利用数据算法等技术手段构建推荐引擎。 技术如果脱离了业务，那么技术应用就无法很好的落地，技术的研究也将失去场景和方向。而业务脱离了技术，那么业务的开展就变得极其昂贵和低效。 所以回过头来我们想想自己没日没夜写了那么多的代码从而构建起来的软件系统，它的价值何在呢？说白了就是为了解决业务问题，所以当你所从事的工作内容并不能为解决业务问题带来多大帮助的时候，你应该要及时做出调整。那么软件系统又是如何体现它自身的价值呢？在我看来有如下几个方面的体现： 业务领域与功能：比如支付宝立足支付领域而推出的转账、收款功能等，比如人工智能自动驾驶系统等。 服务能力：这就好比火车站购票窗口，评判它的服务能力的标准就是它能够同时处理多少用户的购票业务，能不能在指定时间内完成购票业务，能不能7*8小时持续工作。对应到软件系统领域，则表现为以下三个方面： * 系统正确性(程序能够正确表述业务流程，没有Bug) * 可用性（可以7＊24小时＊365不间歇工作） * 大规模（高并发，高吞吐量） 互联网公司正是借助大规模的软件系统承载着繁多的业务功能，使其拥有巨大的服务能力并借助互联网技术突破了空间限制，高效低廉解决了业务问题，创造了丰厚的利润，这是人肉所不可比拟的。 理解了这一层面的概念，你就可以清楚这个价值链条：公司依靠软件系统提供业务服务而创造价值，程序员则是通过构建并持续演进软件系统服务能力以及业务功能以支撑公司业务发展从而创造价值。 有了这个价值链条，我们就可以反思自己的工作学习对软件系统的服务能力提升起到了多大的推动作用？可以反思自己的工作学习是否切实在解决领域的业务问题，还是只是做一些意义不大的重复性工作。 前两天面试了一个候选人，他的工作是从事票务系统开发，他说自己在研究linux内核与汇编语言，我就问他linux内核和汇编语言的学习对你的工作产生了哪些帮助？能否举一个例子？他哑口无言，我内心就觉得这样一个热爱学习的好苗子正迷茫找不到重心，正在做一件浪费精力的事情。正确的学习方式应该是将学习与具体业务场景结合起来，和公司通过软件系统开展业务服务而创造价值，程序员通过提升软件系统服务能力创造价值这一链条串接起来，从对这些价值产生帮助的程度去思考优先级。学习本身没有错，错的往往就是那颗初心。 现在你再来看高并发分布式相关的知识，你会发现并不是因为这些知识比较高深、比较时髦，很多公司有需求才值得学习，而是他们对价值链条有着实实在在的贡献。 价值驱动的架构 一谈到软件系统，人们免不了想起架构这件事来。之所以此处去谈及架构是因为每一个程序员本质都是软件架构体系中的一分子，我们可能深埋于体系流水线之中，感受不到位置和价值。但如果站在架构这一高度去看这些问题则将会非常透彻。那么架构究竟是什么？和上述的价值链又有什么关系呢？ 什么是架构？ 在我看来软件架构就是将人员、技术等资源组织起来以解决业务问题，支撑业务增长的一种活动。可能比较抽象，我想我们可以从架构师的一些具体工作任务来理解这句话含义： 组织业务：架构师通过探索和研究业务领域的知识，构建自身看待业务的”世界观”。他会基于这种认识拆分业务生命周期，确立业务边界，构建出了一套解决特定业务问题的领域模型，并且确认模型之间、领域之间的关系与协作方式，完成了对业务领域内的要素的组织工作。 组织技术：为了能在计算机世界中运作人类社会的业务模型，架构师需要选用计算机世界中合适的框架、中间件、编程语言、网络协议等技术工具依据之前设计方案组织起来形成一套软件系统方案，在我看来软件系统就像是一种技术组织，即技术组件、技术手段依据某种逻辑被组织起来了，这些技术工具被确定了职责，有了明确分工，并以实现业务功能为目标集合在了一起。比如RPC框架或消息队列被用于内部系统之间的通信服务就如同信使一般，而数据库则负责记录结果，它更像是一名书记员。 组织人员：为了能够实现利用软件系统解决业务问题的目标，架构师还需要关注软件系统的构建过程，他以实现软件系统为号召，从公司组织中聚集一批软件工程师，并将这些人员按不同工种、不同职责、不同系统进行组织，确定这些人员之间的协作方式，并关注这个组织系统是否运作良好比如沟通是否顺畅、产出是否达到要求、能否按时间完成等。 组织全局，对外输出：架构师的首要目标是解决业务问题，推动业务增长。所以他非常关心软件的运行状况。因为只有在软件系统运行起来后，才能对外提供服务，才能在用户访问的过程中，解决业务问题。架构师需要关注运行过程中产生的数据比如业务成功率，系统运行资源占用数据、用户反馈信息、业务增长情况等，这些信息将会帮助架构师制定下一步架构目标和方向。 所以软件架构不仅仅只是选用什么框架、选用什么技术组件这么简单。它贯穿了对人的组织、对技术的组织、对业务的组织，并将这三种组织以解决业务问题这一目标有机的结合在了一起。 很多面试的候选人在被问及他所开发的系统采用什么架构的问题时，只会罗列出一些技术组件、技术框架等技术要素，这样看来其根本没有理清架构的深层含义。也有一些架构师只专注对底层技术的研究，以为打造一个卓越的系统是非常牛逼的事情，可是他忽略了软件系统的价值是以解决业务问题的能力、支撑业务增长的能力为衡量标准，所以最后生产出了很多对组织，对业务没有帮助的系统。 成本与收益 正如之前所说软件系统只有在运行的时候才能创造价值，也就是说软件系统能否7*24小时＊365天稳定的工作关系到公司的收益水平。所以开发团队对生产环境的发布总是小心翼翼，对解决生产环境的问题总是加班加点。而软件系统的成本则体现在软件构建过程，这时候我们就能理解那些工程技术如项目管理、敏捷开发、 单元测试、持续集成、持续构建，版本管理等的价值了，他们有的是保证软件系统正确性，有的是为了降低沟通成本，有的是为了提升开发效率等但总的来说就是为了降低软件的构建成本。所以在提升系统服务能力，创造更多业务收益的同时，降低构建成本也是一种提升收益的有效手段。 作为一名软件工程师而言，我们往往处在软件构建过程体系中的某个环节，我们可以基于成本与收益的关系去思考自己每一项技能的价值，学习新的有价值的技能，甚至在工作中基于成本与收益的考量选择合适的技术。比如在逻辑不大发生变化的地方，没有必要去做过多的设计，应用各种花俏的设计模式等浪费时间。这样我们才能成为技术的主人。 架构目标需要适应业务的发展 架构的目标就是为了支撑业务增长，就是提升软件系统的服务能力。可是话虽说如此，但真实却要做很多取舍。比如对初创团队而言，其产品是否解决业务问题这一设想还没得到确认，就立即去构造一个高性能、高可用的分布式系统，这样的架构目标远超出业务发展的需求，最后的结果就是浪费大量人力物力，却得不到任何起色。架构师需要审时度势，仔细衡量正确性、大规模、可用性三者的关系，比如今年业务蓬勃发展日均订单300万，基于对未来的可能预测，明年可能有3000 万的订单，那么架构师应该要着重考虑大规模和可用性。而且每一点提升的程度，也需要架构师衡量把握，比如可用性要达到2个9还是3个9。 回顾自己以往的工作很多时候就是因为没有确立架构目标导致浪费了组织很多资源，比如在之前的创业团队中，由于本人有一定的代码洁癖，经常会花费很多时间和同事计较代码质量，这样本可以更快上线的功能却需要被延迟，当时过度追求正确性的行为是与创业团队快速验证想法的业务需求不匹配的。 另外一点比较深刻的案例则是在本人担任一个技术团队负责人的时候，在一次述职报告的时候，leader问我对接下来团队工作有什么计划？我当时说了一堆什么改进代码质量，每天晨会，任务透明化，建立迭代机制等等，然后就被各种批驳一通。当时团队基本以外包人员为主，人员水平较差，开发出来的金融系统也是千疮百孔而这条业务线最重要的业务价值则是按计划实现潜在投资方的需求，争取拉到投资。所以不久leader就召集测试架构的相关人员与我这边一同梳理对核心功能的测试工作，将研发、测试、上线的流程自动化。 当时并不理解这样做核心价值是什么。但回过头来看这样的工作方式恰好符合了业务发展的需求，即确保系统是符合设计需求的，保证系统达到可接受的正确性，为后续能过快速前进打下基础，最重要的是为企业降低了构建成本。所以程序员想要工作出业绩，必须认清楚系统背后的业务价值，按价值去梳理工作优先级，而不是像我一般过度纠结细节，追求技术理想化。 成也分工，败也分工 正如在程序员的迷茫那一章节提到的：程序员的迷茫因为长期埋没于软件世界的浩大的分工体系中，无法看清从业务到软件架构的价值链条，无法清楚定位自己在分工体系的位置，处理不好自身与技术、业务的关系所致，所以在这里我想谈谈分工。架构师为了使软件系统更好的服务业务，必然将软件系统生命周期进行拆分，比如分出开发生命周期、测试生命周期、用户访问生命周期、软件运维生命周期，并根据不同的生命周期划分出不同的职责与角色。 比如开发人员负责开发周期负责完成软件研发，测试人员负责对开发人员交付的成果进行测试等，于是就形成了分工。一旦分工形成，每一个分工组织都会有自己的价值追求，架构师关注的顶层的价值即软件系统能否支撑业务增长被分工的形式打碎到各个组织中。分工是有其价值的，他使得复杂昂贵的任务可以被简单、并行、可替换的流水线方式解决。但久而久之，价值碎片化的问题就出现了，比如测试人员只关注找出更多问题，开发人员只关注快速开发更多的系统，运维人员只关注保障系统稳定。 三者之间常常都只站在自己的立场去要求对方怎么做，没有人再关注整体价值，产生诸多矛盾增加软件实施成本。而身处流水线中的一员，又因为困扰于重复性工作， 迷茫于工作的意义，甚至感觉自己做为了人的创意与灵感都被扼杀了。所以我的朋友吐槽我说你写了那么多代码然后并没有怎么样是非常有道理的，那是因为我只关注着做为流水工人的价值要求，看不到生态链最顶端的价值。 我们仔细想想那些团队领导，精英领袖哪一个不是为着更广大的价值所负责，比如项目经理只需要关心自身项目的商业价值，而公司CEO则关心公司范畴内所有业务的总体商业价值。所以关注的价值越大且职位也就越高。这些高层领导者们把控着整体的价值链条，及时纠正底层分工组织的价值目标与整体价值目标出现偏差的问题。 从价值出发－找寻学习与工作的新思路 迷茫能引发思考，架构则塑造了视野，而价值则是我们之所以存活，之所以工作的逻辑起点。基于这样一种价值思维，对我们的学习和工作又可以有哪些改启示呢？ 明确自身的业务相关主体：找出你工作的协作关系网内的业务方和客户方，这样你就可以从客户方中找到离你最近的业务价值点，从你的业务方中挖掘更多的资源。甚至你可以按这个思路顺着网络向上或向下挖掘价值链条，整合更多的上下游资源以实现更大的价值。 向前一步，为更大的价值负责：不要因为自己是开发人员就不去关注软件运维，不要因为只是测试就不关注软件开发，因为你关注的越多你越能看清全局的价值目标。如果只关注一亩三分地，那么注定这辈子只能困守在这一亩三分地里，成为一名流水线上焦虑至死的码农。试着转变思维，从架构师的角度思考价值问题，看看能否将技术贯穿到业务、到用户、到最终的价值去。之前我的朋友说过要把产品经理踢到运营位置去，把程序员踢到产品经理位置去，这样才是正确做事方式。这句话也是类似的意思，向前一步才能懂得怎么做的更好。 像架构师一样思考，用价值找寻重心：人的迷茫是因为找不到重心，而价值的意义在于引导我们思考做哪些事情才能实现价值，先做哪些事情会比后做哪些事情更能创造收益。像架构师那样全局性思考，把遇到问题进行拆分，把学习到的事物串联起来，努力构成完整的价值链条。 学会连接，构建体系：前几天看到一篇文章对今日头条的产品形态极尽批判之词，指责它的智能算法将人类封死在自己的喜好之中，将人类社会进一步碎片化。这似乎很有道理，有趣的是互联网将我们连接至广袤的世界，却也把我们封闭在独属于自己的小世界里。依旧是我的那位朋友，他说他的最大价值在于连接，将不同的人连接在一起，有趣的事情可能就会即将发生。 或许算法的天性就是顺从与迎合，但人最终想理解这个世界还是需要依靠自身的行动与不同人之间建立联系，这也是一种摆脱流水线限制的有效方式。另外，我们自身也是某种事物连接的产物，比如架构师，他是业务、技术、管理连接在一起的一种产物。所以我们应当树立自身的知识体系以吸收融合新知识，将孤立的概念连接起来，形成自身的价值链条。比如这篇文章将我从事技术开发经验、与对架构的理解以及自身过往经历结合起来，这也是一种内在的体系梳理。 作者简介：空融，网名“D调的暖冬”。现就职蚂蚁金服，从事支付宝身份认证相关领域的技术开发。]]></content>
      <categories>
        <category>书籍</category>
        <category>《2017阿里技术》</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>阿里技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈MySQL隐式类型转换【转】]]></title>
    <url>%2F2017%2F12%2F18%2F%E8%B0%88%E8%B0%88MySQL%E9%9A%90%E5%BC%8F%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%E3%80%90%E8%BD%AC%E3%80%91%2F</url>
    <content type="text"><![CDATA[前言 今天我们继续回到MySQL系列文章中,谈一谈MySQL中隐式类型转换。(其实我最早知道是在慢SQL优化中知道的隐式类型转换概念的),在说隐式类型转换之前,首先我们通过一个实例来看看是怎么回事。 数据结构本文中所有的操作,都是基于该数据结构(有兴趣的童鞋,可以实验): create table t_base_user( oid bigint(20) not null primary key auto_increment, name varchar(30) null comment &quot;name&quot;, email varchar(30) null comment &quot;email&quot;, age int null comment &quot;age&quot;, telephone varchar(30) null comment &quot;telephone&quot;, status tinyint(4) null comment &quot;0 无效 1 有效&quot;, created_at datetime null default now() comment &quot;创建时间&quot;, updated_at datetime null default now() comment &quot;修改时间&quot; ) ### 新建索引 alter table t_base_user add index idx_email(email); alter table t_base_user add index idx_name(name); alter table t_base_user add index idx_telephone(telephone); ### 新增记录: INSERT INTO `test`.`t_base_user` (`name`, `email`, `age`, `telephone`, `status`, `created_at`, `updated_at`) VALUES (&apos;111111&apos;, &apos;andytohome@gmail.com&apos;, &apos;111&apos;, &apos;12345678901&apos;, &apos;1&apos;, now(),now()); 引子 首先我们基于上述数据结构中,我们来看看下面这个执行计划: explain select * from t_base_user where telephone=12345678901; 执行计划结果: id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra -- | ----------- | ----------- | ---- | ------------- | ---- | ------- | ---- | ---- | ------------ 1 | SIMPLE | t_base_user | ALL | idx_telephone | NULL | NULL | NULL | 1 | Using where 细心的童鞋应该已经看出来了,为什么数据结构中已经在telephone字段上新建了idx_telephone，而上述语句并没有走索引,而是全表扫描。这是为什么呢？带着这疑问,我们来看看今天的主角–MySQL隐式类型转换 什么隐式类型转换？ 在MySQL中: 当操作符与不同类型的操作数一起使用时，会发生类型转换以使操作数兼容。则会发生转换隐式 也就是说,MySQL会根据需要自动将数字转换为字符串，将字符串转换数字。看到这个概念之后,是不是有一种茅塞顿开的感觉。哦… 原来在数据结构中telephone字段为字符串(varchar)类型,而我们传的手机号是数字类型。现在我们将SQL修改下: select * from t_base_user where telephone=”1234567890”; 再看看执行计划上述语句的执行计划: explain select * from t_base_user where telephone=”1234567890”; 结果: id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra -- | ----------- | ----------- | ---- | ------------- | ------------- | ------- | ---- | ---- | ------------ 1 | SIMPLE | t_base_user | ref | idx_telephone | idx_telephone | 63 | const| 1 | Using index condition 从这里看,现在语句已经走索引了。为了加深我们对隐式类型转换的印象,我们再多看看几个隐式类型转换案例:案例一: 字符串转换为数字 mysql &gt; SELECT 1+’1’; 结果: mysql &gt; 2 案例二: 数字转换为字符串 mysql -&gt; SELECT CONCAT(1024,’ andyqian’); 结果:‘1,’ test’; 此时CONCAT(字符拼接)函数就进行了隐式类型转换。 如何避免隐式类型转换? 只有当清楚的知道隐式类型转换的规则，才能从根本上避免产生隐式类型转换。MySQL也在官网描述了进行隐式类型转换的一些规则如下: 1. 隐式类型转换规则: 如果一个或两个参数都是NULL，比较的结果是NULL，除了NULL安全的&lt;=&gt;相等比较运算符。对于NULL &lt;=&gt; NULL，结果为true。不需要转换 如果比较操作中的两个参数都是字符串，则将它们作为字符串进行比较。 如果两个参数都是整数，则将它们作为整数进行比较。 如果不与数字进行比较，则将十六进制值视为二进制字符串 如果其中一个参数是十进制值，则比较取决于另一个参数。 如果另一个参数是十进制或整数值，则将参数与十进制值进行比较，如果另一个参数是浮点值，则将参数与浮点值进行比较 如果其中一个参数是TIMESTAMP或DATETIME列，另一个参数是常量，则在执行比较之前将常量转换为时间戳。 在所有其他情况下，参数都是作为浮点数（实数）比较的。 2. 使用CAST函数显示转换我们可以使用CAST显示的将类型进行转换,如下所示: mysql&gt; SELECT 38.8, CAST(38.8 AS CHAR); 结果: mysql &gt; 38.8, ‘38.8’ 如上述中: select * from t_base_user where telephone=cast(15608464487 as char); 查看执行计划,我们也可以看出,这个时候也走索引了。 3. 类型一致 这里说的类型一致,指的是在写SQL时,参数类型一定要与数据库中的类型一致,避免产生隐式类型转换,就如刚才在文首时,如果多检查,写的SQL的参数类型与数据库中字段类型一致，也就不会不走索引了，你说是不是？ 小心隐式类型转换 这里再重申一次,写SQL时一定要检查参数类型与数据库字段类型一致,(如果参数不一致,也要使用CAST函数显示转换成一致)否则造成隐式类型转换,不走索引,后果简直不堪设想, 在前面《写会MySQL索引》这篇文章中提到过,不走索引,轻则造成慢查询，重则造成数据库服务器CPU100%。唉,说到这里,不瞒你说，我就吃过不少MySQL隐式类型转换的亏,导致了慢查询。 小结 看到这里,是不是有一种，数据表设计还真不是件容易的事情。需要考虑的因素太多太多了,需要考虑字段类型,索引设计,还有各种约束条件等等。也一定要谨慎谨慎再谨慎！其实换个角度就更容易理解了,大家都知道高楼大厦都是需要一个好的地基的,在数据库表设计中,前期的表结构设计就是这个地基，其重要性可想而知。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[互联网行业职位简称总结]]></title>
    <url>%2F2017%2F12%2F14%2F%E4%BA%92%E8%81%94%E7%BD%91%E8%A1%8C%E4%B8%9A%E8%81%8C%E4%BD%8D%E7%AE%80%E7%A7%B0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[常见互联网职位英文缩写：PM、RD、FE、UE、UI、QA、OP、DBA等PM项目经理( Project Manager ) 从职业角度，是指企业建立以项目经理责任制为核心，对项目实行质量、安全、进度、成本管理的责任保证体系和全面提高项目管理水平设立的重要管理岗位。项目经理是为项目的成功策划和执行负总责的人。 项目经理是项目团队的领导者，项目经理首要职责是在预算范围内按时优质地领导项目小组完成全部项目工作内容，并使客户满意。为此项目经理必须在一系列的项目计划、组织和控制活动中做好领导工作，从而实现项目目标。 当然在互联网公司这个有着项目经理or产品经理的意思。 RD研发（Research and Development） 如：软件RD工程师就是软件研发工程师，诸如PHP程序猿，Java程序猿，无论是爱疯的还是安卓的都是属于这一类别。偏向于后端的技术实现。 FE前端（Front-End）；前端开发（Front-End Development） FE是web前端研发、前端开发的意思！ UE用户体验（User Experience，简称UX或 UE） 是一种纯主观的在用户使用一个产品（服务）的过程中建立起来的心理感受。因为它是纯主观的，就带有一定的不确定因素。 个体差异也决定了每个用户的真实体验是无法通过其他途径来完全模拟或再现的。但是对于一个界定明确的用户群体来讲，其用户体验的共性是能够经由良好设计的实验来认识到。 计算机技术和互联网的发展，使技术创新形态正在发生转变，以用户为中心、以人为本越来越得到重视，用户体验也因此被称做创新2.0模式的精髓。 另外还有有个组合叫法：UED（产品交互设计师，用户体验师）。 UI用户界面（User Interface） UI设计则是指对软件的人机交互、操作逻辑、界面美观的整体设计。好的UI设计不仅是让软件变得有个性有品味，还要让软件的操作变得舒适、简单、自由、充分体现软件的定位和特点。 UI还有其它的意义，如Unit Interval，Univ of Iowa，Unlock Instruction，Urgent Interrupt。 QA测试（QUALITY ASSURANCE，中文意思是“质量保证”） 其在ISO8402：1994中的定义是“为了提供足够的信任表明实体能够满足质量要求，而在质量管理体系中实施并根据需要进行证实的全部有计划和有系统的活动”。有些推行ISO9000的组织会设置这样的部门或岗位，负责ISO9000标准所要求的有关质量保证的职能，担任这类工作的人员就叫做QA人员。 OP运维（Operations） OP这个词语代表的意思很多，这个简称来自于英文的Operations一词。我也不清楚谁最早用op代表运维工程师，不过2010年开始，这个词慢慢被很多人所知道。 OP工作内容主要就是维护公司的服务器能够正常提供服务，细分的话包括系统部分，网络部分，应用程序部分，数据库部分，具体根据公司的规模和职位职能不同，运维的定义也不同。现在市面上主要的OP有三种：网络游戏运维，网站运维，大型项目测试和生产环境运维。 DBA数据库管理员（Database Administrator，简称DBA） 是一个负责管理和维护数据库服务器的人。数据库管理员负责全面管理和控制数据库系统。这个职位对不同的人意味着不同的意义。 另外还有DB，既数据库（Database）。 还有就是互联网产品设计常用文档类型的缩写： BRD、MRD、PRD、FSD等BRD商业需求文档（Business Requirement Document） 是基于商业目标或价值所描述的产品需求内容文档（报告）。其核心的用途就是用于产品在投入研发之前，由企业高层作为决策评估的重要依据。其内容涉及市场分析，销售策略，盈利预测等，通常是供决策层们讨论的演示文档，一般比较短小精炼，没有产品细节。 MRD市场需求文档（Market Requirements Document） 获得老大的认同后，产品进入实施，需要先出MRD，具体来说要有更细致的市场与竞争对手分析，通过哪些功能来实现商业目的，功能/非功能需求分哪几块，功能的优先级等等。实际工作中，这个阶段PD可能的产出物有Mind Manager的思维图，Excel的Feature List等。 市场需求文档（MRD）重点放在为一个被提议的新产品或者现有产品的改进定义市场需求。与BRD指出商业问题和解决这些问题的解决方案不同，MRD更深入提议解决方案的细节。它包括一些或者所有这些细节： a. 解决商业问题所需要的特色 b. 市场竞争分析 c. 功能和非功能需求 d. 特色/需求的优先级 e. 用例 MRD通常是由拥有产品经理，产品营销经理或者行业分析师头衔的人撰写的。MRD通常是一份连续的5-25页Word文档，或者正如之后描述那样在一些机构中甚至更长。 PRD产品需求文档（Product Requirements Document） 进步一细化，这部分是PD写得最多的内容，也就是传统意义上的需求分析，我们这里主要指UC（use case）文档。主要内容有，功能使用的具体描述（每个UC一般有用例简述、行为者、前置条件、后置条件、UI描述、流程/子流程/分支流程，等几大块），Visio做的功能点业务流程，界面的说明，demo等。Demo方面，可能用dreamweaver、ps甚至画图板简单画一下，有时候也会有UI/UE支持，出高保真的demo，开发将来可以直接用的那种。 产品需求文档（PRD）重点放在为一个被提议的新产品或者现有产品的改进定义市场需求。与MRD侧重于从市场需要角度看需求的不同，PRD侧重于从产品本身角度看待需求。通常在特点和功能需求上更深入细节，并也可能包括屏幕截图和用户界面流程。在那些MRD不包括具体需求和用例的机构中，PRD就包含这些具体内容。PRD通常是由拥有产品经理，行业分析师或者产品分析师头衔的人撰写的。PRD通常是一份连续的20-50页Word文档，或者针对复杂产品甚至更长。 提醒：一些机构将这里描述的MRD和PRD合并成一个文档，并称最后的文档为MRD。在这种情况下，MRD包括本段描述的内容，也包括上一段描述PRD的内容，并且可能超过50页。 FSD功能详细说明（Functional Specifications Document） 有一点像“概要设计”，这步就开始往开发衔接了，产品UI、业务逻辑的细节都要确定，细化文档并保持更新。相应的，有很多内容，比如表结构设计，要由项目经理来编写了。 功能规格文档（FSD）把焦点集中在实现，定义产品功能需求的全部细节。FSD可能通过一张张的截屏和一条条功能点来定义产品规格。这是一份可以直接让工程师创建产品的文档。 与MRD和PRD侧重于以市场需要和产品角度看需求不同，FSD把重点放在了以表格形式定义产品细节，再让工程师实现这些细节。FSD也可能包括完整的屏幕截图和UI设计细节。 FSD通常是由拥有产品分析师，工程领导或者项目经理头衔的人撰写的 – 作者通常属于工程部门。通常一个连续几十页的Word或类似文档。 剩下的这些都是大佬的职位缩写： CEO、COO、CFO、CTO、CIO等CEO首席执行官(Chief executive officer) 是美国人在20世纪60年代进行公司治理结构改革创新时的产物。 由于市场风云变幻，决策的速度和执行的力度比以往任何时候都更加重要。 传统的“董事会决策、经理层执行”的公司体制已经难以满足决策的需要。而且， 决策层和执行层之间存在的信息传递时滞和沟通障碍、决策成本的增加，已经严 重影响经理层对企业重大决 策的快速反应和执行能力。而解决这一问题的首要一 点，就是让经理人拥有更多自主决策的权力，让经理人更多为自己的决策奋斗、对 自己的行为负责。CEO就是这种变革的产物。CEO在某种意义上代表着将原来董事会 手中的一些决策权过渡到经营层手中。 CEO与总经理，形式上都是企业的“一把手”，CEO既是行政一把手，又是股东 权益代言人————大多数情况下，CEO是作为董事会成员出现的，总经理则不一定 是董事会成员。从这个意义上讲，CEO代表着企业，并对企业经营负责。 由于国外没有类似的上级主管和来自四面八方的牵制，CEO的权威比国内的总经理们更绝对，但他们绝不会像总经理那样过多介入公司的具体事务。CEO作出总体决 策后，具体执行权力就会下放。所以有人说，CEO就像我国50%的董事长加上50%的总经理。 一般来讲，CEO的主要职责有三方面：①对公司所有重大事务和人事任免进行决 策，决策后，权力就下放给具体主管，CEO具体干预的较少；②营造一种促使员工愿 意为公司服务的企业文化；③把公司的整体形象推销出去。 CTO首席技术官或技术主管(Chief technology officer) 基本上就是一个技术大拿，熟练掌握公司地核心技术，并可以带领团队开发或使用新技术来帮助公司达到目标。基本上CTO不会是公司地最高层。 CTO有时候也会成为公司地最高层，特别是一些以技术为核心竞争力地企业来说。首先，我们来解读一下什么是CTO。其实，CTO（首席技术官）作为一个外来名词，在中国还不多见，随着网络热潮传进中国地CXO系列中地一员，CTO给人留下地印象只是技术人员所能达到地最高职位。“但当技术日益成为影响企业发展地决定因素时，CTO也就成为对企业发展起着决定性作用地人群之一。 在美国，CTO除了负责技术支持和技术改良等日常工作外，其主要职责是设计公司地未来工作。从某种意义上说，CTO地首要工作是提出公司未来两三年内地产品和服务地技术发展方向。 尽管CTO这个名词是引进来了，但在角色职能定义方面同国外还存在一定差距。作为一个高科技公司地CTO，其更多地工作应该是前瞻性地，也就是制定下一代产品地策略和进行研究工作，属于技术战略地重要执行者。 在国内来看，大部分地企业里地“CTO”都是过去地“工程师”摇身一变而成地，因此带着很强地技术色彩。在一些通过技术安身立命地高科技企业，这些工程师出身地CTO也往往能够占据核心领导地位。但是在其他地行业中，例如一些传统地行业，一些把市场营销能力作为核心竞争力地企业，CTO地作用就大打折扣，CIO就逐渐浮出水面了。 CIO首席信息官或信息总监(Chief information officer) 具有技术背景或对技术有些了解地公司高层。 通常CIO向CEO汇报，或向CFO汇报。CIO不需要是个技术大拿，但对技术必须非常敏感，并能发掘技术带给公司地潜力。随着IT在各公司地重要性日渐提高，CIO地地位也渐高，有时能进入公司地最高决策层。CIO是个桥梁，把公司地商业模式和技术连接起来。 他通过组织和利用企业地IT资源，为企业创造效益。通过信息化掌握了企业地业务命脉以及战略方向地CIO，很可能向决策管理层地地位继续上升，直到达到权力地顶峰—CEO。 一家美国主导企业地首席执行官和一群首席信息官进行了一次谈话，讨论首席信息官在现代公司中地作用。在谈话进行到一半地时候，他直截了当地说：“首席信息官也许是我最重要地经理人。没有他们，我不知道我地公司会是怎样。”由此可见CIO在企业中地重要作用了。 在CIO成功地基本素质中，其中有一项是要精通企业以及相关行业地知识。要搞信息化，一个CIO至少要熟悉企业地研发、生产、计划、营销、市场、物流等核心业务流程，熟悉企业地财务管理、组织结构、行政程序、人力资源管理等基础资源，以及企业发展地远景、价值观等企业地文化范畴。在这基础上，CIO才能对企业地IT建设和信息资源做出正确地规划。 因此如果你想成为一个成功地CIO，那么最好远离电脑，去积极培养作为企业管理者应该具备地各种能力。对500名CIO所做地调查发现，70%地人认为通往成功地关键是有效地沟通；58%地人选择谙熟商业流程和运作；而46%地人则认为战略性地思想和计划能力很重要。而此前被认为很重要地IT技能，只获得了10%地认可。这不能不说是一个巨大地观念改变。 CFO首席财务官或财务总监(Chief financial officer) 现代公司中最重要、最有价值的顶尖管理职位之一，是掌握着企业的神经系统(财务信息)和血液系统(现金资源)灵魂人物。 做一名成功的CFO需要具备丰富的金融理论知识和实务经验。公司理财与金融市场交互、项目估价、风险管理、产品研发、战略规划、企业核心竞争力的识别与建立以及洞悉信息技术及电子商务对企业的冲击等自然都是CFO职责范围内的事。 在一个大型公司运作中，CFO是一个穿插在金融市场操作和公司内部财务管理之间的角色。担当CFO的人才大多是拥有多年在金融市场驰骋经验的人。在美国，优秀的CFO常常在华尔街做过成功的基金经理人。 COO首席运营官(Chief operating officer) 职责主要是负责公司的日常营运，辅助CEO的工作。 一般来讲，COO负责公司职能管理组织体系的建设，并代表CEO处理企业的日常职能事务。如果公司未设有总裁职务，则COO还要承担整体业务管理的职能，主管企业营销与综合业务拓展，负责建立公司整个的销售策略与政策，组织生产经营，协助CEO制定公司的业务发展计划，并对公司的经营绩效进行考核。]]></content>
      <categories>
        <category>随记</category>
      </categories>
      <tags>
        <tag>随记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP的几种运行模式]]></title>
    <url>%2F2017%2F12%2F13%2FPHP%E7%9A%84%E5%87%A0%E7%A7%8D%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[首先，我们来看看PHP的架构图： SAPI（Server Application Programming Interface服务端应用编程端口）提供了一个和外部通信的接口，使得PHP可以和其他应用进行交互数据。 php默认提供了很多种SAPI，常见的给apache的mod_php，CGI，FastCGI，还有Shell的CLI。 CGI 以CGI的方式运行，CGI英文叫做公共网关接口，就是Apache在遇到PHP脚本的时候会将PHP程序提交给CGI应用程序（php-cgi.exe）解释，CGI方式在遇到连接请求（用户 请求）先要创建cgi的子进程，激活一个CGI进程（解析器会解析php.ini文件，初始化执行环境），然后处理请求，处理完后再以CGI规定的格式返回处理后的结果给Apache，结束这个子进程。之后Apache再将结果响应给请求的用户。这就是fork-and-execute模式。所以用cgi 方式的服务器有多少连接请求就会有多少cgi子进程，子进程反复加载是cgi性能低下的主要原因。都会当用户请求数量非常多时，会大量挤占系统的资源如内 存，CPU时间等，造成效能低下。 （注：php-cgi 是PHP的解释器。） FastCGI 以FastCGI的方式运行，目的是提高CGI程序性能。这种形式是CGI的加强版本，CGI是单进程，多线程的运行方式，程序执行完成之后就会销毁，所以每次都需要加载配置和环境变量fork-and-execute（创建-执行）。而FastCGI则不同，FastCGI 像是一个常驻 (long-live) 型的 CGI，它可以一直执行着，只要激活后，不会每次都要花费时间去 fork 一次。PHP使用PHP-FPM(FastCGI Process Manager)，全称PHP FastCGI进程管理器进行管理。 Web Server启动时载入FastCGI进程管理器(IIS ISAPI或Apache Module)。FastCGI进程管理器自身初始化，启动多个CGI解释器进程 (在任务管理器中可见多个php-cgi.exe)并等待来自Web Server的连接。 当客户端请求到达Web Server时，FastCGI进程管理器选择并连接到一个CGI解释器。Web server将CGI环境变量和标准输入发送到FastCGI子进程php-cgi。 FastCGI子进程完成处理后将标准输出和错误信息从同一连接返回Web Server。当FastCGI子进程关闭连接时，请求便告处理完成。FastCGI子进程接着等待并处理来自FastCGI进程管理器(运行在Web Server中)的下一个连接。 在CGI模式中，php-cgi在此便退出了。 APACHE2HANDLER（mod_php） PHP作为Apache模块，Apache服务器在系统启动后，预先生成多个进程副本驻留在内存中，一旦有请求出 现，就立即使用这些空余的子进程进行处理，这样就不存在生成子进程造成的延迟了。这些服务器副本在处理完一次HTTP请求之后并不立即退出，而是停留在计算机中等待下次请求。对于客户浏览器的请求反应更快，性能较高。注：实现FastCGI协议的还有 PHP-FPM（Nginx）、Spawn-FCGI（Lighttp）。 CLI（Command Line Interface，即命令行接口） cli是php的命令行运行模式，大家经常会使用它，但是可能并没有注意到（例如：我们在linux下经常使用 “php -m”查找PHP安装了那些扩展就是PHP命令行运行模式；]]></content>
      <categories>
        <category>PHP</category>
        <category>PHP原理</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>PHP原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[茅台投资者：最烦被说一夜暴富 这一天我等了十年【转】]]></title>
    <url>%2F2017%2F12%2F13%2F%E8%8C%85%E5%8F%B0%E6%8A%95%E8%B5%84%E8%80%85%EF%BC%9A%E6%9C%80%E7%83%A6%E8%A2%AB%E8%AF%B4%E4%B8%80%E5%A4%9C%E6%9A%B4%E5%AF%8C%20%E8%BF%99%E4%B8%80%E5%A4%A9%E6%88%91%E7%AD%89%E4%BA%86%E5%8D%81%E5%B9%B4%E3%80%90%E8%BD%AC%E3%80%91%2F</url>
    <content type="text"><![CDATA[随记…吴东勤参加茅台股东大会时的自拍。 茅台股票的异常波动造就了一个经济新名词“茅房困境”：我手里有钱，是拿来买房子，还是拿来买茅台？ 这确实是个问题。 11月16日，“贵州茅台”以719.11元的收盘价，成为A股有史以来第一只突破700元的股票。当天，其总市值冲破9000亿元。 茅台自己倒是被吓到了。11月16日晚间茅台公司发布风险提示公告：希望广大投资者和消费者理性看待，审慎决策。 紧接着20日，上交所会员部出了份通报函，批评个别券商对茅台的评估报告客观依据不足，风险揭示不全面、不充分。 此后茅台股票一路走跌。11月28日，贵州茅台以648.23元收盘，上涨4.34%。 从年初的每股334.56元到此后一路走高，不到一年时间，贵州茅台股价累计上涨超100%。转眼又连续下跌，短短几个交易日，市值蒸发千亿。 股市从没有新事物，只有贪婪和恐惧的交替。那些投资茅台的人，他们有着怎样的故事呢？ 给儿子留十套茅台生肖 茅台风波迭起的这几天，浙江舟山投资者吴东勤显得相当淡定，依旧保持着每天晚上喝一点的习惯，不多，两盅，茅台。 在吴东勤15平方米的办公室里有一面堆了将近50箱还没拆封的茅台生肖酒，旁边的架子上还陈列着限量的茅台酒和特供酒，都是近两年陆续收藏的。 他正在装修的新房子里甚至专门为茅台酒留了一间房，做陈列和收藏用。 “我以十年为一个周期做资产配置，每年有15%收益率就已经很开心了，茅台今年涨了多少！我不希望它（涨得）太快。”吴东勤会计学出身，喜欢讨论复利：每年15%收益率，意味着差不多5年以后他的投资就能够翻倍。 事实上，茅台的增长速度远远高于他的预期。“我从150元/股的价格开始买起，最低买到过118元/股，满仓以后就没有再交易。”即使不算分红收益，从开始重仓茅台，一年多时间吴东勤的账户资金已经翻了两番不止。 而他收藏的茅台生肖酒比买入时已经涨了3倍多。 这算暴利吗？“暴利？小票（指小市值股票）才赚得更多。”他说，“创业板我买过乐视，第一家上A股的网络视频公司，两个月不到赚了一倍。现在看呢？我曾问过自己一个问题，‘这家公司5年后会怎么样’，答案是‘不知道’，所以我抛掉了。” 当时的股市几乎是一匹脱缰的野马。“什么时候见顶，谁知道？我不够聪明，所以这个钱你们赚吧，我不赚了。”从那波断崖式下跌的行情里全身而退，吴东勤庆幸自己比较“傻”。“说到底股票投资赚的是三笔钱：一笔是价格低于价值，被低估的钱；一笔是企业发展，市值提高，增值的钱；还有一笔就是价值被高估，泡沫化的钱。我能赚到前两笔就足够满足了。”赚第一笔靠智商，赚到第二笔要耐心，而泡沫化的钱，迟早都要还回去。 “我自己是做实业的，知道实业企业一年能有10%~15%的净利润有多么难。用实业的思维去看一家公司的时候，我们在投资回报上的要求会非常理性。”吴东勤希望可以通过投资实现家庭致富，但这是一个漫长的过程，他并不急，他的计划是20年。“以后给儿子留十套茅台生肖酒，他人生的第一桶金就有了。” 一夜暴富？这一天我等了十年 仲阳和吴东勤同在一个茅粉群里，加群的门槛之一是至少持有5000股茅台。 一开始只是一起讨论股票的网友，投资理念相合，约着喝了顿酒，就成了朋友。他持有茅台的时间比吴东勤更长。 “最烦别人说茅台‘一夜暴富’，这一天我等了十年。”他买入时是2007年春，现在已是2017年冬，“当时茅台股价已经在100元附近徘徊，也是高价股了。” 以当时的“高价”买入，原本就存了长期持有的打算。“给儿子结婚准备的”，对好奇他十年不卖的朋友，仲阳总是这么解释，虽然他儿子今年初中还没毕业。 “我不会说自己锁仓十年都是那么快乐，我是凡人，2008年大熊市的惊涛骇浪，2012年秋后白酒板块的大溃退，都很彷徨。”仲阳说，其实最近这几天他也有点纠结要不要减仓：一方面作为十几年的资深“茅粉”，很清楚这家公司的经营状况并没有出现问题，并且正在进入一轮快速发展周期；另一方面他又多少有点担心历史重演，“我经历过两次股价60%以上的回撤，不想再坐一次电梯。” 他说，股票市场短期看是一台投票器，长期看一定是称重仪。“买股票就是做股东买企业，我要用足够的时间把企业的成长转化为我的财富。”这让他在茅台股价超过400元时决定继续做一个茅台的守望者，他很文艺地说，因为“几乎所有包括投资在内的伟大事业都是一场孤独的旅行”，最好不多看、不多说，也不胡思乱想。 话虽如此，实际很难做到，尤其是茅台股价在极短时间内冲破700元。“机构也好、基金也好都有短期业绩指标，他们不可能像我一样有耐心去等待一家企业成长，（他们）收一茬韭菜就走，能让他们兴奋的只有收益率。”仲阳说起来就生气，直言中国资本市场出现的很多问题每个参与者都有责任，不要抱怨我们没有伟大的公司，因为在他们成为“伟大”之前就已经被一轮轮收割。他忍不住骂了一句，“那些短视的公募基金！” 他决定还是再看看，作为小股东分享了一家企业十年成长的红利，很难说抛就抛。“有感情了。”为了分析这只股票，他甚至专门去研究茅台的历史，了解其酿造过程、搞清楚了每种茅台酒的区别，甚至身边朋友买茅台酒他都要叫人家去茅台电商买，除了“保真”以外，因为这样茅台股份能稍多一部分营收。尽管那几箱酒的买卖影响也未必多大，但他觉得这是投资者应尽之责。 700元以上可能会考虑减持茅台 跟仲阳碰面当天，茅台还没有破700元，另一位投资者“西湖边的男2015”在其个人微博上表态，“以股价700为界，本人在未来6个月内也将择机减持茅台股票；并使本人所持有的茅台仓位降到占全部持仓市值的6~7成。” “西湖边的男2015”姓许，也是资深茅粉，微博网友习惯称呼他“西湖兄”。两人相识，用仲阳的话说，都是“经历过上甘岭的人”。圈内把在2012年秋后白酒板块的大溃退中坚持下来戏称为“守住上甘岭”。 “西湖边的男2015”手里的茅台股票是圈里比较多的，“按目前的市值来算早已过亿”，所以在圈里人看来适度减仓很正常。 11月14日，发微博当天，他说对茅台股价现状的看法是，“不再便宜，但也谈不上极端高估”。 11月16日，茅台股价破700元。 原本想约“西湖边的男2015”见面聊聊，结果他人正在大洋彼岸。“我这人散淡惯了。”他说。 一周之后，他发微博又谈起茅台，当然说的也不仅仅是茅台。谈及减持茅台的初衷，除了仓位原因，他说，“700元以上的茅台股价作为参照，去找寻在未来一个时期投资收益高于茅台的投资标的，是可能的。”当然，他口中的“未来一个时期”是5年，或者10年。大多数希望短期获利的人没有这份耐心。 他想要完成茅台的调仓，实际上并未完成。“这次市场并没有在700元以上给我提供充裕的调仓时间。好在，我善于等待，我不急。” 不要抱怨我们没有伟大的公司，因为在他们成为“伟大”之前就已经被一轮轮收割。]]></content>
      <categories>
        <category>随记</category>
      </categories>
      <tags>
        <tag>随记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache-多站点伪静态配置]]></title>
    <url>%2F2017%2F12%2F13%2FApache-%E5%A4%9A%E7%AB%99%E7%82%B9%E4%BC%AA%E9%9D%99%E6%80%81%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[一、Apache伪静态配置1、开启http.conf中的rewrite模块LoadModule rewrite_module modules/mod_rewrite.so #前的注释去掉即可检测：可使用phpinfo() 查看mod_rewrite模块是否加载2、允许指定目录使用.htaccess123456DocumentRoot &quot;D:/xampp/htdocs&quot;&lt;Directory &quot;D:/xampp/htdocs&quot;&gt;Options Indexes FollowSymLinks Includes ExecCGIAllowOverride AllRequire all granted&lt;/Directory&gt; 3、上面两步操作完之后，后面即可在 /htdocs目录下使用.htaccess文件 二、多站点伪静态配置 前提：根目录下有多个站点，且http.conf 配置文件中的网站根目录不能开启.htaccess文件识别。（否则，根目录开启.htaccess文件识别，那根目录下的子目录都会生效。）此处指的多站点配置伪静态是指对根目录下的某些站点配置。 因此多站点配置可分为两种方式： 根目录开启识别.htaccess文件 根目录下各子站开启识别.htaccess文件 多站点配置伪静态，首先需要有多站点配置。此处不予说明，前提配置与上面相同，直接上代码。12345678910111213&lt;VirtualHost *:80&gt; ServerAdmin webmaster@admin.com DocumentRoot &quot;D:/xampp/htdocs/CI&quot; ServerName myblog.com ServerAlias www.myblog.com &lt;Directory &quot;D:/xampp/htdocs/CI&quot;&gt; Options Indexes FollowSymLinks Includes ExecCGI AllowOverride All Require all granted &lt;/Directory&gt; ErrorLog &quot;logs/myblog.com-error.log&quot; CustomLog &quot;logs/myblog-access.log&quot; common&lt;/VirtualHost&gt; 说明： 此处的节点必须要写，否则站点下的.htaccess文件不生效。 有几个站点可以配置几个站点，都是同理。 三、遇到的问题及操作记录1、启动Apache报错：123456715:10:01 [Apache] Error: Apache shutdown unexpectedly.15:10:01 [Apache] This may be due to a blocked port, missing dependencies, 15:10:01 [Apache] improper privileges, a crash, or a shutdown by another method.15:10:01 [Apache] Press the Logs button to view error logs and check15:10:01 [Apache] the Windows Event Viewer for more clues15:10:01 [Apache] If you need more help, copy and post this15:10:01 [Apache] entire log window on the forums 解决办法： 有错误可知最可能的原因就是端口被占用，因此可围绕端口占用查找windows下查看端口情况及被占用情况命令：列出所有端口的情况： netstat -ano查找指定端口的情况： netstat -ano | findstr “80”还可使用xampp管理界面直接查看使用端口情况，或者使用任务管理器查看进程，比对pid。 此处，经查我的错误不是端口被占用，可在命令行下面执行httpd.exe，查看输出结果。提示错误：123D:\xampp\apache\bin&gt;httpd.exeAH00526: Syntax error on line 47 of D:/xampp/apache/conf/extra/httpd-vhosts.conf:ServerName takes one argument, The hostname and port of the server 可知是由于httpd-vhosts.conf配置文件修改错误，更改后重启成功。]]></content>
      <categories>
        <category>服务器</category>
        <category>Apache</category>
      </categories>
      <tags>
        <tag>Apache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令之grep]]></title>
    <url>%2F2017%2F12%2F12%2FLinux%E5%91%BD%E4%BB%A4%E4%B9%8Bgrep%2F</url>
    <content type="text"><![CDATA[grep命令文件过滤分割与合并 grep（global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。常用选项：-n 在显示符合范本样式的那一列之前，标示出该列的编号。-v 反转查找。-i 忽略字符大小写的差别。 常见用法：1、在文件中搜索一个单词，命令会返回一个包含“match_pattern”的文本行：grep match_pattern file_namegrep “match_pattern” file_name 2、在多个文件中查找：grep “match_pattern” file_1 file_2 file_3 …grep “match_pattern” ./*.php 3、输出除之外的所有行 -v 选项：grep -v “match_pattern” file_name 4、输出包含匹配字符串的行数 -n 选项：grep “text” -n file_name或cat file_name | grep “text” -n #多个文件grep “text” -n file_1 file_2 5、忽略匹配样式中的字符大小写：echo “hello world” | grep -i “HELLO”hello grep的一些进阶选项：12345[root@www ~]# grep [-A] [-B] [--color=auto] &apos;搜寻字符串&apos; filename选项与参数：-A ：后面可加数字，为 after 的意思，除了列出该行外，后续的 n 行也列出来；-B ：后面可加数字，为 befer 的意思，除了列出该行外，前面的 n 行也列出来；--color=auto 可将正确癿那个撷叏数据列出颜色 范例一：要将捉到的关键词显色，且加上行号来显示：123456[root@www ~]# dmesg | grep -n --color=auto &apos;eth&apos;247:eth0: RealTek RTL8139 at 0xee846000, 00:90:cc:a6:34:84, IRQ 10248:eth0: Identified 8139 chip type &apos;RTL-8139C&apos;294:eth0: link up, 100Mbps, full-duplex, lpa 0xC5E1305:eth0: no IPv6 routers present# 你会发现除了 eth 会有特殊颜色来表示之外，最前面还有行号显示 范例二：承上题，在关键词所在行的前两行与后三行也一起捉出来显示1234567891011[root@www ~]# dmesg | grep -n -A3 -B2 --color=auto &apos;eth&apos;245-PCI: setting IRQ 10 as level-triggered246-ACPI: PCI Interrupt 0000:00:0e.0[A] -&gt; Link [LNKB] ...247:eth0: RealTek RTL8139 at 0xee846000, 00:90:cc:a6:34:84, IRQ 10248:eth0: Identified 8139 chip type &apos;RTL-8139C&apos;249-input: PC Speaker as /class/input/input2250-ACPI: PCI Interrupt 0000:00:01.4[B] -&gt; Link [LNKB] ...251-hdb: ATAPI 48X DVD-ROM DVD-R-RAM CD-R/RW drive, 2048kBCache, UDMA(66)# 如上所示，你会发现关键词 247 所在的前两行及 248 后三行也都被显示出来！# 这样可以让你将关键词前后数据捉出来进行分析 grep 是一个很常见也很常用的指令，他最重要的功能就是进行字符串数据的比对，然后将符合用户需求的字符串打印出来。需要说明的是【grep 在数据中查寻一个字符串时，是以 “整行” 为单位来进行数据的撷取的！】也就是说，假如一个档案内有 10行，其中有两行具有你所搜寻的字符串，则将那两行显示在屏幕上，其他的就丢弃了。（注：在关键词的显示方面，grep 可以使用 --color=auto 来将关键词部分使用颜色显示。如果感觉每次使用 grep 都得要自行加上 --color=auto 很麻烦，此时就可以使用 alias 来处理一下，你可以在 ~/.bashrc 内加上这行：【alias grep=&#39;grep --color=auto&#39;】再以【 source ~/.bashrc 】来立即生效即可使用。这样每次执行 grep 都会自动帮你加上颜色显示。）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CGI、FastCGI、PHP-CGI、PHP-FPM详解]]></title>
    <url>%2F2017%2F12%2F12%2FCGI%E3%80%81FastCGI%E3%80%81PHP-CGI%E3%80%81PHP-FPM%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[CGI CGI全称是“公共网关接口”(Common Gateway Interface)，HTTP服务器与你的或其它机器上的程序进行“交谈”的一种工具，其程序须运行在网络服务器上。 CGI可以用任何一种语言编写，只要这种语言具有标准输入、输出和环境变量。如php,perl,tcl等 FastCGI FastCGI像是一个常驻(long-live)型的CGI，它可以一直执行着，只要激活后，不会每次都要花费时间去fork一次(这是CGI最为人诟病的fork-and-execute 模式)。它还支持分布式的运算, 即 FastCGI 程序可以在网站服务器以外的主机上执行并且接受来自其它网站服务器来的请求。 FastCGI是语言无关的、可伸缩架构的CGI开放扩展，其主要行为是将CGI解释器进程保持在内存中并因此获得较高的性能。众所周知，CGI解释器的反复加载是CGI性能低下的主要原因，如果CGI解释器保持在内存中并接受FastCGI进程管理器调度，则可以提供良好的性能、伸缩性、Fail-Over特性等等。 FastCGI也可以称为是一种协议标准，比如下面要说的php-fpm就是支持解析php的一个fastCGI进程管理器/引擎。 FastCGI特点 FastCGI具有语言无关性. FastCGI在进程中的应用程序，独立于核心web服务器运行，提供了一个比API更安全的环境。APIs把应用程序的代码与核心的web服务器链接在一起，这意味着在一个错误的API的应用程序可能会损坏其他应用程序或核心服务器。 恶意的API的应用程序代码甚至可以窃取另一个应用程序或核心服务器的密钥。 FastCGI技术目前支持语言有：C/C++、Java、Perl、Tcl、Python、SmallTalk、Ruby等。相关模块在Apache, ISS, Lighttpd等流行的服务器上也是可用的。 FastCGI的不依赖于任何Web服务器的内部架构，因此即使服务器技术的变化, FastCGI依然稳定不变。 FastCGI的工作原理 Web Server启动时载入FastCGI进程管理器（IIS ISAPI或Apache Module) FastCGI进程管理器自身初始化，启动多个CGI解释器进程(可见多个php-cgi)并等待来自Web Server的连接。 当客户端请求到达Web Server时，FastCGI进程管理器选择并连接到一个CGI解释器。Web server将CGI环境变量和标准输入发送到FastCGI子进程php-cgi。 FastCGI子进程完成处理后将标准输出和错误信息从同一连接返回Web Server。当FastCGI子进程关闭连接时，请求便告处理完成。FastCGI子进程接着等待并处理来自FastCGI进程管理器(运行在Web Server中)的下一个连接。 在CGI模式中，php-cgi在此便退出了。 在上述情况中，你可以想象CGI通常有多慢。每一个Web请求PHP都必须重新解析php.ini、重新载入全部扩展并重初始化全部数据结构。使用FastCGI，所有这些都只在进程启动时发生一次。一个额外的好处是，持续数据库连接(Persistent database connection)可以工作。 FastCGI的不足 因为是多进程，所以比CGI多线程消耗更多的服务器内存，PHP-CGI解释器每进程消耗7至25兆内存，将这个数字乘以50或100就是很大的内存数。 Nginx 0.8.46+PHP 5.2.14(FastCGI)服务器在3万并发连接下，开启的10个Nginx进程消耗150M内存（15M10=150M），开启的64个php-cgi进程消耗1280M内存（20M64=1280M），加上系统自身消耗的内存，总共消耗不到2GB内存。如果服务器内存较小，完全可以只开启25个php-cgi进程，这样php-cgi消耗的总内存数才500M。上面的数据摘自Nginx 0.8.x + PHP 5.2.13(FastCGI)搭建胜过Apache十倍的Web服务器(第6版) php-cgi php-cgi是php提供给web serve也就是http前端服务器的cgi协议接口程序，当每次接到http前端服务器的请求都会开启一个php-cgi进程进行处理，而且开启的php-cgi的过程中会先要重载配置，数据结构以及初始化运行环境，如果更新了php配置，那么就需要重启php-cgi才能生效，例如phpstudy就是这种情况。 PHP-CGIPHP-CGI是PHP自带的FastCGI管理器。 PHP-CGI的不足： php-cgi变更php.ini配置后需重启php-cgi才能让新的php-ini生效，不可以平滑重启。 直接杀死php-cgi进程，php就不能运行了。(PHP-FPM和Spawn-FCGI就没有这个问题，守护进程会平滑从新生成新的子进程。） PHP-FPMPHP-FPM是一个PHP FastCGI管理器（FastCGI Process Manager），是只用于PHP的，可以在http://php-fpm.org/download下载得到。PHP-FPM其实是PHP源代码的一个补丁，旨在将FastCGI进程管理整合进PHP包中。必须将它patch到你的PHP源代码中，在编译安装PHP后才可以使用。PHP5.3.3已经集成php-fpm了，不再是第三方的包了。PHP-FPM提供了更好的PHP进程管理方式，可以有效控制内存和进程、可以平滑重载PHP配置，比spawn-fcgi具有更多有点，所以被PHP官方收录了。在./configure的时候带 –enable-fpm参数即可开启PHP-FPM。 Spawn-FCGISpawn-FCGI是一个通用的FastCGI管理服务器，它是lighttpd中的一部份，很多人都用Lighttpd的Spawn-FCGI进行FastCGI模式下的管理工作。 总结 fastCGI是nginx和php之间的一个通信接口，该接口实际处理过程通过启动php-fpm进程来解析php脚本，即php-fpm相当于一个动态应用服务器，从而实现nginx动态解析php。因此，如果nginx服务器需要支持php解析，需要在nginx.conf中增加php的配置：将php脚本转发到fastCGI进程监听的IP地址和端口（php-fpm.conf中指定）。同时，php安装的时候，需要开启支持fastCGI选项，并且编译安装php-fpm补丁/扩展，同时，需要启动php-fpm进程，才可以解析nginx通过fastCGI转发过来的php脚本。 php-fpm是一个完全独立的程序,不依赖php-cgi,也不依赖php.因为php-fpm是一个内置了php解释器的FastCGI服务,启动时能够自行读取php.ini配置和php-fpm.conf配置. 附:PHP FastCGI进程管理器PHP-FPM的架构一个master进程,支持多个pool,每个pool由master进程监听不同的端口,pool中有多个worker进程.每个worker进程都内置PHP解释器,并且进程常驻后台,支持prefork动态增加.每个worker进程支持在运行时编译脚本并在内存中缓存生成的opcode来提升性能.每个worker进程支持配置响应指定请求数后自动重启,master进程会重启挂掉的worker进程.每个worker进程能保持一个到MySQL/Memcached/Redis的持久连接,实现”连接池”,避免重复建立连接,对程序透明.master进程采用epoll模型异步接收和分发请求,listen监听端口,epoll_wait等待连接,然后分发给对应pool里的worker进程,worker进程accpet请求后poll处理连接,如果worker进程不够用,master进程会prefork更多进程,如果prefork达到了pm.max_children上限,worker进程又全都繁忙,这时master进程会把请求挂起到连接队列backlog里(默认值是511). web server（比如说nginx）只是内容的分发者。比如，如果请求/index.html，那么web server会去文件系统中找到这个文件，发送给浏览器，这里分发的是静态数据。好了，如果现在请求的是/index.php，根据配置文件，nginx知道这个不是静态文件，需要去找PHP解析器来处理，那么他会把这个请求简单处理后交给PHP解析器。Nginx会传哪些数据给PHP解析器呢？url要有吧，查询字符串也得有吧，POST数据也要有，HTTP header不能少吧，好的，CGI就是规定要传哪些数据、以什么样的格式传递给后方处理这个请求的协议。仔细想想，你在PHP代码中使用的用户从哪里来的。 当web server收到/index.php这个请求后，会启动对应的CGI程序，这里就是PHP的解析器。接下来PHP解析器会解析php.ini文件，初始化执行环境，然后处理请求，再以规定CGI规定的格式返回处理后的结果，退出进程。web server再把结果返回给浏览器。 好了，CGI是个协议，跟进程什么的没关系。那fastcgi又是什么呢？Fastcgi是用来提高CGI程序性能的。 提高性能，那么CGI程序的性能问题在哪呢？”PHP解析器会解析php.ini文件，初始化执行环境”，就是这里了。标准的CGI对每个请求都会执行这些步骤（不闲累啊！启动进程很累的说！），所以处理每个时间的时间会比较长。这明显不合理嘛！那么Fastcgi是怎么做的呢？首先，Fastcgi会先启一个master，解析配置文件，初始化执行环境，然后再启动多个worker。当请求过来时，master会传递给一个worker，然后立即可以接受下一个请求。这样就避免了重复的劳动，效率自然是高。而且当worker不够用时，master可以根据配置预先启动几个worker等着；当然空闲worker太多时，也会停掉一些，这样就提高了性能，也节约了资源。这就是fastcgi的对进程的管理。 那PHP-FPM又是什么呢？是一个实现了Fastcgi的程序，被PHP官方收了。 大家都知道，PHP的解释器是php-cgi。php-cgi只是个CGI程序，他自己本身只能解析请求，返回结果，不会进程管理（皇上，臣妾真的做不到啊！）所以就出现了一些能够调度php-cgi进程的程序，比如说由lighthttpd分离出来的spawn-fcgi。好了PHP-FPM也是这么个东东，在长时间的发展后，逐渐得到了大家的认可（要知道，前几年大家可是抱怨PHP-FPM稳定性太差的），也越来越流行。 fastcgi是一个协议，php-fpm实现了这个协议 php-fpm的 管理对象 是php-cgi。但不能说php-fpm是fastcgi进程的管理器，因为前面说了fastcgi是个协议， 以前php-fpm没有包含在PHP内核里面，要使用这个功能，需要找到与源码版本相同的php-fpm对内核打补丁，然后再编译。 后来PHP内核集成了PHP-FPM之后就方便多了，使用–enalbe-fpm这个编译参数即可。 有的说，修改了php.ini配置文件后，没办法 平滑重启，所以就诞生了php-fpm 是的，修改php.ini之后，php-cgi进程的确是没办法平滑重启的。php-fpm对此的处理机制是新的worker用新的配置，已经存在的worker处理完手上的活就可以歇着了，通过这种机制来平滑过度。 答疑解惑网上有的说，fastcgi是一个协议，php-fpm实现了这个协议； 有的说，php-fpm是fastcgi进程的管理器，用来管理fastcgi进程的； 有的说，php-fpm是php内核的一个补丁; 有的说，修改了php.ini配置文件后，没办法平滑重启，所以就诞生了php-fpm； 还有的说PHP-CGI是PHP自带的FastCGI管理器，那这样的话干吗又弄个php-fpm出来，我就更晕了； 答：首先，CGI是干嘛的？CGI是为了保证web server传递过来的数据是标准格式的，方便CGI程序的编写者。 web server（比如说nginx）只是内容的分发者。比如，如果请求/index.html，那么web server会去文件系统中找到这个文件，发送给浏览器，这里分发的是静态数据。好了，如果现在请求的是/index.php，根据配置文件，nginx知道这个不是静态文件，需要去找PHP解析器来处理，那么他会把这个请求简单处理后交给PHP解析器。Nginx会传哪些数据给PHP解析器呢？url要有吧，查询字符串也得有吧，POST数据也要有，HTTP header不能少吧，好的，CGI就是规定要传哪些数据、以什么样的格式传递给后方处理这个请求的协议。仔细想想，你在PHP代码中使用的用户从哪里来的。 当web server收到/index.php这个请求后，会启动对应的CGI程序，这里就是PHP的解析器。接下来PHP解析器会解析php.ini文件，初始化执行环境，然后处理请求，再以规定CGI规定的格式返回处理后的结果，退出进程。web server再把结果返回给浏览器。 好了，CGI是个协议，跟进程什么的没关系。那fastcgi又是什么呢？Fastcgi是用来提高CGI程序性能的。 提高性能，那么CGI程序的性能问题在哪呢？”PHP解析器会解析php.ini文件，初始化执行环境”，就是这里了。标准的CGI对每个请求都会执行这些步骤（不闲累啊！启动进程很累的说！），所以处理每个时间的时间会比较长。这明显不合理嘛！那么Fastcgi是怎么做的呢？首先，Fastcgi会先启一个master，解析配置文件，初始化执行环境，然后再启动多个worker。当请求过来时，master会传递给一个worker，然后立即可以接受下一个请求。这样就避免了重复的劳动，效率自然是高。而且当worker不够用时，master可以根据配置预先启动几个worker等着；当然空闲worker太多时，也会停掉一些，这样就提高了性能，也节约了资源。这就是fastcgi的对进程的管理。 那PHP-FPM又是什么呢？是一个实现了Fastcgi的程序，被PHP官方收了。 大家都知道，PHP的解释器是php-cgi。php-cgi只是个CGI程序，他自己本身只能解析请求，返回结果，不会进程管理（皇上，臣妾真的做不到啊！）所以就出现了一些能够调度php-cgi进程的程序，比如说由lighthttpd分离出来的spawn-fcgi。好了PHP-FPM也是这么个东东，在长时间的发展后，逐渐得到了大家的认可（要知道，前几年大家可是抱怨PHP-FPM稳定性太差的），也越来越流行。 下面是对上面的问题进行的回答： 好了，最后来回来你的问题。 网上有的说，fastcgi是一个协议，php-fpm实现了这个协议对。 有的说，php-fpm是fastcgi进程的管理器，用来管理fastcgi进程的对。php-fpm的管理对象是php-cgi。但不能说php-fpm是fastcgi进程的管理器，因为前面说了fastcgi是个协议，似乎没有这么个进程存在，就算存在php-fpm也管理不了他（至少目前是）。 有的说，php-fpm是php内核的一个补丁以前是对的。因为最开始的时候php-fpm没有包含在PHP内核里面，要使用这个功能，需要找到与源码版本相同的php-fpm对内核打补丁，然后再编译。后来PHP内核集成了PHP-FPM之后就方便多了，使用–enalbe-fpm这个编译参数即可。 有的说，修改了php.ini配置文件后，没办法平滑重启，所以就诞生了php-fpm是的，修改php.ini之后，php-cgi进程的确是没办法平滑重启的。php-fpm对此的处理机制是新的worker用新的配置，已经存在的worker处理完手上的活就可以歇着了，通过这种机制来平滑过度。 还有的说PHP-CGI是PHP自带的FastCGI管理器，那这样的话干吗又弄个php-fpm不对。php-cgi只是解释PHP脚本的程序而已。]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>PHP原理</tag>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP底层的运行机制与原理【摘】]]></title>
    <url>%2F2017%2F12%2F11%2FPHP%E5%BA%95%E5%B1%82%E7%9A%84%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%8E%9F%E7%90%86%E3%80%90%E6%91%98%E3%80%91%2F</url>
    <content type="text"><![CDATA[PHP简单运行过程： 我们从未手动开启过PHP的相关进程，它是随着Apache的启动而运行的； PHP通过mod_php5.so模块和Apache相连（具体说来是SAPI，即服务器应用程序编程接口）； PHP总共有三个模块：内核、Zend引擎、以及扩展层； PHP内核用来处理请求、文件流、错误处理等相关操作； Zend引擎（ZE）用以将源文件转换成机器语言，然后在虚拟机上运行它； 扩展层是一组函数、类库和流，PHP使用它们来执行一些特定的操作。比如，我们需要mysql扩展来连接MySQL数据库； 当ZE执行程序时可能会需要连接若干扩展，这时ZE将控制权交给扩展，等处理完特定任务后再返还； 最后，ZE将程序运行结果返回给PHP内核，它再将结果传送给SAPI层，最终输出到浏览器上。 PHP的执行流程&amp;opcode 我们先来看看PHP代码的执行所经过的流程。 从图上可以看到，PHP实现了一个典型的动态语言执行过程：拿到一段代码后，经过词法解析、语法解析等阶段后，源程序会被翻译成一个个指令(opcodes)，然后ZEND虚拟机顺次执行这些指令完成操作。PHP本身是用C实现的，因此最终调用的也都是C的函数，实际上，我们可以把PHP看做是一个C开发的软件。 PHP的执行的核心是翻译出来的一条一条指令，也即opcode。 Opcode是PHP程序执行的最基本单位。一个opcode由两个参数(op1,op2)、返回值和处理函数组成。PHP程序最终被翻译为一组opcode处理函数的顺序执行。 PHP的四层体系PHP的核心架构如下图： 从图上可以看出，PHP从下到上是一个4层体系： Zend引擎：Zend整体用纯C实现，是PHP的内核部分，它将PHP代码翻译（词法、语法解析等一系列编译过程）为可执行opcode的处理并实现相应的处理方法、实现了基本的数据结构（如hashtable、oo）、内存分配及管理、提供了相应的api方法供外部调用，是一切的核心，所有的外围功能均围绕Zend实现。 Extensions：围绕着Zend引擎，extensions通过组件式的方式提供各种基础服务，我们常见的各种内置函数（如array系列）、标准库等都是通过extension来实现，用户也可以根据需要实现自己的extension以达到功能扩展、性能优化等目的（如贴吧正在使用的PHP中间层、富文本解析就是extension的典型应用）。 Sapi：Sapi全称是Server Application Programming Interface，也就是服务端应用编程接口，Sapi通过一系列钩子函数，使得PHP可以和外围交互数据，这是PHP非常优雅和成功的一个设计，通过sapi成功的将PHP本身和上层应用解耦隔离，PHP可以不再考虑如何针对不同应用进行兼容，而应用本身也可以针对自己的特点实现不同的处理方式。 上层应用：这就是我们平时编写的PHP程序，通过不同的sapi方式得到各种各样的应用模式，如通过webserver实现web应用、在命令行下以脚本方式运行等等。 构架思想：引擎(Zend)+组件(ext)的模式降低内部耦合中间层(sapi)隔绝web server和PHP 如果php是一辆车，那么 车的框架就是php本身 Zend是车的引擎（发动机） Ext下面的各种组件就是车的轮子 Sapi可以看做是公路，车可以跑在不同类型的公路上 而一次php程序的执行就是汽车跑在公路上。 因此，我们需要：性能优异的引擎+合适的车轮+正确的跑道 Sapi 如前所述，Sapi通过通过一系列的接口，使得外部应用可以和PHP交换数据并可以根据不同应用特点实现特定的处理方法，我们常见的一些sapi有： apache2handler：这是以apache作为webserver，采用mod_PHP模式运行时候的处理方式，也是现在应用最广泛的一种。 cgi：这是webserver和PHP直接的另一种交互方式，也就是大名鼎鼎的fastcgi协议，在最近今年fastcgi+PHP得到越来越多的应用，也是异步webserver所唯一支持的方式。 cli：命令行调用的应用模式 LAMP架构从下往上四层： liunx 属于操作系统的底层 apache服务器，属于次服务器，沟通linux和PHP php:属于服务端编程语言，通过php_module 模块 和apache关联 mysql和其他web服务：属于应用服务，通过PHP的Extensions外 挂模块和mysql关联 参考文章：PHP的执行原理/执行流程PHP底层的运行机制与原理]]></content>
      <categories>
        <category>PHP</category>
        <category>PHP原理</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>PHP原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库导入导出命令]]></title>
    <url>%2F2017%2F12%2F11%2FMySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[导出（mysqldump）1、只导出结构，不导出数据 #mysqldump 数据库名 -u root -p -d &gt; xxx.sql 2、只导出数据，不导出结构 #mysqldump 数据库名 -uroot -p -t &gt; xxx.sql 3、既导出数据，也导出结构 #mysqldump 数据库名 -uroot -p &gt; xxx.sql 4、导出指定表 #mysqldump -uroot -p 数据库名 –table 表名 &gt; xxx.sql 5、导出数据库，忽略其中的某些表 #mysqldump -uroot -p 数据库名 –ignore-table 数据库名.表名1 –ignore-table 数据库名.表名2…. &gt; xxx.sql 导入数据由于mysqldump导出的是完整的SQL语句，所以用mysql客户程序很容易就能把数据导入了： #mysql -hlocalhost -P3306 -uroot -p 数据库名 &lt; 文件名 #source /tmp/xxx.sql 备份数据库#mysqldump 数据库名 &gt;数据库备份名 #mysqldump -A -u用户名 -p密码 数据库名&gt;数据库备份名 #mysqldump -d -A –add-drop-table -uroot -p &gt;xxx.sql Mysqldump参数大全（参数来源于mysql5.5.19源码）参数参数说明 –all-databases , -A 导出全部数据库。 mysqldump -uroot -p –all-databases –all-tablespaces , -Y 导出全部表空间。 mysqldump -uroot -p –all-databases –all-tablespaces –no-tablespaces , -y 不导出任何表空间信息。 mysqldump -uroot -p –all-databases –no-tablespaces –add-drop-database 每个数据库创建之前添加drop数据库语句。 mysqldump -uroot -p –all-databases –add-drop-database –add-drop-table 每个数据表创建之前添加drop数据表语句。(默认为打开状态，使用–skip-add-drop-table取消选项) mysqldump -uroot -p –all-databases (默认添加drop语句) mysqldump -uroot -p –all-databases –skip-add-drop-table (取消drop语句) –add-locks 在每个表导出之前增加LOCK TABLES并且之后UNLOCK TABLE。(默认为打开状态，使用–skip-add-locks取消选项) mysqldump -uroot -p –all-databases (默认添加LOCK语句) mysqldump -uroot -p –all-databases –skip-add-locks (取消LOCK语句) –allow-keywords 允许创建是关键词的列名字。这由表名前缀于每个列名做到。 mysqldump -uroot -p –all-databases –allow-keywords –apply-slave-statements 在’CHANGE MASTER’前添加’STOP SLAVE’，并且在导出的最后添加’START SLAVE’。 mysqldump -uroot -p –all-databases –apply-slave-statements –character-sets-dir 字符集文件的目录 mysqldump -uroot -p –all-databases –character-sets-dir=/usr/local/mysql/share/mysql/charsets –comments 附加注释信息。默认为打开，可以用–skip-comments取消 mysqldump -uroot -p –all-databases (默认记录注释) mysqldump -uroot -p –all-databases –skip-comments (取消注释) –compatible 导出的数据将和其它数据库或旧版本的MySQL 相兼容。值可以为ansi、mysql323、mysql40、postgresql、oracle、mssql、db2、maxdb、no_key_options、no_tables_options、no_field_options等， 要使用几个值，用逗号将它们隔开。它并不保证能完全兼容，而是尽量兼容。 mysqldump -uroot -p –all-databases –compatible=ansi –compact 导出更少的输出信息(用于调试)。去掉注释和头尾等结构。可以使用选项：–skip-add-drop-table –skip-add-locks –skip-comments –skip-disable-keys mysqldump -uroot -p –all-databases –compact –complete-insert, -c 使用完整的insert语句(包含列名称)。这么做能提高插入效率，但是可能会受到max_allowed_packet参数的影响而导致插入失败。 mysqldump -uroot -p –all-databases –complete-insert –compress, -C 在客户端和服务器之间启用压缩传递所有信息 mysqldump -uroot -p –all-databases –compress –create-options, -a 在CREATE TABLE语句中包括所有MySQL特性选项。(默认为打开状态) mysqldump -uroot -p –all-databases –databases, -B 导出几个数据库。参数后面所有名字参量都被看作数据库名。 mysqldump -uroot -p –databases test mysql –debug 输出debug信息，用于调试。默认值为：d:t:o,/tmp/mysqldump.trace mysqldump -uroot -p –all-databases –debug mysqldump -uroot -p –all-databases –debug=” d:t:o,/tmp/debug.trace” –debug-check 检查内存和打开文件使用说明并退出。 mysqldump -uroot -p –all-databases –debug-check –debug-info 输出调试信息并退出 mysqldump -uroot -p –all-databases –debug-info –default-character-set 设置默认字符集，默认值为utf8 mysqldump -uroot -p –all-databases –default-character-set=latin1 –delayed-insert 采用延时插入方式（INSERT DELAYED）导出数据 mysqldump -uroot -p –all-databases –delayed-insert –delete-master-logs master备份后删除日志. 这个参数将自动激活–master-data。 mysqldump -uroot -p –all-databases –delete-master-logs –disable-keys 对于每个表，用/!40000 ALTER TABLE tbl_name DISABLE KEYS /;和/!40000 ALTER TABLE tbl_name ENABLE KEYS /;语句引用INSERT语句。这样可以更快地导入dump出来的文件，因为它是在插入所有行后创建索引的。该选项只适合MyISAM表，默认为打开状态。 mysqldump -uroot -p –all-databases –dump-slave 该选项将导致主的binlog位置和文件名追加到导出数据的文件中。设置为1时，将会以CHANGE MASTER命令输出到数据文件；设置为2时，在命令前增加说明信息。该选项将会打开–lock-all-tables，除非–single-transaction被指定。该选项会自动关闭–lock-tables选项。默认值为0。 mysqldump -uroot -p –all-databases –dump-slave=1 mysqldump -uroot -p –all-databases –dump-slave=2 –events, -E 导出事件。 mysqldump -uroot -p –all-databases –events –extended-insert, -e 使用具有多个VALUES列的INSERT语法。这样使导出文件更小，并加速导入时的速度。默认为打开状态，使用–skip-extended-insert取消选项。 mysqldump -uroot -p –all-databases mysqldump -uroot -p –all-databases–skip-extended-insert (取消选项) –fields-terminated-by 导出文件中忽略给定字段。与–tab选项一起使用，不能用于–databases和–all-databases选项 mysqldump -uroot -p test test –tab=”/home/mysql” –fields-terminated-by=”#” –fields-enclosed-by 输出文件中的各个字段用给定字符包裹。与–tab选项一起使用，不能用于–databases和–all-databases选项 mysqldump -uroot -p test test –tab=”/home/mysql” –fields-enclosed-by=”#” –fields-optionally-enclosed-by 输出文件中的各个字段用给定字符选择性包裹。与–tab选项一起使用，不能用于–databases和–all-databases选项 mysqldump -uroot -p test test –tab=”/home/mysql” –fields-enclosed-by=”#” –fields-optionally-enclosed-by =”#” –fields-escaped-by 输出文件中的各个字段忽略给定字符。与–tab选项一起使用，不能用于–databases和–all-databases选项 mysqldump -uroot -p mysql user –tab=”/home/mysql” –fields-escaped-by=”#” –flush-logs 开始导出之前刷新日志。 请注意：假如一次导出多个数据库(使用选项–databases或者–all-databases)，将会逐个数据库刷新日志。除使用–lock-all-tables或者–master-data外。在这种情况下，日志将会被刷新一次，相应的所以表同时被锁定。因此，如果打算同时导出和刷新日志应该使用–lock-all-tables 或者–master-data 和–flush-logs。 mysqldump -uroot -p –all-databases –flush-logs –flush-privileges 在导出mysql数据库之后，发出一条FLUSH PRIVILEGES 语句。为了正确恢复，该选项应该用于导出mysql数据库和依赖mysql数据库数据的任何时候。 mysqldump -uroot -p –all-databases –flush-privileges –force 在导出过程中忽略出现的SQL错误。 mysqldump -uroot -p –all-databases –force –help 显示帮助信息并退出。 mysqldump –help –hex-blob 使用十六进制格式导出二进制字符串字段。如果有二进制数据就必须使用该选项。影响到的字段类型有BINARY、VARBINARY、BLOB。 mysqldump -uroot -p –all-databases –hex-blob –host, -h 需要导出的主机信息 mysqldump -uroot -p –host=localhost –all-databases –ignore-table 不导出指定表。指定忽略多个表时，需要重复多次，每次一个表。每个表必须同时指定数据库和表名。例如：–ignore-table=database.table1 –ignore-table=database.table2 …… mysqldump -uroot -p –host=localhost –all-databases –ignore-table=mysql.user –include-master-host-port 在–dump-slave产生的’CHANGE MASTER TO..’语句中增加’MASTER_HOST=，MASTER_PORT=‘ mysqldump -uroot -p –host=localhost –all-databases –include-master-host-port –insert-ignore 在插入行时使用INSERT IGNORE语句. mysqldump -uroot -p –host=localhost –all-databases –insert-ignore –lines-terminated-by 输出文件的每行用给定字符串划分。与–tab选项一起使用，不能用于–databases和–all-databases选项。 mysqldump -uroot -p –host=localhost test test –tab=”/tmp/mysql” –lines-terminated-by=”##” –lock-all-tables, -x 提交请求锁定所有数据库中的所有表，以保证数据的一致性。这是一个全局读锁，并且自动关闭–single-transaction 和–lock-tables 选项。 mysqldump -uroot -p –host=localhost –all-databases –lock-all-tables –lock-tables, -l 开始导出前，锁定所有表。用READ LOCAL锁定表以允许MyISAM表并行插入。对于支持事务的表例如InnoDB和BDB，–single-transaction是一个更好的选择，因为它根本不需要锁定表。 请注意当导出多个数据库时，–lock-tables分别为每个数据库锁定表。因此，该选项不能保证导出文件中的表在数据库之间的逻辑一致性。不同数据库表的导出状态可以完全不同。 mysqldump -uroot -p –host=localhost –all-databases –lock-tables –log-error 附加警告和错误信息到给定文件 mysqldump -uroot -p –host=localhost –all-databases –log-error=/tmp/mysqldump_error_log.err –master-data 该选项将binlog的位置和文件名追加到输出文件中。如果为1，将会输出CHANGE MASTER 命令；如果为2，输出的CHANGE MASTER命令前添加注释信息。该选项将打开–lock-all-tables 选项，除非–single-transaction也被指定（在这种情况下，全局读锁在开始导出时获得很短的时间；其他内容参考下面的–single-transaction选项）。该选项自动关闭–lock-tables选项。 mysqldump -uroot -p –host=localhost –all-databases –master-data=1; mysqldump -uroot -p –host=localhost –all-databases –master-data=2; –max_allowed_packet 服务器发送和接受的最大包长度。 mysqldump -uroot -p –host=localhost –all-databases –max_allowed_packet=10240 –net_buffer_length TCP/IP和socket连接的缓存大小。 mysqldump -uroot -p –host=localhost –all-databases –net_buffer_length=1024 –no-autocommit 使用autocommit/commit 语句包裹表。 mysqldump -uroot -p –host=localhost –all-databases –no-autocommit –no-create-db, -n 只导出数据，而不添加CREATE DATABASE 语句。 mysqldump -uroot -p –host=localhost –all-databases –no-create-db –no-create-info, -t 只导出数据，而不添加CREATE TABLE 语句。 mysqldump -uroot -p –host=localhost –all-databases –no-create-info –no-data, -d 不导出任何数据，只导出数据库表结构。 mysqldump -uroot -p –host=localhost –all-databases –no-data –no-set-names, -N 等同于–skip-set-charset mysqldump -uroot -p –host=localhost –all-databases –no-set-names –opt 等同于–add-drop-table, –add-locks, –create-options, –quick, –extended-insert, –lock-tables, –set-charset, –disable-keys 该选项默认开启, 可以用–skip-opt禁用. mysqldump -uroot -p –host=localhost –all-databases –opt –order-by-primary 如果存在主键，或者第一个唯一键，对每个表的记录进行排序。在导出MyISAM表到InnoDB表时有效，但会使得导出工作花费很长时间。 mysqldump -uroot -p –host=localhost –all-databases –order-by-primary –password, -p 连接数据库密码 –pipe(windows系统可用) 使用命名管道连接mysql mysqldump -uroot -p –host=localhost –all-databases –pipe –port, -P 连接数据库端口号 –protocol 使用的连接协议，包括：tcp, socket, pipe, memory. mysqldump -uroot -p –host=localhost –all-databases –protocol=tcp –quick, -q 不缓冲查询，直接导出到标准输出。默认为打开状态，使用–skip-quick取消该选项。 mysqldump -uroot -p –host=localhost –all-databases mysqldump -uroot -p –host=localhost –all-databases –skip-quick –quote-names,-Q 使用（`）引起表和列名。默认为打开状态，使用–skip-quote-names取消该选项。 mysqldump -uroot -p –host=localhost –all-databases mysqldump -uroot -p –host=localhost –all-databases –skip-quote-names –replace 使用REPLACE INTO 取代INSERT INTO. mysqldump -uroot -p –host=localhost –all-databases –replace –result-file, -r 直接输出到指定文件中。该选项应该用在使用回车换行对（\r\n）换行的系统上（例如：DOS，Windows）。该选项确保只有一行被使用。 mysqldump -uroot -p –host=localhost –all-databases –result-file=/tmp/mysqldump_result_file.txt –routines, -R 导出存储过程以及自定义函数。 mysqldump -uroot -p –host=localhost –all-databases –routines –set-charset 添加’SET NAMES default_character_set’到输出文件。默认为打开状态，使用–skip-set-charset关闭选项。 mysqldump -uroot -p –host=localhost –all-databases mysqldump -uroot -p –host=localhost –all-databases –skip-set-charset –single-transaction 该选项在导出数据之前提交一个BEGIN SQL语句，BEGIN 不会阻塞任何应用程序且能保证导出时数据库的一致性状态。它只适用于多版本存储引擎，仅InnoDB。本选项和–lock-tables 选项是互斥的，因为LOCK TABLES 会使任何挂起的事务隐含提交。要想导出大表的话，应结合使用–quick 选项。 mysqldump -uroot -p –host=localhost –all-databases –single-transaction –dump-date 将导出时间添加到输出文件中。默认为打开状态，使用–skip-dump-date关闭选项。 mysqldump -uroot -p –host=localhost –all-databases mysqldump -uroot -p –host=localhost –all-databases –skip-dump-date –skip-opt 禁用–opt选项. mysqldump -uroot -p –host=localhost –all-databases –skip-opt –socket,-S 指定连接mysql的socket文件位置，默认路径/tmp/mysql.sock mysqldump -uroot -p –host=localhost –all-databases –socket=/tmp/mysqld.sock –tab,-T 为每个表在给定路径创建tab分割的文本文件。注意：仅仅用于mysqldump和mysqld服务器运行在相同机器上。 mysqldump -uroot -p –host=localhost test test –tab=”/home/mysql” –tables 覆盖–databases (-B)参数，指定需要导出的表名。 mysqldump -uroot -p –host=localhost –databases test –tables test –triggers 导出触发器。该选项默认启用，用–skip-triggers禁用它。 mysqldump -uroot -p –host=localhost –all-databases –triggers –tz-utc 在导出顶部设置时区TIME_ZONE=’+00:00’ ，以保证在不同时区导出的TIMESTAMP 数据或者数据被移动其他时区时的正确性。 mysqldump -uroot -p –host=localhost –all-databases –tz-utc –user, -u 指定连接的用户名。 –verbose, –v 输出多种平台信息。 –version, -V 输出mysqldump版本信息并退出 –where, -w 只转储给定的WHERE条件选择的记录。请注意如果条件包含命令解释符专用空格或字符，一定要将条件引用起来。 mysqldump -uroot -p –host=localhost –all-databases –where=” user=’root’” –xml, -X 导出XML格式. mysqldump -uroot -p –host=localhost –all-databases –xml –plugin_dir 客户端插件的目录，用于兼容不同的插件版本。 mysqldump -uroot -p –host=localhost –all-databases –plugin_dir=”/usr/local/lib/plugin” –default_auth 客户端插件默认使用权限。 mysqldump -uroot -p –host=localhost –all-databases –default-auth=”/usr/local/lib/plugin/”]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell基础知识手册]]></title>
    <url>%2F2017%2F12%2F11%2FShell%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[SHELL脚本具备的内容：（1）脚本声明（#!） 用来告诉系统使用哪种shell解释器来执行该脚本。（2）注释信息（#）是对脚本功能和某些命令的介绍信息（3）命令 脚本执行什么命令 变量1、变量命名 1. 只能使用数字、字母和下划线，且不能以数字开头 2. 变量名区分大小写 3. 建议命令要通俗易懂 注意： 变量不需要声明，初始化不需要指定类型 变量赋值是通过等号（=）进行赋值，在变量、等号和值之间不能出现空格 2、变量分类 1. 本地变量 * 只对当前shell进程有效的，对当前进程的子进程和其它shell进程无效。 * 定义：VAR_NAME=VALUE * 变量引用：${VAR_NAME} 或者 $VAR_NAME * 取消变量：unset VAR_NAME * 相当于java中的私有变量(private)，只能当前类使用，子类和其他类都无法使用。 2. 环境变量 * 自定义的环境变量对当前shell进程及其子shell进程有效，对其它的shell进程无效 * 定义：export VAR_NAME=VALUE * 对所有shell进程都有效需要配置到配置文件中：/etc/profile * 相当于java中的protected修饰符,对当前类，子孙类，以及同一个包下面可以共用。 _（和windows中的环境变量比较类似）_ 3. 局部变量 * 在函数中调用，函数执行结束，变量就会消失 * 对shell脚本中某代码片段有效 * 定义：local VAR_NAME=VALUE * 相当于java代码中某一个方法中定义的局部变量，只对这个方法有效。 4. 位置变量 比如脚本中的参数： $0：脚本自身 $1：脚本的第一个参数 $2：脚本的第二个参数 相当于java中main函数中的args参数，可以获取外部参数。 5. 特殊变量 $?：接收上一条命令的返回状态码 _（返回状态码在0-255之间）_ $#：参数个数 $*：或者$@：所有的参数 $$：获取当前shell的进程号（PID）(可以实现脚本自杀)(或者使用exit命令直接退出也可以使用exit [num]) 运算符测试语句格式： [ 条件表达式 ] （注：两边均有一个空格）按照测试对象来划分，条件测试语句可以分为4种：1、文件测试语句 运算符 作用 -d 测试文件是否为目录类型 -e 测试文件是否存在 -f 判断是否为一般文件 -r 测试当前用户是否有权限读取 -w 测试当前用户是否有权限写入 -x 测试当前用户是否有权限执行 -s 测试文件是否存在并且是否为空 -S 测试文件是否是个socket 2、逻辑测试语句 运算符 作用 &amp;&amp; 逻辑与 &#124;&#124; 逻辑或 ！ 逻辑非 3、整数值比较语句整数比较运算符仅对数字的操作，不能将数字与字符串、文件等内容一起操作，而且不能使用日常生活中的等号、大于号、小于号来判断。因为等号与赋值命令符冲突，大于号和小于号分别于输出重定向命令符和输入重定向命令符冲突。 运算符 作用 -eq 是否等于 -ne 是否不等于 -gt 是否大于 -lt 是否小于 -le 是否等于或小于 -ge 是否大于或等于 4、字符串比较语句字符串比较语句用于判断测试字符串是否为空值，或两个字符串是否相同。 运算符 作用 = 比较字符串内容是否相同 != 比较字符串内容是否不同 -z 判断字符串内容是否为空 运算符常用操作1、算术运算符 格式： let varName=算术表达式 varName=$[算术表达式] varName=$((算术表达式)) varName=\`expr $num1 + $num2\` （使用这种格式要注意两个数字和+号中间要有空格。） 2、逻辑运算符 if [ 条件A &amp;&amp; 条件B ] 在shell中怎么写? if [ 条件A &amp;&amp; 条件B ];then 是不对的（1）需要用到shell中的逻辑操作符 -a 与 -o 或 ！ 非 如if [ 条件A -a 条件B ]（2）if [ 条件A ] &amp;&amp; [条件B ]（3）if((A&amp;&amp;B))（4）if [[ A&amp;&amp;B ]] 字符串操作比较两个字符串的大小&lt; 小于，在ASCII字母顺序下，如： if [[ “$a” &lt; “$b” ]] if [ “$a” \&lt; “$b” ] 大于，在ASCII字母顺序下，如： if [[ “$a” &gt; “$b” ]] if [ “$a” > “$b” ]注意：在[]结构中”&gt;”需要被转义。 数组操作定义： declare -a：表示定义普通数组 特点： 1. 支持稀疏格式 2. 仅支持一维数组 数组赋值方式： 1. 一次对一个元素赋值a[0]=$RANDOM 2. 一次对多个元素赋值a=(a b c d) 3. 按索引进行赋值a=([0]=a [3]=b [1]=c) 数组操作 1. 使用read命令read -a ARRAY_NAME查看元素 ${ARRAY[index]}：查看数组指定角标的元素 ${ARRAY}：查看数组的第一个元素 ${ARRAY[*]}或者${ARRAY[@]}：查看数组的所有元素 2. 获取数组的长度： ${#ARRAY[*]} ${#ARRAY[@]} 3. 获取数组内元素的长度： ${#ARRAY[0]} 4. 从数组中获取某一片段之内的元素（操作类似于字符串操作） 格式： ${ARRAY[@]:offset:length} 注释： offset：偏移的元素个数 length：取出的元素的个数 ${ARRAY[@]:offset:length}：取出偏移量后的指定个数的元素 ${ARRAY[@]:offset}：取出数组中偏移量后的所有元素 5. 数组删除元素： unset ARRAY[index] 流程控制if判断 单分支： if 测试条件;then 选择分支 fi 双分支： if 测试条件 then 选择分支1 else 选择分支2 fi 多分支： if 条件1; then 分支1 elif 条件2; then 分支2 elif 条件3; then 分支3 ... else 分支n fi Case判断（有多个测试条件时，case语句会使得语法结构更清晰） 格式： case 变量引用 in PATTERN1) 分支1 ;; PATTERN2) 分支2 ;; ... *) 分支n ;; esac 注释： PATTERN :类同于文件名通配机制，但支持使用|表示或者 a|b：a或者b *：匹配任意长度的任意字符 ?：匹配任意单个字符 [a-z]：指定范围内的任意单个字符 循环控制for循环 通过使用一个变量去遍历给定列表中的每个元素，在每次变量赋值时执行一次循环体，直至赋值完成所有元素退出循环（注意：for i in {0..9} 等于for i in {0..9..1} , 第三个参数为跨步。） 格式1： for ((i=0;i&lt;10;i++)) do ... Done 格式2： for i in 0 1 2 3 4 5 6 7 8 9 do ... Done 格式3： for i in {0..9} do ... done while循环 适用于循环次数未知，或不便用for直接生成较大的列表时 格式： while 测试条件 do 循环体 done 循环控制命令 循环控制命令——break break命令是在处理过程中跳出循环的一种简单方法，可以使用break命令退出任何类型的循环，包括while循环和for循环 循环控制命令——continue continue命令是一种提前停止循环内命令，而不完全终止循环的方法，这就需要在循环内设置shell不执行命令的条件 条件 bash条件测试 格式： test EXPR [ EXPR ]：注意中括号和表达式之间的空格 整型测试： -gt：大于： -lt：小于 -ge：大于等于 -le：小于等于 -eq：等于 -ne：不等于 例如[ $num1 -gt $num2 ]或者test $num1 -gt $num2 字符串测试： =：等于，例如判断变量是否为空 [ &quot;$str&quot; = &quot;&quot; ] 或者[ -z $str ] !=：不等于 函数格式： function 函数名(){ ... } 1. 引用自定义函数文件时，使用source func.sh 2. 有利于代码的重用性 3. 函数传递参数（可以使用类似于Java中的args，args[1]代表Shell中的$1） 4. 函数的返回值，只能是数字 5. 函数的调用： 函数名 特殊shell中执行命令：`命令`\$(命令) shell脚本中 整数运算一般通过let和expr这两个指令来实现。如： let &quot;s = $s + 1&quot; 或者 s=\expr $s + 1`（注：let的性能远超expr的。建议使用let） 常用于： 自加/自减操作：let no++、let no–、let no += 20`]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git Error: Failed to connect to github.com port 443: Timed out]]></title>
    <url>%2F2017%2F12%2F08%2FGit-Error-Failed-to-connect-to-github-com-port-443-Timed-out%2F</url>
    <content type="text"><![CDATA[错误信息： fatal: unable to access ‘https://github.com/bluce-ben/bluce-ben.github.io.git/&#39;: Failed to connect to github.com port 443: Timed outFATAL Something’s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlError: fatal: unable to access ‘https://github.com/bluce-ben/bluce-ben.github.io.git/&#39;: Failed to connect to github.com port 443: Timed out 依据错误可清晰知道是因为无法连接Git导致的超时错误。而我配置的Hexo+GitHub Pages中使用的Git仓库就是 HTTPS 协议的，所以是使用443端口，也由此我 hexo deploy 部署Git失败。 解决方案：在百度搜索到一个解决方案，现整理如下。1、测试git是否成功连接GitHub，使用： ssh -T git@github.com 连接失败：ssh: connect to host github.com port 22: Connection timed out连接成功：Hi bluce-ben! You’ve successfully authenticated, but GitHub does not provide shell access. 2、修改Git配置文件，首先找到git的安装目录，找到/etc/ssh/ssh_config文件 打开该文件并将下列配置添加末尾：123456Host github.comUser gitHostname ssh.github.comPreferredAuthentications publickeyIdentityFile ~/.ssh/id_rsaPort 443 保存修改。（注：如果保存失败提示没有权限，可用 Notepad++打开会提示使用管理员权限，点击确定即可。） 3、再次测试是否成功连接GitHub。成功后再次 hexo deploy 部署GitHub即可。]]></content>
      <categories>
        <category>版本控制</category>
        <category>Git</category>
        <category>Git问题</category>
        <category>Hexo</category>
        <category>Hexo问题</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Hexo</tag>
        <tag>Git问题</tag>
        <tag>Hexo问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总结-MySQL数据库设计]]></title>
    <url>%2F2017%2F12%2F08%2F%E6%80%BB%E7%BB%93-MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[一、什么是数据库设计１、数据库设计概念 数据库设计就是根据业务系统的具体需求，结合所选择的DBMS，为这个业务系统构造出最优的数据存储模型。并建立好数据库中表的结构，以及表与表之间的关联关系的过程。使之能有效的对应用系统中的数据进行存储，并可以高效的对已经存储的数据进行查询访问。２、设计对比 优良的设计 槽糕的设计 减少数据冗余 存在大量的数据冗余 避免数据维护异常 存在数据插入、更新、删除异常 节约存储空间(偶尔会用空间换时间) 浪费大量存储空间 高效的访问 访问数据低效 二、数据库设计的步骤1). 需求分析 需求分析主要完成的内容：业务系统中有哪些数据？这些数据又有哪些属性？数据和属性的各自特点有哪些？2). 逻辑设计 使用ER图设计工具对数据库进行ER图逻辑建模：首先将需求转化为数据库的逻辑模型，其次，通过ER图形式将逻辑模型描述展示出来，最后这个逻辑设计与所选择的DBMS无关，也就是说这个逻辑设计应该适合所有的DBMS。3). 物理设计 这个阶段是进入到与DBMS相关的阶段，因此首先需要选择DBMS，并且将第二步的逻辑模型转为物理模型。 这个阶段会涉及到数据库中建表，选择字段类型的问题。在MySQL中遵循以下原则： 列的数据类型一方面影响相应数据的存储空间的开销，另一方面也会影响数据查询性能。当一个列可以选择多种数据类型时，应该优先考虑数字类型，其次是日期或二进制，最后是字符串类型。对于相同级别的数据类型，在满足业务的情况下，应该优先选择占用空间小的数据类型。 mysql中部分字段空间占用情况：额外注意几点： 在对数据进行比较(where、join 、order by)操作时，同样的数据，字符串处理比数字处理更慢。 在MySql中，UTF-8占用3个字节。 4). 维护优化 针对新的需求建立新的数据库表(这里补充说明下，在进行数据库最初的设计过程中，不建议在表中预留不确定的字段，这并不能实现数据库良好的扩展性问题，它的代价与后期新加一个字段一样，甚至更大) 索引优化 大表拆分：拆分又分为两种方式 水平拆分：控制表的长度，即数据的行数。会将每个表中的数据量减少。 垂直拆分：数据库中的存储是以页为单位的，当每一行的宽度比较小时(列数比较少时)，每页中存储的内容就多，IO效率就高(数据中的数据是存放在磁盘上，每次IO的内容越多越好)。因此常常对于非常宽的表，进行表的垂直拆分。拆表后的数据量不应该发生变化，但是表的数量增加，每张表的宽度减少。 三、数据库中的几大范式第一范式(1NF) 概念：数据库表中的所有字段值都是不可分解的原子值，就说明该数据库表满足了第一范式。作用：确保每列保持原子性 （每个表中的字段不可再拆分）举例：本来正常情况下，设计地址，只需要一个字段，但是若在实际业务系统中需要访问其中的国家、省、市时，此时就应该将地址这个字段进行拆分才符合1NF。 第二范式(2NF) 概念：基于1NF基础之上，第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。此外：所有单关键字段的表都符合第二范式。作用：确保表中的每列都和主键相关（一个表中只能有一个主键） 第三范式(3NF) 概念：基于2NF基础上的，如果数据表中不存在非关键字段对任意候选关键字段的传递函数依赖则符合第三范式。作用：确保每列都和主键列直接相关,而不是间接相关 （表中字段都与主键直接相关） 四、高手的境界都是无招胜有招 为了设计出优良的数据库，我们需要遵循数据库的范式，但是有时候如果你设计的数据库完全遵循了这些范式，反而会降低你的某些性能。这里提几点： 数据库连接会带来一部分性能损失，因此有时候为了减少冗余，将数据存放在多张表中，往往会降低查询性能，而互联网的世界，读写比例大概是3:1甚至4:1 减少表与表之间的关联数量(减少了对磁盘的IO)，增加数据的读取效率。 反范式化一定要适度。凡事过度适得其反。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总结-MySQL常用函数]]></title>
    <url>%2F2017%2F12%2F08%2F%E6%80%BB%E7%BB%93-MySQL%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[一、聚合函数(常用于GROUP BY从句的SELECT查询中) 下面五个函数会忽略值为NULL的行 函数名 说明 AVG(col) 返回指定列的平均值 COUNT(col) 返回指定列中非NULL值/行的个数（当函数参数为星号*时不会忽略） MIN(col) 返回指定列的最小值 MAX(col) 返回指定列的最大值 SUM(col) 返回指定列的所有值之和 GROUP_CONCAT(col) 返回由属于一组的列值连接组合而成的结果 二、数学函数 函数名 说明 ABS(x) 返回x的绝对值 BIN(x) 返回x的二进制（OCT返回八进制，HEX返回十六进制） EXP(x) 返回值e（自然对数的底）的x次方 GREATEST(x1,x2,…,xn) 返回集合中最大的值 LEAST(x1,x2,…,xn) 返回集合中最小的值 LN(x) 返回x的自然对数 LOG(x,y) 返回x的以y为底的对数 MOD(x,y) 返回x/y的模（余数） PI() 返回pi的值（圆周率） RAND() 返回0到1内的随机值,可以通过提供一个参数(种子)使RAND()随机数生成器生成一个指定的值。 FLOOR(x) 返回小于x的最大整数值，（去掉小数取整） CEILING(x) 返回大于x的最小整数值，（进一取整） ROUND(x,y) 返回参数x的四舍五入的有y位小数的值，（四舍五入） TRUNCATE(x,y) 返回数字x截短为y位小数的结果 SIGN(x) 返回代表数字x的符号的值（正数返回1，负数返回-1，0返回0） SQRT(x) 返回一个数的平方根 三、字符串函数 函数名 说明 ASCII(char) 返回字符的ASCII码值 BIT_LENGTH(str) 返回字符串的比特长度 CONCAT(s1,s2…,sn) 将s1,s2…,sn连接成字符串 CONCAT_WS(sep,s1,s2…,sn) 将s1,s2…,sn连接成字符串，并用sep字符间隔 INSERT(str,x,y,instr) 将字符串str从第x位置开始，y个字符长的子串替换为字符串instr，返回结果 FIND_IN_SET(str,list) 分析逗号分隔的list列表，如果发现str，返回str在list中的位置 LCASE(str)或LOWER(str) 返回将字符串str中所有字符改变为小写后的结果 UCASE(str)或UPPER(str) 返回将字符串str中所有字符转变为大写后的结果 LEFT(str,x) 返回字符串str中最左边的x个字符 RIGHT(str,x) 返回字符串str中最右边的x个字符 LENGTH(str) 返回字符串str中的字符数 POSITION(substr,str) 返回子串substr在字符串str中第一次出现的位置 QUOTE(str) 用反斜杠转义str中的单引号 REPEAT(str,srchstr,rplcstr) 返回字符串str重复x次的结果 REVERSE(str) 返回颠倒字符串str的结果 LTRIM(str) 去掉字符串str开头的空格 RTRIM(str) 去掉字符串str尾部的空格 TRIM(str) 去除字符串首部和尾部的所有空格 四、日期和时间函数 函数名 说明 DATE_ADD(date,INTERVAL int keyword) 返回日期date加上间隔时间int的结果(int必须按照关键字进行格式化),如：SELECTDATE_ADD(CURRENT_DATE,INTERVAL 6 MONTH); DATE_SUB(date,INTERVAL int keyword) 返回日期date加上间隔时间int的结果(int必须按照关键字进行格式化),如：SELECTDATE_SUB(CURRENT_DATE,INTERVAL 6 MONTH); DATE_FORMAT(date,fmt) 依照指定的fmt格式格式化日期date值 FROM_UNIXTIME(ts,fmt) 根据指定的fmt格式，格式化UNIX时间戳ts MONTHNAME(date) 返回date的月份名(英语月份，如October) DAYNAME(date) 返回date的星期名(英语星期几，如Saturday) NOW() 返回当前的日期和时间 如：2016-10-08 18:57:39 CURDATE()或CURRENT_DATE() 返回当前的日期 CURTIME()或CURRENT_TIME() 返回当前的时间 QUARTER(date) 返回date在一年中的季度(1~4) WEEK(date) 返回日期date为一年中第几周(0~53) DAYOFYEAR(date) 返回date是一年的第几天(1~366) DAYOFMONTH(date) 返回date是一个月的第几天(1~31) DAYOFWEEK(date) 返回date所代表的一星期中的第几天(1~7) YEAR(date) 返回日期date的年份(1000~9999) MONTH(date) 返回date的月份值(1~12) DAY(date) 返回date的天数部分 HOUR(time) 返回time的小时值(0~23) MINUTE(time) 返回time的分钟值(0~59) SECOND(time) 返回time的秒值（0-59） DATE(datetime) 返回datetime的日期值 TIME(datetime) 返回datetime的时间值 五、加密函数 函数名 说明 AES_ENCRYPT(str,key) 返回用密钥key对字符串str利用高级加密标准算法加密后的结果，调用AES_ENCRYPT的结果是一个二进制字符串，以BLOB类型存储 AES_DECRYPT(str,key) 返回用密钥key对字符串str利用高级加密标准算法解密后的结果 DECODE(str,key) 使用key作为密钥解密加密字符串str ENCRYPT(str,salt) 使用UNIXcrypt()函数，用关键词salt(一个可以惟一确定口令的字符串，就像钥匙一样)加密字符串str ENCODE(str,key) 使用key作为密钥加密字符串str，调用ENCODE()的结果是一个二进制字符串，它以BLOB类型存储 MD5() 计算字符串str的MD5校验和 PASSWORD(str) 返回字符串str的加密版本，这个加密过程是不可逆转的，和UNIX密码加密过程使用不同的算法。 SHA() 计算字符串str的安全散列算法(SHA)校验和 六、格式化函数 函数名 说明 DATE_FORMAT(date,fmt) 依照字符串fmt格式化日期date值 FORMAT(x,y) 把x格式化为以逗号隔开的数字序列，y是结果的小数位数 INET_ATON(ip) 返回IP地址的数字表示 INET_NTOA(num) 返回数字所代表的IP地址 TIME_FORMAT(time,fmt) 依照字符串fmt格式化时间time值,其中最简单的是FORMAT()函数，它可以把大的数值格式化为以逗号间隔的易读的序列。 七、数据类型转换函数 CAST()函数，将一个值转换为指定的数据类型（类型有：BINARY,CHAR,DATE,TIME,DATETIME,SIGNED,UNSIGNED）SELECT CAST(NOW() AS SIGNED INTEGER),CURDATE()+0; 八、系统信息函数 函数名 说明 DATABASE() 返回当前数据库名 BENCHMARK(count,expr) 将表达式expr重复运行count次 CONNECTION_ID() 返回当前客户的连接ID FOUND_ROWS() 返回最后一个SELECT查询进行检索的总行数 USER()或SYSTEM_USER() 返回当前登陆用户名 VERSION() 返回MySQL服务器的版本 九、条件判断函数 函数名 说明 isnull(expr) 如expr为null，那么isnull()的返回值为1，否则返回值为0。 ifnull(expr1,expr2) 假如expr1不为NULL，则IFNULL()的返回值为expr1; 否则其返回值为expr2。IFNULL()的返回值是数字或是字符串，具体情况取决于其所使用的语境。 nullif(expr1,expr2) 如果expr1=expr2成立，那么返回值为NULL，否则返回值为expr1。 IF(expr1,expr2,expr3) 如果 expr1 是TRUE (expr1 &lt;&gt; 0 and expr1 &lt;&gt; NULL)，则 IF()的返回值为expr2; 否则返回值则为 expr3。IF() 的返回值为数字值或字符串值，具体情况视其所在语境而定。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总结-MySQL主从复制、读写分离配置详解]]></title>
    <url>%2F2017%2F12%2F08%2F%E6%80%BB%E7%BB%93-MySQL%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E3%80%81%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[MySQL主从同步的机制： Slave 上面的IO线程连接上 Master，并请求从指定日志文件的指定位置(或者从最开始的日志)之后的日志内容; Master 接收到来自 Slave 的 IO 线程的请求后，通过负责复制的 IO线程根据请求信息读取指定日志指定位置之后的日志信息，返回给 Slave 端的 IO线程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息在 Master 端的 Binary Log 文件的名称以及在 BinaryLog 中的位置; Slave 的 IO 线程接收到信息后，将接收到的日志内容依次写入到 Slave 端的RelayLog文件(MySQL-relay-bin.xxxxxx)的最末端，并将读取到的Master端的bin-log的文件名和位置记录到master-info文件中，以便在下一次读取的时候能够清楚的高速Master“我需要从某个bin-log的哪个位置开始往后的日志内容，请发给我” Slave 的 SQL 线程检测到 Relay Log 中新增加了内容后，会马上解析该 Log 文件中的内容成为在 Master端真实执行时候的那些可执行的 Query 语句，并在自身执行这些 Query。这样，实际上就是在 Master 端和 Slave端执行了同样的 Query，所以两端的数据是完全一样的。 MySQL主从同步的作用Ø 可以作为一种备份机制，相当于热备份Ø 可以用来做读写分离，均衡数据库负载 MySQL主从同步的步骤1、在Master MySQL上创建一个用户‘repl’，并允许其他Slave服务器可以通过远程访问Master，通过该用户读取二进制日志，实现数据同步。 mysql&gt;create user repl; //创建新用户//repl用户必须具有REPLICATION SLAVE权限，除此之外没有必要添加不必要的权限，密码为mysql。说明一下192.168.0.%，这个配置是指明repl用户所在服务器，这里%是通配符，表示192.168.0.0-192.168.0.255的Server都可以以repl用户登陆主服务器。当然你也可以指定固定Ip。mysql&gt; GRANT REPLICATION SLAVE ON . TO ‘repl‘@’192.168.0.%’ IDENTIFIED BY ‘mysql’; 2、主数据库服务器配置 进入主数据库服务器安装目录,打开my.ini,在文件末尾增加如下配置：123456789101112131415#数据库ID号， 为1时表示为Master,其中master_id必须为1到232–1之间的一个正整数值; server-id = 1 #启用二进制日志； log-bin=mysql-bin #需要同步的二进制数据库名； binlog-do-db=minishop #不同步的二进制数据库名,如果不设置可以将其注释掉; binlog-ignore-db=information_schema binlog-ignore-db=mysql binlog-ignore-db=personalsite binlog-ignore-db=test #设定生成的log文件名； log-bin=&quot;D:/Database/materlog&quot; #把更新的记录写到二进制文件中； （注：此处不限制，是对所有操作进行记录日志）#log-slave-updates 保存文件。重启Mysql服务。 注：主数据库配置文件命令详解： log-bin=mysql-bin 启动二进制日志系统 binlog-do-db=test 二进制需要同步的数据库名 server-id = 1 本机数据库ID 标示为主，该部分还应有一个server-id=Master_id选项，其中master_id必须为1到232–1之间的一个正整数值 log-bin=/var/log/mysql/updatelog 设定生成log文件名，这里的路径没有mysql目录要手动创建并给于它mysql用户的权限。 binlog-ignore-db=mysql 避免同步mysql用户配置，以免不必要的麻烦 查看日志：1234567mysql&gt; SHOW MASTER STATUS;+-------------------+----------+--------------+------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+-------------------+----------+--------------+------------------+| master-bin.000001 | 0| | |+-------------------+----------+--------------+------------------+1 row in set (0.00 sec) （注：此处的日志文件File、Position下面需要用到）3、从数据库服务器配置 进入从数据库服务器安装目录,打开my.ini,在文件末尾增加如下配置：12345678910#如果需要增加Slave库则，此id往后顺延； server-id = 2 log-bin=mysql-bin #如果发现主服务器断线，重新连接的时间差； master-connect-retry=60 #不需要备份的数据库； replicate-ignore-db=mysql #需要备份的数据库 replicate-do-db=minishop #log-slave-update 保存文件。重启Mysql服务。4、连接Master1234567mysql&gt;change master to -&gt;master_host=&apos;192.168.0.104&apos;, //Master 服务器Ip-&gt;master_port=3306,-&gt;master_user=&apos;repl&apos;,-&gt;master_password=&apos;mysql&apos;, -&gt;master_log_file=&apos;master-bin.000001&apos;,//Master服务器产生的日志-&gt;master_log_pos=0; 从服务器配置命令详解： server-id = 2 从服务器ID号，不要和主ID相同 ，如果设置多个从服务器，每个从服务器必须有一个唯一的server-id值，必须与主服务器的以及其它从服务器的不相同。可以认为server-id值类似于IP地址：这些ID值能唯一识别复制服务器群集中的每个服务器实例。 master-host = 172.31.70.51 指定主服务器IP地址 master-user = replication 指定在主服务器上可以进行同步的用户名 master-password = 123456 密码 master-port = 3306 同步所用的端口 master-connect-retry=60 断点重新连接时间 replicate-ignore-db=mysql 屏蔽对mysql库的同步，以免有麻烦 replicate-do-db=test 同步数据库名称 5、启动Slavestart slave;查看从服务器状态mysql&gt; show slave status \G;OK所有配置都完成了。 读写分离 完成MySQL的主从配置，实现数据的实时同步，采用架构的方式实现MySQL的读写分离。 统一认证平台完成数据的增删改的操作，保存数据到MySQL的Master的数据库中，Salve数据库从Master数据库中实时同步数据，应用系统从Salve数据库中读取书据。]]></content>
      <categories>
        <category>MySQL</category>
        <category>架构</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总结-MySQL分库分表理论详解]]></title>
    <url>%2F2017%2F12%2F08%2F%E6%80%BB%E7%BB%93-MySQL%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[为什么要分库/分表？ 当一张的数据达到几百万时，你查询一次所花的时间会变多，如果有联合查询的话，我想有可能会死在那儿了。分表的目的就在于此，减小数据库的负担，缩短查询时间。 mysql中有一种机制是表锁定和行锁定，为什么要出现这种机制，是为了保证数据的完整 性，我举个例子来说吧，如果有二个sql都要修改同一张表的同一条数据，这个时候怎么办呢，是不是二个sql都可以同时修改这条数据呢？很显然mysql 对这种情况的处理是，一种是表锁定（myisam存储引擎），一个是行锁定（innodb存储引擎）。表锁定表示你们都不能对这张表进行操作，必须等我对 表操作完才行。行锁定也一样，别的sql必须等我对这条数据操作完了，才能对这条数据进行操作。如果数据太多，一次执行的时间太长，等待的时间就越长，这 也是我们为什么要分库/分表的原因。 怎么分库/分表？分库：（1）mysql集群与主从复制、读写分离： 主从复制、读写分离只是一种策略。 读写分离策略，最大限度提高了应用中读取数据的速度和并发量。（原理是，由于MySQL中有一种机制是表锁定和行锁定，所以写操作会锁定，而且写操作比读操作更费时，因为不仅要写入数据，还要更新索引。因此，把读写分离，可极大提高数据库的性能。） 主从复制策略，可以降低单台机器的负载，同时最大限度的降低宕机造成的损失。 集群：解决了数据库宕机带来的单点数据库不能访问的问题。 （2）垂直分库：一般根据业务逻辑来分，例如按照用户、商品、日志等。（3）水平分库：就是对业务进行主从复制、读写分离的操作。当一主一从不能满足时，就需要一主多从的形式满足读操作。分表：（1）垂直分表：根据字段的使用情况来进行划分，把一些常使用的字段放在一个表中。不经常使用（读和写）的放在一个表中，可通过一个外键关联。（使用外键时要注意该表是否读大于写，如果不能使用外键，直接将主表搜索ID放到附表即可一样关联。）（2）水平分表：水平分表方式多样，例如： 按时间分表 这种分表方式有一定的局限性，当数据有较强的实效性，如微博发送记录、微信消息记录等，这种数据很少有用户会查询几个月前的数据，如就可以按月分表。 按数据迁移的方式 当一些很久之前的数据，很少再查询。比如员工工资表，我们可以只存今年的工资情况。而历史数据我们可以迁移到一张salary_old表中,保证数据不会丢失。但也可以用来查询。每天定期把今年中的最早一天的记录归入旧表中。这样一方面可以解决性能问题，最多也只需要读2张表就完成了。 按热度拆分 典型的像贴吧这种有高点击率的词条，也有低点击率的词条，如果一个词条一张表，那得多少表啊，所以一般这种情况就会对高点击率的词条生成 一张表，低热度的词条都放在一张大表里，待低热度的词条达到一定的贴数后，比如1W条，再把低热度的表单独拆分成一张表。 （3）分表实现技术方式： 1. 分两张表的情况，可使用对2取余判断奇偶 2. 分10张表以内的，可使用对10或10以内数字取模来实现 3. 数据量大的，可根据MD5哈希取位数来实现（位数越大，表数越大） 4. 利用merge存储引擎来实现分表 实际工作中如何分库/分表？ 做什么事都有一个度，超过这个度就会变得很差，不能一味的做数据库服务器集群，硬件是要花钱买的，也不要一味的分表，分出来1000 表，mysql的存储归根到底还以文件的形势存在硬盘上面，一张表对应三个文件，1000个分表就是对应3000个文件，这样检索起来也会变的很慢 。 建议是：硬件与软件一起使用。 分库/分表后产生的问题？（1）分库分表维度的问题 假如用户购买了商品,需要将交易记录保存取来，如果按照用户的纬度分表，则每个用户的交易记录都保存在同一表中，所以很快很方便的查找到某用户的购买情况，但是某商品被购买的情况则很有可能分布在多张表中，查找起来比较麻烦。反之，按照商品维度分表，可以很方便的查找到此商品的购买情况，但要查找到购买人的交易记录比较麻烦。 所以常见的解决方式有： 1. 通过扫表的方式解决，此方法基本不可能，效率太低了。 2. 记录两份数据，一份按照用户纬度分表，一份按照商品维度分表。 3. 通过搜索引擎解决，但如果实时性要求很高，又得关系到实时搜索。 （2）联合查询的问题 联合查询基本不可能，因为关联的表有可能不在同一数据库中。（3）避免跨库事务 避免在一个事务中修改db0中的表的时候同时修改db1中的表，一个是操作起来更复杂，效率也会有一定影响。（4）尽量把同一组数据放到同一DB服务器上 如将卖家a的商品和交易信息都放到db0中，当db1挂了的时候，卖家a相关的东西可以正常使用。也就是说避免数据库中的数据依赖另一数据库中的数据。 分库时需要注意的问题：（1）联结不能跨库操作（2）InnoDB引擎的外键不能跨库（3）避免跨库事务操作（4）尽量把同一组数据放到同一个数据库上 参考博文：mysql 分库分表的方法数据库水平切分的实现原理解析——分库，分表，主从，集群，负载均衡器数据库读写分离和垂直分库、水平分表linux mysql proxy 的安装，配置，以及读写分离]]></content>
      <categories>
        <category>MySQL</category>
        <category>架构</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL单机到集群的架构变迁【摘】]]></title>
    <url>%2F2017%2F12%2F08%2FMySQL%E5%8D%95%E6%9C%BA%E5%88%B0%E9%9B%86%E7%BE%A4%E7%9A%84%E6%9E%B6%E6%9E%84%E5%8F%98%E8%BF%81%E3%80%90%E6%91%98%E3%80%91%2F</url>
    <content type="text"><![CDATA[1、业务上线初期，web服务规模较小，一般具备以下特点： 服务原型时期,用户基数小,多种业务公用资源,日均写入百万级别,读取千万级别 数据规模小，单机性能能够满足需求 用户规模小，开发重心偏向迭代速度 考虑到上述小型业务特点，为节约资源成本及开发成本，可以采用多个业务混合部署形式 2、当用户增多，数据量、访问量升高(2倍以下)，数据库压力较大，怎样在有限程度提高MySQL吞吐量呢？ SQL优化 硬件升级 压力还在有限的范围内增长，通过简单、低成本优化，可以一定成都上提高有限的服务性能 3、业务持续发展，读取性能出现瓶颈&amp;&amp;各业务互相影响，多个业务出 现资源抢占，如何快速解决业务抢占问题以提高服务性能？ 垂直拆分——按业务进行数据拆分 4、随着业务的继续发展，读取性能出现瓶颈,读写互相影响，如何确保读请求量的增加，不要影响写入性能？相反写入请求量增加如何确保不影响读取性能？(写入性能出现问题会造成数据丢失) 读写分离，写入仅写master，master与slave自动同步；读取仅以slave作为来源 读写分离后，slave仅专注于承担读请求，读取性能得到优化；同里独立的master服务的写入性能也得到优化。 5、一台/一对M-S服务器性能终归是很有限的，当单实例服务性能无法承载线上的请求量时，如何进行优化？ 升级为一主多从架构 一个master承载所有写入请求，理论上master性能不变 多个slave分担读取请求，读取性能提升n倍 6、随着业务量的增长，服务出现如下变化： 数据量增长，意味着原本的存储空间不足 写入量增长，意味着master写入性能存在瓶颈 读取量增长，意味着slave读取性能也存在瓶颈，但通过扩充slave是有限制的：一方面M-Sreplication性能有风险；另一方面扩充slave的成本较高水平拆分 业务经历数据量的增长、读写请求量的增长，数据库服务已经演进为分布式架构，一个业务的数据，怎样合理的分布到上述复杂的分布式数据库是下一个需要解决的问题 7、如何基于上述演进到最后的架构进行数据库设计呢？ 分布式数据库设计 hash拆分方式，既按hash规则，将数据读写请求分散到多个实例上，见上述水平拆分示意图 时间拆分方式，基于确定好的时间划分规则，将数据按时间段分散存储再多个实例中 8、当一台服务器宕机怎么办？（1）Slave(一主多从)宕机？ 1. 剩余健康Slave无风险，则无需紧急操作，例行修复 2. 切换流量到容灾机房(如果具备容灾机房) 3. 紧急扩容[优先]、重启、替换 4. 有损降级部分请求 （2）Master宕机？ 由于master数据的唯一性，致使master出现异常会直接造成数据写入失败 1. 快速下线master 2. 下线一台salve的读服务(如果slave性能有风险，则同时快速扩容) 3. 提升slave为master 4. 生效新master与slave的同步机制 9、注意事项（1）MySQL设计应该注意的问题 1. 表字符集选择UTF8 2. 存储引擎使用InnoDB 3. 使用Varchar/Varbinary存储变长字符串 4. 不在数据库中存储图片、文件等 5. 每张表数据量控制在20000W以下 6. 提前对业务做好垂直拆分 （2）MySQL查询应该遇到的问题 1. 避免使用存储过程、触发器、函数等 让数据库做最擅长的事 降低业务耦合度避开服务端BUG 2. 避免使用大表的JOIN MySQL最擅长的是单表的主键/索引查询 JOIN消耗较多内存,产生临时表 3. 避免在数据库中进行数学运算 MySQL不擅长数学运算 无法使用索引 4. 减少与数据库的交互次数 select条件查询要利用索引 同一字段的条件判定要用in而不要用or 摘抄博客：新兵训练营系列课程——海量数据存储基础分享一篇博客：QPS从0到4000请求每秒，谈达达后台架构演化之路]]></content>
      <categories>
        <category>MySQL</category>
        <category>架构</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL调优三部曲]]></title>
    <url>%2F2017%2F12%2F07%2FMySQL%E8%B0%83%E4%BC%98%E4%B8%89%E9%83%A8%E6%9B%B2%2F</url>
    <content type="text"><![CDATA[1、慢查询 （分析出现出问题的sql）2、Explain （显示了mysql如何使用索引来处理select语句以及连接表。可以帮助选择更好的索引和写出更优化的查询语句）3、Profile（查询到 SQL 会执行多少时间, 并看出 CPU/Memory 使用量, 执行过程中 Systemlock, Table lock 花多少时间等等.） 慢查询 MySQL通过慢查询日志定位那些执行效率较低的SQL 语句，用–log-slow-queries[=file_name]选项启动时，mysqld 会写一个包含所有执行时间超过long_query_time 秒的SQL语句的日志文件，通过查看这个日志文件定位效率较低的SQL 。下面介绍MySQL中如何查询慢的SQL语句： 配置开启配置选项： 配置项 说明 long_query_time 设定慢查询的阀值，超出次设定值的SQL即被记录到慢查询日志，缺省值为10s slow_query_log 指定是否开启慢查询日志 log_slow_queries 指定是否开启慢查询日志(该参数要被slow_query_log取代，做兼容性保留) slow_query_log_file 指定慢日志文件存放位置，可以为空，系统会给一个缺省的文件host_name-slow.log min_examined_row_limit 查询检查返回少于该参数指定行的SQL不被记录到慢查询日志 log_queries_not_using_indexes 不使用索引的慢查询日志是否记录到索引 开启配置查询慢查询是否开启：show variables like &#39;%slow%&#39;;开启慢查询日志：set global log_slow_queries=1;查询慢查询时间：show variables like &#39;%long_query_time%&#39;; 查看方式利用MySQL自带的慢查询日志分析工具mysqldumpslow分析日志。 perlmysqldumpslow –s c –t 10 slow-query.log 具体参数设置如下： -s 表示按何种方式排序，c、t、l、r分别是按照记录次数、时间、查询时间、返回的记录数来排序，ac、at、al、ar，表示相应的倒叙； -t 表示top的意思，后面跟着的数据表示返回前面多少条； -g 后面可以写正则表达式匹配，大小写不敏感。 例子：mysqldumpslow -s c -t 20 host-slow.log #访问次数最多的20个sql语句mysqldumpslow -t 10 -s t -g “left join” host-slow.log #按照时间返回前10条里面含有左连接的sql语句 Explainexplain显示了mysql如何使用索引来处理select语句以及连接表。可以帮助选择更好的索引和写出更优化的查询语句。使用方法，在select语句前加上explain就可以了：例如：explain select surname,first_name form a,b where a.id=b.id显示结果分析： id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra EXPLAIN列的解释 table 显示这一行的数据是关于哪张表的 type 这是重要的列，显示连接使用了何种类型。从最好到最差的连接类型为const、eq_reg、ref、range、indexhe和ALL possible_keys 显示可能应用在这张表中的索引。如果为空，没有可能的索引。可以为相关的域从WHERE语句中选择一个合适的语句 key 实际使用的索引。如果为NULL，则没有使用索引。很少的情况下，MYSQL会选择优化不足的索引。这种情况下，可以在SELECT语句 中使用USE INDEX（indexname）来强制使用一个索引或者用IGNORE INDEX（indexname）来强制MYSQL忽略索引 key_len 使用的索引的长度。在不损失精确性的情况下，长度越短越好 ref 显示索引的哪一列被使用了，如果可能的话，是一个常数 rows MYSQL认为必须检查的用来返回请求数据的行数 Extra 关于MYSQL如何解析查询的额外信息。这里可以看到的坏的例子是Using temporary和Using filesort，意思MYSQL根本不能使用索引，结果是检索会很慢 Extra列返回的描述的意义： Distinct 一旦MYSQL找到了与行相联合匹配的行，就不再搜索了 Not exists MYSQL优化了LEFT JOIN，一旦它找到了匹配LEFT JOIN标准的行，就不再搜索了Range checked for each Record（index map:#）没有找到理想的索引，因此对于从前面表中来的每一个行组合，MYSQL检查使用哪个索引，并用它来从表中返回行。这是使用索引的最慢的连接之一 Using filesort 看到这个的时候，查询就需要优化了。MYSQL需要进行额外的步骤来发现如何对返回的行排序。它根据连接类型以及存储排序键值和匹配条件的全部行的行指针来排序全部行 Using index 列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表的全部的请求列都是同一个索引的部分的时候 Using temporary 看到这个的时候，查询需要优化了。这里，MYSQL需要创建一个临时表来存储结果，这通常发生在对不同的列集进行ORDER BY上，而不是GROUP BY上Where used使用了WHERE从句来限制哪些行将与下一张表匹配或者是返回给用户。如果不想返回表中的全部行，并且连接类型ALL或index，这就会发生，或者是查询有问题不同连接类型的解释（按照效率高低的顺序排序） Type列不同连接类型的解释：（按照效率高低的顺序排序） system 表只有一行：system表。这是const连接类型的特殊情况 const 表中的一个记录的最大值能够匹配这个查询（索引可以是主键或惟一索引）。因为只有一行，这个值实际就是常数，因为MYSQL先读这个值然后把它当做常数来对待 eq_ref 在连接中，MYSQL在查询时，从前面的表中，对每一个记录的联合都从表中读取一个记录，它在查询使用了索引为主键或惟一键的全部时使用 ref 这个连接类型只有在查询使用了不是惟一或主键的键或者是这些类型的部分（比如，利用最左边前缀）时发生。对于之前的表的每一个行联合，全部记录都将从表中读出。这个类型严重依赖于根据索引匹配的记录多少—越少越好 range 这个连接类型使用索引返回一个范围中的行，比如使用&gt;或&lt;查找东西时发生的情况 index 这个连接类型对前面的表中的每一个记录联合进行完全扫描（比ALL更好，因为索引一般小于表数据） ALL 这个连接类型对于前面的每一个记录联合进行完全扫描，这一般比较糟糕，应该尽量避免 Profile Query Profiler是MYSQL自带的一种query诊断分析工具，通过它可以分析出一条SQL语句的性能瓶颈在什么地方。通常我们是使用的explain,以及slow query log都无法做到精确分析，但是Query Profiler却可以定位出一条SQL语句执行的各种资源消耗情况，比如CPU，IO等，以及该SQL执行所耗费的时间等。 Show profiles是5.0.37之后添加的，要想使用此功能，要确保版本在5.0.37之后。 查看数据库版本方法：show variables like &quot;%version%&quot;; 或者 select version(); 1、确定支持show profile 后，查看profile是否开启，数据库默认是不开启的。变量profiling是用户变量，每次都得重新启用。查看方法： show variables like &quot;%pro%&quot;;2、开启和关闭12mysql&gt; set profiling=1;mysql&gt; set profiling=0; information_schema 的 database 会建立一个PROFILING 的 table 记录.3、执行一些语句（自定义语句）mysql&gt;select * from navigation_sub where navPId&lt;6 and navSName=&#39;公司介绍&#39;;4、查询语句执行时间mysql&gt;show profiles;5、查询语句详细执行时间mysql&gt;show profile for query 2;（注：此处的 2 表示再 show profiles 查询后获取的 Query_ID字段。）]]></content>
      <categories>
        <category>MySQL</category>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总结-MySQL索引]]></title>
    <url>%2F2017%2F12%2F07%2F%E6%80%BB%E7%BB%93-MySQL%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[关于MySQL索引的好处，如果正确合理设计并且使用索引的MySQL是一辆兰博基尼的话，那么没有设计和使用索引的MySQL就是一个人力三轮车。对于没有索引的表，单表查询可能几十万数据就是瓶颈，而通常大型网站单日就可能会产生几十万甚至几百万的数据，没有索引查询会变的非常缓慢。 一个简单的对比测试： 以我去年测试的数据作为一个简单示例，20多条数据源随机生成200万条数据，平均每条数据源都重复大概10万次，表结构比较简单，仅包含一个自增ID，一个char类型，一个text类型和一个int类型，单表2G大小，使用MyIASM引擎。开始测试未添加任何索引。 执行下面的SQL语句： mysql&gt; SELECT id,FROM_UNIXTIME(time) FROM article WHERE a.title=’测试标题’ 查询需要的时间非常恐怖的，如果加上联合查询和其他一些约束条件，数据库会疯狂的消耗内存，并且会影响前端程序的执行。这时给title字段添加一个BTREE索引： mysql&gt; ALTER TABLE article ADD INDEX index_article_title ON title(200); 再次执行上述查询语句，其对比非常明显。 MySQL索引的概念 索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。更通俗的说，数据库索引好比是一本书前面的目录，能加快数据库的查询速度。上述SQL语句，在没有索引的情况下，数据库会遍历全部200条数据后选择符合条件的；而有了相应的索引之后，数据库会直接在索引中查找符合条件的选项。如果我们把SQL语句换成“SELECT * FROM article WHERE id=2000000”，那么你是希望数据库按照顺序读取完200万行数据以后给你结果还是直接在索引中定位呢？（注：一般数据库默认都会为主键生成索引）。 索引分为聚簇索引和非聚簇索引两种，聚簇索引是按照数据存放的物理位置为顺序的，而非聚簇索引就不一样了；聚簇索引能提高多行检索的速度，而非聚簇索引对于单行的检索很快。 聚簇索引与非聚簇索引的区别：在《数据库原理》一书中是这么解释聚簇索引和非聚簇索引的区别的：聚簇索引的叶子节点就是数据节点，而非聚簇索引的叶子节点仍然是索引节点，只不过有指向对应数据块的指针。比较MyISAM和InnoDB两种存储引擎说一下，MyISAM引擎的索引文件(.MYI)和数据文件(.MYD)是相互独立的，而InnoDB的数据和索引文件都保存在(.ibd)中。MySQL索引采用的是B+Tree，对于MyISAM引擎来说叶子节点存储的是索引文件，而InnoDB引擎叶子节点存储的既有数据又有索引。当创建数据表时对于InnoDB主键生成的索引为聚簇索引，而MyISAM主键生成的索引为非聚簇索引。对于一个数据表来说只能有一个聚簇索引。（参考文章：MYSQL性能调优: 对聚簇索引和非聚簇索引的认识MySQL聚簇索引和非聚簇索引的原理及使用） MySQL索引的类型1. 普通索引 这是最基本的索引，它没有任何限制，比如上文中为title字段创建的索引就是一个普通索引，MyIASM中默认的BTREE类型的索引，也是我们大多数情况下用到的索引。 –直接创建索引 CREATE INDEX index_name ON table(column(length)) –修改表结构的方式添加索引 ALTER TABLE table_name ADD INDEX index_name ON (column(length)) –创建表的时候同时创建索引 CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL , `content` text CHARACTER SET utf8 COLLATE utf8_general_ci NULL , `time` int(10) NULL DEFAULT NULL , PRIMARY KEY (`id`), INDEX index_name (title(length)) ) –删除索引 DROP INDEX index_name ON table 2. 唯一索引 与普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值（注意和主键不同）。如果是组合索引，则列值的组合必须唯一，创建方法和普通索引类似。 –创建唯一索引 CREATE UNIQUE INDEX indexName ON table(column(length)) –修改表结构 ALTER TABLE table_name ADD UNIQUE indexName ON (column(length)) –创建表的时候直接指定 CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL , `content` text CHARACTER SET utf8 COLLATE utf8_general_ci NULL , `time` int(10) NULL DEFAULT NULL , PRIMARY KEY (`id`), UNIQUE indexName (title(length)) ); 3. 全文索引（FULLTEXT） MySQL从3.23.23版开始支持全文索引和全文检索，FULLTEXT索引仅可用于 MyISAM 表；他们可以从CHAR、VARCHAR或TEXT列中作为CREATE TABLE语句的一部分被创建，或是随后使用ALTER TABLE 或CREATE INDEX被添加。////对于较大的数据集，将你的资料输入一个没有FULLTEXT索引的表中，然后创建索引，其速度比把资料输入现有FULLTEXT索引的速度更为快。不过切记对于大容量的数据表，生成全文索引是一个非常消耗时间非常消耗硬盘空间的做法。 –创建表的适合添加全文索引 CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL , `content` text CHARACTER SET utf8 COLLATE utf8_general_ci NULL , `time` int(10) NULL DEFAULT NULL , PRIMARY KEY (`id`), FULLTEXT (content) ); –修改表结构添加全文索引 ALTER TABLE article ADD FULLTEXT index_content(content) –直接创建索引 CREATE FULLTEXT INDEX index_content ON article(content) 4. 单列索引、多列索引多个单列索引与单个多列索引的查询效果不同，因为执行查询时，MySQL只能使用一个索引，会从多个索引中选择一个限制最为严格的索引。 5. 组合索引（最左前缀匹配原则） 平时用的SQL查询语句一般都有比较多的限制条件，所以为了进一步榨取MySQL的效率，就要考虑建立组合索引。例如上表中针对title和time建立一个组合索引：ALTER TABLE article ADD INDEX index_titme_time (title(50),time(10))。建立这样的组合索引，其实是相当于分别建立了下面两组组合索引：12title,timetitle 为什么没有time这样的组合索引呢？这是因为MySQL组合索引“最左前缀”的结果。简单的理解就是只从最左面的开始组合。并不是只要包含这两列的查询都会用到该组合索引，如下面的几个SQL所示： –使用到上面的索引 SELECT * FROM article WHREE title=&apos;测试&apos; AND time=1234567890; SELECT * FROM article WHREE utitle=&apos;测试&apos;; –不使用上面的索引 SELECT * FROM article WHREE time=1234567890; 6、主键索引 它是一种特殊的唯一索引，不允许有空值。 主键索引与唯一索引区别：唯一索引除了key值允许存在NULL外，其余的和主键索引没有本质性区别。 总结： 主键一定是唯一性索引，唯一性索引并不一定就是主键。 一个表中可以有多个唯一性索引，但只能有一个主键。 主键列不允许空值，而唯一性索引列允许空值。 主键可以被其他字段作外键引用，而索引不能作为外键引用。 7、外键索引（注：InnoDB支持外键，而MyISAM不支持外键。） 是用于建立和加强两个表数据之间的链接的一列或多列。外键约束主要用来维护两个表之间数据的一致性。简言之，表的外键就是另一表的主键，外键将两表联系起来。一般情况下，要删除一张表中的主键必须首先要确保其它表中的没有相同外键（即该表中的主键没有一个外键和它相关联）。 8、索引区别普通索引：最基本的索引，没有任何限制 唯一索引：与&quot;普通索引&quot;类似，不同的就是：索引列的值必须唯一，但允许有空值。 主键索引：它是一种特殊的唯一索引，不允许有空值。 全文索引：针对较大的数据，生成全文索引很耗时好空间。 组合索引：为了更多的提高mysql效率可建立组合索引，遵循”最左前缀“原则。 MySQL索引的优化 上面都在说使用索引的好处，但过多的使用索引将会造成滥用。因此索引也会有它的缺点：虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件。建立索引会占用磁盘空间的索引文件。一般情况这个问题不太严重，但如果你在一个大表上创建了多种组合索引，索引文件的会膨胀很快。索引只是提高效率的一个因素，如果你的MySQL有大数据量的表，就需要花时间研究建立最优秀的索引，或优化查询语句。 下面是一些总结以及收藏的MySQL索引的注意事项和优化方法。 何时使用聚集索引或非聚集索引？ 动作描述 使用聚集索引 使用非聚集索引 列经常被分组排序 使用 使用 返回某范围内的数据 使用 不使用 一个或极少不同值 不使用 不使用 小数目的不同值 使用 不使用 大数目的不同值 不使用 使用 频繁更新的列 不使用 使用 外键列 使用 使用 主键列 使用 使用 频繁修改索引列 不使用 使用 索引不会包含有NULL值的列 只要列中包含有NULL值都将不会被包含在索引中，复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为NULL。 使用短索引 对串列进行索引，如果可能应该指定一个前缀长度。例如，如果有一个CHAR(255)的列，如果在前10个或20个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。（因为索引页存储的数据更多了） 索引列排序 MySQL查询只使用一个索引，因此如果where子句中已经使用了索引的话，那么order by中的列或者OR条件的另一列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引。 like语句操作 一般情况下不鼓励使用like操作，如果非使用不可，如何使用也是一个问题。like “%aaa%” 不会使用索引而like “aaa%”可以使用索引。 不要在列上进行运算避免在where子句中对字段进行表达式操作或函数操作，这将导致引擎放弃使用索引而进行全表扫描。 例如：select * from users where YEAR(adddate)&lt;2007，将在每个行上进行运算，这将导致索引失效而进行全表扫描，因此我们可以改成：select * from users where adddate&lt;’2007-01-01′。 不要在WHERE子句中使用参数如果在 where 子句中使用参数，也会导致全表扫描。可改为强制查询使用索引。 select * from t with(index(索引名)) where num=@num 任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段。 不要在区分度小的列上建立索引在区分度较小的字段上新建索引,基本无效,还会增加大量的索引文件,得不偿失。（区分度: 指字段在数据库中的不重复比。计算规则如下：字段去重后的总数与全表总记录数的商。） 最左前缀匹配原则MySQL会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如select * from t_base_user where type=&quot;10&quot; and created_at&lt;&quot;2017-11-03&quot; and status=1;在上述语句中,status就不会走索引,因为遇到&lt;时,MySQL已经停止匹配,此时走的索引为:(type,created_at),其先后顺序是可以调整的,而走不到status索引,此时需要修改语句为:select * from t_base_user where type=10 and status=1 and created_at&lt;&quot;2017-11-03&quot;即可走status索引。 最后总结一下，MySQL只对一下操作符才使用索引：&lt;,&lt;=,=,&gt;,&gt;=,between,in,以及某些时候的like(不以通配符%或_开头的情形)；而对负向查询（not , not in, not like, &lt;&gt;, != ,!&gt;,!&lt; ） 不会使用索引。理论上每张表里面最多可创建16个索引，不过除非是数据量真的很多，否则过多的使用索引也不是那么好玩的，比如我刚才针对text类型的字段创建索引的时候，系统差点就卡死了。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总结-MySQL语句的语法]]></title>
    <url>%2F2017%2F12%2F07%2F%E6%80%BB%E7%BB%93-MySQL%E8%AF%AD%E5%8F%A5%E7%9A%84%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[MySQL语句的语法：1、ALTER TABLE 更新已存在的表的模式。2、COMMIT 用来将事务处理写到数据库。3、CREATE INDEX 用于在一个或多个列上创建索引。4、CREATE PROCEDURE 用于创建存储过程。5、CREATE TABLE 用于创建数据库表。6、CREATE USER 用于向系统中添加新的用户账号。7、CREATE VIEW 用来创建一个或多个表上的新视图。8、DELETE 从表中删除一行或多行。9、DROP 永久地删除数据库对象（表、视图、索引等）。10、INSERT 给表增加一行。11、INSERT SELECT 插入SELECT的结果到一个表。12、ROLLBACK 用于撤销一个事务处理块。13、SAVEPOINT 为使用ROLLBACK语句设立保留点。14、SELECT 用于从一个或多个表（视图）中检索数据。15、START TRANSACTION 表示一个新的事务处理块的开始。16、UPDATE 更新表中的一行或多行。 详解：1、ALTER TABLE 更新已存在的表的模式。 1. 删除列 ALTER TABLE 【表名字】 DROP 【列名称】 2. 增加列 ALTER TABLE 【表名字】 ADD 【列名称】 【列类型】 COMMENT &apos;注释说明&apos; 3. 修改列的类型信息 ALTER TABLE 【表名字】 MODIFY 【列名称】【列类型】 4. 重命名列 ALTER TABLE 【表名字】 CHANGE 【列名称】【新列名称】【新列类型】 5. 重命名表 ALTER TABLE 【表名字】 RENAME 【表新名字】 6. 更改表存储引擎 ALTER TABLE ENGINE=InnoDB; 7. 删除表中主键 Alter TABLE 【表名字】 DROP PRIMARY KEY （注：该主键不能为自增，否则会报错。） 8. 添加主键 ALTER TABLE 【表名字】 ADD PRIMARY KEY (`resid`,`resfromid`); （注：如果表中有数据，直接添加主键会报错。需要①字段新建普通索引②字段增加自增属性③字段新建主键④删除字段的普通索引。） 9. 添加索引 ALTER TABLE 【表名字】 ADD INDEX index_name (name); 10. 添加唯一索引 ALTER TABLE 【表名字】 ADD UNIQUE emp_name2(cardnumber); 11. 删除索引 ALTER TABLE 【表名字】 DROP INDEX emp_name; 12. 添加外键索引 ALTER TABLE 【表名字】 ADD FOREIGN KEY index_name(name) REFERENCES foreign_table(name); （注：添加外键需要两个表的存储引擎都是InnoDB型。） 13. 删除外键索引 ALTER TABLE 【表名字】 DROP FOREIGN KEY index_name; 14. 添加复合索引 ALTER TABLE 【表名字】 ADD INDEX index_name(`column1`, `column2`); 2、CREATE TABLE 创建数据库表 12345678CREATE TABLE [IF NOT EXISTS] `tbl_name`(`col_name` type [NOT NULL | NULL] [DEFAULT default_value] [AUTO_INCREMENT]PRIMARY KEY (`index_col_name`,...)or KEY [index_name] (`index_col_name`,...)or INDEX [index_name] (`index_col_name`,...)or UNIQUE [INDEX] [index_name] (`index_col_name`,...)or FOREIGN KEY index_name (`index_col_name`,...）)ENGINE=MyISAM DEFAULT CHARSET=UTF8; 3、CREATE USER 创建用户 1. 创建用户 CREATE USER user01@&apos;localhost/%&apos; IDENTIFIED BY &apos;password1&apos;; 2. 修改密码 SET PASSWORD FOR &apos;user01&apos;@&apos;localhost/%&apos; = PASSWORD(&apos;password2&apos;); 4、GRANT 赋予权限 1. 设置权限 grant 权限 on 数据库对象 to 用户 2. 取消权限 revoke 权限 on 数据库对象 from 用户 3. 查看权限 show grants for dba@localhost; 5、CREATE VIEW 创建视图 1. 创建视图 CREATE VIEW view_name AS SQL; 2. 删除视图 DROP VIEW [IF EXISTS] view_name; （注：视图不能修改，只能删除，然后重建。） 6、DROP 删除（表、视图、存储过程） DROP TABLE/VIEW/PROCEDURE [IF EXISTS] name; 7、CREATE PROCEDURE 创建存储过程 1. 创建存储过程 CREATE PROCEDURE procedure_name( IN id int, OUT price DECIMAL(8,2) ) BEGIN SQL; END; 2. 调用存储过程 CALL procedure_name(); 8、触发器 1. 创建触发器 CREATE TRIGGER trigger_name [AFTER/BEFORE] [INSERT/UPDATE/DELETE] ON table_name FOR EACH ROW 操作; （在对表操作之前或之后，执行操作，EACH ROW表示每行变动都需要执行操作） 2. 删除触发器 DROP TRIGGER trigger_name; 3. 如何使用触发器 此处不需要人为操作触发器，当对表操作的时候自动执行。 9、事务处理12345操作格式：START TRANSACTION;SQL;COMMIT;ROLLBACK; 数据表的基本操作：1、SELECTselect [ALL/DISTINCE] 字段表达式子句 [from子句] [where子句] [group by子句] [having子句] [order by子句] [limit子句]; 2、UPDATEUPDATE tbl_name SET col_name1=expr1 [, col_name2=expr2 ...][WHERE子句] [ORDER BY子句] [LIMIT子句]; 3、DELETEDELETE FROM tbl_name [WHERE子句] [ORDER BY子句] [LIMIT子句]; 4、INSERTINSERT INTO tbl_name(col_name1,col_name2...) VALUES(col_vallue1,col_value2...); 附属：修改数据表编码及查询数据库编码：（1）查询数据库编码1234567891011121314mysql&gt; show variables like &apos;%char%&apos;;+--------------------------+--------------------------------+| Variable_name | Value |+--------------------------+--------------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | D:\xampp\mysql\share\charsets\ |+--------------------------+--------------------------------+8 rows in set 如果想要修改编码的话，可使用命令：set character_set_database = utf8; （2）修改数据表编码ALTER TABLE table_name character set utf8;]]></content>
      <categories>
        <category>MySQL</category>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总结-MySQL数据类型]]></title>
    <url>%2F2017%2F12%2F07%2F%E6%80%BB%E7%BB%93-MySQL%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[数据类型是定义列中可以存储什么数据以及该数据实际怎样存储的基本规则。数据类型用于以下目的： 数据类型允许限制可存储在列中的数据。例如，数值数据类型列只能接受数值。 数据类型允许在内部更有效地存储数据。 可以用一种比文本串更简洁的格式存储数值和日期时间值。 数据类型允许变换排序顺序。如果所有数据都作为串处理，则1位于10之前，而10又位于2之前（串以字典顺序排序，从左边开始比较，一次一个字符）。作为数值数据类型，数值才能正确排序。 在设计表时，应该特别重视所用的数据类型。使用错误的数据类型可能会严重地影响应用程序的功能和性能。更改包含数据的列不是一件小事（而且这样做可能会导致数据丢失）。 1、串数据类型 最常用的数据类型是串数据类型。有两种基本的串类型，分别为定长串和变长串。 定长串接受长度固定的字符串，其长度是在创建表时指定的。 变长串存储可变长度的文本。 （注：MySQL处理定长列远比处理变长列快的多，性能好得多。而且，MySQL不允许对变长列（或一个列的可变部分）进行索引。这也会极大地影响性能。）串数据类型 数据类型 说明 CHAR 1~255个字符的定长串。它的长度必须在创建时指定，否则MySQL假定为CHAR(1) VARCHAR 长度可变，最多不超过255字节。 TINYTEXT 与TEXT相同，但最大长度为255字节 TEXT 最大长度为64K的变长文本 MEDIUMTEXT 与TEXT相同，但最大长度为16K LONGTEXT 与TEXT相同，但最大长度为4GB ENUM 接受最多64K个串组成的一个预定义集合的某个串 SET 接受最多64个串组成的一个预定义集合的零个或多个串 （注：不管使用何种形式的串数据类型，串值都必须括在引号内（通常单引号更好）。）特殊说明： `ENUM` 枚举类型 只能由一个枚举字符串值（最大值是65535）如果默认值为not null，那么允许列表中的第一个值为默认值 `SET` 集合类型 字符串对象可以有零个或多个SET成员（最多64个成员） 当数值不是数值时：你可能会认为电话号码和邮政编码应该存储在数值字段中（数值字段只存储数值数据），但是，这样做却是不可取的。如果在数值字段中存储邮政编码01234，则保存的将是数值1234，实际上丢失了一位数字。需要遵守的基本规则是：如果数值时计算（求和、平均等）中使用的数值，则应该存储在数值数据类型列中。如果作为字符串（可能只包含数字）使用，则应该保存在串数据类型列中。 2、数值数据类型 数值数据类型存储数值。MySQL支持多种数值数据类型，每种存储的数值具有不同的取值范围。 （注：有符号或无符号：所有数值数据类型（除BIT和BOOLEAN外）都可以有符号或无符号。有符号数值列可以存储正或负的数值，无符号数值列只能存储正数。默认情况为有符号，但如果你知道自己不需要存储负值，可以使用UNSIGNED关键字，这样做将允许你存倍大小的值。）数值数据类型 数据类型 说明 BOOLEAN(或BOOL) 布尔标志，或者为0或者为1，主要用于开/关(on/off)标志 TINYINT 整数值，1个字节，支持-128~127（如果为UNSIGNED，为0~255）的数 SMALLINT 整数值，2个字节，支持-32768-32767（如果是UNSIGNED，为0-65535）的数 MEDIUMINT 整数值，3个字节，支持-8388608~8388607（如果是UNSIGNED，为0~16777215）的数 INT(或INTEGER) 整数值，4个字节，支持-2147483648~2147483647 BIGINT 整数值，8个字节，支持+-9.22*10的18次方 FLOAT 单精度浮点值，8位精度（4字节） DOUBLE 双精度浮点值，16位精度（8字节） DECIMAL(或DEC) 精度可变的浮点值 REAL 4字节的浮点值 BIT 位字段，1~64位。（在MySQL5之前，BIT功能等价于TINYINT） （注：与串不一样，数值不应该括在引号内。） 3、日期和时间数据类型 MySQL使用专门的数据类型来存储日期和时间值。日期和时间数据类型 数据类型 说明 DATE 表示1000-01-01~9999-12-31的日期，格式为YYYY-MM-DD TIME 格式为HH:MM:SS DATETIME DATE和TIME的组合 TIMESTAMP 功能和DATETIME相同（但范围较小） YEAR 用2位数字表示，范围是70(1970年)~69(2069年)，用4位数字表示，范围是1901年~2155年 4、二进制数据类型 二进制数据类型可存储任何数据（甚至包括二进制信息），如图像、多媒体、字处理文档等。二进制数据类型 数据类型 说明 BLOB Blob最大长度为64KB MEDIUMBLOB Blob最大长度为16MB LONGBLOB Blob最大长度为4GB TINYBLOB Blob最大长度为255字节 注：数据类型的属性 MySQL关键字 含义 NULL 数据列可包含NULL值 NOT NULL 数据列不允许包含NULL值 DEFAULT 默认值 PRIMARY KEY 主键 AUTO_INCREMENT 自动递增，适用于整数类型 UNSIGNED 无符号 CHARACTER SET name 指定一个字符集]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《MySQL必知必会》-高级知识]]></title>
    <url>%2F2017%2F12%2F07%2FMySQL%E3%80%8A%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E3%80%8B-%E9%AB%98%E7%BA%A7%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[二十二、使用视图1、视图是什么？（注：MySQL 5添加了对视图的支持，因此，使用视图需要版本在MySQL 5及以后的版本。）视图是虚拟的表。与包含数据的表不一样，视图只包含使用时动态检索数据的查询。例如： SELECT cust_name,cust_contact FROM customers,orders,orderitems WHERE customers.cust_id = orders.cust_id AND orderitems.order_num = oders.order_num AND prod_id = &#39;TNT2&#39;;此查询用来检索订购了某个特定产品的用户。假如可以把整个查询包装成一个名为productcustomers的虚拟表，则可以如下轻松地检索出相同的数据： SELECT cust_name,cust_contact FROM productcustomers WERE prod_id = &#39;TNT2&#39;;这就是视图的作用。productcustomers是一个试图，作为视图，它不包含表中应该有的任何列或数据，它包含的是一个SQL查询（与上面用以正确联结表的相同的查询）。1.2、为什么使用视图？下面是视图的一些常见应用： 1. 重用SQL语句 2. 简化复杂的SQL操作。在编写查询后，可以方便地重用它而不必知道它的基本查询细节。 3. 使用表的组成部分而不是整个表。 4. 保护数据。可以给用户授予表的特定部分的访问权限而不是整个表的访问权限。 5. 更改数据格式和标识。视图可返回与底层表的表示和格式不同的数据。 在视图创建之后，可以用与表基本相同的方式利用它们。可以对视图执行SELECT操作，过滤和排序数据，将视图联结到其他视图或表，甚至能添加和更新数据（添加和更新数据存在某些限制。）。 需要知道的是：视图仅仅是用来查看存储在别处的数据的一种设施。视图本身不包含数据，因此它们返回的数据是从其他表中检索出来的。在添加或更改这些表中的数据时，视图将返回改变过的数据。（性能问题：因为视图不包含数据，所以每次使用视图时，都必须处理查询执行时所需的任一个检索。如果你用多个联结和过滤创建了复杂的视图或者嵌套了视图，可能会发现性能下降得很厉害。因此，在部署使用了大量视图的应用前，应该进行测试。） 1.3、视图的规则和限制下面是关于视图创建和使用的一些最常见的规则和限制： 1. 与表一样，视图必须唯一命名（不能给视图取与别的视图或者表相同的名字）。 2. 对于可以创建的视图数目没有限制。 3. 为了创建视图，必须具有足够的访问权限。这些限制通常由数据库管理人员授予。 4. 视图可以嵌套，既可以利用从其它视图中检索数据的查询来构造一个试图。 5. ORDER BY可以用在视图中，但如果从该视图检索数据SELECT中也含有ORDER BY，那么该视图中的ORDER BY将被覆盖。 6. 视图不能索引，也不能有关联的触发器或默认值。 7. 视图可以和表一起使用。例如，编写一条联结表和视图的SELECT语句。 2、视图是怎样工作的？ 2.1视图的创建： 1. 视图用CREATE VIEW语句来创建。 2. 使用SHOW CREATE VIEW viewname;来查看创建视图的语句。 3. 用DROP删除视图，其语法为DROP VIEW viewname;。 4. 更新视图时，可以先用DROP再用CREATE，也可以直接用CREATE OR REPLACE VIEW。 如果要更新的视图不存在，则第2条更新语句会创建一个试图；如果更新的视图存在，则第2条更新语句会替换原有视图。 2.2 常用的视图应用 1. 隐藏复杂的SQL，这通常都会涉及联结。 12345CREATE VIEW productcustomers AS SELECT cust_name,cust_contact,prod_id FROM customers,orders,orderitemsWHERE customers.cust_id = orders.cust_id AND orderitems.order_num = orders.order_num; 这条语句创建一个名为productcustomers的视图，联结了3个表，以返回已订购了任意产品的所有客户的列表。 为检索订购了产品的TNT2的客户，可如下进行： `SELECT cust_name,cust_contact FROM productcustomers WHERE prod_id=&apos;TNT2&apos;;` （注：创建可重用的视图：创建不受特定数据限制的视图是一种好办法。） 2. 用视图重新格式化检索出的数据。 123SELECT Concat(RTrim(vend_name), &apos;(&apos;, RTrim(vend_country), &apos;)&apos;) AS vend_title FROM vendors ORDER BY vend_name; 假如经常需要这个格式的结果，可创建一个试图，每次需要时使用它。 1234CREATE VIEW vendorlocations ASSELECT Concat(RTrim(vend_name), &apos;(&apos;, RTrim(vend_country), &apos;)&apos;) AS vend_titleFROM vendors ORDER BY vend_name; 3. 用视图过滤不想要的数据 视图对于应用普通的WHERE子句也很有用。例如，可以定义customeremaillist视图，它过滤没有电子邮件地址的客户。 123CREATE VIEW customeremaillist SELECT cust_id,cust_name,cust_email FROM customers WHERE cust_email IS NOT NULL; （注：WHERE子句与WHERE子句：如果从视图检索数据时使用了一条WHERE子句，则两组子句（一组在视图中，另一组是传递给视图的）将自动组合。） 4. 使用视图与计算字段 视图对于简化计算字段的使用特别有用。下面是它检索某个特定订单中的物品，计算每种物品的总价格： 1234CREATE VIEW orderitemsexpanded AS SELECT order_num,prod_id,quantity,item_price, quantity*item_price AS expanded_price FROM orderitems; 为检索订单20005的详细内容可如下操作： `SELECT * FROM orderitemsexpanded WHERE order_num = 20005;` 2.3 更新视图 通常，视图是可更新的（即可以对它们使用INSERT、UPDATE和DELETE）。更新一个视图将更新其基表（可以回忆一下，视图本身没有数据）。 但是，并非所有视图都是可更新的。基本上如果MySQL不能正确地确定被更新的基数据，则不允许更新（包括插入和删除）。这实际上意味着，如果视图定义中有以下操作，则不能进行视图的更新： 1. 分组（使用GROUP BY和HAVING） 2. 联结 3. 子查询 4. 并 5. 聚集涵涵素（Min()、Count()、Sum()等） 6. DISTINCT 7. 导出（计算）列。 因此，一般应该将视图用于检索（SELECT语句）而不用于更新（INSERT、UPDATE和DELETE）。 二十三、使用存储过程1、什么是存储过程？（注：MySQL 5添加了对存储过程的支持，因此需要MySQL 5及以后的版本。） 迄今为止，使用的大多数SQL语句都是针对一个或多个表的单条语句。并非所有操作都这么简单，经常会有一个完整的操作需要多条语句才能完成。例如下面的情形： 1. 为了处理订单，需要核对以保证库存中有相应的物品。 2. 如果库存有物品，这些物品需要预定以便不将它们再卖给别人，并且要减少可用的物品数量以反映正确的库存量。 3. 库存中没有的物品需要订购，这需要与供应商进行某种交互。 4. 关于哪些物品入库（并且可以立即发货）和哪些物品退订，需要通知相应的客户。 这显然不是一个完整的例子，执行这个处理需要针对许多表的多条MySQL语句。此外，需要执行的具体语句及其次序也不是固定的，它们可能会（和将）根据哪些物品在库存中哪些不在而变化。 可以创建存储过程。存储过程简单来说，就是为以后的使用而保存的一条或多条MySQL语句的集合。可将其视为批文件，虽然它们的作用不仅限于批处理。 2、为什么要使用存储过程？ 下面列出一些主要的理由： 1. 通过把处理封装在容易使用的单元中，简化复杂的操作。 2. 由于不要求反复建立一系列处理步骤，这保证了数据的完整性。如果所有开发人员和应用程序都使用同一（试验和测试）存储过程，则所使用的代码都是相同的。 这一点的延伸就是防止错误。需要执行的步骤越多，出错的可能性就越大。防止错误保证了数据的一致性。 3. 简化对变动的管理。如果表名、列名或业务逻辑（或别的内容）有变化，只需要更改存储过程的代码。使用它的人员甚至不需要知道这些变化。 这一点的延伸就是安全性。通过存储过程限制对基础数据的访问减少了数据讹误（无意识的或别的原因所导致的数据讹误）的机会。 4. 提高性能。因为使用存储过程比使用单独的SQL语句更快。 5. 存在一些只能用在单个请求中的MySQL元素和特性，存储过程以使用它们来编写功能更强更灵活的代码。 换句话说，使用存储过程有3个主要的好处，即简单、安全、高性能。 不过，在将SQL代码转换为存储过程前，也必须知道它的一些缺陷： 1. 一般来说，存储过程的编写比基本的SQL语句复杂，编写存储过程需要更高的技能，更丰富的经验 2. 你可能没有创建存储过程的安全访问权限。许多数据库管理员限制存储过程的创建权限，允许用户使用存储过程，但不允许他们创建存储过程。 3、如何使用存储过程？ 存储过程的执行远比其定义更经常遇到，因此，我们将从执行存储过程开始介绍。然后再介绍创建和使用存储过程。 3.1 执行存储过程 MySQL称存储过程的执行为调用，因此MySQL执行存储过程的语句为CALL。CALL接受存储过程的名字以及需要传递给它的任意参数。 例如： productpricing(@pricelow,12@pricehigh,@priceaverage); 其中，执行名为productpricing的存储过程，它计算并返回产品的最低、最高和平均价格。 3.2 创建存储过程 例子：一个返回产品平均价格的存储过程： 1234CREATE PROCEDURE productpricing()BEGIN SELECT Avg(prod_price) AS priceaverage FROM products;END; 此存储过程名为productpricing，用CREATE PROCUDURE productpricing()语句定义。 如果存储过程接受参数，它们将在()中列举出来。此存储过程没有参数，但后跟的()仍然需要。BEGIN和END语句用来限定存储过程体，过程体本身仅是一个简单的SELECT语句。 在MySQL处理这段代码时，它创建一个新的存储过程productpricing。没有返回数据，因为这段代码并未调用存储过程，这里只是为以后使用而创建它。 （注：mysql命令行客户机的分隔符，如果你使用的是mysql命令行实用程序，请往下看：默认的MySQL语句分隔符;（正如你已经在迄今为止所使用的MySQL语句中所看到的那样）。mysql命令行实用程序也使用;作为语句分隔符。如果命令行使用程序要解释存储过程自身内的;字符，则它们最终不会成为存储过程的成分，这会使存储过程中的SQL出现句法错误。解决办法是临时更改命令行使用程序的语句分隔符，如下所示： 12345678DELIMITER //CREATE PROCEDURE productpricing()BEGIN SELECT Avg(prod_price) AS priceaverage FROM products;END //DELIMITER ; 其中，DELIMITER // 告诉命令行实用程序使用//作为新的语句结束分隔符，可以看到标志存储过程结束的END定义为END//而不是END;。这样，存储过程体内的;仍然保持不动，并且正确地传递给数据库引擎。最后，为恢复为原来的语句分隔符，可使用DELIMITER ;。除\符号外，任何字符都可以用作语句分隔符。如果你使用的是mysql命令行实用程序，在阅读本章时请记住这里的内容。） 3.3 使用存储过程 CALL productpricing(); 执行刚创建的存储过程并显示返回的结果。因为存储过程实际上是一种函数，所以存储过程名后需要有()符号（即使不传递参数也需要）。 3.4 删除存储过程 DROP PROCEDURE productpricing; 请注意没有使用后面的()，只给出存储过程名。 （注：如果指定的过程不存在，则DROP PROCEDURE将产生一个错误。当过程存在想删除它时（如果过程不存在也不产生错误）可使用DROP PROCEDURE IF EXISTS。） 3.4 使用参数 以下是productpricing的修改版本(如果不先删除此存储过程，则不能再次创建它)： 12345678910CREATE PROCEDURE productpricing( OUT pl DECIMAL(8, 2), OUT ph DECIMAL(8, 2), OUT pa DECIMAL(8, 2))BEGIN SELECT Min(prod_price) INTO pl FROM products; SELECT Max(prod_price) INTO ph FROM products; SELECT Avg(prod_price) INTO pa FROM products;END; 此存储过程接受3个参数：pl存储产品的最低价格，ph存储产品的最高价格，pa存储产品的平均价格。每个参数必须具有指定的类型，这里使用十进制值。 关键字OUT指出相应的参数用来从存储过程传出一个值（返回给调用者）。 MySQL支持IN（传递给存储过程）、OUT（从存储过程传出）和INOUT（对存储过程传入和传出）类型的参数。 （注：存储过程的参数允许的数据类型与表中使用的数据类型相同。） 调用此修改过的存储过程，必须指定3个变量名。 CALL productpricing(@pricelow, @pricehigh, @priceaverage); （注：所有MySQL变量都必须以@开始。） 为了显示检索出的值，可以使用以下语句： SELECT @pricehigh, @pricelow, @priceaverage; 3.5 建立智能存储过程 只有在存储过程内包含业务规则和智能处理时，它们的威力才真正显现出来。 考虑这个场景。你需要获得与以前一样的订单合计，但需要对合计增加营业税，不过只针对某些顾客。那么，你需要做下面几件事情： 1. 获得合计（与以前一样）； 2. 把营业税有条件地添加到合计； 3. 返回合计（带或不带税）。 存储过程的完整工作如下： 1234567891011121314151617181920212223242526272829-- Name: ordertotal-- Parameters: onumber = order number-- taxable = 0 if not taxable,1 if taxable-- ototal = order total variableCREATE PROCEDURE ordertotal( IN onumber INT, IN taxable BOOLEAN, OUT ototal DECIMAL(8,2)) COMMENT &apos;Obtain order total, optionally adding tax&apos;BEGIN -- Declare variable for total DECLARE total DECIMAL(8,2); -- Declare tax percentage DECLARE taxrate INT DEFAULT 6; -- Get the order total SELECT Sum(item_price*quantity) FROM orderitems WHERE order_num = onumber INTO total; -- Is this taxable? IF taxable THEN -- Yes, so add taxrate to the total SELECT total+(total/100*taxrate) INTO total; END IF; -- AND finally, save to out variable SELECT total INTO ototal;END; 此存储过程有很大的变动。首先，增加了注释（前面放置–）。添加了另外一个参数taxable，它是一个布尔值。在存储过程体中，用DECLARE语句定义了两个局部变量。DECLARE要求指定变量名和数据类型，它也支持可选的默认值（这个例子中的taxable的默认值被设置为6%）。SELECT语句已经改变，因此其结果存储到total（局部变量）而不ototal。IF语句检查taxable是否为真，如果为真，则用另一SELECT语句增加营业税到局部变量total。最后，用另一SELECT语句将total（它增加或许不增加营业税）保存到ototal。 （注：COMMENT关键字：它不是必需的，但如果给出，将在SHOW PROCEDURE STATUS的结果中显示。） （注：IF语句：IF语句还支持ELSEIF和ELSE子句（前者还使用THEN子句，后者不使用）。） 3.6 检查存储过程 为显示用来创建一个存储过程的CREATE语句，使用SHOW CREATE PROCEDURE语句： SHOW CREATE PROCEDURE ordertotal; 为了获得包括何时、由谁创建等详细信息的存储过程列表，使用SHOW PROCUDURE STATUS语句： SHOW PROCEDURE STATUS LIKE &#39;ordertotal&#39;; （注：SHOW PROCEDURE STATUS列出所有存储过程，为了限制其输出，可使用LIKE指定一个过滤模式。） 二十四、使用游标1、什么是游标？（注：MySQL 5添加了对游标的支持） 使用简单的SELECT语句，没有办法得到第一行、下一行或前10行，也不存在每次一行地处理所有行的简单方法。有时，需要在检索出来的行中前进或后退一行或多行。这就是使用游标的原因。 游标（cursor）是一个存储在MySQL服务器上的数据库查询，它不是一条SELECT语句，而是被该语句检索出来的结果集。在存储了游标之后，应用程序可以根据需要滚动或浏览其中的数据。 游标主要用于交互式应用，其中用户需要滚动屏幕上的数据，并对数据进行浏览或做出更改。（注：MySQL游标只能用于存储过程（和函数）。） 2、如何使用游标？ 2.1使用游标涉及几个明确的步骤： 1. 在能够使用游标前，必须声明（定义）它。这个过程实际上没有检索数据，它只是定义要使用的SELECT语句。 2. 一旦声明后，必须打开游标以供使用。这个过程用前面定义的SELECT语句把数据实际检索出来。 3. 对于填有数据的游标，根据需要取出（检索）各行。 4. 在结束游标使用时，必须关闭游标。 2.2 创建游标 游标用DECLARE语句创建。DECLARE命名游标，并定义相应的SELECT语句，根据需要带WHERE和其它子句。 例如：下面语句定义了名为ordernumbers的游标： 123456CREATE PROCEDURE processorders()BEGIN DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders;END; 2.3 打开和关闭游标 在定义游标之后，可以打开它。 游标用OPEN CURSOR语句来打开： OPEN ordernumbers; 游标处理完成后，应当使用如下语句关闭游标： CLOSE ordernumbers; 在一个游标关闭后，如果没有重新打开，则不能使用它。但是，使用声明过的游标不需要再次声明，用OPEN语句打开它就可以了。 2.4 使用游标数据 在一个游标被打开后，可以使用FETCH语句分别访问它的每一行。FETCH指定检索什么数据（所需的列），检索出来的数据存储在什么地方。它还向前移动游标中的内部行指针，使下一条FETCH语句检索下一行（不重复读取同一行）。 下面给出游标存储过程对取出的数据进行某种实际的处理： 12345678910111213141516171819202122232425262728293031323334353637CREATE PROCEDURE processorders()BEGIN -- Declare local variables DECLARE done BOOLEAN DEFAULT 0; DECLARE o INT; DECLARE t DECIMAL(8,2); -- Declare the cursor DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; -- Declare continue handler DECLARE CONTINUE HANDLER FOR SQLSTATE &apos;02000&apos; SET done=1; -- Create a table to store the results CREATE TABLE IF NOT EXISTS ordertotals (order_num INT, total DECIMAL(8,2)); -- Open the cursor OPEN ordernumbers; -- Loop through all rows REPEAT -- Get order number FETCH ordernumbers INTO o; -- Get the total for this order CALL ordertotal(o, 1, t); -- Insert order and total into ordertotals INSERT INTO ordertotals(order_num, total) VALUES(o, t); -- End of loop UNTIL done END REPEAT; -- Close the cursor CLOSE ordernumbers;END; 此例子中，FETCH是在REPEAT内，因此它反复执行直到done为真（由UNTIL done END REPEAT;规定）。为使它起作用，用一个DEFAULT 0(假，不结束)定义变量done。 而done是通过以下语句设置结束条件： DECLARE CONTINUE HANDLER FOR SQLSTATE &#39;02000&#39; SET done=1; 这条语句定义了一个CONTINUE HANDLER，它是在条件出现时被执行的代码。这里，它指出当SQLSTATE ‘02000’出现时，SET done=1。SQLSTATE ‘02000’是一个未找到条件，当REPEAT由于没有更多的行供循环而不能继续时，出现这个条件。 （注：DECLARE语句的次序：用DECLARE语句定义的局部变量必须在定义任意游标或句柄之前定义，而句柄必须在游标之后定义。不遵守此顺序将产生错误信息。） （注：重复或循环：除这里使用的REPEAT语句外，MySQL还支持循环语句，它可用来重复执行代码，直到使用LEAVE语句手动退出位置。通常REPEAT语句的语法使它更适合于对游标进行循环。） 二十五、触发器（注：MySQL 5增加了对触发器的支持。）1、什么是触发器？ MySQL语句在需要时被执行，存储过程也是如此。但是，如果你想要某条语句（或某些语句）在事件发生时自动执行。例如： 1. 每当增加一个顾客到某个数据表时，都检查其电话号码格式是否正确等； 2. 每当订购一个产品时，都从库存数量中减去订购的数量； 3. 无论何时删除一行，都在某个存档表中保留一个副本。 所有这些例子的共同之处是它们都需要在某个表发生更改时自动处理。这确切地说就是触发器。触发器是MySQL相应以下任意语句而自动执行的一条MySQL语句（或位于BEGIN和END语句之间的一组语句）： DELETE、INSERT、UPDATE 其它MySQL语句不支持触发器。 2、为什么要使用触发器？ 2.1 创建触发器 在创建触发器时，需要给出4条信息： 1. 唯一的触发器名；（保持每个数据库的触发器名唯一） 2. 触发器关联的表； 3. 触发器应该响应的活动（DELETE、INSERT或UPDATE）； 4. 触发器何时执行（处理之前或之后）。 触发器用CREATE TRIGGER语句创建。下面是一个例子： 12CREATE TRIGGER newproduct AFTER INSERT ON productsFOR EACH ROW SELECT &apos;Product added&apos;; 在这个例子中，文本Product added将对每个插入的行显示一次。 （注：只有表才支持触发器，视图不支持（临时表也不支持）。） 触发器按每个表每个事件每次地定义，每个表每个事件每次只允许一个触发器。因此，每个表最多支持6个触发器（每条INSERT、UPDATE和DELETE的之前和之后）。单一触发器不能与多个时间或多个表关联，所以，如果你需要一个对INSERT和UPDATE操作执行的触发器，则应该定义两个触发器。 （注：如果BEFORE触发器失败，则MySQL将不执行请求的操作。此外，如果BEFORE触发器或语句本身失败，MySQL将不执行AFTER触发器（如果有的话）。） 2.2 删除触发器 使用 DROP TRIGGER语句，如下所示： DROP TRIGGER newproduct; 触发器不能更新或覆盖。为了修改一个触发器，必须先删除它，然后再重新创建。 3、如何使用触发器？ 查看已有触发器： show triggers; 3.1 INSERT触发器 INSERT触发器在INSERT语句执行之前或之后执行。需要知道以下几点： 1. 在INSERT触发器代码内，可引用一个名为NEW的虚拟表，访问被插入的行； 2. 在BEFORE INSERT触发器中，NEW中的值也可以被更新（允许更改被插入的值）； 3. 对于AUTO_INCREMENT列，NEW在INSERT执行之前包含0，在INSERT执行之后包含新的自动生成值。 示例： 12CREATE TRIGGER neworder AFTER INSERT ON ordersFOR EACH ROW SELECT NEW.order_num; 此代码创建一个名为neworder的触发器，它按照AFTER INSERT ON orders执行。在插入一个新订单到orders表时，MySQL生成一个新订单号并保存到order_num中。触发器从NEW.order_num取得这个值并返回它。此触发器必须按照AFTER INSERT执行，因为在BEFORE INSERT语句执行之前，新order_num还没有生成。对于orders的每次插入使用这个触发器将总是返回新的订单号。 （注：通常，将BEFORE用于数据验证和净化（目的是保证插入表中的数据确实是需要的数据）。本提示也适用于UPDATE触发器。） 3.2 DELETE触发器 DELETE触发器在DELETE语句执行之前或之后执行。需要知道以下两点： 1. 在DELETE触发器代码内，你可以引用一个名为OLD的虚拟表，访问被删除的行； 2. OLD中的值全都是只读的，不能更新。 示例： 123456CREATE TRIGGER deleteorder BEFORE DELETE ON ordersFOR EACH ROWBEGIN INSERT INTO archive_orders(order_num, order_date, cust_id) VALUES(OLD.order_num, OLD.order_date, OLD.cust_id);END; 在任意订单被删除前将执行此触发器。它使用一条INSERT语句将OLD中的值（要被删除的订单）保存到一个名为archive_orders的存档表中。 使用BEFORE DELETE触发器的优点（相对于AFTER DELETE触发器来说）为，如果由于某种原因，订单不能存档，DELETE本身将被放弃。 （注：多语句触发器。触发器deleteorder使用BEGIN和END语句标记触发器体。使用BEGIN END块的好处是触发器能容纳多条SQL语句。） 3.3 UPDATE触发器 UPDATE触发器在UPDATE语句执行之前或之后执行。需要知道以下几点： 1. 在UPDATE触发器代码中，你可以引用一个名为OLD的虚拟表访问以前（UPDATE语句前）的值，引用一个名为NEW的虚拟表访问新更新的值。 2. 在BEFORE UPDATE触发器中，NEW中的值可能也被更新（允许更改将要用于UPDATE语句中的值）； 3. OLD中的值全都是只读的，不能更新。 下面的例子保证州名缩写总是大写（不管UPDATE语句中给出的是大写还是小写）： 12CREATE TRIGGER updatevendor BEFORE UPDATE ON vendorsFOR EACH ROW SET NEW.vend_state = Upper(NEW.vend_state); 显然，任何数据净化都需要在UPDATE语句之前进行，就像这个例子中一样。每次更新一个行时，NEW.vend_state中的值（将用来更新表行的值）都用Upper(NEW.vend_state)替换。4、关于触发器的进一步介绍 再介绍一些使用触发器时需要记住的重点： 1. 与其他DBMS相比，MySQL 5中支持的触发器相当初级。未来会有一些改进和增强触发器支持的计划。 2. 创建触发器可能需要特殊的安全访问权限，但是，触发器的执行时自动的。如果INSERT、UPDATE、DELETE语句能够执行，则相关的触发器也能执行。 3. 应该用触发器来保证数据的一致性（大小写、格式等）。在触发器中执行这种类型的处理优点是它总是进行这种处理，而且是透明地进行，与客户机应用无关。 4. 触发器的一种非常有意义的使用是创建审计跟踪。使用触发器，把更改（如果需要，甚至还有之前和之后的状态）记录到另一个表非常容易。 5. 遗憾的是，MySQL触发器中不支持CALL语句。这表示不能从触发器内调用存储过程。所需的存储过程代码需要复制到触发器内。 二十六、管理事务处理（MyISAM和InnoDB是两种常用的引擎，前者不支持事务处理，而后者支持。）1、什么是事务处理 事务处理（transaction processing）可以用来维护数据库的完整性，它保证成批的MySQL操作要么完全执行，要么完全不执行。 下面是关于事务处理需要知道的几个术语： 1. 事务（transaction）指一组SQL语句； 2. 回退（rollback）指撤销指定SQL语句的过程； 3. 提交（commit）指将未存储的SQL语句结果写入数据库表； 4. 保留点（savepoint）指事务处理中设置的临时占位符（placeholder），你可以对它发布回退（与回退整个事务处理不同）。 2、如何利用COMMIT和ROLLBACK语句来管理事务处理 二十七、全球化和本地化1、字符集和校对顺序 数据库表被用来存储和检索数据。不同的语言和字符集需要以不同的方式存储和检索。 重要术语： 1. 字符集 为字母和符号的集合； 2. 编码 为某个字符集成员的内部表示； 3. 校对 为规定字符如何比较的指令。 （注：校对为什么重要。考虑词APE、apex和Apple。它们处于正确的排序顺序吗？这有赖于你是否想区分大小写。使用区分大小写的校对顺序，这些词有一种排序方式，使用不区分大小写的校对顺序有另外一种排序方式。这不仅影响（如用ORDER BY排序数据），还影响搜索（例如，寻找apple的WHERE子句是否能够找到APPLE）。）2、使用字符集和校对顺序 MySQL支持众多的字符集。 为查看所有支持的字符集完整列表，使用以下语句： SHOW CHARACTER SET; 这条语句显示所有可用的字符集以及每个字符集的描述和默认校对。 为了查看所支持校对的完整列表，使用以下语句： SHOW COLLATION; 此语句显示所有可用的校对，以及它们适用的字符集。可以看到有的字符集具有不止一种校对。 通常系统管理在安装时定义一个默认的字符集和校对。此外，也可以在创建数据库时，指定默认的字符集和校对。为了确定所用的字符集和校对，可以使用以下语句： SHOW VARIABLES LIKE &#39;character%&#39;; SHOW VARIABLES LIKE &#39;collation%&#39;; 实际上，字符集很少是服务器范围（甚至数据库范围）的设置。不同的表，甚至不同的列都可能需要不同的字符集，而且两者都可以创建表时指定。 一般，MySQL如下确定使用什么样的字符集和校对： 1. 如果指定CHARACTER SET和COLLATE两者，则使用这些值。 2. 如果只指定CHARACTER SET，则使用此字符集及其默认的校对（如SHOW CHARACTER SET的结果中所示）。 3. 如果既不指定CHARACTER SET，也不指定COLLATE，则使用数据库默认。 最后，值得注意的是，如果绝对需要，串可以在字符集之间进行转换。为此，使用Cast()或Convert()函数。 二十八、安全管理1、访问控制 MySQL服务器的安全基础是：用户应该对他们需要的数据具有适当的访问权，既不能多也不要能少。换句话说，用户不能对过多的数据具有过多的访问权。 考虑以下内容： 1. 多数用户只需要对表进行读和写，但少数用户甚至需要能创建和删除表。 2. 某些用户需要读表，但可能不需要更新表； 3. 你可能想允许用户添加数据，但不允许它们删除数据； 4. 某些用户（管理员）可能需要处理用户账号的权限，但多数用户不需要； 5. 你可能想让用户通过存储过程访问数据，但不允许他们直接访问数据； 6. 你可能想根据用户登录的地点限制对某些功能的访问。 2、管理用户 MySQL用户账号和信息存储在名为mysql的MySQL数据库中。一般不需要直接访问mysql数据库和表，但有时需要直接访问。需要直接访问它的时机之一是需要获得所有用户的账号列表时。 mysql数据库有一个名为user的表，它包含所有用户账号。 2.1 创建用户账号 使用CREATE USER语句： CREATE USER ben IDENTIFIED BY &#39;p@$$wOrd&#39;; 2.2 重新命名一个用户账号 使用RENAME USER语句： RENAME USER ben TO bforta; （注：仅MySQL5或之后的版本支持 RENAME USER，为了在以前的MySQL中重命名一个用户，可使用UPDATE直接更新user表。） 2.3 删除用户账号 使用DROP USER语句： DROP USER bforta; （注：自MySQL5以来，DROP USER删除用户账号和所有相关的账号权限。在MySQL5之前，DROP USER只能用来删除用户账号，不能删除相关账号权限。因此，如果使用旧版本的MySQL，需要先用REVOKE删除和账号相关的权限，然后再用DROP USER删除账号。） 2.4 设置访问权限 1. 查看赋予用户账号的权限，使用SHOW GRANTS FOR语句，如下： `SHOW GRANTS FOR bforta;` 2. 设置权限，使用GRANT语句。GRANT要求至少给出以下信息： （1）要授予的权限； （2）被授予访问权限的数据库或表； （3）用户名。 如下例子给出GRANT的用法： `GRANT SELECT ON crashcourse.* TO bforta;` 此GRANT允许用户在crashcourse.*（crashcourse数据库的所有表）上使用SELECT。通过只授予SELECT访问权限，用户bforta对crashcourse数据库中的所有数据具有只读访问权限。 3. GRANT的反操作为REVOKE，用它来撤销特定的权限。举个例子： `REVOKE SELECT ON crashcourse.* FROM bforta;` 这条REVOKE语句取消刚赋予用户bforta的SELECT访问权限。被撤销的访问权限必须存在，否则会出错。 GRANT和REVOKE可在几个层次上控制访问权限： （1）整个服务器，使用GRANT ALL和REVOKE ALL; （2）整个数据库，使用ON database.*; （3）特定的表，使用ON database.table; （4）特定的列； （5）特定的存储过程。 可以授予或撤销的每个权限： 权限 | 说明 ----------------|------------------------------ ALL | 除GRANT OPTION外的所有权限 SELECT | 使用SELECT INSERT | 使用INSERT UPDATE | 使用UPDATE DELETE | 使用DELETE CREATE | 使用CREATE TABLE DROP | 使用DROP TABLE GRANT OPTION | 使用GRANT和REVOKE ALTER | 使用ALTER TABLE ALTER ROUTINE | 使用ALTER PROCEDURE和DROP PROCEDURE CREATE ROUTINE | 使用CREATE PROCEDURE CREATE TEMPORARY TABLES | 使用CREATE TEMPORARY TABLE CREATE USER | 使用CREATE USER、DROP USER、RENAME USER和REVOKE ALL PRIVILEGES CREATE VIEW | 使用CREATE VIEW INDEX | 使用CREATE INDEX和DROP INDEX LOCK TABLES | 使用LOCK TABLES EXECUTE | 使用CALL和存储过程 FILE | 使用SELECT INTO OUTFILE和LOAD DATA INFILE PROCESS | 使用SHOW FULL PROCESSLIST RELOAD | 使用FLUSH REPLICATION CLIENT | 服务器位置的访问 REPLICATION SLAVE | 由复制从属使用 SHOW DATABASES | 使用SHOW DATABASES SHOW VIEW | 使用SHOW CREATE VIEW SHUTDOWN | 使用mysqladmin shutdown(用来关闭MySQL) SUPER | 使用CHANGE MASTER、KILL、LOGS、PURGE、MASTER和SET GLOBAL。还允许mysqladmin调试登录 USAGE | 无访问权限 使用GRANT和REVOKE，在结合列出的权限，你能对用户可以就你的宝贵数据做什么事情和不能做什么事情具有完全的控制。 （注：多个权限用逗号分隔） （注：未来授权：在使用GRANT和REVOKE时，用户账号必须存在，但对所涉及的对象没有这个要求。这允许管理员在创建数据库和表之前设计和实现安全措施。 这样做的副作用是，当某个数据库或表删除时（用DROP语句），相关的访问权限仍然存在。而且，如果将来重新创建该数据库或表，这些权限仍然起作用。） 二十九、数据库维护MySQL主要的日志文件有以下几种： 1. 错误日志。它包含启动和关闭问题以及任意关键错误的细节。此日志通常名为hostname.err，位于data目录中。此日志名可用--log-error命令行选项更改。 2. 查询日志。它记录所有MySQL活动，在诊断问题时非常有用。此日志文件可能会很快地变得非常大，因此不应该长期使用它。此日志通常名为hostname.log，位于data目录中。此名字可以用--log命令行选项更改。 3. 二进制日志。它记录更新过数据（或者可能更新过数据）的所有语句。此日志通常名为hostname-bin，位于data目录内。此名字可以用--log-bin命令行选项更改。注意，这个日志文件是MySQL5中添加的，以前的MySQL版本中使用的是更新日志。 4. 缓慢查询日志。顾名思义，此日志记录执行缓慢的任何查询。这个日志在确定数据库何处需要优化很有用。此日志通常名为hostname-slow.log，位于data目录中。此名字可以用--log-slow-queries命令行选项更改。 在使用日志时，可用FLUSH LOGS语句来刷新和重新开始所有日志文件。 三十、改善新能下面是一些性能优化的技巧： 1. 一般来说，使用存储过程比一条一条地执行其中的各条MySQL语句快； 2. 使用正确的数据类型； 3. 绝不要检索比需求还要多的数据。换言之，不要用SELECT *（除非你真正需要每个列）。 4. 必须索引数据库表以改善数据检索的性能；但注意，索引改善数据检索的性能，但损害数据插入、删除和更新的性能； 5. 使用多条SELECT语句和连接它们的UNION语句，能够获得极大的性能改进； 6. LIKE很慢。一般来说，最好是使用FULLTEXT而不是LIKE； 7. 最重要的规则就是，每条规则在某些条件下都会被打破。]]></content>
      <categories>
        <category>书籍</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《MySQL必知必会》-基础知识]]></title>
    <url>%2F2017%2F12%2F07%2FMySQL%E3%80%8A%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E3%80%8B-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[一、数据库基础1、数据库：数据库（database）保存有组织的数据的容器（通常是一个文件或一组文件）。数据库软件应称为DBMS（数据库管理系统）。数据库是通过DBMS创建和操纵的容器。你使用的是DBMS，它替你访问数据库。 2、表：表（table）某种特定类型数据的结构化清单。（注：表名：在不同的数据库中可以使用相同的表名。但是在同一数据库却不能使用相同的表名。 表名 = 数据库_表名）3、列和数据类型：列（column）表中的一个字段。所有的表都是由一个或多个字段构成的。数据类型（datatype）所容许的数据的类型。每个表列都有相应的数据类型，它限制（或容许）该列中存储的数据。 4、行：行（row）表中的一个记录（record）。 5、主键：主键（primary key）一列（或一组列），其值能够唯一区分表中每个行。（注：主键列不允许NULL值。 还有一种非常重要的键，称为外键。） 6、什么是SQLSQL是结构化查询语言（Structured Query Language)的缩写。SQL是一种专门用来与数据库通信的语言。 二、MySQL简介MySQL是一种DBMS，即它是一种数据库软件。学习MySQL最好使用专门用途的实用程序，特别有3个工具需要提及： mysql命令行实用程序 MySQL Administrator（MySQL管理器）是一个图形交互客户机，用来简化MySQL服务器的管理。 MySQL Query Browser为一个图形交互客户机，用来编写和执行MySQL命令。 三、使用MySQLmysql命令行命令： USE database; #选择使用数据库 SHOW DATABASES; #显示可用数据库的一个列表 SHOW TABLES; #返回当前数据库内可用表的列表 SHOW STATUS; #用于显示广泛的服务器状态信息 SHOW CREATE DATABASE和SHOWCREATE TABLES name #显示创建特定数据库或表的MySQL语句 SHOW GRANTS #用来显示授予用户（所有用户或特定用户）的安全权限 SHOW ERRORS和SHOWWARNINGS #用来显示服务器错误或警告信息 四、检索数据关键字： SELECT、DISTINCT、LIMIT 五、排序检索数据子句（clause）SQL语句由子句构成，有些子句是必须的，而有的是可选的。一个子句通常由一个关键字和所提供的数据组成。子句的例子有SELECT语句的FROM子句。排序子句：ORDER BY 六、过滤数据过滤子句：WHERE（注：在同时使用ORDER BY和WHERE子句时，应该让ORDER BY位于WHERE之后）WHERE子句操作符： >、&lt;、&gt;=、&lt;=、=、!=、&lt;&gt;、BETWEEN(必须用AND关键字分割) 七、数据过滤操作符（operator）用来联结或改变WHERE子句中的子句的关键字。也称为逻辑操作符（logical operator)。操作符：AND、OR、IN、NOT（注：SQL在处理OR操作符之前，优先处理AND操作符。如果需要加括号）（注：IN的最大优点是可以包含其他SELECT语句，使得能够更动态地建立WHERE子句。）（注：MySQL支持使用NOT对IN、BETWEEN和EXISTS子句取反） 八、用通配符进行过滤通配符（wildcard）用来匹配值的一部分的特殊字符。（在搜索子句中使用通配符，必须使用LIKE操作符。）通配符： 百分号（%）通配符：表示任何字符出现任意次数。（注：尾空格可能会干扰通配符匹配。办法是去掉首尾空格。） 下划线（_）通配符：用途与%一样，但下划线只匹配单个字符而不是多个字符。 九、正则表达式REGEXP 与 LIKE 使用类似 十、创建计算字段字段拼接函数：Concat()空格删除函数：RTrim()、LTrim()、Trim()关键字：AS （别名用AS赋予）MySQL算数操作符：+、-、*、/ 十一、使用数据处理函数（注：函数的可移植性不强。因此决定使用函数，应该保证做好代码的注释，以便以后能确切地知道所编写SQL代码的含义。）文本处理函数： 函数名 说明 Upper() 将文本转换为大写 Lower() 将串转换为小写 Length() 返回串的长度 LTrim() 去掉串左边的空格 RTrim() 去掉串右边的空格 Left() 返回串左边的字符 Right() 返回串右边的字符 Locate() 找出串的一个子串 SubString() 返回子串的字符 Soundex() 返回串的SOUNDEX值日期和时间处理函数： AddDate() 增加一个日期（天、周等） AddTime() 增加一个时间（时、分等） CurDate() 返回当前日期 CurTime() 返回当前时间 Date() 返回日期时间的日期部分 DateDiff() 计算两个日期之差 Date_Add() 高度灵活的日期运算函数 Date_Format() 返回一个格式化的日期或时间串 Day() 返回一个日期的天数部分 DayOfWeek() 对于一个日期，返回对应的星期几 Hour() 返回一个时间的小时部分 Minute() 返回一个时间的分钟部分 Month() 返回一个日期的月份部分 Now() 返回当前日期和时间 Second() 返回一个时间的秒部分 Time() 返回一个日期时间的时间部分 Year() 返回一个日期的年份部分 （注：不管是插入或更新表值还是用WHERE子句进行过滤，日期必须为格式yyyy-mm-dd。防止歧义性。） 例子：（1）SELECT cust_id,order_num FROM orders WHERE order_date=&#39;2005-09-01&#39;;此例子中值全部具有时间值 00:00:00，但实际中很可能并不总是这样。因此，需要比较日期部分，而把时间部分忽略。如下： SELECT cust_id,order_num FROM orders WHERE Date(order_date)=&#39;2005-09-01&#39;;如果想要比较的是日期部分，Date()是一个很好的选择。（2）如果你想检索出2005年9月下的所有订单？ SELECT cust_id,order_num FROM orders WHERE Date(order_date) BETWEEN &#39;2005-09-01&#39; AND &#39;2009-09-30&#39;;弊端：如果搜索的月份是闰年2月的话，还需要注意是否闰月。 SELECT cust_id,order_num FROM orders WHERE Year(order_date)=2005 AND Month(order_date)=9; 数值处理函数： 函数名 说明 Abs() 返回一个数的绝对值 Cos() 返回一个角度的余弦 Exp() 返回一个数的指数值 Mod() 返回除操作的余数 Pi() 返回圆周率 Rand() 返回一个随机数 Sin() 返回一个角度的正弦 Sqrt() 返回一个数的平方根 Tan() 返回一个数的正切 十二、汇总数据聚集函数（aggregate function)运行在行组上，计算和返回单个值的函数。SQL聚集函数： AVG()、COUNT()、MAX()、MIN()、SUM() 十三、分组数据分组数据设计两个SELECT语句子句：GROUP BY子句和HAVING子句分组数据：GROUP BY过滤分组：HAVING（区别：WHERE过滤行，HAVING过滤分组。且HAVING支持所有WHERE操作符。还有一种理解方法：WHERE在数据分组前进行过滤，HAVING在数据分组后进行过滤。）（注：一般在使用GROUP BY子句时，应该也给出ORDER BY子句，这是保证数据正确排序的唯一方法，千万不要仅依赖GROUP BY排序数据。） SELECT子句及其顺序： 子句 说明 是否必须使用 SELECT 要返回的列或表达式 是 FROM 从中检索数据的表 仅在从表选择数据时使用 WHERE 行级过滤 否 GROUP BY 分组说明 仅在按组计算聚集时使用 HAVING 组级过滤 否 ORDER BY 输出排序顺序 否 LIMIT 要检索的行数 否 十四、使用子查询子查询最常见的使用是在WHERE子句的IN操作符中，以及用来填充计算列。 十五、联结表外键（foreign key）：外键为某个表中的一列，它包含另一个表的主键值，定义了两个表之间的关系。可伸缩性（scale）：能够适应不断增加的工作量而不失败。设计良好的数据库或应用程序称之为可伸缩性好（scale well）。笛卡儿积（cartesian product）：由没有联结条件的表关系返回的结果为笛卡儿积。检索出的行的数数目将是第一个表中的行数乘以第二个表中的行数。联结：联结是一种机制，用来在一条SELECT语句中关联表，因此称之为联结。 使用WHERE创建联结：SELECT vend_name,prod_name,prod_price FROM vendors,products WHERE vendors.vend_id = products.vend_id ORDER BY vend_name,prod_name;（注：如果不加WHERE子句，结果将变成笛卡儿积。） 内部联结（INNER JOIN）SELECT vend_name,prod_name,prod_price FROM vendors INNER JOIN products ON vendors.vend_id = products.vend_id;（注：ANSI SQL规范首选INNER JOIN语法。）（性能考虑：1. MySQL在运行时关联指定的每个表以处理联结。这种处理可能非常耗费资源的，因此不要联结不必要的表。联结的表越多，性能下降越厉害；2. 利用联结而不使用子查询。） 十六、创建高级联结使用表别名。外部联结（OUTER JOIN）：包含没有关联行的那些行。 SELECT c.cust_id,o.order_num FROM customers as c LEFT OUTER JOIN orders as o ON c.cust_id = o.cust_id;在使用OUTER JOIN语法时，必须使用RIGHT或LEFT关键字指定包括其所有行的表。 十七、组合查询MySQL允许执行多个查询（多条SELECT语句），并将结果作为单个查询结果集返回。这些组合查询通常称为并（union）或复合查询（compound query）。关键字：UNION UNION和UNION ALL的区别：UNION从查询结果集中自动去除了重复的行。UNION ALL返回所有匹配的行。 在用UNION组合查询时，只能使用一条ORDER BY子句，它必须出现在最后一条SELECT语句之后。不允许使用多条ORDER BY子句。 十八、全文本搜索（注：并非所有引擎都支持全文本搜索，MyISAM支持，而InnoDB不支持。）全文索引：FULLTEXT使用两个函数Match()和Against()执行全文本搜索，其中Match()指定被搜索的列，Against()指定要使用的搜索表达式。例如： SELECT note_text FROM productnotes WHERE Match(note_text) Against(&#39;rabbit&#39;);（注：传递给Match()的值必须与FULLTEXT()定义中的相同。） 十九、插入数据关键字：INSERT INSERT INTO table(fields) VALUES(values);（注：多行插入时，values值可以是数组。） 二十、更新和删除数据关键字：UPDATE、DELETE（注：更新和删除操作时，不要省略WHERE子句，否则会删除或更新表中所有行。） UPDATE table SET field1=value1,...fieldn=valuen WHERE ...; DELETE FROM table WHERE ...; 二十一、创建和操纵表1、表的创建：CREATE TABLE 语句1234567CREATE TABLE `customers`( &apos;cust_id&apos; int(11) NOT NULL AUTO_INCREMENT, &apos;username&apos; varchar(50) NOT NULL DEFAULT=&apos;guest&apos;, &apos;email&apos; varchar(20) NOT NULL, &apos;sex&apos; enum(&apos;0&apos;, &apos;1&apos;) NOT NULL, PRIMARY KEY(&apos;cust_id&apos;)) ENGINE=MyISAM DEFAULT CHARSET=utf8; 几种常用引擎： InnoDB：是一个可靠的事务处理引擎，它不支持全文本搜索； MEMORY：在功能等同于MyISAM，但由于数据存储在内存（不是磁盘）中，速度很快（特别适合于临时表）； MyISAM：是一个性能极高的引擎，它支持全文本搜索，但不支持事务处理。（注：引擎类型可以混用。外键不能跨引擎，即使用一个引擎的表不能引用具有使用不同引擎的表的外键。） 2、表的更改：ALTER TABLE 语句 增加一个列：ALTER TABLE customers ADD phone varchar(11); 删除一个列：ALTER TABLE customers DROP COLUMN phone; 定义外键： 3、表的删除：DROP TABLE 语句 DROP TABLE customers;4、重命名表：RENAME TABLE 语句 RENAME TABLE customers TO guests;]]></content>
      <categories>
        <category>书籍</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>书籍</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo问题总结]]></title>
    <url>%2F2017%2F12%2F06%2Fhexo%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[博客栏目分类、标签不显示内容问题描述：网站栏目通过 hexo new page &#39;pagename&#39; 新增了分类、标签、关于等页面。然后陆陆续续写了几篇文章。但是，点击分类、标签页面确没有生成对应的分类和标签信息。解决方案：需要了解，通过 hexo new page &#39;pagename&#39; 生成的文件内容（以tags为例）：首先执行命令： hexo new page &#39;tags&#39;生成页面，之后再更改生成页面的内容12345---title: tagsdate: 2017-12-05 18:47:42type:--- 改为：123456---title: TagClouddate: 2017-12-05 18:47:42type: &quot;tags&quot;comments: false //多说或者Disqus评论--- 再次生成静态文件，访问成功。同理，分类页面照此修改即可！ hexo部署后，CNAME会被自动删除，怎么办？（自己的独立域名需要用CNAME连接到github pages）解决方案：将需要上传至github的内容放在source文件夹，例如CNAME、favicon.ico、images等，每次上传就不会消失了 Hexo 部署到GitHub报错：fatal: could not read Username for ‘https://github.com&#39;: No error执行 hexo d 部署时报错，信息如下： 由错误信息可知，GitHub 不能够读取 Username。因此我验证了一下本地的SSH KEY是否能够连接到GitHub，执行命令：12$ ssh -T git@github.comHi bluce-ben! You&apos;ve successfully authenticated, but GitHub does not provide shell access. 出现上面的语句说明你的ssh key已经配置好了，并没有错误，能够与 GitHub 连通。 从网上查询说是使用 ssh 登录方式来代替 https，因此进行了代码调整，如下：1234567# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git #repository: https://github.com/bluce-ben/bluce-ben.github.io.git repository: git@github.com:bluce-ben/bluce-ben.github.io.git branch: master 然后，再次执行 hexo d 命令部署成功！]]></content>
      <categories>
        <category>Hexo</category>
        <category>Hexo问题</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Hexo问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown操作指南]]></title>
    <url>%2F2017%2F12%2F06%2FMarkdown%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[标题注意要点： 标题间的空行是没有效果的，除非插入数据（这个是Markdown语法原因） 标题大小以标题首的“#”数量为准，标题末尾可添加可不添加“#” 列表Markdown 支持有序列表和无序列表。 无序列表使用星号、加号或是减号作为列表标记： 有序列表则使用数字接着一个英文句点：（注：很重要的一点是，你在列表标记上使用的数字并不会影响输出的 HTML 结果） 引用（注意：引用的区块内也可以使用其他的 Markdown 语法，包括标题、列表、代码区块等） 粗体和斜体如果要在文字前后直接插入普通的星号或底线，你可以用反斜线进行转义。（注：不管是粗体还是斜体，只有在一行才会有效果，否则会被当作普通的符号。） 代码引用（注：代码引用里面，使用markdown其它语法不识别。且反引号的数量代码后数量&lt;=代码前数量） 表格（注：输出表格前需要空行。且表格中表头不能为段落(即空格不能超过4个)，其它没要求。） 链接 此处图片地址可使用绝对路径，也可使用相对路径。（注：参考式链接此处不做说明。）Markdown 支持以比较简短的自动链接形式来处理网址和电子邮件信箱，只要是用尖括号包起来， Markdown 就会自动把它转成链接。一般网址的链接文字就和链接地址一样，例如：&lt;http://example.com/&gt;Markdown 会转为：&lt;a href=&quot;http://example.com/&quot;&gt;http://example.com/&lt;/a&gt; 图片行内式图片语法：![Alt text](img &quot;title&quot;)（注：参考式图片此处不做说明。） 分割线你可以在一行中用三个以上的星号、减号、底线来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。下面每种写法都可以建立分隔线：123456789* * *********- - ---------------------------------------- 缩进和段落缩进：可使用全角下的空格即可实现缩进段落：半角下使用4个缩进即可实现 特殊字符在写博客时，一定注意不要写这些字符，如果要写，就进行转义有些人可能说可以使用反斜杠 \ 来处理，我这里不建议。下面是比较常见的几个:1234567891011121314151617181920212223 &amp;#32; — 空格Space! &amp;#33; — 惊叹号Exclamation mark ” &amp;#34; &amp;quot; — 双引号Quotation mark # &amp;#35; — 数字标志Number sign $ &amp;#36; — 美元标志Dollar sign % &amp;#37; — 百分号Percent sign &amp; &amp;#38; &amp;amp; — Ampersand ‘ &amp;#39; — 单引号Apostrophe ( &amp;#40; — 小括号左边部分Left parenthesis ) &amp;#41; — 小括号右边部分Right parenthesis * &amp;#42; — 星号Asterisk + &amp;#43; — 加号Plus sign &lt; &amp;#60; &amp;lt; — 小于号Less than = &amp;#61; — 等于符号Equals sign &gt; &amp;#62; &amp;gt; — 大于号Greater than ? &amp;#63; — 问号Question mark @ &amp;#64; — Commercial at [ &amp;#91; — 中括号左边部分Left square bracket \ &amp;#92; — 反斜杠Reverse solidus (backslash) ] &amp;#93; — 中括号右边部分Right square bracket &#123; &amp;#123; — 大括号左边部分Left curly brace | &amp;#124; — 竖线Vertical bar &#125; &amp;#125; — 大括号右边部分Right curly brace 嵌入HTML字体颜色使用&lt;font&gt;标签：&lt;font color=&quot;red&quot;&gt;&lt;/font&gt; 图片大小控制使用&lt;img&gt;标签： &lt;img src=&quot;url&quot; width=&quot;&quot; height=&quot;&quot; /&gt; 表格宽度设定 &lt;table&gt; 中表格的宽度由标题的 &lt;th&gt; 决定，我们只需要利用上 CSS 操作一番即可达到目的。在 Markdown 中，在原表格前添加 CSS 代码，类似这样：123456789&lt;style&gt;table th:first-of-type &#123; width: 100px;&#125;&lt;/style&gt;# 下方是表格的 Markdown 语法名称|值|备注---|---|--- 这里涉及到CSS选择器的问题，首先 &lt;th&gt; 存在于 &lt;table&gt; 中；其次 th:first-of-type 的意思是每个 &lt;th&gt; 为其父级的第一个元素，这里指的就是围绕着【名称】的 &lt;th&gt;。同理第二、三个使用 th:nth-of-type(2)、th:nth-of-type(3) 就可以了，以此类推。 表格内插入 |（1）若直接在表格内插入 | ，会导致表格混乱。需要将其转化为 &amp;#124; 即可。（2）若表格表头的首格为空，如果什么都不填写，会导致表格混乱。可填写 &amp;#32;或&amp;nbsp; 即可，表示空格（不会显示到界面，同时填充了首格为空的需求）。]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo命令操作指南]]></title>
    <url>%2F2017%2F12%2F06%2Fhexo%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[基本命令hexo g == hexo generate #生成静态文件hexo s == hexo server #启动本地预览。默认情况下，访问网址为： http://localhost:4000/hexo d == hexo deploy #远程部署hexo n &quot;文章标题&quot; == hexo new &quot;文章标题&quot; #新建一篇博文 组合命令hexo s -g #等同于先输入 hexo g，再输入 hexo shexo d -g #等同于先输入 hexo g，再输入 hexo d 写作命令hexo new &quot;postName&quot; #新建文章hexo new page &quot;pageName&quot; #新建页面hexo new draft &quot;filename&quot; #新建草稿hexo publish draft &quot;filename&quot; #发布草稿注意：&lt;!--more--&gt;之上的内容为摘要。 其它命令hexo clean #清除缓存文件 (db.json) 和已生成的静态文件 (public)。hexo version #查看hexo版本号hexo publish filename #发表草稿。详解传送门hexo list [type] #列出网站资料 type:page、post、route、tag、category 官方命令：传送门]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo主题及配置文件说明]]></title>
    <url>%2F2017%2F12%2F06%2FHexo%E4%B8%BB%E9%A2%98%E5%8F%8A%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[Hexo主题本网站使用的是 Next 主题，以此为例来说明如何配置主题。Hexo官方主题：传送门Next主题：传送门 1、下载主题直接使用Git克隆到本地指定目录即可：12cd your-hexo-site$ git clone https://github.com/iissnan/hexo-theme-next themes/next 2、启用主题进入Hexo目录，打开站点配置文件，找到theme字段，并将值改为 next。theme: next到此，NexT 主题安装完成。下一步我们将验证主题是否正确启用。在切换主题之后、验证之前， 我们最好使用 hexo clean 来清除 Hexo 的缓存。 3、验证主题启动服务：hexo s此时即可使用浏览器访问 http://localhost:4000，检查站点是否正确运行。 4、主题设定通过打开主题目录下面的主题配置文件修改即可。1、选择 Scheme123456789# ---------------------------------------------------------------# Scheme Settings# ---------------------------------------------------------------# Schemes# scheme: Muse# scheme: Mist# scheme: Piscesscheme: Gemini 2、设置语言编辑站点配置文件，将 language 设置成你所需要的语言。例如选用简体中文，配置如下：language: zh-Hans3、设置菜单编辑主题配置文件，修改以下内容：123456789menu: home: / || home categories: /categories/ || th tags: /tags/ || tags archives: /archives/ || archive about: /about/ || user #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap commonweal: /404/ || heartbeat 此处 || 之后代表的是 Font Awesome 图标的名字。注意：如果上面设置了简体中文，而此处配置了相应栏目，而网站显示仍然是英文。那就是主题目录下的languages/{language}.yml语言包没有对应的翻译，需要手动添加即可。4、设置头像编辑主题配置文件， 修改字段 avatar， 值设置成头像的链接地址。头像的链接地址可以是：①完整的互联网URI（绝对路径） ②站点内的地址（相对路径）avatar: /images/IMG_2796.JPG以上就是我自己修改的配置，如果还想了解更多配置详情，可到Next官网查看Next 5、插件安装1、百度统计：传送门2、阅读次数统计：传送门 配置文件1、站点配置文件站点配置：传送门 2、主题配置文件Next主题配置：传送门 RSS1、RSS是什么？ 简易信息聚合（也叫聚合内容）是一种RSS基于XML标准，在互联网上被广泛采用的内容包装和投递协议。RSS(Really Simple Syndication)是一种描述和同步网站内容的格式，是使用最广泛的XML应用。RSS搭建了信息迅速传播的一个技术平台，使得每个人都成为潜在的信息提供者。发布一个RSS文件后，这个RSS Feed中包含的信息就能直接被其他站点调用，而且由于这些数据都是标准的XML格式，所以也能在其他的终端和服务中使用，是一种描述和同步网站内容的格式。RSS可以是以下三个解释的其中一个： Really Simple Syndication；RDF (Resource Description Framework) Site Summary； Rich Site Summary。但其实这三个解释都是指同一种Syndication的技术。 RSS目前广泛用于网上新闻频道，blog和wiki，主要的版本有0.91, 1.0, 2.0。使用RSS订阅能更快地获取信息，网站提供RSS输出，有利于让用户获取网站内容的最新更新。网络用户可以在客户端借助于支持RSS的聚合工具软件，在不打开网站内容页面的情况下阅读支持RSS输出的网站内容。 说白了，RSS就是获取订阅的feed流，且是最新的。 2、如何使用RSS？ RSS就是个数据源，要订阅RSS，就必须先知道RSS的地址。一般来说，各个网站的首页都会用显著位置标明。名称可能会有些不同，比如RSS、XML、FEED，大家知道它们指的都是同样的东西就可以了。有时RSS后面还会带有版本号，比如2.0、1.0，甚至0.92，这个不必理会，它们只是内部格式不同，内容都是一样。 获取到数据源之后，内容是一堆程序语言，想要转换成正常形式阅读，就需要阅读器。RSS的阅读器多种多样，大致分为两种，一种是桌面型的，需要安装；另一种是在线型，直接使用浏览器进行阅读。 Hexo 增加阅读数下面介绍两种方式：LeanClound 和 不蒜子统计。 （1）LeanClound 统计注：配置LeanClound统计需要提前配置好HTTPS协议。可参考下面的说明。 注册LeanClound账号 创建应用 -&gt; 可获取到APP ID和APP Key。（用于Hexo主题配置文件。） 创建Class -&gt; 用于专门保存博客的文章访问量等数据。此处Class名称必须为 Counter。 添加安全域名 -&gt; 只有添加了安全域名，才有权访问后台的数据。（注：此处安全域名需要使用HTTPS协议的域名） 修改主题配置文件 -&gt; 即把2步骤获取的APP ID和APP Key添加进去，并开启。 至此，重新部署代码后生效，不需要自己添加样式或加载JS等操作。 （2）不蒜子统计直接打开 themes/_config.yml 文件，搜索 “busuanzi_count” 即可找到主题中默认已经配置了不蒜子统计。你要做的就是： 开启不蒜子： enable: true 进入 http://ibruce.info/2015/04/04/busuanzi/ 网站，依据上面说明更改对应的域名即可。 更改域名：如果依据网站说明的地址 themes/你的主题/layout/_partial/footer.ejs 找不到对应的文件。可直接在“你的主题”路径下面直接搜索：grep -r &#39;busuanzi&#39; ./* 即可找到对应的主题默认加载的不蒜子JS，更改其域名即可。 至此，重新部署代码即可生效。 Hexo http升级到https基于Hexo + GitHub Page搭建的个人博客，配置https超级简单。两步搞定：1、进入GitHub Page所在的Repository，点击Settings 2、在Options（默认）选项下方找到GitHub Page一栏，如下图所示，勾选 Enforce HTTPS设置即可。 Hexo 增加搜索功能安装插件npm install hexo-generator-searchdb --save 修改配置文件修改 站点配置 文件12345search: path: search.xml field: post format: html limit: 10000 修改 主题配置 文件12local_search: enable: true]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows下使用Hexo+导入Github博客]]></title>
    <url>%2F2017%2F12%2F06%2Fwindows%E4%B8%8B%E4%BD%BF%E7%94%A8Hexo%2B%E5%AF%BC%E5%85%A5Github%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[准备工作1、安装Node.js下载地址：传送门去 NodeJs 官网下载相应版本，进行安装即可。可以通过node -v的命令来测试NodeJS是否安装成功 2、安装Git下载地址：传送门去 Git 官网下载相应版本，进行安装即可。可以通过git –version的命令来测试git是否安装成功 3、注册Github账号（如果已有GIthub账号可忽略）去 Github 官网进行注册即可。注册完之后记得添加 SSH Key。这个 SSH Key是一个认证，让github识别绑定这台机器，允许这台机器提交。在Git Bash执行如下命令：cd ~/. ssh~这个符号，表示在用户目录下执行代码如果提示：No such file or directory 说明你是第一次使用git。下面就说下怎么配置SSH Key，生产新的SSH Key配置（windows）。在Git Bash执行代码：ssh-keygen -t rsa -C &quot;zheng_benwu@163.com&quot;记得修改成你自己邮箱地址。成功后会在家目录下生成一个隐藏文件夹.ssh，里面包含id_rsa 以及id_rsa.pub。 4、添加SSH Key到Github打开刚才生成的文件id_rsa.pub文件，复制其中的公钥。之后在Github添加SSH Key，在任意界面右上角，点击你的头像，选择Settings-&gt; SSH keys-&gt;New SSH key把公钥添加进去就ok了。至此，本地已经将环境搭建完成。 搭建博客1、安装Hexo在本地新建一个Blog文件夹，文件右键，选择Git Bash（进入Bash界面）。输入指令安装hexo（安装hexo命令）：npm install -g hexo等安装完毕，通过输入hexo的命令来测试hexo是否安装成功。接着初始化Hexo（安装Hexo博客）：hexo init hexo初始化成功会显示Start blogging with Hexo!这时在你刚才创建的Blog里面会多出一个hexo文件。安装完成！ 2、安装依赖文件接着，进入到hexo目录，输入指令npm install，安装依赖文件以及部署形成文件。安装依赖文件：12cd hexonpm install 部署形成文件（生成静态文件）：hexo generate 3、测试启动server服务：hexo server这时提示Hexo is running at http://loalhost:4000/.接着我们打开浏览器，输入http://localhost:4000/便可看到默认的博客。到这里，Hexo已经安装完毕。 配置Github Page1、新建博客仓库登录Github，点击”New repository”，新建一个版本库输入仓库名：你的Github名称.github.io。然后点击Create repository。 注意：这边的创建名字，一定要用的github的用户名，不然显示不出来，因为githubPage只能你的用户名。 2、启动GitHub Pages点击右边的“Setting”菜单进入设置,点击”Choose a theme”，即可选择一个模板。保存发布即可。到这里，可通过 https://bluce-ben.github.io 打开自己创建的静态站点模板。 将本地hexo项目托管到Github1、修改配置文件打开修改hexo目录下配置文件_config.yml（网站配置文件）。编辑最后面的deploy属性，修改如下代码：123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: https://github.com/bluce-ben/bluce-ben.github.io.git branch: master 注意：此处type为git，不是github；repository仓库使用https形式的，不要使用SSH形式的。 2、安装hexo-deployer-git插件此插件的作用就是关联hexo与git部署问题，当使用hexo deploy部署到git上时报错，可能就是缺少该插件。执行命令：npm install hexo-deployer-git --save 3、部署你本地的主题到Github上执行命令：123hexo cleanhexo generator #简写 hexo ghexo deploy #简写 hexo d 此处命令可简写为：12hexo cleanhexo d -g 到此，就将本地Hexo博客与GitHub Pages关联成功。 异地管理hexo首先，需要在这里说明一下我的问题。起初建博客的时候，当我将本地的hexo部署到GitHub上时，发现GitHub上版本库中的数据与本地是不一致的，GitHub上仅仅只是保存了页面显示部分，即.deploy_git下的内容（将本地部署到GitHub上的部分，命令为hexo d）。这并不是我想要的，当初没有异地管理的需求，就把这事忘了。 而此时，需要在异地管理，因此就把过程记录下来。 如果在搭建Hexo+GitHub Page的初始阶段，可阅读 https://blog.csdn.net/zwx2445205419/article/details/66970640 该篇文章。 如果是在后期新增异地管理的需求，可继续往下阅读。原理类似。 一、创建hexo文件存放分支1、创建分支 2、将hexo分支设为默认分支（注：设置为默认分支之后，下次再执行clone的时候，导入到本地的是hexo分支。） 二、将hexo分支仓库clone到本地，并清空分支数据1、将分支clone到本地，并把数据清空，注意要保留.git文件，因为.git是版本库的信息。clone仓库的目的就是要取得 .git文件。没有该文件，就不能对hexo分支进行管理了。 2、将清空后的hexo分支推到GitHub，即将GitHub上hexo分支数据清空。（注：此分支清空并不影响master分支数据） 三、将本地hexo文件数据添加到hexo分支版本下，并推到GitHub1、将本地hexo文件数据中的.git文件删除，使用自己 clone下来的hexo分支中的 .git文件。 2、然后把本地的hexo文件数据（不包含.git文件，否则两个会有冲突，一个文件夹下只能有一个同名文件）放到hexo分支下，并进行访问，查看是否可以使用。123hexo cleanhexo ghexo s 3、之后，进行部署。访问博客是否修改成功。（注：可通过增加一篇文件测试）注：hexo文件下 _config.yml中 deploy参数如下：123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: git@github.com:xxx.github.io.git branch: master 仍然是部署到 master主分支。开始部署：hexo d 4、最后，再将本地的hexo分支同步到GitHub上的hexo分支。123git add .git commit -m &apos;&apos;git push origin hexo 可登录GitHub查看 hexo分支是否有变化。 四、异地clone hexo分支进行管理异地通过 git clone git@github.com:xxx.github.io.git进行导入本地进行管理。注：每次进行本地分支管理前，可使用git pull来同步分支数据。以防在其它地方进行了更改数据，造成数据冲突。 原理说明：其实，说白了就是有两个分支，一个主分支用来存放GitHub Page页面，一个hexo分支用来存放本地hexo文件（即博客文件）。异地管理的时候，导出hexo文件即可。当部署文件的时候就自动的部署到主分支的GitHub Page。然后再执行命令git push origin hexo，将本地hexo文件推到origin/hexo分支，用于其他电脑更新分支信息。注：相当于比以前多了一步，即管理本地的hexo分支。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
